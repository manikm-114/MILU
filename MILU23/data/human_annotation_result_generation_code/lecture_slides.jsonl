{"lecture": "Lecture 1", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 1 - Slide1.txt", "file_path": "Lecture 1\\Texts\\Slide1.txt", "content": "Hi everyone, and welcome to this online course on Medical Imaging. I‚Äôm Professor Ge Wang, and I‚Äôll be guiding you through this exciting journey.\n\nMedical imaging is a fascinating and powerful field. It brings together science, engineering, and medicine to help us see what‚Äôs happening inside the human body‚Äîwithout making a single cut. Whether your background is in engineering, physics, computer science, or biology, you‚Äôll find that medical imaging offers something valuable and fascinating.\n\nIn the weeks ahead, we‚Äôll build a strong foundation in the core principles of imaging and describe a variety of technologies that are transforming modern healthcare. I'm excited to get started‚Äîand I hope you are too.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 1 - Slide2.txt", "file_path": "Lecture 1\\Texts\\Slide2.txt", "content": "We‚Äôll begin with the fundamentals ‚Äî systems, Fourier analysis, signal processing ‚Äî and hands-on MATLAB sessions. Then we‚Äôll explore major imaging techniques: X-ray and radiography, CT, PET, SPECT, MRI, ultrasound, optical, and deep imaging. Along the way, we‚Äôll have three exams to help you review and reinforce what you learn\n\n\nNow, just a quick note: in a traditional classroom, I‚Äôd mention office hours; however, since this is an online course, I encourage you to participate actively in our discussion forums. Don‚Äôt hesitate to post questions or thoughts as they come up. Learning is always more effective when it's interactive.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 1 - Slide3.txt", "file_path": "Lecture 1\\Texts\\Slide3.txt", "content": "So, to kick things off, let me give you a quick overview of what we‚Äôll cover today.\n\nThere are four main questions we‚Äôll explore:\n\nFirst, what is medical imaging?\n\nSecond, why should we study it?\n\nThird, who are involved in this course?\n\nAnd finally, how are we going to approach the material?\n\nNow, when we talk about medical imaging‚Äîsometimes we use the terms biomedical imaging or bio-instrumentation‚Äîwe‚Äôre referring to a wide range of methods, technologies and systems that help us see inside the human body, animals, or biological samples. These methods are grounded in engineering, physics, and biomedicine.\n\nAnd this isn‚Äôt just about theory. You‚Äôll also get hands-on experience with tools like MATLAB, and you‚Äôll work with selected reading materials and online resources. By the end of the course, you‚Äôll not only understand how these imaging systems work but also be able to apply that knowledge in a meaningful way.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 1 - Slide4.txt", "file_path": "Lecture 1\\Texts\\Slide4.txt", "content": "Let‚Äôs continue explaining our idea of medical imaging as a form of inner vision.\n\nYou know, as humans, our natural vision is powerful‚Äîbut it‚Äôs limited. We can see the world around us, we can observe surfaces, recognize faces, navigate through our environment. But our vision is tied to visible light, and that means we can‚Äôt see through most objects. We don‚Äôt know what‚Äôs behind a wall, inside a box, or within our own bodies.\n\nThat‚Äôs where medical imaging comes in. It gives us the ability to look beyond the surface‚Äîto see inside the human body without making a single incision. This is what makes medical imaging so extraordinary. It extends our natural vision, allowing us to explore what‚Äôs hidden, to understand both the structure and function of our organs, and to help guide medical care.\n\nThink about it‚Äîwhen someone doesn‚Äôt feel well, when there‚Äôs discomfort or concern, what do we do? We turn to imaging. It becomes the eyes of the physician, revealing what‚Äôs happening beneath the surface. Through imaging, doctors can measure physiological function, detect disease, and ultimately provide treatment that can ease suffering or even save lives.\n\nThat‚Äôs the true power of medical imaging‚Äîand why we call it the inner vision of modern medicine.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 1 - Slide5.txt", "file_path": "Lecture 1\\Texts\\Slide5.txt", "content": "Let me ask you this‚Äîwhat if we could see through the human body, almost like having X-ray vision? Imagine being able to look at tissues, organs, even down to the cellular level. Sounds like science fiction, right?\n\nWell, that‚Äôs exactly the dream of medical imaging. This field gives us a kind of \"super vision\"‚Äîan inner vision‚Äîthat lets us explore the human body from the inside out, completely noninvasively.\n\nAnd the truth is, what once seemed futuristic is now part of everyday medical practice. With the tools we‚Äôll talk about in this course, clinicians can detect problems earlier, treat patients more precisely, and monitor progress more closely. Many of the advances in imaging came from the needs that once seemed impossible‚Äîuntil technology caught up. You‚Äôll see how these breakthroughs happened, and why they matter.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 1 - Slide6.txt", "file_path": "Lecture 1\\Texts\\Slide6.txt", "content": "Now, let‚Äôs go back to where it all began. In 1895, a German physicist named Wilhelm Conrad Roentgen was experimenting with cathode rays when he discovered something totally unexpected‚Äîan invisible form of radiation that could pass through solid objects. He called them X-rays because he didn‚Äôt quite know what they were.\n\nTo test his discovery, he took the first X-ray image‚Äîa picture of his wife‚Äôs hand. You could clearly see the bones, and even her wedding ring. It was the first time humans saw inside the body without surgery. That moment changed medicine forever.\n\nX-rays could depict dense structures like bones and metal with incredible clarity. This breakthrough was so significant that Roentgen received a Nobel Prize in 1901.\n\nThat single discovery launched the field of radiology‚Äîand over a century later, we‚Äôre still building on it.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 1 - Slide7.txt", "file_path": "Lecture 1\\Texts\\Slide7.txt", "content": "Fast forward to today, and X-ray imaging‚Äîspecifically X-ray radiography‚Äîis one of the most common tools used in hospitals and clinics around the world. It‚Äôs fast, affordable, and incredibly useful.\n\nLet‚Äôs say someone falls and hurts their arm. An X-ray can quickly show whether there's a fracture. Or in breast cancer screening, mammography‚Äîa specialized type of X-ray radiography‚Äîcan reveal tiny calcifications or early tumors that might not be detectable by touch. Early detection like this can save lives.\n\nBut there‚Äôs a limitation with X-ray radiographs. They‚Äôre 2D projections, so everything in the body along the X-ray path gets stacked into one image. That makes it hard to see soft tissues clearly or to isolate specific structures.\n\nSo, the next logical step was to ask‚Äîwhat if we could look from multiple directions and reconstruct a full 3D view of what‚Äôs inside? That question led to the development of Computed Tomography, or CT, which we‚Äôll get into next.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 1 - Slide8.txt", "file_path": "Lecture 1\\Texts\\Slide8.txt", "content": "Alright, so earlier we talked about how regular X-rays give us a flat, two-dimensional view of the body. But there‚Äôs a catch‚Äîeverything in that path gets layered on top of each other. It‚Äôs like looking at shadows overlapped together.\n\nNow imagine if we could take many X-ray radiographs around the body‚Äîvirtually, of course‚Äîand figure out what‚Äôs happening on transverse sections. That‚Äôs exactly what computed tomography, or CT, allows us to do.\n\nThe word \"tomography\" comes from Greek‚Äî\"tomos\" means slice, and \"graphy\" means writing or drawing. So, we're basically drawing slices of the body. With CT, we collect X-ray projections from many angles, then use a computer to reconstruct a cross-sectional image.\n\nThe result? A crystal-clear view of internal anatomy, without overlapping tissues. Today‚Äôs CT scanners are incredibly precise‚Äîsome can even resolve features smaller than a millimeter. Think of it like slicing a watermelon and looking inside but doing it without a knife‚Äîjust scanning. It‚Äôs an amazing tool, and one that has completely transformed modern medicine.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 1 - Slide9.txt", "file_path": "Lecture 1\\Texts\\Slide9.txt", "content": "Now, if you want to get a fun and visual explanation of how X-rays work, there‚Äôs a great animated video I did for TED-Ed. It‚Äôs called ‚ÄúHow X-rays See Through Your Skin.‚Äù\n\nIn just a few minutes, it walks you through how X-rays are produced, how they interact with the body, and how detected signals are turned into a cross-sectional image. It‚Äôs a great refresher, especially if you‚Äôre a visual learner.\n\nYou‚Äôll also see how this technology became the foundation for other types of imaging we‚Äôll cover‚Äîlike PET, SPECT, and MRI. The video‚Äôs optional, but I really recommend checking it out when you have a moment.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 1 - Slide10.txt", "file_path": "Lecture 1\\Texts\\Slide10.txt", "content": "So far, we've focused on X-rays‚Äîbut medical imaging goes way beyond that. In fact, it covers almost the entire electromagnetic spectrum.\n\nDepending on what part of the spectrum you use, you get different kinds of information. For example:\n\nGamma rays are used in nuclear imaging, like PET and SPECT\n\nRadio waves are instrumental in MRI\n\nVisible and infrared light are for optical imaging\n\nAnd even ultrasound, though not part of the EM spectrum, gives us mechanical wave-based imaging, referred to as ultrasound imaging.\n\nEach imaging modality has its own strengths and weaknesses. Some are great for structure, others for function. Some are good at seeing bones, others at detecting molecular processes like metabolism or gene expression.\n\nIn fact, no single technique gives us the whole picture. That‚Äôs why modern medicine often combines them‚Äîwhat we call multimodal imaging. When we link them together, we get a rather rich view of what‚Äôs going on inside the body.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 1 - Slide11.txt", "file_path": "Lecture 1\\Texts\\Slide11.txt", "content": "Let‚Äôs talk a bit more about what makes tomography special.\n\nWhen you take a regular photo with your phone, you get a picture‚Äîthat‚Äôs it. It‚Äôs a direct result of what the camera sees. But tomographic imaging is different. You don‚Äôt directly capture the image you want. Instead, you collect data from many different angles, and that data is a kind of indirect measurement‚Äîa projection of what's inside.\n\nNow here‚Äôs the key part: to get a cross-sectional image, we need to do something called inversion. In other words, we take the measurements and reverse-engineer the internal structure that produced them.\n\nThis is an inverse problem. It‚Äôs different from the more familiar forward problem, where you know all the inputs, and a forward model, and calculate the outcome. Like in physics: you know the force and the mass, so you calculate how something moves.\n\nIn tomography, it‚Äôs the opposite. You see the outcome‚Äîthe measurement‚Äîand you ask: what could have caused this? That‚Äôs a lot harder.\n\nHere‚Äôs a fun analogy: think of boiling an egg. That‚Äôs a straightforward process. But what if someone gave you a boiled egg and said, ‚ÄúUnboil it‚Äù? That‚Äôs the inverse problem. Sounds impossible‚Äîbut researchers have found ways to reverse that process under specific conditions. It‚Äôs challenging, but not hopeless. And that‚Äôs what makes inversion in imaging both difficult and exciting.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 1 - Slide12.txt", "file_path": "Lecture 1\\Texts\\Slide12.txt", "content": "Here‚Äôs another example that might feel a bit more familiar.\n\nLet‚Äôs say you take a photo with your phone, but it turns out a little blurry‚Äîmaybe the camera shook, or the focus was off, or the camera is cheap and of poor quality. What you get is not the true image, but a version that‚Äôs been blurred in several ways.\n\nNow, how do we describe that blurring? In image processing, we model it using something called a point spread function‚Äîor PSF for short. This describes how a single point of light spreads out in the image.\n\nThe process that causes blurring is called convolution. It‚Äôs a mathematical operation that combines the original image with the PSF. Convolution is sort of like multiplication‚Äîbut in the so-called Fourier space that we will explain later in this course. Indeed, in the Fourier domain, convolution becomes regular multiplication. The Fourier transform is often performed using a fast algorithm, FFT in short. That simplifies a lot of computations.\n\nBut now comes the hard part: what if all you have is the blurry image? Can you work backward and recover the original, sharp version?\n\nThat‚Äôs another inverse problem. And just like with tomography, it‚Äôs tricky. You're trying to undo the effect of the blur, based on your knowledge of the system. It takes clever algorithms and good models‚Äîbut it can be done. And that‚Äôs the kind of thinking we‚Äôll develop throughout this course.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 1 - Slide13.txt", "file_path": "Lecture 1\\Texts\\Slide13.txt", "content": "Let‚Äôs take a step back for a moment and think about how mathematics builds over time.\n\nWhen you were in school, you learned the basics: addition, subtraction, multiplication, division. These are the building blocks. But once you reach higher levels‚Äîlike where we are now‚Äîthose simple operations take on more advanced forms.\n\nTake addition, for example. At this level, we‚Äôre not just adding a few numbers‚Äîwe might be summing over an infinite number of values. That‚Äôs what integration is: it's a continuous version of addition, used when we add up contributions from every point in a region.\n\nWhat about subtraction? In calculus, that becomes differentiation. Instead of just finding a difference between two numbers, we‚Äôre looking at how fast something is changing at an exact point by evaluating a very small difference of a smooth function between two rather close points.\n\nNow let‚Äôs look at multiplication. In imaging, we move from regular multiplication to something called convolution‚Äîa fundamental concept in systems and signal processing. Convolution helps us model how a spatially-invariant linear system modifies a signal or an image. You‚Äôll see this again and again in medical imaging. Again, a convolution in a spatial or time domain is a multiplication in the Fourier domain.\n\nAnd finally, what about division? Here‚Äôs where things get interesting. The advanced version of division‚Äîwhere you‚Äôre trying to reverse a process‚Äîis what we call an inverse solution; the deblurring problem is a good example. You‚Äôre given a blurry image, and your job is to figure out what is the underlying ground truth image. And that‚Äôs essentially what we do in tomographic imaging.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 1 - Slide14.txt", "file_path": "Lecture 1\\Texts\\Slide14.txt", "content": "Now, here‚Äôs the point: inversion is hard. Much harder than the forward process.\n\nIn a forward problem, you start with all the information, and you calculate the outcome. That‚Äôs usually pretty straightforward. But an inverse problem is the opposite: you‚Äôre starting with the outcome, and trying to infer what caused it.\n\nAnd this is where things get tricky. There might be multiple possible answers, or the data might be noisy or incomplete. You need to bring in clever mathematics, algorithms, physical models, and prior knowledge to make sense of it all.\n\nThat‚Äôs why this course isn‚Äôt just about memorizing facts. It‚Äôs about developing your quantitative thinking and really understanding what‚Äôs going on underneath the surface. It‚Äôs challenging‚Äîbut if you put in the effort, it‚Äôs also incredibly rewarding.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 1 - Slide15.txt", "file_path": "Lecture 1\\Texts\\Slide15.txt", "content": "Now let‚Äôs take a moment to appreciate just how interdisciplinary medical imaging really is.\n\nWe‚Äôre combining mathematics, physics, chemistry, biology, and engineering‚Äîall in one field. Think about that. Whether we‚Äôre modeling how X-rays interact with tissue, analyzing ultrasound signals, or designing contrast agents for optical imaging‚Äîwe‚Äôre pulling from all corners of science and technology.\n\nThis means that medical imaging isn‚Äôt just about one skill set. It‚Äôs about thinking broadly and connecting the dots between different fields. And if you enjoy solving complex problems, this is one of the best places to be.\n\nSo by now, you should have a pretty good sense of what this course is about‚Äîand what makes it so special. We‚Äôll focus on modern biomedical imaging technologies, and we‚Äôll build the knowledge and tools needed to understand how they work, and how they‚Äôre used to help people.\n\nAnd honestly, it‚Äôs really cool stuff.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 1 - Slide16.txt", "file_path": "Lecture 1\\Texts\\Slide16.txt", "content": "So by now, you might be thinking‚Äîthis is all very cool, but why do I really need to learn medical imaging?\n\nWell, I‚Äôd say there are at least two solid reasons.\n\nFirst‚Äîand perhaps most importantly‚Äîit gives you essential health-related knowledge. Whether you pursue a career in engineering, medicine, or something entirely different, understanding how imaging works can help you make sense of your own health and the health of those around you, during your hospital visits and afterwards.\n\nThe second reason is career-related. We‚Äôll get to that more. But for now, just keep in mind‚Äîthis is cool knowledge for a niche group of people. It‚Äôs personal and extremely useful in several ways; to say the least, it is a required course for your college education.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 1 - Slide17.txt", "file_path": "Lecture 1\\Texts\\Slide17.txt", "content": "Let me share something personal with you.\n\nA couple of years ago, I had a kidney stone. Not fun. I ended up in the hospital, they injected a contrast agent, ran a CT scan, and just like that‚Äîthey could see the size and location of the stone.\n\nNow, because I understood what was going on, I wasn‚Äôt worried or confused. I could follow the diagnosis, and I knew what the doctors were looking for.\n\nAnd honestly, sooner or later, most of us will have a medical scan‚Äîwhether it‚Äôs an X-ray, a CT, or an MRI. And perhaps, all of them and multiple times. It could be you, or someone you love. When that happens, wouldn‚Äôt it be helpful to understand what those images mean?\n\nThat‚Äôs why this knowledge matters‚Äînot just in theory, but in real life.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 1 - Slide18.txt", "file_path": "Lecture 1\\Texts\\Slide18.txt", "content": "Looking ahead, many of you will have families of your own. When a family member gets a scan or a test result, wouldn‚Äôt you want to understand what‚Äôs happening?\n\nThat‚Äôs where this course really empowers you.\n\nEven if you don‚Äôt go into imaging research, even if you don‚Äôt become a radiologist or an imaging scientist, just being able to interpret results and ask the right questions can make a big difference.\n\nSo that‚Äôs the first reason: personal and practical value. Now let‚Äôs move on to the second one‚Äîyour career.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 1 - Slide19.txt", "file_path": "Lecture 1\\Texts\\Slide19.txt", "content": "Let‚Äôs zoom out for a moment and look at biomedical engineering as a field.\n\nIt‚Äôs one of the newest branches of engineering. Engineering started with disciplines like civil and mechanical, then moved into electrical, chemical, and aerospace. More recently, biomedical engineering emerged to bridge the gap between technology and medicine.\n\nAnd it‚Äôs grown fast‚Äîbecause it‚Äôs relevant, it‚Äôs interdisciplinary, and it‚Äôs impactful.\n\nIn many universities, biomedical engineering includes at least two major focus areas. One is imaging, which is what we‚Äôre covering here. The other is tissue engineering and regenerative medicine, often called TERM.\n\nWhether or not you plan to specialize in imaging, this course plays a foundational role. It builds your technical knowledge, strengthens your systems thinking, and helps you understand one of the most important technologies in healthcare today.\n\nSo if you‚Äôre here as part of your degree requirements‚Äîgreat. But don‚Äôt just treat it like a checkbox. Learn it well. It will pay off, academically, professionally, and personally.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 1 - Slide20.txt", "file_path": "Lecture 1\\Texts\\Slide20.txt", "content": "Now, for those of you considering a concentration in imaging‚Äîor even just taking this course seriously‚Äîit can really pay off in your career.\n\nMedical imaging is a technical field, but it‚Äôs also full of opportunities. If you understand how imaging systems work, and you can apply that knowledge to real-world problems, you‚Äôll bring serious value to any team‚Äîwhether in research, healthcare, or industry.\n\nAnd in case you‚Äôre still wondering: yes, biomedical engineering is a great profession. It sits at the intersection of life sciences, medicine, and engineering‚Äîand that makes it both exciting and employable.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 1 - Slide21.txt", "file_path": "Lecture 1\\Texts\\Slide21.txt", "content": "In fact, if you look at trends in job growth, biomedical engineering consistently ranks among the fastest-growing careers. Just take a glance at some recent employment reports‚Äîyou‚Äôll see biomedical engineers right near the top.\n\nWhy is that? Well, part of the reason is that healthcare is becoming more tech-driven. Devices, sensors, data analysis, machine learning‚Äîit all depends on people who can bridge the gap between biology and engineering.\n\nSo not only is this field intellectually exciting‚Äîit also comes with strong job prospects and competitive salaries.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 1 - Slide22.txt", "file_path": "Lecture 1\\Texts\\Slide22.txt", "content": "Now let‚Äôs zoom out to the global scale.\n\nMedical imaging isn‚Äôt just a field‚Äîit‚Äôs a major global industry. And it‚Äôs growing fast. Countries around the world are investing heavily in imaging infrastructure, service, and innovation.\n\nIf you look at the data, regions like Asia and Europe are rapidly expanding their market share. Asia, in particular, has seen a surge in R&D spending and adoption of advanced imaging technologies. That means more jobs, more research funding, and more innovation coming from across the globe.\n\nSo, whether you're aiming for industry, academia, or entrepreneurship‚Äîthis is a great space to be in.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 1 - Slide23.txt", "file_path": "Lecture 1\\Texts\\Slide23.txt", "content": "When it comes to imaging modalities, CT and MRI continue to lead in market share and clinical use.\n\nThat‚Äôs no surprise‚Äîboth are incredibly versatile and widely used across medical specialties. CT scans are fast and great for detailed anatomical views. MRI provides outstanding soft tissue contrast, which is essential in areas like cardiology and neurology.\n\nYou might be surprised, though, that nuclear imaging‚Äîlike PET and SPECT‚Äîis also a major player. These techniques are critical for functional imaging and cancer imaging. They are becoming more popular as we move toward personalized and molecular medicine.\n\nEach year, investment in these technologies grows. The message is clear: the field is fast evolving, and there‚Äôs a strong momentum behind it.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 1 - Slide24.txt", "file_path": "Lecture 1\\Texts\\Slide24.txt", "content": "So now that we‚Äôve seen how vital CT, MRI, and other imaging technologies are in practice and in the global market‚Äîhere‚Äôs something interesting to reflect on.\n\nA few years ago, a survey asked physicians and scientists to name the most important technological innovation of the entire 20th century. Think about that‚Äîthis included everything: the internet, smartphones, even space travel.\n\nAnd what came out on top? Magnetic Resonance Imaging (MRI) and Computed Tomography (CT). Not even close. These tomographic imaging technologies were voted number one by a considerable margin.\n\nWhy? Because they fundamentally changed how we diagnose and treat diseases. They let us see inside the human body in ways that were never possible before‚Äîaccurately, noninvasively, and in real time.\n\nAs you go deeper into this course, you‚Äôll learn how these systems work‚Äîso not only will you understand their value, you‚Äôll also be able to explain it confidently in, say, a job interview or a professional conversation. That‚Äôs powerful knowledge.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 1 - Slide25.txt", "file_path": "Lecture 1\\Texts\\Slide25.txt", "content": "Speaking of innovation‚Äîlet‚Äôs talk about one of the major players in the field: General Electric, or GE, in the neighborhood of Rensselaer Polytechnic Institute.\n\nThis is a company with a deep history of technological contributions, not just in medical imaging, but across many industries. And interestingly, ‚ÄúGE‚Äù also happens to be my initials‚Äîbut theirs is with a capital E. So, no confusion!\n\nGE was founded back in 1892, not long after X-rays were discovered. And Rensselaer Polytechnic Institute was founded even earlier. So these two organizations have grown side by side, both focused on pushing the boundaries of science and technology.\n\nGE has been part of some most transformative developments in imaging‚Äîfrom early X-ray machines to advanced MR scanners. They‚Äôve really helped shape this field.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 1 - Slide26.txt", "file_path": "Lecture 1\\Texts\\Slide26.txt", "content": "Now, within GE, one of the most important innovation engines is their Global Research Center. This is where long-term, high-impact R&D happens.\n\nTheir record speaks for itself‚Äîtwo Nobel Prizes, major contributions to physics, chemistry, materials, energy, and of course, medical imaging.\n\nIn 1984, they played a critical role in the development of MRI systems. And in 1999, digital X-ray systems came out of their labs. They‚Äôve consistently been at the forefront of translating ideas into real-world technologies. My lab and Global Research Center has been closely collaborating on medical imaging over decades. \n\nSo when we say medical imaging sits at the cutting edge of innovation‚Äîwe mean it.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 1 - Slide27.txt", "file_path": "Lecture 1\\Texts\\Slide27.txt", "content": "And what makes GE even more impressive is the global reach of their research.\n\nThey have major centers across the U.S., Europe, Asia, and South America. For example, the Global Research Center in Niskayuna, New York‚Äîright near RPI‚Äîis home to over a hundred labs and nearly two thousand technologists.\n\nFor a student interested in imaging, that‚Äôs a golden opportunity. You can collaborate, intern, or even launch a career through the GE-RPI partnership.\n\nSome of you may want to go into academia, others into entrepreneurship or industry. Wherever you go, connecting with innovation hubs like GE gives you a head start. Especially in imaging, they are a top-tier industrial partner.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 1 - Slide28.txt", "file_path": "Lecture 1\\Texts\\Slide28.txt", "content": "Before we wrap up this introduction,  I‚Äôll also give you a simple task‚Äîjust a brief introductory slide you can prepare. It‚Äôs not graded, but it helps me get a sense of who you are, what your background is, your GPA, research experience, and what you hope to learn. That way, I can adjust the flow of this course to better support your journey.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 1 - Slide29.txt", "file_path": "Lecture 1\\Texts\\Slide29.txt", "content": "As we continue our journey into medical imaging, let me take a moment to introduce myself in a little more depth.\n\nI‚Äôm Professor Ge Wang, and in addition to teaching this course, I‚Äôm deeply involved in research on medical imaging. My work spans both the theoretical and applied sides of the field. I‚Äôve dedicated much of my career to advancing medical imaging science so that we can better diagnose and treat disease.\n\nIf you‚Äôre interested in exploring more about my work, feel free to check out my home page and lab‚Äôs website, where we share updates on our latest projects.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 30, "slide_filename": "Slide30.txt", "slide_annotation": "Lecture 1 - Slide30.txt", "file_path": "Lecture 1\\Texts\\Slide30.txt", "content": "Now let me share a bit more about the imaging program at RPI.\n\nMy main interests are in X-ray imaging and optical imaging. I direct our Biomedical Imaging Center, where we work on advancing these technologies. One exciting direction is how artificial intelligence can help improve imaging‚Äîmaking it faster, more accurate, and more insightful.\n\nThis work doesn‚Äôt happen in isolation. I collaborate with outstanding colleagues like Professor Xavier Intes, a leader in optical molecular tomography, Professor Pingkun Yan, a leader in AI-based image analysis and healthcare analytics, and many others. Together, we team up with world-class researchers from GE, Yale, Wake Forest, MGH, Stanford, FDA, and other top institutions. Our shared goal is to create imaging solutions that are innovative, influential, and that truly make differences in patient care.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 31, "slide_filename": "Slide31.txt", "slide_annotation": "Lecture 1 - Slide31.txt", "file_path": "Lecture 1\\Texts\\Slide31.txt", "content": "These collaborations all come together in a fantastic building‚Äîthe Center for Biotechnology and Interdisciplinary Studies, or CBIS.\n\nCBIS is more than just a building. It‚Äôs a hub where experts from biology, engineering, and computing come together to tackle big problems in science and healthcare. It‚Äôs where our imaging center is rooted, and where much of the work that I‚Äôve been describing happens.\n\nOver the past decade, CBIS has been at the forefront of interdisciplinary research, and it continues to grow in both scope and impact.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 32, "slide_filename": "Slide32.txt", "slide_annotation": "Lecture 1 - Slide32.txt", "file_path": "Lecture 1\\Texts\\Slide32.txt", "content": "Now, I often like to say: if CBIS were a coordinate system, my office would be right at the origin!\n\nOf course, for this online course, we‚Äôre not meeting in person. But the idea is the same‚ÄîI‚Äôm here as a resource for you. The lectures, materials, and any additional resources I share are all designed to help you succeed and make the most of this learning experience.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 33, "slide_filename": "Slide33.txt", "slide_annotation": "Lecture 1 - Slide33.txt", "file_path": "Lecture 1\\Texts\\Slide33.txt", "content": "We‚Äôve designed this course with two major phases in mind: a foundation phase and a modality-specific phase. In the foundation phase‚Äîhighlighted in light blue‚Äîyou‚Äôll build the critical background knowledge necessary to understand modern biomedical imaging systems. This includes essential concepts in linear systems, convolution, and Fourier analysis. These are the mathematical and conceptual tools at the heart of almost every imaging modality we‚Äôll cover later.\n\nWhy this structure? Well, my previous experience with diverse students tells me that not everyone enters this course with the same preparation. Some of you may have strong backgrounds in physics, applied math, and electrical engineering‚Äîand that‚Äôs great. You might already be familiar with system theory and Fourier transform. But many others come from biology, chemistry, or material science backgrounds, where these topics are not emphasized. That‚Äôs completely okay.\n\nMedical imaging is interdisciplinary‚Äîand complex. So my job is to guide all of you, regardless of background, to the point where you understand these core ideas clearly and intuitively. It won't be enough to just listen and watch these lectures. You‚Äôll need to invest time in previewing and reviewing the materials and doing the assignments. But rest assured‚Äîyour effort will pay off. These foundational topics will resurface again and again, especially in modalities like CT and MRI, where the Fourier transform is a key player.\n\nAlso, linear systems might look simple on the surface‚Äîafter all, they obey additivity and scaling rules‚Äîbut they come with nuances. One particularly subtle concept is convolution, which plays a crucial role in both systems analysis and image reconstruction. We‚Äôll make sure you understand not just how to compute it, but also why it matters and how it shows up in real-world scanners.\n\nSo, we begin with the foundation. Later, we‚Äôll shift to the modality phase, shown in dark green‚Äîwhere we delve into imaging techniques like X-ray CT, nuclear imaging, MRI, ultrasound, and optical imaging. Each one has unique physics, engineering principles, and clinical applications.\n\nThink of it this way: the foundation is your universal imaging 'language'. Once you master it, learning each new modality becomes much easier, and far more meaningful.\n\nLet me walk you through our tentative timeline for the course‚Äîthough as with any plan, we may make some adjustments along the way.\n\nThis lecture kicks off the course. In our next session, you‚Äôll be introduced to MATLAB. If you're unfamiliar with it, don‚Äôt worry‚Äîour teaching assistant will help you get started. MATLAB is a vital tool in imaging research, especially when modeling systems or reconstructing images from tomographic data. After the introduction, you‚Äôll have a short MATLAB-related assignment to sharpen your skills.\n\nThen we move into the foundation module‚Äîthe light blue section. This is where we cover the theoretical concepts that apply across imaging technologies. After that, we transition to the green section‚Äîwhere we examine individual imaging modalities.\n\nYou‚Äôll also be evaluated through three closed-book exams spaced across the course: one after the foundation module, one after the first set of modalities (CT and nuclear imaging), and one to cover everything else. Your final grade will be based on a combination of these exams, your homework, and participation.\n\nNow, this is an online course, so we‚Äôve adapted all the content accordingly. Pre-recorded lectures, interactive demos, and exercises will all be made available. If you have questions, we encourage you to reach out via the discussion forum or email. Quick responses are part of our commitment to help you succeed.\n\nA quick tip: spend quality time understanding the Fourier transform early on. It‚Äôs a recurring theme. And if you‚Äôve ever wondered why exponential functions like \nùëí^ùëñùúÉ show up everywhere, this course will make that connection crystal clear. We‚Äôll discuss how they lead to elegant mathematical representations and how these expressions play into image formation.\n\nFinally, one last point. This course is not just about passing exams‚Äîit‚Äôs about equipping you with a deep understanding that you can carry forward into research, engineering design, and healthcare work. Let‚Äôs work together and learn from each other.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 34, "slide_filename": "Slide34.txt", "slide_annotation": "Lecture 1 - Slide34.txt", "file_path": "Lecture 1\\Texts\\Slide34.txt", "content": "For the modality-specific portion of the course‚Äîthe dark green section‚Äîwe‚Äôll be using the green textbook: Introduction to Biomedical Imaging by Andrew Webb. It‚Äôs available online or through our university bookstore.\n\nThis book isn‚Äôt very thick, but it‚Äôs well-structured and written with clarity. Even though it was published a while ago, the fundamental principles it covers haven‚Äôt changed. Physics is timeless in that way. We‚Äôll supplement the book with more recent findings, particularly in areas like deep learning for image reconstruction, low-dose imaging, and multi-modal fusion‚Äîadvances that have significantly shaped the field over past several years.\n\nThe textbook will guide your learning, especially in the second part of the course. For the foundation topics like linear systems and Fourier theory, we‚Äôll use tailored lecture notes and curated readings, because unfortunately, there‚Äôs no single book that explains everything in a concise and consistent manner for students from all backgrounds.\n\nSo, keep this in mind: while you may rely on the textbook for the modality section, the theoretical part of the course will require close attention to lecture materials and exercises. That‚Äôs where most students need the extra help‚Äîand that‚Äôs why we‚Äôre here to support you.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 35, "slide_filename": "Slide35.txt", "slide_annotation": "Lecture 1 - Slide35.txt", "file_path": "Lecture 1\\Texts\\Slide35.txt", "content": "In addition to our course materials, I‚Äôd like to mention a particularly helpful supplementary resource that aligns well with the foundation we‚Äôre covering‚Äîespecially Fourier transforms and linear systems.\n\nStanford University offers an excellent course titled 'EE261: The Fourier Transform and Its Applications'. It‚Äôs taught by Professor Brad Osgood, and the best part is that it is freely available online through Stanford‚Äôs SEE platform. You can access the full textbook there and watch high-quality lecture videos.\n\nNow, a quick word of caution: the Stanford course spans an entire semester. The book is over 400 pages long. Don‚Äôt feel overwhelmed. You absolutely do not need to cover all of it. Think of it as a reference companion to our course. In fact, for our purposes, reading even 80‚Äì100 pages from selected sections will be more than enough to reinforce your understanding.\n\nWhat makes this resource valuable is the clarity and consistency of its explanations and mathematical treatment. It builds the theory from the ground up, with elegant examples and a style that‚Äôs both rigorous and readable. If you ever find yourself puzzled by a concept I introduce‚Äîsay, the derivation of the Fourier series, or the intuition behind convolution‚Äîthis book can offer a second viewpoint. Sometimes, a slightly different presentation is all you need for clarity.\n\nHowever, please note that we will follow our own structured path in this course. For example, I introduce linear systems before Fourier transforms, because in imaging, systems thinking provides the conceptual grounding. The Stanford course takes the reverse order. Both are valid‚Äîbut here, we start with linear systems to better prepare for their applications in imaging modalities like CT and MRI.\n\nPlease use this Stanford material as a valuable supplement, not a replacement. You‚Äôll benefit most by following our course flow carefully and referring to the Stanford book only when you want to go deeper or review challenging topics.\n\nLastly, although I have reached out for formal permission, the material seems publicly accessible and openly shared for educational use. So, feel free to explore it from your own dorm room, your library, or anywhere you study.\n\nThis is an exciting time to learn these topics‚Äîtools like ChatGPT make it easier than ever to gain a deeper understanding. Let‚Äôs use them wisely.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 36, "slide_filename": "Slide36.txt", "slide_annotation": "Lecture 1 - Slide36.txt", "file_path": "Lecture 1\\Texts\\Slide36.txt", "content": "In this course, you‚Äôll have access to a complete set of digital resources to support your learning.\n\nWe‚Äôve included professionally recorded video lectures, so you can review key concepts anytime. All PowerPoint slides used in the course are also available for download and review.\n\nFor the modality sections, the textbook is available as hardcopy and PDF, making it easy to follow along even to search specific topics digitally. The course schedule follows a well-structured plan from previous offerings, with minor updates to reflect some latest progress in imaging science.\n\nThese materials are here to help you succeed‚Äîuse them actively as we explore the foundation materials and modalities of medical imaging.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 37, "slide_filename": "Slide37.txt", "slide_annotation": "Lecture 1 - Slide37.txt", "file_path": "Lecture 1\\Texts\\Slide37.txt", "content": "Continuing from our previous discussions, let‚Äôs dive into the physics of X-ray imaging and how we can collect and process data to reconstruct an image.\n\nIn X-ray imaging, we start with the concept of a line integral. Essentially, the X-rays pass through the object, and what we measure is the energy of penetrated x-ray photons and then compute the line integral along the x-ray path. Such line integrals together form the Radon transform. To make it simpler, imagine you have an object, and you put all line integrals from a specific direction to form an x-ray projection. This projection is a one-dimensional profile, which is the information you can get along all the X-ray paths passing through the object in that direction.\n\nNow, let‚Äôs take this idea into the realm of mathematics. We start with a 2D image, denoted as \nf(x,y). When we have all x-ray projections or the Radon transform, we get a new 2D function  p(Œ∏,t), where  t is the coordinate along the 1D detector array and  Œ∏ is the projection orientation. By changing the angle Œ∏, we rotate the data acquisition system to gather x-ray projections.\n\nNow, the magic happens when we flip the process around. Given this collected projection dataset, our goal is to reconstruct the original 2D image f(x,y). This is the inverse process, and here‚Äôs where things get interesting: by perform the 1D Fourier trans of each and every of the 1D projections, we move into the Fourier domain.\n\nTo put it simply, the 1D projections we collect represent profiles in the frequency domain. By rotating the data acquisition system, we eventually capture the entire Fourier space. And once we have all the data in the Fourier space, we can apply the inverse Fourier transform to reconstruct the original image. This process connects the data from the physical object to the Fourier spectrum of the underlying image, and from the Fourier spectrum, we can reconstruct the original image.\n\nWhile this overview gives you the fundamental idea, later in the course, we‚Äôll dive deeper into the Fourier transform, why they can be inverted, and how this mathematical framework ties directly to the imaging process. But for now, keep in mind that the forward process (how we collect data) and the inverse process (how we reconstruct the image) are two sides of the same coin, closely linked by Fourier analysis.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 38, "slide_filename": "Slide38.txt", "slide_annotation": "Lecture 1 - Slide38.txt", "file_path": "Lecture 1\\Texts\\Slide38.txt", "content": "Continuing with our exploration of imaging modalities, let‚Äôs now discuss Positron Emission Tomography, or PET. While the previous imaging techniques focused on capturing the anatomical structure, PET offers a powerful way to visualize the functional activity in the human body.\n\nSo, how does it work? Well, the key to PET is the introduction of radioactive tracers into the body. These tracers are typically radioactive chemicals that participate in your body‚Äôs metabolic processes. For example, tumors tend to consume a lot of glucose. If we couple glucose molecules with a radioactive tracer, we can track how much glucose the tumor is using, which directly correlates with its activity. Tumors, which are often highly metabolic, will naturally absorb more of this tracer.\n\nOnce the PET tracer is in the body, it begins to emit gamma ray photons in pairs, and two photons in a pair will travel in opposite directions. Gamma rays, much like X-rays, can penetrate through tissues and be detected externally. When we receive gamma ray photons from two different detector locations, we know they originated from the same event somewhere on the line connecting the two detectors.\nThis process is similar to the measurement of line integrals used for CT. Then, we can apply Fourier analysis to reconstruct the images. In PET, Fourier transforms help us create detailed, functional images of the body‚Äôs processes. The goal here is to understand how much activities are happening at specific locations. So, while we‚Äôve seen how CT gives us anatomical details, PET allows us to observe the biological functions‚Äîlike metabolism‚Äîwithin those structures.\n\nThis modality is especially useful for identifying cancerous tissues, studying brain function, and even monitoring heart conditions. Later, we‚Äôll explain how the gamma ray photons are detected and processed to reconstruct these functional images. But for now, remember that PET imaging provides us with a way to look inside the body not just for its structure, but also for how physiology is actively working.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 39, "slide_filename": "Slide39.txt", "slide_annotation": "Lecture 1 - Slide39.txt", "file_path": "Lecture 1\\Texts\\Slide39.txt", "content": "Now, let‚Äôs explore a closely related imaging technique in nuclear medicine called Single Photon Emission Computed Tomography, or SPECT. While Positron Emission Tomography (PET) relies on the detection of paired gamma ray photons emitted from the tracer, SPECT works differently.\n\nIn SPECT, the radio tracer emits gamma ray photons randomly and individually, without being paired. Essentially, instead of detecting two photons coming from opposite directions (as we do in PET), we measure single gamma ray photons emitted from a radioactive tracer within the body.\n\nWhen a photon is detected through a collimator, we know it came from a particular path through the patient body, but since the photons are not paired, we need to use different data preprocessing and image reconstruction methods. SPECT provides valuable information, but it‚Äôs a bit different from PET. In SPECT, we capture these single photons from various angles around the body and use dedicated tomographic reconstruction techniques to create an image.\n\nWe‚Äôll discuss the details of SPECT imaging later in the course, but for now, keep in mind that while PET relies on paired photons to provide highly precise spatial information, SPECT uses single photons to gather functional information in a different way. Both PET and SPECT produce tomographic images of radio-tracer activities.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 40, "slide_filename": "Slide40.txt", "slide_annotation": "Lecture 1 - Slide40.txt", "file_path": "Lecture 1\\Texts\\Slide40.txt", "content": "Now, let‚Äôs turn our attention to the third major imaging modality: Magnetic Resonance Imaging, or MRI. MRI uses a completely different approach compared to X-ray and nuclear imaging techniques including PET and SPECT. In MRI, we utilize radio frequency signals to gather detailed images of the body.\n\nHere‚Äôs how it works: First, we subject the patient to a very strong magnetic field. This magnetic field aligns the hydrogen nuclei‚Äîwhich are abundant in water molecules in your body. Then, we introduce a radio frequency (RF) signal that briefly disrupts this alignment, causing the hydrogen nuclei to emit their own radio frequency signals as they return to their original stable state.\n\nThe key advantage of MRI is that it is sensitive to the water content in different tissues and interactions of hydrogen nuclei with their micro-environments, which makes MRI extremely useful for imaging soft tissues like the brain, muscles, and organs. The signal we collect can give us a lot of valuable information, such as chemical shifts in tissues, which help us distinguish between different types of tissues and even detect abnormalities like tumors or inflammation.\n\nMRI is also great for studying brain activity, as the technique can be used to observe changes in blood flow over time. This makes MRI not only a structural imaging tool but also a functional one, adding a layer of depth to our understanding of the body.\n\nTogether with CT and nuclear imaging techniques, MRI forms the trio of most important imaging modalities. While CT focuses on capturing detailed anatomical structures using X-rays, and nuclear imaging provides insight into physiological functions, MRI offers unmatched soft tissue contrast and the ability to examine both structure and function in a non-invasive way, even into our thinking processes.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 41, "slide_filename": "Slide41.txt", "slide_annotation": "Lecture 1 - Slide41.txt", "file_path": "Lecture 1\\Texts\\Slide41.txt", "content": "Now, let's look at Ultrasound Imaging. While it might not be as widely discussed or as advanced as the big three‚ÄîCT, nuclear imaging, and MRI‚Äîultrasound remains a powerful, affordable, and safe imaging tool in medical practice, and seems playing an increasingly important role for point of care imaging.\n\nSo, how does ultrasound work? It operates based on the principle of mechanical vibrations. In ultrasound imaging, we use a device called a transducer, which contains piezoelectric materials. These materials can generate high-frequency sound waves when subjected to an electric current. These sound waves are then sent into the body.\n\nAs the sound waves travel through tissues, they interact with various structures inside the body. Some waves are reflected, and others continue through the tissues. The reflected waves are detected by the transducer, and from this data, we can reconstruct an image of the internal structures. Essentially, we‚Äôre mapping the body‚Äôs anatomy based on how sound waves bounce off tissues.\n\nOne of the greatest benefits of ultrasound is that it‚Äôs non-invasive and completely safe‚Äîunlike X-rays or CT scans, ultrasound doesn't involve ionizing radiation. It‚Äôs also incredibly cost-effective and high-speed, which makes it a go-to tool for many real-time imaging tasks. You‚Äôll often see it used during fetal imaging in pregnancy, as well as in blood flow monitoring and guiding biopsies.\n\nWhile it may not provide the same level of detail as MRI or CT, ultrasound‚Äôs affordability, speed, and safety make it a valuable tool for many clinical applications. Coupled with AI methods, ultrasound imaging has yet more to offer.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 42, "slide_filename": "Slide42.txt", "slide_annotation": "Lecture 1 - Slide42.txt", "file_path": "Lecture 1\\Texts\\Slide42.txt", "content": "Now, let‚Äôs talk about Optical Imaging, another fascinating imaging modality that uses visible and infrared light. Unlike the other imaging techniques we've discussed, optical imaging focuses on how light interacts with cells, allowing us to reveal molecular and cellular interactions in the body.\n\nOne unique aspect of optical imaging is its ability to extract signatures from these biological interactions, providing valuable information about what‚Äôs happening at a microscopic level. This is where things get exciting! For example, we can use luminescence probes to tag specific proteins or gene expressions. Once tagged, these proteins will emit a luminescent light and make them visible to our imaging system.\n\nThink of it like the glow of a firefly in a summer night. When certain proteins or cells are tagged, they emit bioluminescent light, which we can detect. This is a form of passive imaging, meaning the light we need to detect is emitted by the body itself, rather than steered by any external light source.\n\nFor example, in a typical experiment, an animal might be placed in a dark room, and we would observe the light emissions coming from specific areas on its body. We want to perform tomographic reconstruction to find the internal distribution of the bioluminescent sources. For that purpose, we combine an optical imager with a separate tomographic scanner, which helps us get both the anatomy and the optical properties of organs and structures to build a forward imaging model. After that, we can invert this forward model for reconstruction of a 3D distribution of bioluminescent sources.\n\nThe real challenge in optical imaging isn‚Äôt in the forward process‚Äîwhere we predict the external signal based on the known internal structures. The difficulty is in the inverse process, where we have this inverse problem: Given the external light signals we measure, how do we reconstruct the distribution of bioluminescent sources inside the body?\n\nTo do this, we use multiple mirrors to collect all the emitted light, creating a complete set of external views. We then use this data to reconstruct the internal light source distribution. This method has been known as bioluminescence tomography that we pioneered and has become quite popular in the literature.\n\nNote that optical imaging includes a good number of techniques.  Bioluminescence tomography is only one of them. We have fluorescence tomography as well, with fluorescence probes excitable by an external laser source. More clinically important is optical coherence tomography or OCT, which is used to check your retina in eye clinic.\n\nIn summary, optical imaging, like other modalities, allows us to visualize and understand structures, probes or tracers inside the body. It‚Äôs a unique and powerful way to study biology from the inside out.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 43, "slide_filename": "Slide43.txt", "slide_annotation": "Lecture 1 - Slide43.txt", "file_path": "Lecture 1\\Texts\\Slide43.txt", "content": "Now that we‚Äôve explored the different imaging modalities and how they allow us to visualize features inside the body, the next logical step is to talk about what we can do with the images we‚Äôve acquired. Once you have an image, you don‚Äôt just look at it‚Äîyou analyze it to gain task-specific information. This is where the domain of image processing grows.\n\nIn medicine, image analysis is crucial for extracting actionable insights. There are various techniques we use, such as image segmentation, where we identify specific regions or structures within the image‚Äîlike tumors, organs, or blood vessels. By doing this, we can focus on areas of interest, making it easier to understand what‚Äôs happening inside the body.\n\nAnother important tool is image classification, which allows us to categorize different structures or tissues based on their visual features. This helps doctors make diagnosis, track changes over time, or even guide interventional procedures. \n\nWe also use techniques like image enhancement, which improves the quality of the image by making certain features more visible, such as image deblurring, which sharpens images that might have been blurry due to motion or technical issues.\n\nSome textbooks on medical imaging even include image visualization. This is where new technologies like augmented reality (AR) become relevant. I recently read an article, where the use of AR is discussed for medical imaging-guided surgery. With this innovative method, surgeons wear specialized glasses that overlay imaging data directly onto the patient‚Äôs body during surgery, guiding their actions in real time. While this is an exciting application of medical imaging, for the scope of this course, we‚Äôll focus more on bio-imaging and bio-instrumentation, with an emphasis on tomographic imaging.\n\nIn this course, we‚Äôll briefly cover some key aspects of image analysis, but we won‚Äôt delve too deeply into every aspect of processing. By the end, you‚Äôll have a solid understanding of how image analysis ties into the broader field of medical imaging.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 44, "slide_filename": "Slide44.txt", "slide_annotation": "Lecture 1 - Slide44.txt", "file_path": "Lecture 1\\Texts\\Slide44.txt", "content": "To put everything in perspective, let‚Äôs take a step back and look at the big picture. In understanding the human body, we often talk about two key concepts: phenotype and genotype.\n\nGenotype refers to your genetic makeup‚Äîthe DNA that gives rise to your traits. Meanwhile, phenotype refers to the observable characteristics of an individual, such as physical traits, biological functions, and disease states. Biomedical imaging plays a crucial role in studying the phenotype. Through imaging, we can visualize and analyze the biological structures and functions that define an individual‚Äôs phenotype.\n\nNow, bioinformatics and genetic profiling give us a lot of information about the genotype‚Äîthings like your genetic predispositions and variations. However, these genetic insights need to be connected to the physical manifestation of the genes‚Äîthe phenotype. We can use biomedical imaging to observe and understand the effects of genetic factors on the body.\n\nWhile we‚Äôre entering the world of imaging in this course, it‚Äôs important to recognize that we‚Äôre focusing primarily on phenotype‚Äîhow imaging allows us to visualize and analyze the body's structures and functions. However, linking genotype (your genetic information) with phenotype (the observable traits and conditions) is an essential goal for personalized medicine. This connection between the two will allow for more accurate diagnosis, more effective treatment, and unprecedented prediction in the future.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 45, "slide_filename": "Slide45.txt", "slide_annotation": "Lecture 1 - Slide45.txt", "file_path": "Lecture 1\\Texts\\Slide45.txt", "content": "Looking ahead, the next lecture will focus on building a solid foundation in MATLAB, which will be essential as we move into topics like Fourier transforms and linear systems. Rather than just listening my lecture passively, you will be using MATLAB directly, as this hands-on approach will help you understand key concepts like convolution and the Fourier transform in a deeper, more practical way.\n\nMATLAB is an invaluable tool used across many disciplines; for example, medical imaging and biomedical engineering in general. It will be an important part of your learning journey. If you don't have it installed yet, you can easily download it from the official MATLAB website‚Äîthis is freely available to you. If you run into any installation issues, support is readily available through your university‚Äôs resources, such as the IT support team.\n\nTo get started, I recommend completing the MATLAB On-ramp tutorial, which is a two-hour interactive course that will walk you through the basics. I've personally found this tutorial very effective in building MATLAB skills, and it‚Äôs designed for learners of all levels. Rather than only spending time in the next lecture on MatLab, I encourage you to complete this on your own time before the next lecture. This will give you the background you need and prepare you for the upcoming lessons, where we‚Äôll apply these concepts together.\n\nLet me make it your homework: going through the MATLAB On-ramp. By the end of that session, you should be comfortable with MATLAB‚Äôs interface and basic functions. Then, you‚Äôll be ready to learn more.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 46, "slide_filename": "Slide46.txt", "slide_annotation": "Lecture 1 - Slide46.txt", "file_path": "Lecture 1\\Texts\\Slide46.txt", "content": "You‚Äôll notice that this course is not just about the theory‚Äîit‚Äôs also about putting those theories into practice. For example, you can use MATLAB to simulate various types of data processing. It‚Äôs not about learning complicated tricks; it‚Äôs about using basic MATLAB functions to analyze data, visualize results, and perform tasks that will help with your assignments.\n\nRemember, the goal here is not to get bogged down in fancy techniques. Rather, we want to ensure that you have a solid foundation in MATLAB so that when it‚Äôs time to do your homework or apply concepts like the Fourier transform or linear systems, you‚Äôll have the tool you need to execute them effectively. Now, ChatGPT is a great tool to help you develop MatLab codes.  Please keep this in mind and become familiar with ChatGPT in this context.\n\nWith hands-on practice, you'll get familiar with the interface and the functionality, making your learning process or workflow more effective and more efficient. It‚Äôs all about building that confidence through practical experience.\n\nSo, let‚Äôs keep learning with MATLAB. By engaging with it now, you‚Äôll set yourself up for success in the course.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 47, "slide_filename": "Slide47.txt", "slide_annotation": "Lecture 1 - Slide47.txt", "file_path": "Lecture 1\\Texts\\Slide47.txt", "content": "Now, I want to say something that‚Äôs transforming the world of medical imaging: the huge wave of machine learning, artificial intelligence (AI), and robotics. These technologies hold huge potential, and they are revolutionizing how we analyze and interpret medical images, to say the least.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 48, "slide_filename": "Slide48.txt", "slide_annotation": "Lecture 1 - Slide48.txt", "file_path": "Lecture 1\\Texts\\Slide48.txt", "content": "As we continue to explore the exciting advancements in medical imaging, it's clear that this field is still young. Through a recent survey, I found that medical imaging is one of the hottest areas in research today, with a significant increase in the number of published papers every year. However, what‚Äôs really catching attention is machine learning‚Äîit's quickly becoming an even hotter topic than medical imaging itself.\n\nWhat does this mean? The combination of medical imaging with machine learning, especially deep learning, holds enormous potential for the future of diagnostics, treatment planning, and personalized medicine. This intersection is where we can expect to see transformative changes in how we approach medical imaging. The ability to reconstruct, analyze and interpret medical images with AI-empowered algorithms opens doors to endless possibilities.\n\nIn 2016 I wrote the first perspective article titled Perspective on Deep Imaging, where I share my thoughts on how deep learning and AI are poised to revolutionize medical imaging in the coming years. If you‚Äôre curious about my views on this, feel free to search for it and dive deeper into the future of medical imaging.\n\nHowever, in this course, we‚Äôre going to stick with the traditional classroom teaching, but for those of you who are highly motivated and eager to push beyond the syllabus, there‚Äôs an exciting option. You can choose to work on a project that integrates machine learning with medical imaging. This would involve hands-on research, where your work could contribute directly to a meaningful outcome in the field.\n\nWhile it‚Äôs not a requirement, this is an opportunity for those of you who want to challenge yourselves and explore advanced concepts further. For students who choose this option, 30% of your grade could come from the research project, with 70% from other course elements. Many students find that they don‚Äôt have the time to take such a project, but if you‚Äôre passionate about it, the opportunity is there. In fact, some of my former students, even high school students, have worked with me on research projects, and have gone on to get their work published in conferences and journals, which can be a huge boost to their career.\n\nSo, if you feel like the course is a bit easy and you‚Äôre ready to go beyond, this project option could be a perfect fit. The office door is always open, and if you choose this path, you'll get real-world experience and potentially contribute to cutting-edge research.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 49, "slide_filename": "Slide49.txt", "slide_annotation": "Lecture 1 - Slide49.txt", "file_path": "Lecture 1\\Texts\\Slide49.txt", "content": "Building on what we have discussed so far about the intersection of medical imaging and machine learning, let‚Äôs take a moment to look further ahead.\n\nI truly believe the future of medical imaging‚Äîand medicine in general‚Äîis full of incredible possibilities. As artificial intelligence continues to advance, we‚Äôre already seeing systems that can match or even surpass human performance in interpreting medical images. It‚Äôs very possible that, in the not-so-distant future, many of the tasks traditionally done by radiologists will be basically handled by AI.\n\nAnd it goes beyond that. Imagine robotic surgeons, guided by multimodal imaging and machine learning, performing procedures with precision that‚Äôs hard for even the best human surgeon to match. Over time, we might see AI and robotics take on more and more roles in healthcare, not to replace professionals, but to enhance what we can achieve‚Äîand to make healthcare safer, faster, and more accessible.\n\nThis is the direction the field is moving in, and it‚Äôs one of the reasons why understanding imaging technologies today is so valuable and timely. You're not just learning how things work now‚Äîyou‚Äôre preparing to shape the future.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 1", "slide_number": 50, "slide_filename": "Slide50.txt", "slide_annotation": "Lecture 1 - Slide50.txt", "file_path": "Lecture 1\\Texts\\Slide50.txt", "content": "As we think about how artificial intelligence and robotics might transform medical imaging and healthcare, it‚Äôs hard not to be reminded of how often science fiction points the way to future innovations.\n\nFor example, have you seen the movie Elysium? In that film, there‚Äôs this futuristic machine‚Äîa kind of all-in-one medical scanner. It can detect and treat any disease, regenerate or even replace organs, and remarkably, reverse the aging process. It‚Äôs the ultimate vision of technology and medicine coming together to provide perfect healthcare.\n\nOf course, today we aren‚Äôt there yet. But the ideas behind that machine‚Äîintegrated imaging, precise diagnostics, personalized treatment‚Äîare exactly what we‚Äôre working toward with advanced medical imaging, machine learning, and robotics. It‚Äôs a powerful reminder of what could be possible.\n\nI hope this inspires you as we move forward in the course. By learning the foundations of medical imaging today, you‚Äôre taking the first step toward contributing to the invention of our future.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 2 - Slide1.txt", "file_path": "Lecture 2\\Texts\\Slide1.txt", "content": "Welcome to this session, where we‚Äôll begin exploring MATLAB ‚Äî one of the most widely used computational platforms in science and engineering, especially in biomedical imaging.\nThis will serve as your gateway to MATLAB: how it works, why we use it, and how it helps us handle real-world data. You don‚Äôt need any prior experience ‚Äî we‚Äôll start with the basics and build up step by step.\nAs we move forward, you‚Äôll use MATLAB to generate signals, reconstruct images, and analyze datasets ‚Äî all key tasks in medical imaging. The goal is not just to learn syntax, but to develop intuition and confidence in working with computational tools that are directly relevant to biomedical research and clinical practice.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 2 - Slide2.txt", "file_path": "Lecture 2\\Texts\\Slide2.txt", "content": "We are right on schedule in our journey through this material.\n\nIf you‚Äôve already previewed the reading materials related to today‚Äôs lecture, excellent! That head start will help you connect concepts more easily. If not, no worries. I do encourage you to follow along closely and review key ideas afterward. This habit of reinforcing concepts as you progress will help you build a stronger, more intuitive understanding, especially as the ideas become more mathematically involved.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 2 - Slide3.txt", "file_path": "Lecture 2\\Texts\\Slide3.txt", "content": "Now let me address a common issue students often encounter early on ‚Äî installation problems with MATLAB.\nHere‚Äôs a message I received from a student. He was unable to install MATLAB on his computer because of authentication issues with CAS ‚Äî something that happens occasionally, especially at the beginning of the semester. The good news is that this is fixable, and you‚Äôre not alone.\nIf you experience something similar, please reach out early. Visit the RPI Help Center, and they‚Äôll assist you with installing MATLAB. And of course, please feel free to contact me or the teaching assistant ‚Äî we‚Äôre here to help.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 2 - Slide4.txt", "file_path": "Lecture 2\\Texts\\Slide4.txt", "content": "If you're unable to install MATLAB on your own machine, don‚Äôt worry ‚Äî you still have options.\nOne solution is MATLAB Online, a browser-based version provided by MathWorks. You can access it from any modern browser without installing anything on your device. Just search ‚Äúrun MATLAB online,‚Äù and you‚Äôll find the link right away.\nAnother option is Octave Online. It‚Äôs a free, open-source alternative that supports many MATLAB commands. While it doesn‚Äôt include all the advanced toolboxes, it‚Äôs good enough for our assignments.\nSo remember, lack of installation shouldn‚Äôt be a blocker preventing you from doing homework. There are various ways to stay engaged and complete your tasks ‚Äî all you need is a browser and a willingness.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 2 - Slide5.txt", "file_path": "Lecture 2\\Texts\\Slide5.txt", "content": "Let‚Äôs take a closer look at Octave.\nOctave is a scientific programming language that mirrors MATLAB‚Äôs syntax and functionality. It‚Äôs free, open-source, and available for Windows, macOS, and Linux. You can run it in the browser using Octave Online.\nThe interface looks different, but under the hood, many of the same commands will work ‚Äî especially those used for basic matrix operations, plotting, and simple scripts. This makes Octave a reliable backup if MATLAB access is delayed or limited.\nWhile it might not support all specialized toolboxes or image processing features, it‚Äôs more than enough for getting started and building foundational skills, at least for this course.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 2 - Slide6.txt", "file_path": "Lecture 2\\Texts\\Slide6.txt", "content": "Let me walk you through what we‚Äôll cover in this lab module.\nWe‚Äôll start by getting MATLAB installed on your machine. Once it‚Äôs up and running, you‚Äôll go through the module MATLAB On Ramp ‚Äî an interactive tutorial that introduces the core features of the platform in a hands-on way.\nNext, we‚Äôll work through two examples that show MATLAB in action. First, we‚Äôll look at spectral shifts of starlight ‚Äî a great illustration of how MATLAB can be used in astrophysics. Then, we‚Äôll switch gears to generate a 3D cone surface, which introduces MATLAB‚Äôs powerful graphics.\nAnd once you‚Äôve the basics, we can take steps further. As good examples, we can explore several advanced MATLAB toolboxes ‚Äî like image processing, instrument control, optimization, statistics and machine learning, and symbolic computation. These toolboxes allow MATLAB to go far beyond basic math and become a most powerful scientific computing platform.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 2 - Slide7.txt", "file_path": "Lecture 2\\Texts\\Slide7.txt", "content": "The very first step is to install and activate MATLAB.\nIf you‚Äôre at RPI, you can download MATLAB directly from the campus software portal: dotcio.rpi.edu/services/software-labs. Make sure to follow the installation instructions that match your operating system ‚Äî whether you're on Windows, macOS, or Linux.\nDuring the installation, you‚Äôll be asked to enter a product key to activate the software. This key is provided by the institution, and it unlocks your licensed access.\nIf you run into any issues during the setup ‚Äî like installation errors or activation trouble ‚Äî don‚Äôt hesitate to reach out. You can consult the help center, or message me directly. We're here to help you get started smoothly.\nHaving MATLAB set up and ready prepares us well before diving into hands-on examples.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 2 - Slide8.txt", "file_path": "Lecture 2\\Texts\\Slide8.txt", "content": "Now that you have MATLAB installed, let‚Äôs talk about how to launch it.\nIf you‚Äôre using Windows, just go to your start menu or applications folder and open MATLAB like any other program. On Linux, you‚Äôll typically launch it from the terminal by typing matlab.\nOnce MATLAB starts, you‚Äôll see what‚Äôs called the MATLAB desktop ‚Äî this is the main environment where everything happens. You‚Äôll notice a command window for typing code, a workspace that keeps track of your variables, a file browser, and an editor where you can write and save scripts.\nTake a few minutes to get familiar with this layout. Learning your way around now will help you feel more confident and efficient as we go forward.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 2 - Slide9.txt", "file_path": "Lecture 2\\Texts\\Slide9.txt", "content": "As you begin using MATLAB, you might occasionally wonder, ‚ÄúWhat does this function do?‚Äù That‚Äôs where the built-in help system comes in.\nLet‚Äôs say you want to understand how to create a plot. Just type help plot in the command window, and MATLAB will show you the syntax, arguments, and examples for that function.\nIf you don‚Äôt know the name of the function you need, use the lookfor command. For example, typing lookfor matrix will return a list of functions that relate to matrices.\nThese built-in tools are powerful and often overlooked. Knowing how to access documentation and search for functions will make you more independent and effective as you learn.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 2 - Slide10.txt", "file_path": "Lecture 2\\Texts\\Slide10.txt", "content": "Once you start writing code in MATLAB, you‚Äôll be creating and manipulating variables ‚Äî and the workspace is used to hold them.\nThe who and whos commands show you what variables are currently stored. who gives you a simple list, while whos gives more detail ‚Äî like size, type, and memory usage.\nYou can save all your current variables to a file using the save command and later bring them back with load. If you want to start fresh, clear all wipes out everything in the workspace, clc clears the command window, and close all shuts any open figures.\nThese are simple but essential tools. Once you know how to manage your workspace, you‚Äôll find MATLAB easier and more enjoyable to use.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 2 - Slide11.txt", "file_path": "Lecture 2\\Texts\\Slide11.txt", "content": "Let‚Äôs take a moment to look at some fundamental syntax in MATLAB that you‚Äôll see all the time.\nThe percent symbol ‚Äî % ‚Äî is used for writing comments. Anything after that symbol is ignored by MATLAB, so it‚Äôs a great way to document your code. \nA semicolon ‚Äî ; ‚Äî tells MATLAB not to display the output of a command, which keeps your console cleaner.\nIf a command is getting too long, you can break it across multiple lines using three dots ‚Äî .... This helps keep your code readable.\nMATLAB also includes a few important constants: eps stands for machine precision, inf is infinity, and NaN ‚Äî which stands for ‚ÄúNot a Number‚Äù ‚Äî appears when a calculation isn‚Äôt mathematically defined, like dividing some number by zero.\nTo control how numbers appear on your screen, use formatting commands. For example, format long shows more decimal places, and format short shows fewer.\nYou also have access to a large library of built-in functions ‚Äî like sqrt for square roots, exp for exponentials, and trigonometric functions such as sin and cos.\nBasic arithmetic is intuitive ‚Äî you‚Äôll use plus, minus, times, and divide symbols just as you'd expect. And for constants, MATLAB gives you pi, and Euler‚Äôs number, which you can access with exp 1.\nTogether, these form the foundation of all numerical work in MATLAB.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 2 - Slide12.txt", "file_path": "Lecture 2\\Texts\\Slide12.txt", "content": "Suppose we define:\u000bv as a row vector equals negative two, three, zero, four point five, and negative one point five.\u000bTo make it a column, we write: v equals v prime ‚Äî using the transpose.\nTo access elements:\u000bv of one gives the first,\u000bv of two to four gives the second to fourth,\u000band v of three and five gives just those two.\nYou can also create sequences.\u000bFor example: v equals four to two, stepping by negative one ‚Äî gives four, three, two.\nAnd combining vectors is easy.\u000bIf a equals one to three, and b equals two to three,\u000bthen c equals a b ‚Äî gives one, two, three, two, three.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 2 - Slide13.txt", "file_path": "Lecture 2\\Texts\\Slide13.txt", "content": "Let‚Äôs see how to create and work with matrices in MATLAB.\nTo get evenly spaced numbers, use linspace.\u000bFor example:\u000bx equals linspace from minus pi to pi, with 10 points ‚Äî gives 10 values across that range.\u000bFor logarithmic spacing, use logspace.\nTo define a matrix, write:\u000bA equals square bracket one two three; four five six ‚Äî that gives a two-by-three matrix.\nTo access elements:\u000bA of one, two gives the value in the first row, second column.\u000bA colon, two returns the second column.\u000bA two, colon gives the second row.\nYou can add, subtract, or scale matrices like:\u000bA plus B, or two times A.\nUse A star B for matrix multiplication,\u000band A dot star B for element-by-element.\nTranspose with A prime,\u000band get the determinant using det of A.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 2 - Slide14.txt", "file_path": "Lecture 2\\Texts\\Slide14.txt", "content": "MATLAB also makes it easy to generate special-purpose matrices, which are often useful for setting up or testing algorithms.\nFor example, diag of v creates a diagonal matrix from a vector. You can also reverse it: diag of A pulls the diagonal elements from a matrix.\nIf you need an identity matrix ‚Äî that‚Äôs a square matrix with ones on the diagonal and zeros elsewhere ‚Äî just write eye of n.\nTo create a matrix full of zeros, use zeros of m and n. If you need one filled with ones, use ones of m and n.\nThese kinds of matrices are especially useful when initializing arrays before filling them with computed values.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 2 - Slide15.txt", "file_path": "Lecture 2\\Texts\\Slide15.txt", "content": "Now let‚Äôs look at logical operations in MATLAB.\nYou can compare values using operators like:\u000bdouble equals, less than, greater than, or not equals, which is written as tilde equals.\nThe tilde symbol means not.\u000bFor combining conditions, use ampersand for AND, and vertical bar for OR.\nTo find specific values in an array, use the find function.\u000bFor example: find A equals three ‚Äî gives you the positions where A is exactly three.\nThese logical tools are great for filtering data and writing conditional code.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 2 - Slide16.txt", "file_path": "Lecture 2\\Texts\\Slide16.txt", "content": "Alright, let‚Äôs shift gears and talk about solving systems of linear equations ‚Äî something that comes up all the time in engineering, physics, and, yes, even in medical imaging.\nSuppose we‚Äôre working with a system like A times x equals b. Now, you might think ‚Äî okay, just take the inverse of A and multiply it by b. So, x equals inv of A times b.\nAnd sure, that‚Äôs mathematically valid ‚Äî but in practice, it‚Äôs a bad idea. It‚Äôs slow, it‚Äôs unstable, and it can lead to all kinds of numerical issues, especially with large or ill-conditioned matrices.\nInstead, MATLAB gives us a much better tool: the backslash operator. So we write x equals A backslash b. This tells MATLAB to choose the most efficient way to solve the system behind the scenes ‚Äî no explicit inverse needed.\nLater on, we‚Äôll learn how to implement our own solvers, but for now, this built-in method is what you should use. It‚Äôs clean, fast, and accurate ‚Äî everything we want.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 2 - Slide17.txt", "file_path": "Lecture 2\\Texts\\Slide17.txt", "content": "Now that we‚Äôve seen how to work with equations, let‚Äôs take a moment to explore how we can inspect the structure of the data we‚Äôre working with ‚Äî vectors and matrices.\nTo find out how long a vector is, just use length ‚Äî so length of v tells you how many elements it has.\nIf you‚Äôre working with a matrix, and you want to know its shape ‚Äî how many rows and columns ‚Äî just use size of A.\nWant to check if a matrix is full rank? That‚Äôs easy ‚Äî rank of A will do the job.\nAnd when we want to understand how large a matrix or vector is in terms of magnitude, we use the norm function. Just norm of A gives the 2-norm by default ‚Äî that‚Äôs the square root of the sum of squares. You can also ask for specific norms like the 1-norm using norm of A, one, or the infinity norm with norm of A, inf.\nThese commands give you insight into the structure and stability of your data ‚Äî and they‚Äôre especially useful when you‚Äôre preparing data for algorithms or interpreting results.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 2 - Slide18.txt", "file_path": "Lecture 2\\Texts\\Slide18.txt", "content": "Let‚Äôs talk about control flow ‚Äî specifically, loops.\nStart with a for loop. Use it when you know how many times you want to repeat something.\nFor example:\u000bSet x to zero.\u000bThen, for i from 1 to 5 in steps of 2 ‚Äî so, 1, 3, and 5 ‚Äî\u000bwe add i to x.\u000bAt the end, x equals 9.\nfor loops are useful when working through a known sequence.\nNow, a while loop runs as long as a condition is true.\nIn this example:\u000bStart with x equal to 7.\u000bWhile x is greater than or equal to zero, subtract 2 each time.\u000bThe loop stops when x becomes negative.\nJust be careful ‚Äî if the condition never becomes false, the loop won‚Äôt stop.\nFinally, break lets you exit a loop early.\u000bIt‚Äôs useful if you find what you need before the loop finishes.\u000bIn nested loops, it only exits the innermost one.\u000bUse it wisely ‚Äî don‚Äôt skip steps that still matter.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 2 - Slide19.txt", "file_path": "Lecture 2\\Texts\\Slide19.txt", "content": "Let‚Äôs talk about how we make decisions in MATLAB.\n\nFirst, the if statement.\nYou might say:\u000b‚ÄúIf x is equal to 3, display: the value of x is 3.\u000bIf x is equal to 5, display: the value of x is 5.\u000bOtherwise, display: the value of x is not 3 or 5.‚Äù\nThis allows the program to choose what to do based on the value of x.\nNow, when you want to check one variable against several specific values, a switch statement is better.\n\nFor example:\u000b‚ÄúIf face is 1, say: rolled a one.\u000bIf face is 2, say: rolled a two.\u000bOtherwise, say: rolled a number greater than or equal to three.‚Äù\nThis is easier to read than writing multiple if statements. And in MATLAB, the switch ends automatically ‚Äî no need for a break.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 2 - Slide20.txt", "file_path": "Lecture 2\\Texts\\Slide20.txt", "content": "Now let‚Äôs talk about vectorization ‚Äî one of the most important habits in MATLAB programming.\nHere‚Äôs the idea. Instead of looping through each element one by one, we apply an operation to the whole array at once.\nFor example:\u000bLet‚Äôs say x is the vector ‚Äî one, two, three.\u000bUsing a loop, we would add five to each element, one at a time.\u000bBut with vectorization, we just say: x equals x plus five ‚Äî and MATLAB does it all at once.\nSame result, much faster, much cleaner.\nWhenever you can, avoid loops. Use vectorized operations. MATLAB is built for that.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 2 - Slide21.txt", "file_path": "Lecture 2\\Texts\\Slide21.txt", "content": "Let‚Äôs look at how MATLAB handles basic plots.\nStart by generating x values from minus one to one using linspace. Then take the sine of x to get y.\nUse plot x and y to create a basic line graph.\nTo change the appearance, you can specify style and color. For example, plot x, y, black line draws the line in black.\nUse hold on if you want to overlay another plot on the same figure.\u000bUse figure to open a new figure window for a separate plot.\nPlotting helps visualize data ‚Äî and it‚Äôs built into MATLAB as a core strength.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 2 - Slide22.txt", "file_path": "Lecture 2\\Texts\\Slide22.txt", "content": "Sometimes, you want to show several plots in one window.\nUse the subplot command to divide the figure into a grid.\u000bFor example, subplot 2, 3, 1 creates two rows and three columns ‚Äî and selects the first cell.\nEach subplot acts like an independent plot. You can add titles, labels, and style just like a regular figure.\nThis is helpful when comparing outputs or organizing results cleanly on a single screen.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 2 - Slide23.txt", "file_path": "Lecture 2\\Texts\\Slide23.txt", "content": "Now let‚Äôs add a third dimension.\nUse plot 3 of x, y, z to make a 3D line plot.\nFor surfaces, use the mesh command ‚Äî it draws a grid over the surface defined by z values.\nIf you want a contour plot ‚Äî use contour z matrix. That shows level curves of a surface.\nYou can also control the axis limits with axis, add a title with title, and label the axes using x label and y label.\nTo explain the meaning of different curves, use legend.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 2 - Slide24.txt", "file_path": "Lecture 2\\Texts\\Slide24.txt", "content": "Here are a few examples of what you can do with MATLAB plots.\nYou‚Äôll see grouped bar charts, stacked bars, 2D lines, and 3D surfaces ‚Äî all using basic commands.\nTry different styles, colors, and layouts to get the look you want.\nGood visualization tells a story ‚Äî not just numbers, but patterns, relationships, and insights.\nUse plots not just to check results, but to communicate them clearly.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 2 - Slide25.txt", "file_path": "Lecture 2\\Texts\\Slide25.txt", "content": "Now let‚Äôs talk about reading and writing data files.\nTo open a file, use f open, and pass the file name and read mode.\u000bFor example: f open in dot dat, r t.\nThis gives you a file ID ‚Äî or fid.\nUse f scanf to read numbers from the file.\u000bWhen you're done, always close the file using f close.\nIf you need formatting help, use help textread or help f printf.\nBelow is an example of a data file ‚Äî it has names, types, numbers, and yes or no labels.\nYou‚Äôll often need to work with mixed data ‚Äî so learning how to read from and write to files is essential in MATLAB workflows.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 2 - Slide26.txt", "file_path": "Lecture 2\\Texts\\Slide26.txt", "content": "Let‚Äôs now learn how to read data files in MATLAB ‚Äî both fully and partially.\nTo read an entire structured dataset, you first open the file using fopen, then use the textread function to extract data from all columns.\nEach data type in the file corresponds to a format symbol ‚Äî like %s for strings, %f for floating-point numbers, and %d for integers. MATLAB assigns each column to a variable automatically.\nThis works well when you need to load the whole dataset into memory ‚Äî like names, types, values, and so on.\nNow, if you only want to read part of the data ‚Äî say just the first column ‚Äî you can still use textread, but with asterisks added to the format string to skip over the columns you don‚Äôt need. MATLAB will still move through the file correctly, but it‚Äôll only save the data you asked for.\nThis selective reading is great for reducing memory usage or focusing on relevant features.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 2 - Slide27.txt", "file_path": "Lecture 2\\Texts\\Slide27.txt", "content": "Sometimes you don‚Äôt need the full file ‚Äî you just want a specific line. Maybe line two contains Joe‚Äôs entry, and that‚Äôs all you care about.\nIn this case, you can tell MATLAB to skip one line, and then read just one. You still use textread, but now with two extra options: the number of lines to read, and the number of header lines to skip.\nThis lets you directly target a record without loading everything ‚Äî useful for indexed data, or when you‚Äôre debugging or sampling from a large file.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 2 - Slide28.txt", "file_path": "Lecture 2\\Texts\\Slide28.txt", "content": "So far we‚Äôve looked at reading files ‚Äî now let‚Äôs switch to writing.\nTo write data out, open a file in write mode using fopen. Then use fprintf to format the output line, inserting variables in the order you want them saved.\nFor example, you can save name, type, and numerical values all in one line. And as always, once writing is done, make sure to close the file with fclose.\nThis gives you full control over the format, spacing, and layout of your data. Very useful when preparing results for review, or generating output for other software to read.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 2 - Slide29.txt", "file_path": "Lecture 2\\Texts\\Slide29.txt", "content": "Next, let‚Äôs see how to keep a record of your work using the diary command.\nStart with diary followed by a filename. From that point on, everything you type and everything MATLAB prints will be saved in a log file. Once you‚Äôre done, stop the recording by typing diary off.\nThis is great for tracking your workflow, recording sessions, or saving what you‚Äôve done for future reference or reporting.\nNow, another useful tool is timing. If you want to measure how long your code takes to run, just put tic before the commands, and toc after them. MATLAB will show you the elapsed time in seconds.\nThis helps you compare different methods and optimize performance.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 30, "slide_filename": "Slide30.txt", "slide_annotation": "Lecture 2 - Slide30.txt", "file_path": "Lecture 2\\Texts\\Slide30.txt", "content": "One of the key ideas in MATLAB is that everything is a matrix.\nA single number? That‚Äôs just a one-by-one matrix.\u000bA row of numbers is a row vector. A column is a column vector.\u000bAnd an image? That‚Äôs a two-dimensional matrix.\u000bIf it‚Äôs in color, it becomes three-dimensional ‚Äî width, height, and color channels.\nThis matrix mindset makes MATLAB powerful and consistent across tasks ‚Äî whether you're doing math, image processing, or even machine learning.\nNow, let‚Äôs talk about indexing. If you have a matrix A:\u000bA of i comma j gives the element in row i and column j.\u000bA of i comma colon gives the entire i-th row.\u000bA of colon comma j gives the whole j-th column.\nYou can reshape matrices, slice out parts, or use logical masks to filter data.\nTo modify the matrix, use expressions like A plus five ‚Äî which adds five to every element.\u000bOr A dot star B ‚Äî for element-by-element multiplication.\nThis is the foundation of MATLAB ‚Äî matrix thinking in everything you do", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 31, "slide_filename": "Slide31.txt", "slide_annotation": "Lecture 2 - Slide31.txt", "file_path": "Lecture 2\\Texts\\Slide31.txt", "content": "In MATLAB, you can do both matrix and element-wise operations ‚Äî and the difference is all in the dot.\n‚ÄúA times B‚Äù ‚Äî with a single star ‚Äî means matrix multiplication.\u000b‚ÄúA dot star B‚Äù ‚Äî with a dot ‚Äî means multiply element by element.\nSame goes for powers and division.\u000b‚ÄúA caret 2‚Äù raises the matrix to a power.\u000b‚ÄúA dot caret 2‚Äù squares each element.\u000bAnd for division, ‚ÄúA divided by B‚Äù solves equations, while ‚ÄúA dot slash B‚Äù divides element-wise.\nThese small differences are key when working with data, equations, or signals in MATLAB.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 32, "slide_filename": "Slide32.txt", "slide_annotation": "Lecture 2 - Slide32.txt", "file_path": "Lecture 2\\Texts\\Slide32.txt", "content": "Whenever you can, avoid loops ‚Äî and write in matrix form.\nInstead of looping to square elements, just write ‚ÄúA dot caret 2‚Äù.\nWant row-wise sums? Use ‚Äúsum of A, comma 2‚Äù.\nNeed probabilities per row? Divide each row by its total using matrix broadcasting ‚Äî it‚Äôs cleaner and faster than nested loops.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 33, "slide_filename": "Slide33.txt", "slide_annotation": "Lecture 2 - Slide33.txt", "file_path": "Lecture 2\\Texts\\Slide33.txt", "content": "The peaks function creates a sample surface. Use mesh to visualize it.\nTo smooth it out, create a finer grid with meshgrid, and use interp2 to estimate new Z-values.\nThis technique is useful in graphics, simulations, and image processing ‚Äî anytime you want smoother, higher-resolution data.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 34, "slide_filename": "Slide34.txt", "slide_annotation": "Lecture 2 - Slide34.txt", "file_path": "Lecture 2\\Texts\\Slide34.txt", "content": "Here‚Äôs how we morph faces using matrix tools.\nWe warp two face images using interpolation on each color channel. Then blend them using a weighted average ‚Äî with alpha controlling the mix.\nTo detect unusual pixels, we reshape the image and compute the Mahalanobis distance from a reference color.\nNext, we create a binary mask by thresholding the distance, and apply a median filter to clean it up.\nIt‚Äôs a compact example of how MATLAB combines image warping, statistics, and filtering ‚Äî all with matrices.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 35, "slide_filename": "Slide35.txt", "slide_annotation": "Lecture 2 - Slide35.txt", "file_path": "Lecture 2\\Texts\\Slide35.txt", "content": "This example shows how plot resolution affects curve smoothness.\nIn a loop, we increase the number of x-values using linspace, then compute y equals x over one plus x squared.\nWe plot each result in a subplot grid, with labels and titles showing the resolution.\nWith each pause, you can see the graph getting smoother ‚Äî a great way to visualize the impact of sampling density.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 36, "slide_filename": "Slide36.txt", "slide_annotation": "Lecture 2 - Slide36.txt", "file_path": "Lecture 2\\Texts\\Slide36.txt", "content": "Let‚Äôs talk about the two main types of files you‚Äôll create in MATLAB ‚Äî scripts and functions.\nScripts are simple. They run a series of commands and use variables already in your workspace. You don‚Äôt pass anything in, and they don‚Äôt return anything out. They‚Äôre great for quick experiments or automation.\nFunctions, on the other hand, are more structured. You give them inputs, and they return outputs. Inside a function, all variables are local ‚Äî meaning they don‚Äôt interfere with anything else in your workspace.\nKnowing when to use a script or a function is key to writing clean, reliable code.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 37, "slide_filename": "Slide37.txt", "slide_annotation": "Lecture 2 - Slide37.txt", "file_path": "Lecture 2\\Texts\\Slide37.txt", "content": "Here‚Äôs a basic function example ‚Äî one that calculates the area and circumference of a circle.\nWe define a function called circle that takes one input: the radius. Inside, we calculate the area as pi times radius squared, and the circumference as two times pi times the radius.\nSave this in a file named circle dot m.\nNow, to use this function, we write a script. For example, if the radius is 7, we call the function with that value and display the result using disp.\nYou‚Äôd save this script as myscript dot m.\nThis is how functions and scripts work together. You build reusable tools with functions and run them from scripts.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 38, "slide_filename": "Slide38.txt", "slide_annotation": "Lecture 2 - Slide38.txt", "file_path": "Lecture 2\\Texts\\Slide38.txt", "content": "Let‚Äôs try a simple numerical task: summing one over i squared, from i equals 1 to 10.\nWe can do this two ways ‚Äî forward, starting from i equals 1, or backward, starting from i equals 10.\nThe code is nearly identical, just with the loop going in reverse.\nNow, in theory, both results should be the same. But because of floating-point rounding, the order can make a small difference. This is a great example of how numerical stability matters ‚Äî especially in scientific computing.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 39, "slide_filename": "Slide39.txt", "slide_annotation": "Lecture 2 - Slide39.txt", "file_path": "Lecture 2\\Texts\\Slide39.txt", "content": "Now let‚Äôs write our own matrix multiplication function ‚Äî no built-in functions allowed.\nWe create a function called matrix_multiply that takes two square matrices and a size n.\nUsing three nested loops, we compute the product row by row and column by column. For each entry in the result, we take a dot product between a row of A and a column of B.\nThis hands-on approach shows exactly how matrix multiplication works under the hood ‚Äî which is important to understand before we talk about optimization.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 40, "slide_filename": "Slide40.txt", "slide_annotation": "Lecture 2 - Slide40.txt", "file_path": "Lecture 2\\Texts\\Slide40.txt", "content": "To test our new function, we write a quick script.\nWe choose a matrix size n, generate two random n-by-n matrices A and B, and call our matrix_multiply function to compute the result.\nYou can also compare it to MATLAB‚Äôs built-in multiplication to check for accuracy.\nAnd here‚Äôs something to think about ‚Äî this triple-loop method works, but it‚Äôs not fast. Later on, we‚Äôll talk about how to rewrite this using vectorized operations to make it much more efficient.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 41, "slide_filename": "Slide41.txt", "slide_annotation": "Lecture 2 - Slide41.txt", "file_path": "Lecture 2\\Texts\\Slide41.txt", "content": "Now let‚Äôs look at a real-world scientific task ‚Äî measuring how stars move.\nWe start with spectral data from a star. It includes many wavelengths, evenly spaced. The key is to find a feature called the Hydrogen-alpha line. If that line shifts, the star is moving ‚Äî either away or toward us.\nWith MATLAB, we‚Äôll identify that line, calculate how much it‚Äôs shifted, and then use the Doppler formula to estimate the star‚Äôs speed.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 42, "slide_filename": "Slide42.txt", "slide_annotation": "Lecture 2 - Slide42.txt", "file_path": "Lecture 2\\Texts\\Slide42.txt", "content": "Here‚Äôs how we do that in MATLAB.\nFirst, we compute the wavelength range using the start point, number of data points, and spacing. Then, we build a vector of wavelengths.\nNext, we plot the spectrum and locate the lowest point ‚Äî which marks the Hydrogen-alpha line. Its position gives us the shifted wavelength.\nUsing the Doppler shift formula, we compute the redshift, then multiply by 300,000 ‚Äî the speed of light in kilometers per second ‚Äî to get the star‚Äôs velocity.\nJust a few lines of code, and we‚Äôre measuring motion across space.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 43, "slide_filename": "Slide43.txt", "slide_annotation": "Lecture 2 - Slide43.txt", "file_path": "Lecture 2\\Texts\\Slide43.txt", "content": "Let‚Äôs switch gears to 3D graphics.\nHere, we‚Äôre creating a cone using MATLAB‚Äôs cylinder function.\nWe define a shape with the vector: t equals zero, one, zero. This gives us a cone shape.\nThen we call the cylinder function to get the X, Y, and Z coordinates, and use surf to draw the 3D surface.\nThis shows how easily you can build and visualize 3D shapes in MATLAB ‚Äî helpful for both learning and engineering.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 44, "slide_filename": "Slide44.txt", "slide_annotation": "Lecture 2 - Slide44.txt", "file_path": "Lecture 2\\Texts\\Slide44.txt", "content": "Here‚Äôs the full example, all in one place.\nWe define t as a vector with values zero, one, zero. Then we call the cylinder function to generate the geometry, and use surf to plot it.\nThat‚Äôs it ‚Äî three lines of code give us a smooth 3D cone you can rotate, zoom, and customize. You can even add lights, shadows, or change the viewing angle.\nIt‚Äôs a simple example, but it shows the power of MATLAB for 3D visualization.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 45, "slide_filename": "Slide45.txt", "slide_annotation": "Lecture 2 - Slide45.txt", "file_path": "Lecture 2\\Texts\\Slide45.txt", "content": "Now let‚Äôs explore the Image Processing Toolbox.\nIt includes powerful tools for loading, modifying, analyzing, and segmenting images.\nYou can sharpen edges, enhance contrast, or isolate bright regions. It even lets you analyze object shapes and intensities ‚Äî which is essential in fields like medical imaging.\nYou can also use it to register images, align features, and prepare large datasets for analysis. And if needed, you can even accelerate the workflow with C or C++ code.\nThis toolbox makes MATLAB a great platform for both learning and real-world applications in imaging.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 46, "slide_filename": "Slide46.txt", "slide_annotation": "Lecture 2 - Slide46.txt", "file_path": "Lecture 2\\Texts\\Slide46.txt", "content": "Now let‚Äôs take a look at the Instrument Control Toolbox.\nThis toolbox allows MATLAB to talk directly to hardware ‚Äî like oscilloscopes, sensors, or signal generators. That means you can send commands, receive data, and even control external devices ‚Äî all from your MATLAB environment.\nYou can also create your own instrument drivers, manage device sessions, and work with supported protocols like VISA or TCP/IP.\nSo, if you're working in a lab with physical instruments, this toolbox helps you connect MATLAB to the real world.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 47, "slide_filename": "Slide47.txt", "slide_annotation": "Lecture 2 - Slide47.txt", "file_path": "Lecture 2\\Texts\\Slide47.txt", "content": "Next up ‚Äî the Optimization Toolbox.\nThis one‚Äôs all about solving problems where you want to find the best outcome ‚Äî like minimizing cost, or maximizing performance.\nYou can solve linear programs, quadratic programs, and nonlinear problems. It also supports multi-objective optimization.\nIt‚Äôs useful in many fields ‚Äî from data fitting and machine learning to operations research. If your work involves tuning models or decision-making, this toolbox is essential.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 48, "slide_filename": "Slide48.txt", "slide_annotation": "Lecture 2 - Slide48.txt", "file_path": "Lecture 2\\Texts\\Slide48.txt", "content": "Now we move into data science with the Statistics and Machine Learning Toolbox.\nThis toolbox helps you analyze data, test hypotheses, and build predictive models.\nYou can use it for clustering, regression, dimensionality reduction, or training classifiers. It also supports parallel computing, which is great for handling large datasets.\nAnd the best part? Many of these features work with just a few lines of code ‚Äî so you can try powerful techniques without writing complex algorithms from scratch.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 49, "slide_filename": "Slide49.txt", "slide_annotation": "Lecture 2 - Slide49.txt", "file_path": "Lecture 2\\Texts\\Slide49.txt", "content": "The Symbolic Math Toolbox brings exact math into MATLAB.\nInstead of working with numbers alone, you can define symbolic variables and compute exact derivatives, integrals, or solutions to equations.\nFor example, you can write a function like f of x equals x squared times sine x and ask MATLAB to find its derivative. You‚Äôll get the answer in symbolic form ‚Äî not an approximation.\nYou can also plot these expressions or export them into Simulink for simulation.\nIt‚Äôs a great toolbox for exploring math symbolically, especially in calculus and algebra-heavy problems.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 2", "slide_number": 50, "slide_filename": "Slide50.txt", "slide_annotation": "Lecture 2 - Slide50.txt", "file_path": "Lecture 2\\Texts\\Slide50.txt", "content": "Finally, let‚Äôs talk about how to keep learning on your own.\nThere‚Äôs a helpful online tutorial by Dr. Azernikov that introduces MATLAB basics step by step. It‚Äôs clear and beginner-friendly ‚Äî a great way to reinforce what we‚Äôve learned here.\nAlso, don‚Äôt forget about MATLAB‚Äôs built-in help tools. If your cursor is on a command, just press F1. Or type help followed by the function name in the Command Window.\nFor more support, you can check the MathWorks website, Stack Overflow, or MATLAB Central. These are great places to ask questions and find real code examples.\nIn short ‚Äî keep exploring, keep experimenting, and you'll keep getting better.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 3", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 3 - Slide1.txt", "file_path": "Lecture 3\\Texts\\Slide1.txt", "content": "Welcome to this lecture on medical imaging. In our earlier sessions, we introduced some general concepts in medical imaging and provided a brief overview of essential tools, including MATLAB programming. Today, we begin what I consider the foundation section of this course. This foundation will help you build the knowledge needed to understand the major medical imaging modalities we‚Äôll explore throughout the semester.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 3 - Slide2.txt", "file_path": "Lecture 3\\Texts\\Slide2.txt", "content": "We are right on schedule in our journey through this material.\n\nIf you‚Äôve had a chance to look over the reading materials for today‚Äôs lecture, that‚Äôs great ‚Äî it will make it easier to connect the ideas we cover. If not, that‚Äôs fine too. I encourage you to follow along closely and take time afterward to review the main points. Consistently reinforcing concepts as you go will help you develop a stronger and more intuitive grasp, especially as the material becomes more mathematically detailed", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 3 - Slide3.txt", "file_path": "Lecture 3\\Texts\\Slide3.txt", "content": "Today‚Äôs lecture is straightforward. We‚Äôre going to talk about the concept of a system‚Äîa core idea that underpins everything we‚Äôll cover in this course. If you‚Äôve downloaded the draft of my textbook, you‚Äôll see that this topic forms the first chapter. Reading just the first ten pages will give you a solid grasp of the key ideas.\nIn each chapter, I aim for a balanced structure: we start with three main sections, followed by a concluding section with remarks and points for deeper thought. In this lecture, we‚Äôll begin with general systems, then narrow our focus to linear systems‚Äîwhich, as the name suggests, behave in a straightforward, predictable way, much like linear functions in mathematics.\nTo qualify as a linear system, two properties must be satisfied: additivity and homogeneity. Together, these form what we call the superposition principle. We‚Äôll break these ideas down, explore when one property implies the other, and consider whether they are truly independent requirements. Lastly, we‚Äôll touch on nonlinear systems‚Äîwhich are simply systems that don‚Äôt meet these linearity conditions. This will give you a broad overview of the system concepts essential for medical imaging.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 3 - Slide4.txt", "file_path": "Lecture 3\\Texts\\Slide4.txt", "content": "Let‚Äôs build on that by comparing systems to something you‚Äôre already familiar with‚Äîfunctions. A function is a mathematical rule that maps each input from one set, called the domain, to an output in another set, called the range. Systems work in a similar way. They take an input, process it through interconnected elements, and produce an output.\nThese elements could represent anything: parts of a social system, components of a physiological system, or pieces of a mechanical system. The important point is that both functions and systems describe relationships between inputs and outputs. Our goal is to define these relationships clearly so that we can apply them to engineering and medical imaging problems.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 3 - Slide5.txt", "file_path": "Lecture 3\\Texts\\Slide5.txt", "content": "Here‚Äôs a simple illustration of what a function does. Imagine selecting a value from the domain‚Äîsay, a number‚Äîand applying the function to find its corresponding value in the range. This basic idea of mapping inputs to outputs might seem simple, but it‚Äôs fundamental to everything we‚Äôre discussing.\nIn my slides, I use symbols to highlight key points: a red diamond marks something essential‚Äîconcepts you should remember clearly. A green circle points out interesting but optional material, things that go beyond what you strictly need for engineering purposes. While we don‚Äôt aim to become mathematicians in this course, understanding these key ideas deeply will strengthen your grasp of medical imaging systems.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 3 - Slide6.txt", "file_path": "Lecture 3\\Texts\\Slide6.txt", "content": "Functions come in many forms‚Äîlike sinusoidal functions, linear functions, and even more complex structures such as those defined recursively in fractal geometry. If you‚Äôre unfamiliar with fractals, they‚Äôre fascinating patterns that repeat at every scale, and I encourage you to explore them further if you‚Äôre curious. But for our purposes, it‚Äôs enough to be comfortable with regular functions and their properties.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 3 - Slide7.txt", "file_path": "Lecture 3\\Texts\\Slide7.txt", "content": "Another powerful tool in mathematics is the Taylor expansion. This allows us to approximate any smooth, well-behaved function using a sum of terms: starting with a constant (the function‚Äôs value at a point), then adding linear terms based on the first derivative, and quadratic terms based on the second derivative, and so on.\nEach added term improves the approximation, as long as certain conditions are met to ensure the series converges. In engineering practice, though, we often stop at the simpler terms‚Äîconstant or linear‚Äîbecause they offer a good balance of accuracy and simplicity. This idea of building approximations is central to how we model systems efficiently.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 3 - Slide8.txt", "file_path": "Lecture 3\\Texts\\Slide8.txt", "content": "In fact, for most practical purposes, we‚Äôre satisfied with linear functions. In one dimension, this means a straight line; in two dimensions, it‚Äôs a plane; and in higher dimensions, we extend the idea further. These simple, linear relationships are easy to work with and provide the foundation for much of engineering analysis‚Äîincluding in medical imaging.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 3 - Slide9.txt", "file_path": "Lecture 3\\Texts\\Slide9.txt", "content": "As engineers, we often think of the world in terms of systems. We use mathematical operators‚Äîlike L or O‚Äîto describe how a system transforms an input into an output. If you feed an input signal, V, into a system, the output, W, represents how that signal is modified.\nThese inputs and outputs can take many forms: time-dependent signals, images, tensors, or even discrete data sets like color values in an image. The concept is general: a system processes an input and produces an output, just as a function maps domain to range.\nIn medical imaging, we apply this thinking to devices like CT scanners, where the entire system transforms physical signals into diagnostic images. And as we analyze these systems, we rely on mathematical relationships to describe their behavior in precise terms. This distinction between thinking as engineers about systems and as mathematicians about abstract functions is subtle but important as we move forward.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 3 - Slide10.txt", "file_path": "Lecture 3\\Texts\\Slide10.txt", "content": "As engineers, much of our work revolves around systems, and one of the first things we need to do is measurement. Without measurement, we can‚Äôt really understand or control anything. In fact, measurement is at the heart of both engineering and physics. For example, in quantum mechanics, measurement plays a critical role‚Äîuntil we measure, we don‚Äôt truly know a system‚Äôs state. It‚Äôs like the famous thought experiment with Schr√∂dinger‚Äôs cat: is the cat alive or dead? Only measurement collapses the uncertainty into reality.\nWhen we perform measurements, we typically have input variables and sometimes a reference. Consider weighing an object: we place a known weight on one side of a balance and the object on the other. If the balance is level, we know the object‚Äôs weight equals the standard. Similarly, in electrical measurements, we might compare an unknown resistance or voltage against a reference using an ohmmeter or a multimeter. These devices let us measure things like current, resistance, and capacitance with precision.\nMore broadly, think of an instructor as a system: when the calendar says it‚Äôs time for class, that‚Äôs the input. The output is the lecture. But if something changes‚Äîsay, the instructor is unwell or called away‚Äîthe output changes too. This is a reminder that systems can have modifiers that influence their behavior.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 3 - Slide11.txt", "file_path": "Lecture 3\\Texts\\Slide11.txt", "content": "An imaging system is a specialized type of measurement system. Consider an MRI scanner: its purpose is to measure and produce an image of a cross-section of the body‚Äôs anatomy. Ideally, this image is a faithful representation‚Äîa sharp, clear point corresponds exactly to a sharp point in the body.\nBut not all systems are created equal. A high-quality system will produce sharp, high-contrast images, while a less precise system might blur details, much like how a low-resolution camera produces softer, less defined pictures. This blurring is a key characteristic of the system and tells us a lot about its performance. In essence, imaging systems, sensing systems, and measurement systems all belong to the same family‚Äîthey process inputs to produce meaningful outputs through measurement.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 3 - Slide12.txt", "file_path": "Lecture 3\\Texts\\Slide12.txt", "content": "Another type of system is the control system. Unlike simple measurement systems, control systems don‚Äôt just generate an output‚Äîthey use that output as feedback to adjust the input or the system‚Äôs behavior.\nFor instance, think of a radar tracking an airplane. The radar detects the plane‚Äôs position and adjusts itself to keep the target within view. This feedback loop helps the system achieve its goal, even as conditions change. Control systems are essential in engineering whenever we want systems to adapt or correct themselves in real time.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 3 - Slide13.txt", "file_path": "Lecture 3\\Texts\\Slide13.txt", "content": "Today, robotic systems represent a hot area of control system development. From robots that provide assistance in hotels or nursing homes to those that help care for infants, these systems combine control with a degree of machine intelligence. Their adaptability and responsiveness open up incredible possibilities in both industry and healthcare.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 3 - Slide14.txt", "file_path": "Lecture 3\\Texts\\Slide14.txt", "content": "Let‚Äôs not forget neurological systems, like the human brain. The brain is a complex system of billions of neurons that process inputs from the environment and generate intelligent actions. Neurons decide whether to fire based on inputs, passing signals that enable everything from basic reflexes to complex reasoning. Together, these neural networks form the foundation of our physiological systems.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 3 - Slide15.txt", "file_path": "Lecture 3\\Texts\\Slide15.txt", "content": "The human system as a whole is incredibly intricate. It spans multiple levels‚Äîfrom genes and cells up to organs and entire physiological systems like the circulatory or digestive systems. When disease strikes, understanding these interconnections is crucial. This is the realm of systems biology or systems medicine‚Äîa field dedicated to studying the body as a network of interacting parts, so we can better understand, diagnose, and treat disease.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 3 - Slide16.txt", "file_path": "Lecture 3\\Texts\\Slide16.txt", "content": "Looking ahead, we can expect increasing convergence between robotic systems and human systems. Technologies like brain-machine interfaces are already emerging, with the potential to enhance memory, reasoning, and other cognitive functions. This merging of neuroscience and artificial intelligence represents an exciting frontier. As I once heard in a talk: physics helps us understand the external world, while neuroscience helps us understand ourselves. Bringing these fields together may fundamentally reshape what it means to be human.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 3 - Slide17.txt", "file_path": "Lecture 3\\Texts\\Slide17.txt", "content": "Given the vast complexity of all these systems, we need to narrow our focus. That‚Äôs why in this course, we concentrate on linear systems‚Äîthe simplest and most important class of systems, especially in medical imaging. Linear systems are manageable to study, yet powerful in their applications. Once we master them, we‚Äôll be ready to tackle the greater complexity of nonlinear systems.\nIn a linear system, two key properties must hold: additivity and homogeneity. Together, these form the superposition principle. We‚Äôll explore these in detail, starting with simple cases and building up to more rigorous requirements.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 3 - Slide18.txt", "file_path": "Lecture 3\\Texts\\Slide18.txt", "content": "Let‚Äôs look at additivity first. If you know how a system responds to input F1, producing output K1, and separately how it responds to input F2, producing K2, then additivity tells us the system‚Äôs response to F1 + F2 is simply K1 + K2. This property allows us to analyze complex inputs by breaking them into simpler parts‚Äîa key advantage in engineering.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 3 - Slide19.txt", "file_path": "Lecture 3\\Texts\\Slide19.txt", "content": "Now consider homogeneity, or the scaling property. If a system produces output K1 for input F1, then scaling the input‚Äîmultiplying it by some factor‚Äîshould scale the output by the same factor. For example, if we double the brightness of an image input, the output image should also be twice as bright.\nThese two properties‚Äîadditivity and homogeneity‚Äîare at the heart of linear systems. Understanding them clearly is essential, because they allow us to predict how systems behave, simplify complex problems, and build reliable imaging technologies.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 3 - Slide20.txt", "file_path": "Lecture 3\\Texts\\Slide20.txt", "content": "Let‚Äôs test these ideas with a few examples. Suppose we have a system that maps positive numbers, x, to the square root of x. Is this system linear? Well, while the output is zero when the input is zero, that alone doesn‚Äôt make a system linear. Remember, true linearity requires both additivity and homogeneity. The square root function doesn‚Äôt satisfy either‚Äîif you take two inputs and sum their square roots, that doesn‚Äôt equal the square root of the sum, and scaling the input doesn‚Äôt scale the output proportionally.\nOn the other hand, consider a linear transformation in three-dimensional space, like multiplying a vector by a 3√ó3 matrix. This kind of operation satisfies both additivity and homogeneity, making it a linear system. Similarly, mathematical operations like integration and differentiation also qualify as linear operators. They obey the superposition principle, which combines both key properties of linearity in a compact form.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 3 - Slide21.txt", "file_path": "Lecture 3\\Texts\\Slide21.txt", "content": "Over the years, some have argued that the definition of a linear system might be redundant‚Äîperhaps additivity alone implies homogeneity, or vice versa. This is an intriguing idea, and it reminds us that crafting precise definitions, like those in mathematics or geometry, is both an art and a science. Just as Euclid defined geometry with five foundational postulates, and mathematicians later debated whether all were independent, we can ask similar questions about system properties.\nIn the next few slides, we‚Äôll look at when additivity and homogeneity are truly independent and when they might be connected. This kind of deep thinking helps us build stronger foundations for engineering practice.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 3 - Slide22.txt", "file_path": "Lecture 3\\Texts\\Slide22.txt", "content": "For continuous functions, additivity and homogeneity can indeed be shown to be equivalent. For example, if a system satisfies additivity, you can use that property repeatedly to demonstrate homogeneity for integer scalars, rational scalars, and ultimately for all real scalars, thanks to continuity. Conversely, if a system satisfies homogeneity, you can reason your way back to additivity.\nBut this also points to a larger truth: when building formal systems‚Äîwhether in mathematics or engineering‚Äîwe must ensure they are logically consistent. This is no trivial task, and it‚Äôs part of what makes formal reasoning so challenging and rewarding.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 3 - Slide23.txt", "file_path": "Lecture 3\\Texts\\Slide23.txt", "content": "However, equivalence doesn‚Äôt always hold. Consider the complex conjugate operation. This system satisfies additivity: the conjugate of a sum equals the sum of the conjugates. But it fails homogeneity when the scalar is complex, because conjugating a scalar times a complex number doesn‚Äôt give the same result as scaling the conjugate by that scalar. This shows that additivity doesn‚Äôt always imply homogeneity‚Äîyou really do need to check both.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 3 - Slide24.txt", "file_path": "Lecture 3\\Texts\\Slide24.txt", "content": "Similarly, it‚Äôs possible to have homogeneity without additivity. Imagine a function that applies one slope if the input is rational and another slope if it‚Äôs irrational. Scaling works consistently because multiplying by a rational scalar preserves the input‚Äôs type (rational or irrational), so homogeneity holds. But if you add two irrational inputs that sum to a rational number, the slopes change‚Äîand additivity breaks down.\nThese examples remind us why linear systems require both properties. We can‚Äôt shortcut the definition by assuming one implies the other in all cases.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 3 - Slide25.txt", "file_path": "Lecture 3\\Texts\\Slide25.txt", "content": "Together, additivity and homogeneity form what we call the superposition principle. This principle allows us to break down complex inputs and predict how the system will respond by summing simpler outputs.\nLinear systems are easier to work with than nonlinear ones, and tools like Fourier analysis and convolution give us powerful ways to study them. Importantly, even nonlinear systems can often be approximated as a collection of piecewise linear systems‚Äîso mastering linear systems lays the groundwork for handling more complex situations.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 3 - Slide26.txt", "file_path": "Lecture 3\\Texts\\Slide26.txt", "content": "One subtle idea is relative linearity. A system‚Äôs behavior might depend on the chosen coordinate system or reference point. For example, a linear function with an intercept (like y = mx + b) is still linear in its relative changes. If you redefine the origin to subtract out b, it behaves like a linear function passing through zero.\nSimilarly, for systems, we often focus on relative changes in input and output rather than absolute values. This helps us apply linear system theory even when the system‚Äôs baseline isn‚Äôt perfectly aligned with the origin.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 3 - Slide27.txt", "file_path": "Lecture 3\\Texts\\Slide27.txt", "content": "Let‚Äôs consider classic electrical components. A resistor follows Ohm‚Äôs law: V = IR. This is a linear relationship‚Äîdouble the current, and the voltage doubles. Whether we write V = IR or I = V/R, the linearity remains clear. This is a simple, familiar example of a linear system.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 3 - Slide28.txt", "file_path": "Lecture 3\\Texts\\Slide28.txt", "content": "A capacitor introduces a bit more complexity. Its voltage depends on the integral of the current over time, plus an initial condition. If we focus on the relative change‚Äîthe change from that initial state‚Äîthe system behaves linearly. This shows how initial conditions affect our view of linearity, and why considering relative changes can be so useful.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 3 - Slide29.txt", "file_path": "Lecture 3\\Texts\\Slide29.txt", "content": "An inductor works similarly, but with current depending on the integral of voltage over time, or voltage depending on the derivative of current. Again, if we account for initial conditions, we can apply linear system theory to analyze these components effectively.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 30, "slide_filename": "Slide30.txt", "slide_annotation": "Lecture 3 - Slide30.txt", "file_path": "Lecture 3\\Texts\\Slide30.txt", "content": "In medical imaging and other engineering applications, we often focus not just on linear systems, but on shift-invariant linear systems. This means that if the input shifts‚Äîsay, you move an object from one location to another‚Äîthe output shifts in the same way.\nFor example, if I take your picture in this room, then take it again in my office, the image of you should look the same. The system‚Äôs response stays consistent despite the shift in location. This stability, or shift invariance, is a key property of many imaging systems.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 31, "slide_filename": "Slide31.txt", "slide_annotation": "Lecture 3 - Slide31.txt", "file_path": "Lecture 3\\Texts\\Slide31.txt", "content": "Think of ripples in a pond. If you drop a stone at one point, the ripples spread out. If you drop it at another point, the same pattern emerges‚Äîjust shifted in space. This is an example of spatial invariance in a system.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 32, "slide_filename": "Slide32.txt", "slide_annotation": "Lecture 3 - Slide32.txt", "file_path": "Lecture 3\\Texts\\Slide32.txt", "content": "Similarly, a music player is temporally invariant. Whether you play your favorite song today or next week, it sounds the same. This consistency over time reflects shift invariance with respect to the time domain.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 33, "slide_filename": "Slide33.txt", "slide_annotation": "Lecture 3 - Slide33.txt", "file_path": "Lecture 3\\Texts\\Slide33.txt", "content": "Mathematically, shift invariance means that if the input shifts by some amount‚Äîsay a‚Äîthe output shifts by the same amount. The system‚Äôs operation stays consistent, no matter where or when the input occurs.\nShift invariance, combined with linearity, gives us powerful tools for analyzing systems. Together, they allow us to model and predict system behavior with confidence, whether in medical imaging, signal processing, or other fields of engineering.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 34, "slide_filename": "Slide34.txt", "slide_annotation": "Lecture 3 - Slide34.txt", "file_path": "Lecture 3\\Texts\\Slide34.txt", "content": "Let‚Äôs finish this part of our discussion by touching briefly on nonlinear systems. This is a topic marked with a green button‚Äîso it‚Äôs something nice to know, not required at the core level of this course. Nonlinear science is a relatively new and complex area. Unlike linear systems, nonlinear equations are often difficult, or even impossible, to solve exactly. But thanks to modern computing, we can discretize these equations and compute solutions numerically‚Äîeven if we have to do so through brute force.\nBeyond practical applications, nonlinear systems are fascinating because they produce surprising, often counterintuitive phenomena. And anything that defies our intuition naturally grabs our interest.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 35, "slide_filename": "Slide35.txt", "slide_annotation": "Lecture 3 - Slide35.txt", "file_path": "Lecture 3\\Texts\\Slide35.txt", "content": "One famous example of a nonlinear system is the logistic map, a simple but powerful mathematical model. It describes how a population‚Äîsay, of rabbits‚Äîevolves over time. The equation looks straightforward: next year‚Äôs population depends on this year‚Äôs population and a growth rate factor, r. But because of the nonlinear term involving the population itself, the system‚Äôs behavior changes dramatically depending on the value of r.\nWhen the population is small, growth is nearly linear. But as the population approaches the environment‚Äôs limits, growth slows‚Äîresources become scarce, and the population stabilizes or even declines. This simple model reflects how nature regulates itself.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 36, "slide_filename": "Slide36.txt", "slide_annotation": "Lecture 3 - Slide36.txt", "file_path": "Lecture 3\\Texts\\Slide36.txt", "content": "What‚Äôs remarkable about the logistic map is how its behavior changes as the growth rate increases. At low rates, the population settles to a single stable value. As the rate rises, the population starts oscillating between two values, then four, then many‚Äîand eventually behaves unpredictably, in what‚Äôs called chaotic behavior.\nThis is chaos in a mathematical sense: a deterministic system, governed by precise rules, but with outputs that seem random because they‚Äôre so sensitive to initial conditions. This is why long-term predictions, like weather forecasts, become unreliable‚Äîsmall differences in starting conditions can lead to dramatically different outcomes. This idea ties back to quantum mechanics, where we see that at a fundamental level, nature behaves probabilistically.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 37, "slide_filename": "Slide37.txt", "slide_annotation": "Lecture 3 - Slide37.txt", "file_path": "Lecture 3\\Texts\\Slide37.txt", "content": "Another example of a nonlinear system is the biological neuron. A neuron accumulates inputs over time‚Äîsmall signals have no effect, but once inputs surpass a certain threshold, the neuron fires, sending an electrical impulse. This threshold mechanism helps the nervous system filter out noise and only respond to meaningful stimuli.\nWe can model this mathematically as an artificial neuron, where inputs are combined through a weighted sum (the linear part), and this sum is passed through a nonlinear function that decides whether or not to fire. This is the basic building block of artificial neural networks.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 38, "slide_filename": "Slide38.txt", "slide_annotation": "Lecture 3 - Slide38.txt", "file_path": "Lecture 3\\Texts\\Slide38.txt", "content": "When we connect these artificial neurons in layers, we create a neural network. With many layers, we have what‚Äôs called a deep neural network‚Äîthe foundation of modern deep learning.\nIn these networks, the lower layers detect simple features, like edges or corners, while higher layers learn more abstract patterns, like faces or traffic signs. Deep neural networks now drive advances in fields from facial recognition to self-driving cars. This is a powerful example of combining linear and nonlinear elements to solve complex problems.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 39, "slide_filename": "Slide39.txt", "slide_annotation": "Lecture 3 - Slide39.txt", "file_path": "Lecture 3\\Texts\\Slide39.txt", "content": "As we wrap up, let me remind you that alongside the exams and homework, those of you who are particularly motivated are welcome to pursue a class project. I recommend looking into machine learning‚Äîit‚Äôs an exciting and fast-growing area. If you‚Äôre interested, you can read my perspective article on this topic, and we can discuss potential directions for your project.", "total_slides_in_lecture": 40}
{"lecture": "Lecture 3", "slide_number": 40, "slide_filename": "Slide40.txt", "slide_annotation": "Lecture 3 - Slide40.txt", "file_path": "Lecture 3\\Texts\\Slide40.txt", "content": "If a project doesn‚Äôt interest you, that‚Äôs perfectly fine‚Äîyou can focus on the regular course structure of lectures, exams, and homework. Speaking of which, I‚Äôll upload today‚Äôs homework assignment for you to download. Please submit your solutions by midnight next Tuesday. After the deadline, we‚Äôll post the answers for your review.\nThank you for your attention today!", "total_slides_in_lecture": 40}
{"lecture": "Lecture 4", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 4 - Slide1.txt", "file_path": "Lecture 4\\Texts\\Slide1.txt", "content": "Today, we move on to our fourth lecture, where our focus will be on convolution.\n\nThis is a direct continuation of our recent discussion on systems, especially linear systems that are shift-invariant ‚Äî two foundational concepts we began unpacking in the previous lecture. Understanding convolution will allow us to model how these systems respond to different types of input, and it's a concept that shows up across many areas in engineering, physics, and especially medical imaging.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 4 - Slide2.txt", "file_path": "Lecture 4\\Texts\\Slide2.txt", "content": "We are right on schedule in our journey through this material.\n\nIf you‚Äôve already gone through the reading materials for today, that‚Äôs excellent ‚Äî it will help you link the concepts more quickly. If you haven‚Äôt, that‚Äôs okay. Just stay engaged during the lecture and review the key points afterward. Building this habit of revisiting ideas will strengthen your understanding, especially as we move into more math-heavy topics", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 4 - Slide3.txt", "file_path": "Lecture 4\\Texts\\Slide3.txt", "content": "As we step into the topic of convolution, let‚Äôs outline what we‚Äôll be exploring today.\nWe‚Äôll begin by revisiting shift-invariant linear systems, grounding ourselves in a concept we touched on earlier. Then, we‚Äôll move into two important formulations of convolution: continuous convolution and discrete convolution.\n\nEach of them plays a crucial role depending on the context ‚Äî continuous for signals that evolve smoothly over time or space, and discrete for digital signals and images. Through this lecture, we‚Äôll examine both perspectives and their practical implications.\nBy the end of the course, you‚Äôll not only understand how convolution works ‚Äî you'll also appreciate why it matters in system analysis, image reconstruction, and the broader field of medical imaging.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 4 - Slide4.txt", "file_path": "Lecture 4\\Texts\\Slide4.txt", "content": "Let‚Äôs refresh our picture of a linear, shift-invariant system.\nImagine dropping a pebble into a still pond. No matter where the pebble lands, the ripples that form spread out in the same circular pattern. This uniformity ‚Äî where the system‚Äôs response doesn‚Äôt change based on where the input occurs ‚Äî is what we call shift-invariance.\n\nNow, relate this to medical imaging. Suppose you undergo a scan at one hospital today and another hospital next week. If your physiological condition hasn‚Äôt changed, you‚Äôd expect the scans to look the same. That‚Äôs shift-invariance in action ‚Äî it ensures reliability across time and space.\n\nNext, we have linearity. It has two key components:\nAdditivity ‚Äî if two features exist, like two separate tumors, the system should show the combination of both clearly.\nHomogeneity ‚Äî if a feature changes in intensity, like a tumor growing larger, the imaging should reflect that change proportionally.\n\nTogether, these properties form what‚Äôs known as the superposition principle ‚Äî the foundation for much of modern system analysis, including medical imaging.\nSo, as we move ahead, keep the ripple analogy in mind. It's a simple yet powerful way to visualize how linear, shift-invariant systems behave ‚Äî consistently, predictably, and truthfully.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 4 - Slide5.txt", "file_path": "Lecture 4\\Texts\\Slide5.txt", "content": "To understand convolution deeply, let‚Äôs frame it through a familiar story from physics.\nYou might recall the idea ‚Äî sometimes attributed to Newton ‚Äî that the universe began with a single impulse, a sort of divine kick that set everything into motion. Whether metaphor or mechanics, this idea captures something fundamental.\nIn physics, we often model this idea of an initial force as an impulse ‚Äî a very short burst of energy that sets a system into motion. For instance, think of hitting a golf ball with a club. That brief push is an impulse, and the ball's subsequent motion is the system‚Äôs response.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 4 - Slide6.txt", "file_path": "Lecture 4\\Texts\\Slide6.txt", "content": "Let‚Äôs take a closer look at what that impulse does.\n\nWhen you strike a stationary object, like a golf ball, the force you apply doesn‚Äôt last long ‚Äî it‚Äôs concentrated over a short time. But that momentary push has a lasting effect: it changes the ball‚Äôs velocity and sets it into motion.\nThis simple interaction captures an essential principle: an impulse causes change.\n\nIn the context of system analysis, we‚Äôre interested in how a system responds over time after receiving an impulse. That response ‚Äî often called the impulse response ‚Äî tells us how the system behaves dynamically. Once we know the impulse response, we can use convolution to determine the system‚Äôs output for any more complex input signal.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 4 - Slide7.txt", "file_path": "Lecture 4\\Texts\\Slide7.txt", "content": "Let‚Äôs look at how impulse relates to momentum ‚Äî a key concept in both physics and system analysis.\nMomentum, as you may recall, is simply mass multiplied by velocity. \n\nSo if an object is at rest, it has zero momentum. When you apply a force over a period of time ‚Äî say, when you hit a tennis ball with a racket ‚Äî the object gains velocity. That change in velocity reflects a change in momentum.\n\nNow here‚Äôs the essential link:\u000bImpulse is defined as force multiplied by the time over which it acts.\n\u000bWe start with Newton‚Äôs second law, which tells us:\u000bForce equals mass times acceleration ‚Äî or simply,\u000bF equals m a.\n\nNow, acceleration is defined as the change in velocity divided by the change in time ‚Äî so we write:\u000ba equals delta v over delta t.\n\nIf we substitute that into Newton‚Äôs law, we get:\u000bF times delta t equals m times delta v.\n\nThis equation shows us something powerful:\u000bImpulse ‚Äî that‚Äôs force times time ‚Äî equals the change in momentum.\ntells us that impulse causes the change in momentum.\n\n\n It‚Äôs the way to model how mechanical systems respond to external inputs, and the idea applies to other input-output relationships as well.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 4 - Slide8.txt", "file_path": "Lecture 4\\Texts\\Slide8.txt", "content": "Building on what we just discussed, here we observe something quite elegant:\nThe shape of the force curve ‚Äî whether it‚Äôs a tall narrow spike or a wide shallow push ‚Äî doesn‚Äôt actually matter when it comes to total impulse. What truly matters is the area under the force-time curve.\n\nThis area represents the cumulative effect of the force. So, a small force applied over a long duration can have the same impulse as a large force applied for a short time.\n\nIn system terms, different kinds of inputs can result in the same response if their total impact ‚Äî the area ‚Äî is identical.\nThis idea is central to convolution: we‚Äôre not just tracking individual values, but the accumulated effect of an input across time.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 4 - Slide9.txt", "file_path": "Lecture 4\\Texts\\Slide9.txt", "content": "Now let‚Äôs extend that idea.\nImpulse isn‚Äôt limited to rectangular shapes. We can see in this graph that different Gaussian curves ‚Äî whether wide or narrow, flat or sharp ‚Äî can all serve as impulse-like inputs.\n\nWhat matters is the area under each curve. This area represents the total impulse delivered by the force, regardless of how the function looks. As long as that area remains the same, the impact on the system is the same.\nThis insight is critical in both physics and engineering: we often abstract away the exact shape of a signal and focus instead on its integrated effect ‚Äî the impulse.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 4 - Slide10.txt", "file_path": "Lecture 4\\Texts\\Slide10.txt", "content": "This brings us to an important mathematical idea ‚Äî the limiting process, which gives us a way to model an ideal impulse.\nLet‚Äôs define a simple function ‚Äî we‚Äôll call it d tau of t.\nIt‚Äôs equal to one divided by two tau when t is between minus tau and tau.\n\u000bAnd it‚Äôs equal to zero everywhere else.\nSo again:\nd tau of t equals one over two tau, if t is greater than minus tau and less than tau.\u000bOtherwise, it‚Äôs zero.\nNow, as tau gets smaller and smaller, this function becomes taller and narrower, but the area under the curve remains exactly one.\n\nThat‚Äôs the key. As we shrink the width to zero ‚Äî and the height grows without bound ‚Äî we approach the Dirac delta function.\nMathematically, we say:\nThe limit of d tau of t, as tau approaches zero, is zero everywhere except at t equals zero.\u000bAnd at the same time,\u000bThe limit of the area under the curve ‚Äî capital I of tau ‚Äî is equal to one.\nIn other words, the delta function is infinitely narrow, infinitely tall, and yet has a finite area of one.\n\nThis makes it a perfect tool for modeling ideal impulses, especially when we need to represent quick, concentrated energy or a sharp event in time.\nAnd this idea will become central as we move into our discussion of convolution, where impulse responses help us understand how systems behave in response to various inputs.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 4 - Slide11.txt", "file_path": "Lecture 4\\Texts\\Slide11.txt", "content": "Let‚Äôs take a moment to reflect on why the limiting process is essential when defining an impulse.\nYou might wonder ‚Äî if we already know an impulse gives us the desired effect, like setting a golf ball in motion, why go through the trouble of idealizing it mathematically? The reason lies in precision. By applying the limiting process, we create a perfect, universal model of an impulse, one that‚Äôs as rigorous as the definition of a derivative in calculus.\nThink of how we define a derivative: we take a small change, and by letting that change approach zero, we achieve an exact result. We‚Äôre doing something similar here with the Dirac delta function.\n\nNow, regarding the shape of the delta function ‚Äî does it matter? In fact, no. What matters is the area under the curve, not how the curve looks. It‚Äôs like receiving payment: whether it comes in one sum or in smaller installments, the total value is what counts.\n\nInterestingly, if you start worrying about the exact shape without measuring it, you‚Äôre in uncertain territory. Without measurement, all shapes are possible ‚Äî a bit like the probabilistic world of quantum mechanics! Only when we observe or measure do we know what‚Äôs truly there. This connection offers a fascinating lens on how we idealize and model impulses in system theory.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 4 - Slide12.txt", "file_path": "Lecture 4\\Texts\\Slide12.txt", "content": "Let‚Äôs take a deeper look at the Dirac delta function ‚Äî one of the most powerful concepts in system analysis.\nThe delta function is what we call a generalized function, or in mathematical terms, a distribution. It helps us model a perfect impulse ‚Äî something that delivers a concentrated effect at a single moment in time.\n\nNow, here‚Äôs how we describe it mathematically:\nWe say that delta of t minus t naught equals zero for all values of t not equal to t naught.\u000bAnd, when we integrate this delta function from minus infinity to infinity, the result is one.\nWhat does that mean?\nIt means the function is zero everywhere ‚Äî except at one specific point in time, which we call t naught. At that single point, it has an infinitely sharp spike, but the area under the spike is exactly one. That area is what we call the impulse.\nSo even though we can‚Äôt see or draw the spike perfectly, we know that its total effect ‚Äî the impulse ‚Äî is one. That‚Äôs what matters. The effect is important, but the exact shape doesn‚Äôt matter.\n\nNow, let‚Äôs shift to the discrete version of the delta function ‚Äî something we use in digital systems.\nWe define delta of n like this:\nIt equals one when n is zero,\u000band it equals zero for all other values of n.\n\nIn other words, this function is zero everywhere ‚Äî except at n equals zero, where it has a value of one.\u000bIt‚Äôs a single pulse ‚Äî like a quick tap ‚Äî at a specific point in time.\n\nWe can also shift this pulse. For example, delta of n minus two just moves the pulse to position n equals two.\n\nThis simple pulse is incredibly useful. It forms the building block for discrete-time convolution, which we‚Äôll talk about next.\nSo both the continuous and discrete versions of the delta function help us analyze how systems respond to input ‚Äî whether we‚Äôre dealing with real-world signals or digital data.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 4 - Slide13.txt", "file_path": "Lecture 4\\Texts\\Slide13.txt", "content": "Before we explore how the delta function models sampling, let‚Äôs take a quick look at the integral mean value theorem ‚Äî a fundamental concept from calculus.\nImagine we have a continuous function plotted over the interval from a to b. The area under the curve ‚Äî shaded in green on the slide ‚Äî is found by integrating the function from a to b.\n\nNow, here‚Äôs what the mean value theorem says:\u000bSomewhere between a and b, there‚Äôs at least one point, which we‚Äôll call c, where the height of the function at that point ‚Äî f of c ‚Äî multiplied by the width of the interval, gives you the same area as the one under the curve.\nIt‚Äôs like finding a rectangle with the same base and the same area ‚Äî it represents the \"average\" value of the function across the interval.\n\nThis concept of \"average value over a short interval\" will help us understand how the delta function works when it comes to sampling a continuous function.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 4 - Slide14.txt", "file_path": "Lecture 4\\Texts\\Slide14.txt", "content": "Let‚Äôs now connect this idea directly to the delta function and its role in sampling.\nImagine you have a continuous function ‚Äî let‚Äôs call it f of t. Now, if you multiply this function by a shifted delta function, specifically delta of t minus t naught, and then integrate, something powerful happens:\n\u000bYou end up extracting the value of f at t naught. In other words, the delta function acts like a perfect sampling tool.\nMathematically, this is shown through a limiting process. As the width of the delta function shrinks toward zero, its height increases ‚Äî but the total area remains one. That‚Äôs key.\nInside this narrow window around t naught, the function f is essentially flat ‚Äî and using the mean value theorem, we know there must be some point where the function's value equals the average over that interval. As the window becomes infinitesimally small, that point converges exactly to t naught.\nSo what do we get?\n\nThe integral from minus infinity to infinity of delta of t minus t naught, times f of t, d t ‚Äî equals f of t naught.\nThis is called the sampling property of the delta function. It‚Äôs as if the delta function \"reaches into\" the function and pulls out the value at a single point.\nEven more fascinating ‚Äî we can actually rebuild any continuous function using this idea. By stacking many delta functions across time, each weighted by the function‚Äôs value at that point, we can reconstruct the entire function. Think of it like slicing the function into tiny pieces ‚Äî each slice tells us something, and when you add them all together, you get the full picture again.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 4 - Slide15.txt", "file_path": "Lecture 4\\Texts\\Slide15.txt", "content": "Let‚Äôs now think about how this idea translates to the discrete case.\nYou can visualize a discrete function as a collection of rectangular bars ‚Äî sometimes called gate functions. These bars represent values at specific, evenly spaced points ‚Äî like sampling a continuous function at fixed intervals.\nHere you can see the mathematical expression in this slide.\n\nThis function is defined piecewise as follows:\nIt equals zero when the absolute value of t is greater than one-half,\nIt equals one-half when the absolute value of t is exactly one-half,\nAnd it equals one when the absolute value of t is less than one-half.\nIn other words, the rectangular function is centered at zero, has a total width of one, and represents a simple, finite pulse ‚Äî like a short ‚Äúon‚Äù signal.\n\nAt the bottom of the slide, we define the discrete delta function, written as delta of n.\nIt equals one when n is zero, and zero otherwise.\nSo delta of n is a single spike located at n equals zero.\u000bIf we shift it, like delta of n minus two, the spike moves to n equals two.\n\nThis function serves as the discrete version of the continuous delta function. By shifting and scaling these unit impulses, we can represent any discrete signal as a sum of these basic components ‚Äî just like reconstructing a picture with pixels.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 4 - Slide16.txt", "file_path": "Lecture 4\\Texts\\Slide16.txt", "content": "Here, you can see the idea visually.\nImagine the original continuous function shown in gray.\u000bWe can approximate it piecewise using constant slices ‚Äî small rectangles, each with a height that matches the continuous function‚Äôs value at that point.\nEach rectangle corresponds to a delta function, or a rectangular bar, scaled to match the original function‚Äôs height.\n\nThe key point is: the area under each rectangle is equivalent to the area under the corresponding delta function at that location.\nIn this way, everything ties together between the continuous and discrete perspectives.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 4 - Slide17.txt", "file_path": "Lecture 4\\Texts\\Slide17.txt", "content": "In fact, you can use discrete delta functions to represent any arbitrary discrete function.\nEach value of the function can be represented as a scaled delta function at that point.\nWhen we sum these scaled delta functions, we reconstruct the original discrete function.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 4 - Slide18.txt", "file_path": "Lecture 4\\Texts\\Slide18.txt", "content": "Now that we‚Äôve developed an understanding of the delta function and its role in modeling discrete events, let‚Äôs take this idea a step further and see how we can use it to represent an entire discrete function.\n\nHere we observe a powerful mathematical concept: any discrete function ‚Äî let‚Äôs call it g of n ‚Äî can be expressed as a weighted sum of shifted delta functions. That is, we take the delta function, shift it to the appropriate position, and scale it by the function‚Äôs value at that point. When we add all these together, we recover the original function.\nHere you can see the mathematical expression in this slide.\n\nThis formula tells us that each sample of g contributes one impulse, located at n equals k, and scaled by g at that k. When we add up all these scaled and shifted impulses, we rebuild the function g.\n\nLet‚Äôs look at a specific example to make this concrete.\n\nSuppose your discrete function consists of just a few nonzero points. The first point is at n equals 0 with value 1 ‚Äî that‚Äôs just delta of n.\n\u000bThe second point is at n equals 1 with a value of negative 1 ‚Äî so we write that as minus delta of n minus 1.\n\u000bAnd the third point is at n equals 2 with a value of 1.5 ‚Äî which becomes 1.5 times delta of n minus 2.\n\nBy adding these three impulses together, we effectively reconstruct the original signal, point by point.\n\nThis approach is not only elegant, it‚Äôs also foundational in signal processing and linear systems. It allows us to treat any discrete-time signal as a sum of building blocks ‚Äî scaled and shifted impulses ‚Äî making analysis and filtering much easier later on.\nWe‚Äôll continue building on this idea when we introduce convolution, which is just the response of a system to this kind of delta-based representation.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 4 - Slide19.txt", "file_path": "Lecture 4\\Texts\\Slide19.txt", "content": "So far, we‚Äôve explored both the continuous and discrete versions of the delta function.\nBoth approaches give us ways to express functions as combinations of basic building blocks.\nWhether a system or phenomenon is fundamentally continuous or discrete isn‚Äôt always clear ‚Äî think of water.\nWhen you see a flowing stream, it seems continuous. But if you zoom in, you‚Äôll find discrete molecules.\u000bAnd if you go even deeper, those molecules follow the rules of quantum mechanics, where a continuous probability wave describes behavior.\n\nIn engineering, we switch between these views depending on what‚Äôs useful.\u000bA summation represents the discrete case. An integral represents the continuous case.\u000bThey are really two sides of the same coin.\nWith this foundation, we‚Äôre ready to move forward ‚Äî\u000band explore how delta functions connect directly to shift-invariant systems,\u000bwhich play a key role in imaging and signal processing.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 4 - Slide20.txt", "file_path": "Lecture 4\\Texts\\Slide20.txt", "content": "Let‚Äôs take a look at this graph.\nThis impulse response, shown here as h of n, is our key piece of knowledge.\nI‚Äôm not asking you to start from nothing.\u000bI‚Äôm giving you the system‚Äôs reaction to a standardized input ‚Äî the delta function applied at time zero.\nOnce you have that, you‚Äôre ready.\nBecause if I give the system a more general input ‚Äî not just a single impulse, but any arbitrary shape ‚Äî\u000byou can still figure out the output.\nSo the question becomes:\u000bHow do you compute the output from this known impulse response and a general input?\nThat‚Äôs exactly where convolution comes in.\nConvolution is the operation that combines the input signal with the impulse response,\u000bto produce the final output.\nIt‚Äôs absolutely essential in imaging and signal processing.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 4 - Slide21.txt", "file_path": "Lecture 4\\Texts\\Slide21.txt", "content": "In a shift-invariant system, if you know the system‚Äôs response to an impulse at one location, you can predict its response at any other location or time.\nThat‚Äôs because shift-invariance means the system behaves the same way, no matter where or when you apply the impulse.\nIf you apply an impulse at a different time or position, you can infer the output by simply shifting the known response accordingly.\nAnd because the system is linear, you have additivity and homogeneity.\nIf you scale an input, the output will be scaled in the same way.\nIf you add multiple inputs, the output will be the sum of individual outputs due to each input.\nThis is exactly why knowing the impulse response of a system tells you everything about how that system behaves ‚Äî no matter what input you give it.\n\nTo summarize, for a shift-invariant linear system, given any input, we want to compute the output.\u000bThe critical information we rely on is the system‚Äôs impulse response.\u000bIf we know how the system reacts to a delta function at time zero,\u000bwe can use that as a building block to find the output for any input.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 4 - Slide22.txt", "file_path": "Lecture 4\\Texts\\Slide22.txt", "content": "To be specific, now let‚Äôs see how we actually compute the output y of n of a shift-invariant linear system to an arbitrary input x of n, formulated on the 1st line.\nSuppose I give you a system characterized by its impulse response, written as h of n, which is shown on the 2nd line.\u000bThat means: if you feed the system a discrete delta function as input, the output will be h of n.\u000bThis response ‚Äî h of n ‚Äî is the system‚Äôs character.\nBut what if you give the system a more general input, say x of n?\u000bWhat will the output y of n be, which is what I just formulated on the 1st line, and a very important question.\nLet‚Äôs break it down step by step.\nSince the system is shift-invariant, if you shift the input by k, the output also shifts by k, shown on the 3rd line.\u000bSo if the input is delta of n minus k, the output becomes h of n minus k.\nThen, because the system is linear, if we scale the input by x of k,\u000bConsequently, the output is also scaled by x of k.\u000bThat gives us x of k times h of n minus k on the 4th line.\nNow, to get the total output, we add up all of these scaled and shifted responses.\nMathematically, this gives us the convolution sum, on the right-hand side of the last two lines:\ny of n equals the sum over k from minus infinity to infinity of x of k times h of n minus k.\nThis is how the convolution is defined, and also why we need this concept.\n\u000bIt shows why the impulse response is so powerful ‚Äî it‚Äôs all you need to compute the system‚Äôs output for any input.\nNow, convolution might seem a bit tricky at first, especially because of the flipping and shifting involved ‚Äî\u000bthat negative k inside h of n minus k could appear confusing.\n\u000bBut if you follow the logic step-by-step, the structure becomes clear.\nThis convolution concept is fundamental in linear systems and engineering ‚Äî\u000band that‚Äôs why we dedicate so much attention to understanding convolution.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 4 - Slide23.txt", "file_path": "Lecture 4\\Texts\\Slide23.txt", "content": "Now let‚Äôs apply the same idea in the continuous case.\nAgain, you‚Äôre given the system‚Äôs response to a delta function, which we call h of t.\nIf we shift the delta to t minus tau, the output becomes h of t minus tau.\u000bIn other words, shifting the input shifts the output in the same way ‚Äî that‚Äôs shift-invariance.\nNext, we scale the input by x of tau.\u000bBecause the system is linear, the output also gets scaled ‚Äî giving us x of tau times h of t minus tau.\nFinally, to build the full output for a general input signal,\u000bwe integrate over all time ‚Äî summing these scaled and shifted responses.\nThis gives us the continuous-time convolution integral:\n\nYou can think of this as collecting all the weighted impulse responses across time,\u000band combining them to form the system‚Äôs output, where we have utilized the sampling property of the delta function.\nThis is how linear, shift-invariant systems behave in continuous time ‚Äî\u000band this operation, convolution, lies at the heart of modeling and understanding them.\nFinally, we have assumed that all involved integrals or summations on the previous slide converge, so mathematically well defined.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 4 - Slide24.txt", "file_path": "Lecture 4\\Texts\\Slide24.txt", "content": "So whether we‚Äôre dealing with discrete or continuous systems,\u000bConvolution is the tool we use to compute the output.\nIn both cases, the idea is the same:\n\nFirst, express the input as a collection of impulses.\u000bThen, for each impulse, you already know the output ‚Äî that's the impulse response.\u000bFinally, sum or integrate all of those individual responses to get the final output.\nThis process is captured using these two key equations:\nWe can see here discrete and continuous mathematical formulations.\u000b\nBut I know from experience that convolution can feel confusing at first.\u000bThat‚Äôs why it really helps to look at a concrete example.\nLet‚Äôs move on to one now.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 4 - Slide25.txt", "file_path": "Lecture 4\\Texts\\Slide25.txt", "content": "Let‚Äôs consider a hands-on example using two discrete functions, each with five data points.\nNow imagine holding out both hands:\u000byour left hand represents one function ‚Äî let‚Äôs call it x of n,\u000band your right hand represents the other ‚Äî this will be h of n, the impulse response.\nBecause of that negative k in the convolution sum,\u000bone function needs to be flipped ‚Äî think of it like flipping your right hand so you see the back of it.\u000bThat‚Äôs how we visualize h of n minus k.\nNow we shift this flipped function across the input and calculate the output step-by-step.\n\nFor n equals zero, you align the flipped function with the origin,\u000bmultiply the overlapping points, then sum the products ‚Äî\u000bthis gives you the output y of zero.\n\nFor n equals one, shift the flipped function one step to the right,\u000bmultiply again, sum the result ‚Äî and you get y of one.\nYou keep shifting, multiplying, and summing for each value of n.\nYou‚Äôll notice that when the two functions fully overlap,\u000byou get the largest value in the convolution.\u000bAnd as they slide past each other, the overlap decreases,\u000bso the convolution value gradually falls back to zero.\n\nThis visual analogy with your hands is a powerful way to understand discrete convolution ‚Äî\u000bespecially how flipping, shifting, multiplying, and summing all come together.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 4 - Slide26.txt", "file_path": "Lecture 4\\Texts\\Slide26.txt", "content": "Let‚Äôs assign specific numbers to this example.\nSuppose on your left hand, your thumb has value 5, your index finger is 4,\u000bthen 3, 2, and 1 for the middle, ring, and little fingers.\nOn your right hand, the values go the other way ‚Äî your little finger is 1, ring finger is 2,\u000band it increases up to 5 at the thumb.\nNow, flip the right hand ‚Äî just like we flip h of n in the convolution process ‚Äî\u000bthen slide it across the left hand, computing the pairwise products at each shift,\u000band summing them to get each value of the output.\nThis is exactly what‚Äôs happening in the MATLAB code shown on the slide.\nWe define x as the vector [5, 4, 3, 2, 1]\u000band h as [1, 2, 3, 4, 5].\nThen we call y equals conv of x and h\u000band finally, we plot the result.\nThe plot shows a symmetric peak, just as we expect ‚Äî the center point reflects full overlap between the two sequences,\u000band the values decrease symmetrically as they slide past each other.\nAlso remember: when you convolve two sequences of lengths n 1 and n 2,\u000bthe result will have a length of n 1 plus n 1 minus one.\nPracticing with simple examples like this helps you visualize and internalize the convolution process ‚Äî\u000bflipping, shifting, multiplying, and adding.\nAnd in imaging systems, which are designed to be linear and shift-invariant,\u000bconvolution defines how the system responds to any input.\nThat‚Äôs why mastering this operation is essential.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 4 - Slide27.txt", "file_path": "Lecture 4\\Texts\\Slide27.txt", "content": "Let‚Äôs see another example ‚Äî this time from medical imaging.\nImagine you‚Äôre injecting a contrast agent, like iodine, into the bloodstream to highlight blood vessels in an X-ray image.\nWithout contrast, the blood, vessels, and surrounding tissue can look very similar.\u000bBut when you add contrast, the vessels light up more clearly, making them easier to detect and interpret.\nNow here‚Äôs the key part: as the iodine travels through the body, it moves along multiple paths.\nSome paths are fast ‚Äî like major arteries. Others are slower ‚Äî like capillaries or microvasculature.\u000bEach of these paths introduces a different delay before the contrast reaches the detector.\nIf you could inject an ideal, instantaneous impulse of contrast ‚Äî in other words, a perfect delta function ‚Äî\u000bthen what you would see is a collection of impulse responses, labeled here as H 1, H 2, H 3, and so on.\n\nEach of these responses corresponds to a different path, and each arrives at a different time.\nBut in reality, injections are not perfect impulses. They take time.\n\u000bThis means the actual contrast input is more spread out ‚Äî and the measured output at the gamma camera\u000bbecomes a sum of all these delayed responses, weighted by how much contrast passes through each path.\n\nIn other words, the system‚Äôs output is a convolution of the injected input with the system‚Äôs impulse response.\nThis illustrates exactly why convolution is so important in medical imaging:\u000bit helps us model and interpret how signals ‚Äî like contrast or radiation ‚Äî travel through complex biological systems,\u000band how we capture those signals with imaging devices like a gamma camera.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 4 - Slide28.txt", "file_path": "Lecture 4\\Texts\\Slide28.txt", "content": "In reality, the contrast isn‚Äôt delivered as a perfect impulse ‚Äî\u000bit‚Äôs injected over a period of time, with more at the start and less later, forming a time curve.\nWe can think of this non-ideal input as a sum of small impulses occurring at different times ‚Äî\u000beach one representing a fraction of the total injected dose.\nFor each of these small impulses, we already know the system‚Äôs response ‚Äî\u000bthat‚Äôs the impulse response we‚Äôve discussed.\nSo what do we do?\nWe scale each impulse response based on how much contrast was injected at that moment,\u000bshift it in time according to when it was injected,\u000band then add all the individual responses together.\nWhat we get is the system‚Äôs total response ‚Äî the final signal that‚Äôs measured by the detector.\nThis is a perfect visualization of discrete convolution in a real-world setting.\u000bIt shows how imaging systems handle non-ideal, time-varying inputs using the exact same principles we‚Äôve learned.\nIt may take some time for these ideas to fully sink in,\u000bbut they form the foundation for understanding how imaging systems process signals and generate useful images.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 4 - Slide29.txt", "file_path": "Lecture 4\\Texts\\Slide29.txt", "content": "Now, let‚Äôs work through a more abstract ‚Äî and slightly more mathematical ‚Äî example.\u000bI like to call this a minds-on example.\nThis example is based on continuous convolution, and while it‚Äôs not difficult, it does require you to think like in calculus.\nSo let‚Äôs take our time with it.\nWith convolution, the basic process is this:\u000bWe flip one function, shift it, multiply them point by point, and then integrate over the region where they overlap.\nAt first, that might sound a little mechanical. So let‚Äôs break it down clearly, step by step.\nWe‚Äôre working with two functions here:\nf of t, shown in red. This is a triangular function ‚Äî it starts at zero, rises to a peak at t equals 2, and then drops back to zero.\ng of t, shown in purple. This is a rectangular pulse that spans from t equals minus 2 to plus 2, with a constant value of 3.\nNow, because g of t is symmetric, flipping it across the vertical axis doesn‚Äôt change its shape ‚Äî\u000bwhich actually makes our job easier.\nSo we‚Äôll flip g to get g of t minus tau, and then slide it across f of tau,\u000bevaluating the convolution at each position t.\nHere‚Äôs the key part:\u000bBoth functions are compactly supported, meaning they‚Äôre only non-zero over a specific range.\u000bThat means the overlap only happens during certain intervals.\nSo to compute the convolution, we‚Äôll handle it case by case ‚Äî based on the value of t.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 30, "slide_filename": "Slide30.txt", "slide_annotation": "Lecture 4 - Slide30.txt", "file_path": "Lecture 4\\Texts\\Slide30.txt", "content": "üëâ Case 1: When t is less than ‚Äì2\nAt this stage, the flipped and shifted version of g, written as g of t minus tau,\u000bis positioned so far to the left that it does not overlap with f of tau at all.\nThat means the product of the two functions is zero everywhere,\u000bso the convolution result y of t is also zero.\nThis is our first insight:\u000bNo overlap means no contribution to the integral, and the output is zero.\n\n\n Case 2: When t is between ‚Äì2 and 0\nNow we begin to see partial overlap.\nAs g of t minus tau slides to the right, part of it starts to enter the region where f of tau is non-zero.\nLet‚Äôs break this down mathematically:\nOver the region where they overlap, f of tau is defined as\u000bf of tau equals negative tau plus 2, valid from tau equals 0 to tau equals 2.\nAnd the flipped rectangle, g of t minus tau, still has a constant height of 3 wherever it overlaps.\nSo we‚Äôre integrating the product of these two functions across the overlapping interval.\n\nThe simplifies convolution becomes:\u000b3 times the integral of (‚Äìtau + 2), from 0 to 2 plus t.\nIf you evaluate this integral, you get:\u000by of t equals negative three-halves t squared plus 6.\nSo the convolution result in this case is a parabolic curve, rising as t approaches zero.\n\nThis illustrates an important idea:\u000bConvolution is constructed from slices of overlapping areas ‚Äî\u000band in each case, we‚Äôre summing the product over those intervals of overlap.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 31, "slide_filename": "Slide31.txt", "slide_annotation": "Lecture 4 - Slide31.txt", "file_path": "Lecture 4\\Texts\\Slide31.txt", "content": "Case 3: When t is between 0 and 2\nIn this case, the flipped and shifted version of g, that is, g of t minus tau,\u000bcompletely overlaps the triangular function f of tau.\nThis means we integrate across the entire support of f of tau,\u000bfrom tau equals 0 to 2.\nWithin this interval:\nf of tau equals negative tau plus 2\ng of t minus tau is constant at 3\nSo the convolution becomes:\u000by of t equals the integral from 0 to 2 of 3 times (‚Äìtau + 2), d tau\nWhen we evaluate this, we get:\u000by of t equals 6\nSo, for t between 0 and 2, the convolution gives a flat region with a constant value of 6.\u000bThis happens because the area under the product is the same across the entire interval.\n\n Case 4: When t is between 2 and 4\nNow, f of tau begins to slide out of the rectangular window of g of t minus tau.\nWe‚Äôre back to a partial overlap, but this time on the opposite side ‚Äî\u000bthe right edge of g is moving past f.\nThe new overlapping interval runs from\u000btau equals t minus 2 to tau equals 2\nAgain, we use:\nf of tau equals negative tau plus 2\ng of t minus tau equals 3\nSo the convolution becomes:\u000by of t equals the integral from t minus 2 to 2 of 3 times (‚Äìtau + 2), d tau\nEvaluating this gives:\u000by of t equals three-halves t squared minus twelve t plus 24\nThis result traces out the descending part of the convolution curve,\u000bas the overlap shrinks toward zero.\n\n Case 5: When t is greater than or equal to 4\nAt this point, g of t minus tau has completely moved past f of tau.\nThere is no overlap between the two functions,\u000bso the convolution result once again becomes:\u000by of t equals 0", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 32, "slide_filename": "Slide32.txt", "slide_annotation": "Lecture 4 - Slide32.txt", "file_path": "Lecture 4\\Texts\\Slide32.txt", "content": "Now, let‚Äôs bring all the cases together to write the full piecewise definition of the convolution result.\nWe‚Äôve carefully stepped through five distinct intervals,\u000beach showing how the amount of overlap between f of t and g of t determines the value of the convolution.\nPutting everything together, we now have the complete solution.\n\nEach case ‚Äî from full overlap, to partial overlap, to no overlap ‚Äî\u000bcontributes a segment of the final output.\n\nThis leads to a piecewise-defined function, expressed as follows:\ny of t equals 0,‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉfor t less than ‚Äì2\ny of t equals ‚Äìthree-halves t squared plus 6,‚ÄÉfor t between ‚Äì2 and 0\ny of t equals 6,‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉfor t between 0 and 2\ny of t equals three-halves t squared minus 12 t plus 24,‚ÄÉfor t between 2 and 4\ny of t equals 0,‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉfor t greater than or equal to 4\nWhen we plot these expressions, we get a smooth, continuous curve ‚Äî\u000bit rises, flattens out in the middle, and then gradually falls again.\u000bIt forms a kind of bump, which is exactly what we expect\u000bas g of t slides over and past f of t.\n\nThis example gives us a complete, mathematical walkthrough of continuous convolution ‚Äî\u000bfrom defining the problem, to analyzing it case by case, to combining all pieces into the final solution.\nIt beautifully demonstrates how convolution is built:\u000bby accumulating the area under the product of two functions ‚Äî\u000beach one flipped, shifted, multiplied, and then integrated, step by step.\nIt‚Äôs a powerful blend of geometry and calculus,\u000band it‚Äôs absolutely foundational in signal processing and medical imaging.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 33, "slide_filename": "Slide33.txt", "slide_annotation": "Lecture 4 - Slide33.txt", "file_path": "Lecture 4\\Texts\\Slide33.txt", "content": "Now, let‚Äôs look at a practical example that ties all of this together ‚Äî the RC circuit.\nIf you‚Äôre not familiar with electrical circuits, don‚Äôt worry.\u000bFor our purposes, you can think of this setup as a black box.\u000bThe input is a signal, and the output depends on the system‚Äôs behavior ‚Äî specifically, its impulse response.\nIn this circuit, we have a resistor R and a capacitor C connected in series.\u000bThe voltage signal going in is x of t, and the voltage across the capacitor is the output y of t.\nNow here‚Äôs the key point:\u000bIf we apply a delta function as the input ‚Äî meaning a sharp, instantaneous impulse ‚Äî\u000bthe output is the circuit‚Äôs impulse response, denoted h of t.\n\nThe h of t of the RC circuit is a decaying exponential. It tells us how the circuit reacts over time after a sudden input.\nBut what happens if we apply a more general input ‚Äî like a square pulse or a step function?\nWell, just like we learned, we can decompose that input into a sum of small impulse components.\u000bEach tiny impulse generates a scaled copy of the impulse response, and the total output is the convolution of the input and h of t.\n\nWe can see this visually on the right:\u000bDifferent inputs ‚Äî like short pulses or longer ones ‚Äî are convolved with h of t,\u000band the resulting output curves smoothly reflect how the circuit responds over time.\nTry experimenting with this in MATLAB or Python.\u000bDecompose the signal, apply convolution, and see how the shape of the output curve emerges naturally.\nThis kind of modeling is not just useful for circuits ‚Äî it‚Äôs a powerful tool in medical imaging, signal filtering, and physiological modeling, where systems respond to inputs in time-dependent ways.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 34, "slide_filename": "Slide34.txt", "slide_annotation": "Lecture 4 - Slide34.txt", "file_path": "Lecture 4\\Texts\\Slide34.txt", "content": "Now that you understand how 1D convolution works,\u000bit‚Äôs actually quite easy to extend the idea to two dimensions, especially when working with images.\n\nIn 2D convolution, you‚Äôre essentially applying the same process ‚Äî\u000bbut now, instead of just one variable like t, you‚Äôre working with two:\u000bx and y in the discrete case, or tau one and tau two in the continuous case.\n\nLet‚Äôs take a look at the discrete form first.\nWe can see the formula at the top of the slide:\nf of x, y convolved with g of x, y equals the double sum\u000bover n 1 and n 2 of f at n 1, n 2 times g at x minus n 1, y minus n 1.\nJust like in 1D, we flip the kernel, shift it, multiply, and sum ‚Äî\u000bbut now we do this across both rows and columns.\nIn the continuous case, we do the same thing using integrals.\u000bWe integrate over both variables ‚Äî tau one and tau two ‚Äî\u000bto compute the total area under the product.\n\nSo the core principle stays the same:\u000bFlip, shift, multiply, and sum or integrate ‚Äî\u000bbut now it happens in two directions at once, across a 2D grid or surface.\nThis extension is especially important in image processing and medical imaging,\u000bwhere we often apply filters, masks, or system responses across 2D signals like slices or projection data.\nWhether you‚Äôre summing or integrating,\u000bconvolution gives you a systematic way to model how a signal transforms when passed through a system.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 35, "slide_filename": "Slide35.txt", "slide_annotation": "Lecture 4 - Slide35.txt", "file_path": "Lecture 4\\Texts\\Slide35.txt", "content": "In imaging, 2D convolution is a powerful tool for modeling blurring.\nLet‚Äôs take a look at how this works using a simple example.\nOn the left, we see an input image represented as a matrix of pixel intensities.\u000bEach number corresponds to the brightness level of a pixel.\nIn the middle, we see a 3 by 3 averaging mask.\u000bAll entries are 1, and we divide the result by 9 ‚Äî\u000bwhich means we‚Äôre averaging the values in a 3 by 3 window.\nThis operation is a 2D convolution:\u000bwe slide the mask across the image, and at each position,\u000bwe multiply corresponding elements, sum them up, and store the result in the center pixel of the output.\nLet‚Äôs focus on the region that‚Äôs highlighted.\nFor the top-left example, the 3 by 3 neighborhood includes values like 1, 2, 3, and so on.\u000bWhen we sum those values and divide by 9, we get the blurred output value, shown in yellow.\nThe same thing happens throughout the image.\u000bEach pixel in the output image becomes the average of itself and its eight neighbors.\nThe result?\u000bSharp transitions are smoothed out, fine details are softened ‚Äî\u000band the image takes on a blurred appearance.\nThis is a fundamental application of 2D convolution ‚Äî\u000band it‚Äôs used not only for visual effects, but also as a preprocessing step in many medical imaging tasks,\u000blike noise reduction and feature extraction.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 36, "slide_filename": "Slide36.txt", "slide_annotation": "Lecture 4 - Slide36.txt", "file_path": "Lecture 4\\Texts\\Slide36.txt", "content": "Let‚Äôs take a look at how blurring arises naturally in imaging systems ‚Äî not just from digital filters, but from physical limitations.\nHere we observe a key concept in optics called the point spread function, or PSF.\nIn theory, if you image a perfect point of light, it should appear as a sharp dot on the detector.\u000bBut in reality, that point gets blurred into a small disk ‚Äî often called an Airy disk ‚Äî\u000bbecause of how light diffracts through the lens system.\nThis is the system‚Äôs impulse response in two dimensions.\nSo even if your input is just a single point ‚Äî like a star in the night sky ‚Äî\u000bthe output captured by your imaging system is a small, spread-out shape.\nAnd when two bright points are placed too close together,\u000btheir point spread functions begin to overlap.\u000bThis overlap makes it difficult ‚Äî or even impossible ‚Äî to tell them apart in the final image.\nThis phenomenon is at the core of what we call the resolution limit in optical systems.\nIn fact, PSFs are not just a curiosity ‚Äî\u000bthey‚Äôre mathematically modeled and measured,\u000band they define how sharp or blurry every part of an image will be.\nSo in medical imaging, astronomy, microscopy ‚Äî\u000bunderstanding the point spread function is essential to both image formation and image reconstruction.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 37, "slide_filename": "Slide37.txt", "slide_annotation": "Lecture 4 - Slide37.txt", "file_path": "Lecture 4\\Texts\\Slide37.txt", "content": "Now let‚Äôs talk about the reverse process of blurring ‚Äî a technique called deconvolution.\nWhen an image is blurred, it‚Äôs often because the original scene has been convolved with a point spread function ‚Äî or PSF.\u000bWe saw earlier that even a perfect point source becomes a blurred spot due to system limitations.\nDeconvolution is the process of trying to undo that effect ‚Äî\u000bto recover the original image from its blurred version.\nYou can think of convolution like a kind of advanced multiplication:\u000bwe blend the signal with the system‚Äôs response.\u000bIn that same spirit, deconvolution behaves like an advanced form of division ‚Äî\u000bwe‚Äôre trying to undo the blending, and isolate the original input.\nWe can see that clearly in the example on this slide.\u000bThe left image is a blurred photo of a car ‚Äî the license plate is unreadable.\u000bAfter deconvolution, the right image restores the clarity ‚Äî now we can read the plate.\nThis kind of technique is used not just in photography, but in microscopy, astronomy, and especially in medical imaging,\u000bwhere resolving fine detail can be critical.\nIt‚Äôs important to note that deconvolution is not always perfect ‚Äî\u000bin real-world systems, noise and distortion make it a challenging problem.\u000bBut with good modeling of the PSF and proper regularization,\u000bwe can often greatly enhance image quality.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 38, "slide_filename": "Slide38.txt", "slide_annotation": "Lecture 4 - Slide38.txt", "file_path": "Lecture 4\\Texts\\Slide38.txt", "content": "Here‚Äôs the idea in action.\nThis slide shows how inverse filtering works in the frequency domain.\nA sharp image, when convolved with a point spread function, results in a blurred image.\nIn the Fourier domain, convolution becomes multiplication.\u000bSo, the Fourier transform of the blurred image equals\u000bthe product of the transforms of the original image and the PSF.\nTo reverse this, we perform division in the frequency domain ‚Äî\u000bthis is the essence of inverse filtering.\nIn the red box below, the image on the right is the transform of the blurred image,\u000bthe center is the transform of the PSF,\u000band the left is the result of dividing them ‚Äî recovering the original image spectrum.\nApplying the inverse Fourier transform brings us back to the spatial domain.\nThis process forms the basis of many image restoration techniques.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 39, "slide_filename": "Slide39.txt", "slide_annotation": "Lecture 4 - Slide39.txt", "file_path": "Lecture 4\\Texts\\Slide39.txt", "content": "You might not fully grasp this yet ‚Äî and that‚Äôs perfectly fine.\nOnce we cover Fourier analysis, the connection between convolution and deconvolution will become much clearer.\nFor now, think of convolution and deconvolution as the advanced forms of multiplication and division,\u000bjust like integration is the continuous counterpart of summation.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 40, "slide_filename": "Slide40.txt", "slide_annotation": "Lecture 4 - Slide40.txt", "file_path": "Lecture 4\\Texts\\Slide40.txt", "content": "Returning to one-dimensional signals, convolution behaves like multiplication in several key ways:\nCommutative:‚ÄÉh convolved with f equals f convolved with h.\nAssociative:‚ÄÉh convolved with the result of f convolved with g equals the result of h convolved with f, then convolved with g.\nDistributive:‚ÄÉh convolved with the sum of f and g equals h convolved with f plus h convolved with g.\nThese properties show that convolution is algebraically structured, and they will connect directly to Fourier analysis.\nYou‚Äôre invited to try a quick quiz:\u000bIf y of n equals h of n convolved with x of n,\u000bprove that y of n minus k equals h of n convolved with x of n minus k.\nThis follows naturally from the meaning of convolution.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 41, "slide_filename": "Slide41.txt", "slide_annotation": "Lecture 4 - Slide41.txt", "file_path": "Lecture 4\\Texts\\Slide41.txt", "content": "We can even prove these properties. I won‚Äôt go through the full proof now, but it follows directly from how convolution is defined.\n\nHere, we start with the standard convolution definition:\u000by of n equals the sum over k of x of k times h of n minus k.\nBy substituting k equals n minus capital K,\u000bwe change the index of summation and rewrite the expression in reversed order.\nThis leads directly to:\u000bh convolved with x equals x convolved with h.\nSo, convolution is commutative ‚Äî just like multiplication.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 42, "slide_filename": "Slide42.txt", "slide_annotation": "Lecture 4 - Slide42.txt", "file_path": "Lecture 4\\Texts\\Slide42.txt", "content": "A related concept to convolution is cross-correlation.\nMathematically, the key difference is in the sign of the shift.\nIn convolution, we compute the integral of f of x minus u times g of u.\nIn cross-correlation, it‚Äôs f of x plus u times g of u.\nSo, convolution flips one of the functions before shifting and multiplying,\u000bwhile cross-correlation does not.\nBoth are useful in signal processing ‚Äî\u000bbut for different purposes, like system response versus pattern matching.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 43, "slide_filename": "Slide43.txt", "slide_annotation": "Lecture 4 - Slide43.txt", "file_path": "Lecture 4\\Texts\\Slide43.txt", "content": "Let‚Äôs take a closer look at the graphical differences between convolution, cross-correlation, and autocorrelation.\nOn the left, we see convolution.\u000bFunction f is in blue, and function g is in red.\u000bIn convolution, g is first flipped horizontally, then shifted, multiplied with f, and finally integrated or summed.\u000bAs g slides across f, the overlapping area is computed at each step ‚Äî producing the black output curve below.\nIn the middle, we have cross-correlation.\u000bIt follows the same steps as convolution, except there‚Äôs no flipping of g.\u000bWe simply shift g across f, multiply at each point, and accumulate the result.\u000bSo visually, the shapes are the same, but the symmetry is different ‚Äî this matters in pattern alignment and signal comparison.\nOn the right, we see autocorrelation.\u000bThis is a special case of cross-correlation where the same function is compared with itself.\u000bSo g and f are the same, and we measure how well the signal matches its own shifted versions.\u000bThe result usually peaks at zero shift ‚Äî showing maximum similarity with itself ‚Äî and falls off on either side.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 44, "slide_filename": "Slide44.txt", "slide_annotation": "Lecture 4 - Slide44.txt", "file_path": "Lecture 4\\Texts\\Slide44.txt", "content": "Cross-correlation is deeply connected to the Cauchy‚ÄìSchwarz inequality.\nThis fundamental result tells us that the inner product of two signals\u000bis always less than or equal to the product of their norms.\nMathematically, this absolute value of the inner product is less than or equal to\u000bthe square root of the sum of all the a k squared\u000btimes the square root of the sum of all the b k squared.\n\nEquality holds only when the two signals are proportional.\nThis is why cross-correlation works in this way:\u000bthe output is maximized when the signal and the template are aligned in shape ‚Äî\u000ba key idea in pattern recognition and signal detection.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 45, "slide_filename": "Slide45.txt", "slide_annotation": "Lecture 4 - Slide45.txt", "file_path": "Lecture 4\\Texts\\Slide45.txt", "content": "Here we observe a practical example of cross-correlation in action ‚Äî\u000ba technique known as matched filtering, widely used in signal detection.\nIn the top plot, the blue curve shows a received signal that includes both the desired pulse and a lot of background noise.\u000bThe red curve represents the matched filter ‚Äî a reference copy of the signal we‚Äôre trying to detect.\nThe bottom plot shows the cross-correlation output.\u000bNotice the sharp peak centered around one second ‚Äî this is where the received signal best aligns with the matched filter.\u000bThat spike tells us that the target signal is present at that location in time.\nThis is exactly what the Cauchy‚ÄìSchwarz inequality predicted:\u000bthe inner product, or correlation, reaches its maximum value when the two signals are best aligned.\nMatched filtering like this is essential in many fields:\u000bfrom radar and sonar, to wireless communication, and even in medical imaging ‚Äî\u000banywhere we want to pull a weak signal out of noisy data.\nCross-correlation gives us a way to do that precisely and efficiently.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 46, "slide_filename": "Slide46.txt", "slide_annotation": "Lecture 4 - Slide46.txt", "file_path": "Lecture 4\\Texts\\Slide46.txt", "content": "Let‚Äôs take a look at how convolution helps with feature extraction ‚Äî especially edge detection ‚Äî which is fundamental in medical imaging and computer vision.\nIn this example, we‚Äôre applying a Sobel filter, often used to detect edges in the horizontal direction.\nWe begin with the source image, represented as a grid of pixel intensity values.\u000bThe Sobel kernel, shown here as a 3-by-3 matrix, contains weights:\u000bnegative one, zero, and positive one across columns ‚Äî designed to emphasize horizontal changes.\nHere‚Äôs what happens:\u000bWe align the filter over a 3-by-3 region of the input image.\u000bThen, we compute the sum of products between the kernel weights and the underlying pixel values.\nFor instance, at this highlighted region, the computation looks like this:\n‚ÄÉ‚Äì1 times 3, plus 0 times 0, plus 1 times 1,\u000b‚ÄÉplus ‚Äì2 times 2, plus 0 times 6, plus 2 times 2,\u000b‚ÄÉplus ‚Äì1 times 2, plus 0 times 4, plus 1 times 1.\nWhen we add those up, we get ‚Äì3.\u000bThat value becomes the output for the corresponding destination pixel.\nNow, as the filter slides across the image ‚Äî pixel by pixel ‚Äî this process repeats.\u000bWherever there‚Äôs a strong horizontal transition, the filter produces a large response.\u000bWhere the region is uniform, the response is small or zero.\nThis is how edge detection works:\u000bIt uses convolution to detect where intensity changes rapidly, allowing us to extract important structural features from an image.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 47, "slide_filename": "Slide47.txt", "slide_annotation": "Lecture 4 - Slide47.txt", "file_path": "Lecture 4\\Texts\\Slide47.txt", "content": "To detect edges effectively across all directions, we apply multiple convolution filters ‚Äî each oriented differently.\nThe image on the left shows the original grayscale input. On the right, we see the result of applying the Canny edge detector, one of the most widely used edge detection algorithms.\nWhat makes the Canny method powerful is its multi-stage approach. It starts with smoothing the image to suppress noise, then applies gradient-based filters in horizontal and vertical directions. From there, it calculates the gradient magnitude and direction at each pixel.\nThe algorithm then performs non-maximum suppression to thin out the edges, and finally applies double thresholding and edge tracking by hysteresis to preserve only the most significant contours.\nThe result is a crisp, binary edge map ‚Äî like the one shown here ‚Äî which highlights boundaries of objects and fine details.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 48, "slide_filename": "Slide48.txt", "slide_annotation": "Lecture 4 - Slide48.txt", "file_path": "Lecture 4\\Texts\\Slide48.txt", "content": "To sum up:\u000bWe explored linear systems, especially shift-invariant systems.\u000bWe saw how convolution helps compute the system output.\u000bWe saw how cross-correlation relates to convolution and supports feature detection.\nWe learned how decomposing functions into impulses connects to these operations.\nConvolution can feel tricky at first, but with practice, it becomes a powerful, intuitive tool.", "total_slides_in_lecture": 49}
{"lecture": "Lecture 4", "slide_number": 49, "slide_filename": "Slide49.txt", "slide_annotation": "Lecture 4 - Slide49.txt", "file_path": "Lecture 4\\Texts\\Slide49.txt", "content": "Now it‚Äôs time to apply what we‚Äôve learned.\nSuppose the time constant RC equals 1 second. Use MATLAB to generate the plots shown in the first row.\nYour goal is to compute the convolution of the input signal with the system‚Äôs impulse response:\n‚ÄÉh of t equals R C times e to the power of minus t over R C, multiplied by the unit step function\nFocus on how the input signal and the impulse response interact to produce the output.\nPlease submit:\n‚Äì Your MATLAB script\u000b‚Äì The resulting figures\u000b‚Äì All combined in a Word document", "total_slides_in_lecture": 49}
{"lecture": "Lecture 5", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 5 - Slide1.txt", "file_path": "Lecture 5\\Texts\\Slide1.txt", "content": "Welcome back, everyone. Let me remind you of the best way to get the most out of this course. First, make sure you read the assigned chapter before each lecture. Join the lecture with the preview in mind and listen carefully.  Ask if you have questions proactively. Afterward, work through the homework and revisit the material again. This layered approach ‚Äî reading, watching, listening, asking, practicing ‚Äî really helps deepen your understanding.\n\nToday, we‚Äôre going to explore a very powerful concept in signal processing: the Fourier series. This topic takes us a step beyond convolution, both in complexity and in importance. It‚Äôs a bit more challenging, but it‚Äôs absolutely central to many applications in engineering, physics, and medical imaging. So, I‚Äôll do my best to break it down and highlight the key ideas so that you can follow along with confidence.\nLet‚Äôs get started.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 5 - Slide2.txt", "file_path": "Lecture 5\\Texts\\Slide2.txt", "content": "Seems like we are going as per our schedule.\n\nIn some courses or textbooks, you might be handed a formula for Fourier analysis and told to apply it mechanically. And sure ‚Äî you might be able to follow the steps, plug in numbers, and get the right answer. But that kind of approach often leaves you without a real understanding of what‚Äôs happening under the hood.\n\nHere, we‚Äôre going to do things differently. Instead of just giving you the equations, we‚Äôll take the time to explore the why and how behind them. What does the Fourier series represent? Why does it work? And how does it help us understand complex signals?\n\nThis deeper understanding is especially important in a field like medical imaging, where you‚Äôre not just solving math problems ‚Äî you‚Äôre working with real-world data that affects real people. So, our goal is not just to teach you the tools, but to help you develop a solid intuition for how and why they work.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 5 - Slide3.txt", "file_path": "Lecture 5\\Texts\\Slide3.txt", "content": "Now, let me share what I hope you'll take away from this lecture. We‚Äôre going to approach the Fourier series from a geometric perspective, starting with the idea of high-dimensional space. This means thinking about vectors, angles, distances, and inner products ‚Äî not just in two or three dimensions, but in spaces with many dimensions.\n\nWhen we combine all of these geometric tools, we begin to see Fourier series and Fourier transforms not as abstract formulas, but as structured transformations ‚Äî transformations that have a clear geometric interpretation.\n\nAnd that‚Äôs powerful. Because once you can visualize what‚Äôs going on, everything becomes more intuitive. You not only understand the formulas better, but you also remember them more easily. You know what each term is doing. You‚Äôre no longer just running code or solving equations without knowing why they work. You can see the big picture. In our last lecture, we talked about linear systems and their behavior in both continuous and discrete settings. \n\nWe introduced convolution and briefly touched on concepts such as the inner product and cross-correlation. Some of you might have found those connections a bit abstract, and that‚Äôs completely normal at first. But today, we‚Äôll start bringing those pieces together, using geometry as our guide.\nLet‚Äôs dive in.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 5 - Slide4.txt", "file_path": "Lecture 5\\Texts\\Slide4.txt", "content": "Let‚Äôs now talk about a foundational concept that underpins much of what we‚Äôre doing ‚Äî the inner product, also known as the dot product.\n\nSuppose you have two vectors, A and B. The inner product is calculated by pairing each element of vector A with the corresponding element of vector B, multiplying those pairs, and then summing the results.\n\nIn mathematical terms, if vector A has components A1 through An, and vector B has components B1 through Bn, then the inner product of A and B is the sum from i equals 1 to n of Ai times Bi.\n\nLet‚Äôs look at an example. Take vector A as 1, 3, negative 5, and vector B as 4, negative 2, negative 1.\n\nWe compute the inner product as follows:\u000bOne times four, plus\u000bthree times negative two, plus\u000bnegative five times negative one.\nThat gives us 4 minus 6 plus 5, which equals 3.\n\nNow, why is this important? Because when we perform convolution or cross-correlation, we are essentially performing inner products ‚Äî over and over again. Each time we slide one signal over another and compute the inner product, we generate one output value.\nThis is exactly how matched filtering works in radar systems. When an incoming signal ‚Äî such as an echo from an aircraft ‚Äî aligns with a known pattern, the inner product reaches its peak, making it possible to detect the target.\n\nFrom the perspective of linear systems, cross-correlation involves flipping and shifting one signal, and then computing a whole series of inner products. The result becomes the output response of a shift-invariant system. So understanding inner products is not just helpful ‚Äî it‚Äôs essential.\n\nThere‚Äôs also a beautiful geometric interpretation. The inner product of two vectors equals the product of their magnitudes multiplied by the cosine of the angle between them.\n\nIn other words, the dot product of A and B equals the norm of A times the norm of B times cosine theta ‚Äî where theta is the angle between the two vectors.\n\nSo, if the vectors point in the same direction, the angle is zero, and cosine of zero is one ‚Äî giving you the maximum value.\nIf the vectors are orthogonal ‚Äî that is, they are at 90 degrees ‚Äî then cosine theta is zero, and so is the inner product.\n\nAnd here‚Äôs something really elegant: if you take the inner product of a vector with itself, you get the square of its length. That means the magnitude of a vector can be defined as the square root of its inner product with itself.\n\nIn summary, the inner product captures three things at once ‚Äî length, angle, and alignment ‚Äî all in a single operation. And that makes it the foundation for everything we‚Äôll do with Fourier series.\n\nHere‚Äôs another way to think about it ‚Äî by drawing a biological analogy.\n\nThink of the inner product like the base pairing in DNA. Just as DNA sequences are built from matched pairs of bases ‚Äî adenine with thymine, guanine with cytosine ‚Äî our inner product is built by matching one element from vector A with one from vector B. Each matched pair contributes to the total result.\nAnd just like those base pairings carry the essential instructions for life, this mathematical pairing carries the structural foundation for data analysis. The overall sum of those matched pairs ‚Äî the inner product ‚Äî gives us meaningful information about the relationship between two datasets or signals.\n\nIn many ways, you can think of the inner product as the DNA of data science. It‚Äôs what lets us quantify similarity, compute distances, analyze angles, and perform transformations. We‚Äôll keep coming back to this idea throughout the course, so keep it in mind as we go deeper into Fourier theory.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 5 - Slide5.txt", "file_path": "Lecture 5\\Texts\\Slide5.txt", "content": "Let‚Äôs now revisit the concept of a shift-invariant linear system, which we introduced earlier in the course. This diagram gives us a step-by-step breakdown of how such systems behave ‚Äî and how the inner product plays a central role.\n\nHere‚Äôs the key idea: once you're given the impulse response of a system ‚Äî that is, how it reacts to a single impulse ‚Äî and you know the system is both linear and shift-invariant, then you can compute the system‚Äôs output for any input signal.\nThis is exactly where convolution comes into play.\n\nAs shown in the diagram, the input signal x of n can be represented as a weighted sum of shifted delta functions.\u000bThat is, x of n equals the sum over k, from negative infinity to infinity, of x of k times delta of n minus k.\n\nNow, for each of those shifted deltas ‚Äî delta of n minus k ‚Äî the system responds with a shifted impulse response:\u000bh of n minus k.\nAnd this response is scaled by the input value at that shift, which is x of k.\n\nSo what do we do next?\n\u000bWe add up all those scaled responses. The result is:\u000bx of n equals the sum over k, from minus infinity to infinity, of x of k times h of n minus k.\nThat‚Äôs the standard convolution formula.\n\nBut here‚Äôs what‚Äôs really important:\u000bWhen you compute the convolution at a particular time n, you are essentially performing an inner product.\nYou take the input signal and the flipped, shifted impulse response ‚Äî multiply them element by element ‚Äî and sum the result.\n\u000bThis process is just like computing the dot product of two vectors.\nThis same concept is used in matched filtering ‚Äî like in radar signal processing.\n\u000bIn this process, we take a known pattern ‚Äî which we refer to as the filter ‚Äî and slide it across the incoming signal.\u000bWhen the filter aligns well with a portion of the signal, the inner product reaches a maximum value.\u000bThis peak indicates that a strong match has been detected.\n\nWhether we call it convolution or cross-correlation, the operation always boils down to computing inner products.\nSo once again, the inner product isn‚Äôt just a nice-to-have mathematical idea.\u000bIt is the fundamental engine behind convolution, filtering, correlation, and soon, our study of Fourier analysis.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 5 - Slide6.txt", "file_path": "Lecture 5\\Texts\\Slide6.txt", "content": "Let‚Äôs now take a step back and look at the inner product from a geometric perspective. This viewpoint helps us build strong intuition, especially as we move into higher dimensions.\n\nStart by imagining two-dimensional space. You have a vector A, which we can think of as a point in that space. Now, add another non-zero vector, let‚Äôs call it vector B. Along with the zero vector ‚Äî that‚Äôs the origin ‚Äî these two define a straight line. This line contains all the scalar multiples of vector B. In other words, the line L of k equals k times vector B, where k is any real number.\n\nNow here‚Äôs the key idea: we want to project vector A onto that line. In other words, we want to find the point on the line that‚Äôs closest to vector A. Mathematically, this means finding the value of k that minimizes the distance between vector A and k times vector B.\n\nThe expression shown here represents that squared distance. It‚Äôs written as the sum, from i equals 1 to N, of the square of the quantity ai minus k times bi. That‚Äôs essentially the Euclidean distance, squared ‚Äî but applied component-wise in vector form.\n\nThis is an extension of the classic distance formula you saw in high school geometry: the square root of x one minus x two squared, plus y one minus y two squared. But now, we‚Äôre doing it in higher dimensions.\n\nTo minimize the distance, we take the derivative of that squared distance with respect to k, set it to zero, and solve for k. When we do that, we arrive at this important formula:\nk equals the inner product of vector A and vector B, divided by the squared norm of vector B.\u000b\nThat gives us the scaling factor for the projection.\u000bAnd then, the distance from the origin to the projected point is:\nD equals A dot B, divided by the norm of B.\n\u000bThis leads us back to a central identity in geometry:\nA dot B equals the length of A times the length of B times the cosine of theta, where theta is the angle between the two vectors.\nSo what does this all mean geometrically?\n\nThe inner product tells us how much of vector A lies in the direction of vector B.\u000bIt‚Äôs the projection of A onto B, scaled by how aligned the two are ‚Äî and that alignment is captured by the cosine of the angle.\nIf A and B are orthogonal ‚Äî that means they‚Äôre at a ninety-degree angle ‚Äî then the cosine is zero, and so is the inner product.\u000bBut if they point in the same direction, the cosine is one, and the projection is at its maximum.\n\nSo this isn‚Äôt just an abstract formula. It gives us a powerful visual interpretation of what the inner product is really doing ‚Äî measuring alignment.\nAnd this interpretation extends naturally into higher dimensions, which makes it especially useful as we transition into Fourier series.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 5 - Slide7.txt", "file_path": "Lecture 5\\Texts\\Slide7.txt", "content": "Let‚Äôs now turn our attention to a very important and elegant result in linear algebra and geometry ‚Äî the Cauchy‚ÄìSchwarz inequality.\n\nIf you haven‚Äôt yet walked through the proof of this inequality, I encourage you to do so. It‚Äôs not just a theoretical result ‚Äî it‚Äôs a tool that offers deep insight into how vectors relate to each other.\n\nNow, the inequality is written here in both summation and vector form.\nYou can see here.\n\nIn vector form, we write:\u000bThe absolute value of a dot b is less than or equal to the norm of a times the norm of b.\n\nIn plain terms, if you take the dot product of two vectors, its value will always be smaller than or equal to the product of their magnitudes.\n\u000bEquality holds only when one vector is a scalar multiple of the other ‚Äî in other words, when the vectors are perfectly aligned.\n\nTo make this more concrete, let‚Äôs consider the two-dimensional version:\u000bThe quantity a squared plus b squared, multiplied by c squared plus d squared, is greater than or equal to the square of a times c plus b times d.\n\nThis is a specific case of the inequality and shows how it works with actual vector components.\nNow, how do we prove it?\n\nA typical approach involves setting up a quadratic expression that includes both vectors and a variable, x.\u000bWe write the sum over i of the square of the quantity a·µ¢ times x plus b·µ¢.\u000bThis can also be written as the sum of a·µ¢ squared, multiplied by the square of x plus b·µ¢ divided by a·µ¢\u000b\nWe set this whole expression equal to zero and solve the resulting quadratic equation.\nThis works because the square of a real quantity is always non-negative, so the sum equals zero only when all terms vanish.\u000bThat only happens when the vectors are linearly dependent ‚Äî and that‚Äôs what gives us the equality condition.\nNow, let‚Äôs connect this back to geometry. Remember from the last slide, the inner product of two vectors equals the product of their magnitudes times the cosine of the angle between them.\n\u000bThat is:\u000bA dot B equals the norm of A times the norm of B times cosine theta.\nSince cosine of theta is always between negative one and one, the dot product can never exceed the product of the magnitudes.\u000bThat‚Äôs the geometric heart of the Cauchy‚ÄìSchwarz inequality.\n\nSo this isn‚Äôt just an algebraic result ‚Äî it‚Äôs a powerful guarantee about how closely two vectors can align.\nAnd as we move into Fourier analysis, this principle plays a crucial role in how we represent signals, enforce orthogonality, and ensure numerical stability.\nSo keep this inequality in mind ‚Äî we‚Äôll revisit it again when we start working with basis functions and Fourier coefficients.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 5 - Slide8.txt", "file_path": "Lecture 5\\Texts\\Slide8.txt", "content": "Let‚Äôs now return to the Cauchy‚ÄìSchwarz inequality, but this time through a more intuitive, geometric lens.\nEarlier, we explored the algebraic version. Now, let‚Äôs visualize what it really means. \n\nImagine two vectors, A and B, drawn as arrows in space. The inner product between them ‚Äî also called the dot product ‚Äî can be expressed as:\n\u000bA dot B equals norm of A times norm of B times cosine of the angle theta between them.\nNow, keep in mind: cosine theta is always between minus one and plus one. So unless theta is exactly zero degrees ‚Äî which means the vectors are pointing in the exact same direction ‚Äî the inner product is always strictly less than the product of the magnitudes. That‚Äôs why the inequality holds.\nBut what about equality? \n\nWell, equality happens when cosine theta is equal to one. In other words, when theta equals zero. Geometrically, that means vector A lies perfectly along the line defined by vector B. One vector is simply a scaled version of the other.\n\nIn mathematical terms, we say:\u000bbi divided by ai equals c, for all values of i.\n\u000bThis means each component of B is a constant multiple of the corresponding component of A.\nThat constant, c, tells us how much we stretch or shrink vector A to get to vector B. When that relationship holds for every component, the two vectors are not only aligned ‚Äî they are perfectly aligned, differing only in length.\nThis concept is extremely useful in signal processing. Take matched filtering, for example. Imagine you transmit a radar pulse. That pulse travels, reflects off an object ‚Äî say, an airplane ‚Äî and comes back. Because of factors like distance or material type, the return signal may be delayed or weakened, but its shape stays the same.\n\nSo how do we detect it?\nWe use cross-correlation. That means we slide the known signal over the incoming data and measure how similar they are. When the alignment is strongest ‚Äî that is, when the cross-correlation reaches its peak ‚Äî the two signals are most similar in shape.\nThis is where the Cauchy‚ÄìSchwarz inequality comes in. When two signals align perfectly, their inner product is maximized. Cosine theta approaches one. That tells us: these vectors ‚Äî or signals ‚Äî are almost perfectly matched.\n\nSo this triangle diagram isn't just a piece of geometry. It‚Äôs a powerful mental image that shows how the inner product captures alignment. And equality ‚Äî the case when the inequality becomes an equality ‚Äî only happens when vectors point exactly in the same direction. Whether you're in two dimensions, three dimensions, or even much higher-dimensional space ‚Äî the story remains the same.\nThat‚Äôs the geometric heart of the Cauchy‚ÄìSchwarz inequality.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 5 - Slide9.txt", "file_path": "Lecture 5\\Texts\\Slide9.txt", "content": "Now let‚Äôs take everything we‚Äôve learned so far and generalize it to n-dimensional space ‚Äî what we call ‚ÄúR n,‚Äù or more formally, R superscript n.\nSuppose we have two vectors:\u000bVector V has components v i,\u000band vector W has components w i.\n\nThese indices ‚Äî the little i ‚Äî run from 1 to n, where n is the number of dimensions.\u000bSo V and W are points in high-dimensional space.\u000bBut just like in two or three dimensions, we can still calculate the distance between them.\n\nThat distance is given by this formula:\u000bD squared of V and W equals the sum over i of v i minus w i, squared.\nThis is just a generalization of the Pythagorean theorem.\n\u000bIn two dimensions, it gives the diagonal of a square.\u000bIn three dimensions, it gives the diagonal of a cube.\u000bIn n dimensions, it gives the straight-line ‚Äî or Euclidean ‚Äî distance between two points.\n\nNext, let‚Äôs talk about projection ‚Äî projecting one vector onto the line defined by the other.\u000bFor instance, we can project vector V onto the direction of vector W.\u000bTo do this, we find the scalar value k that minimizes the distance between V and the scaled version of W.\nMathematically, that means minimizing the squared distance ‚Äî\u000bthe sum over i of v i minus k times w i, squared.\n\nThe optimal value of k ‚Äî the one that minimizes that distance ‚Äî is given by:\u000bk equals V dot W, over the norm of W squared.\nThis result comes directly from taking the derivative of the squared distance with respect to k,\u000bsetting it equal to zero, and solving.\u000bAnd look what appears naturally in the formula: the dot product, or inner product.\u000bSo this isn‚Äôt a random definition ‚Äî it‚Äôs rooted in geometry.\nThat brings us to another essential concept: angle and orthogonality.\n\nWe can also express the inner product as:\u000bV dot W equals norm of V times norm of W times cosine theta ‚Äî\u000bwhere theta is the angle between the two vectors.\nSo when theta equals 90 degrees, cosine theta is zero, and the inner product is zero.\u000bThat‚Äôs the condition for orthogonality ‚Äî when vectors are perpendicular.\n\nEven in R n ‚Äî in any number of dimensions ‚Äî the inner product continues to define distance, projection, angle, and orthogonality.\u000bThese aren‚Äôt just abstract mathematical ideas.\u000bThey are the foundation for understanding how high-dimensional geometry works.\u000bAnd they‚Äôre essential tools in areas like signal analysis, data science, and machine learning.\nFinally, don‚Äôt forget about basis and dimensionality.\n\u000bIn two dimensions, you need two basis vectors ‚Äî usually along the x and y axes.\u000bIn n-dimensional space, you need n basis vectors to describe any direction or position fully.\nAnd in our upcoming discussion of Fourier series, we‚Äôll see how basis functions let us express complex signals as combinations of simple building blocks ‚Äî just like coordinates in R n.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 5 - Slide10.txt", "file_path": "Lecture 5\\Texts\\Slide10.txt", "content": "Now let‚Äôs focus on a familiar and intuitive case ‚Äî three-dimensional space.\u000bThis diagram provides a clear geometric visualization that many of you have likely encountered before.\nWe‚Äôre looking at a vector ‚Äî let‚Äôs call it vector A ‚Äî situated in three-dimensional space.\u000bYou can think of vector A as an arrow pointing from the origin to some point in space.\u000b\nAlternatively, you can think of it simply as a collection of coordinates.\nEither way, vector A is completely described by its three components ‚Äî\u000bx A, y A, and z A ‚Äî\u000bwhich correspond to its projections along the x, y, and z axes.\n\nNow, here‚Äôs the key idea:\u000bEach of these components represents how much vector A \"points\" in the direction of one of the coordinate axes.\nIf you project vector A onto the x-axis, you get x A.\u000bProject it onto the y-axis, and you get y A.\u000bProject it onto the z-axis, and you get z A.\nThis method of description is based on an orthogonal basis,\u000bwhich simply means the axes are all perpendicular to each other ‚Äî\u000bat right angles.\nAnd here‚Äôs the important part:\u000bIf you know x A, y A, and z A,\u000byou have everything you need to reconstruct the entire vector A.\nThat‚Äôs because vector A is just the sum of its three projections ‚Äî\u000beach one scaled by a unit vector in the x, y, or z direction.\n\nMathematically, we write:\u000bvector A equals\u000bx A times i-hat,\u000bplus y A times j-hat,\u000bplus z A times k-hat.\nHere, i-hat is the unit vector along the x-axis,\u000bj-hat is the unit vector along the y-axis,\u000band k-hat is the unit vector along the z-axis.\nThis idea generalizes beautifully to higher dimensions.\u000bInstead of working with x, y, and z,\u000byou‚Äôll have more axes ‚Äî more basis directions ‚Äî\u000bbut the principle stays the same.\nAnd here‚Äôs where this becomes really exciting:\u000b\nIn Fourier analysis, we do something very similar,\u000bbut instead of projecting onto spatial axes,\u000bwe project onto a set of basis functions ‚Äî\u000blike sine and cosine waves.\nSo, what you‚Äôre seeing here in 3D\u000bis not just about vectors in space.\u000bIt‚Äôs the same foundational idea that lets us\u000banalyze and reconstruct complex signals\u000bin the frequency domain.\nUnderstanding this orthogonal representation\u000bsets us up perfectly for the world of Fourier series.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 5 - Slide11.txt", "file_path": "Lecture 5\\Texts\\Slide11.txt", "content": "Previously, we saw how a vector in three-dimensional space can be represented using its projections onto the x, y, and z axes. Now, let‚Äôs extend this same idea to n-dimensional space ‚Äî using orthogonal basis vectors to represent any arbitrary vector.\n\u000bLet‚Äôs quickly review the 3D case first.\u000bWe have three standard unit vectors: along the x-axis, e x equals the vector 1, 0, 0, along the y-axis, e y equals 0, 1, 0, and along the z-axis, e z equals 0, 0, 1.\u000bAny 3D vector ‚Äî let‚Äôs say vector v ‚Äî with components x, y, and z, can be written as a weighted sum of these basis vectors.\u000bSo, we write: vector v equals v dot e x times e x, plus v dot e y times e y, plus v dot e z times e z.\u000bEach of these terms represents the projection of vector v onto one of the coordinate directions. Because our basis vectors are orthogonal and of unit length, we don‚Äôt need to normalize ‚Äî the inner product directly gives us the component in that direction.\n\nNow let‚Äôs generalize to n dimensions.\u000bSuppose we have a vector ‚Äî we‚Äôll call it vector W ‚Äî that lives in n-dimensional space. We can express this vector as a linear combination of n orthonormal basis vectors, denoted as e n, where n runs from 1 to capital N.\u000bThe formula looks like this: vector W equals the sum from n equals 1 to capital N of W dot e n times e n.\n\nIn words, for each basis direction e n, we project W onto that direction by computing the inner product W dot e n. That gives us the scalar coefficient for that direction. Then we scale e n by that coefficient and sum everything together.\u000b\nThis is a very powerful idea: you can express any vector ‚Äî no matter how high the dimension ‚Äî as a sum of projections onto orthonormal basis vectors.\u000bAnd what‚Äôs even more powerful is that those basis vectors don‚Äôt have to be just the standard coordinate axes.\u000bThey could be sine and cosine functions, as we use in Fourier series. Or they could be eigenvectors in principal component analysis. Or wavelets, or anything else ‚Äî as long as they are orthonormal.\u000bSo what we‚Äôve described on this slide is the general framework of orthogonal representation in n-dimensional space. It‚Äôs the core mathematical idea that underpins many tools in signal processing, data analysis, and machine learning.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 5 - Slide12.txt", "file_path": "Lecture 5\\Texts\\Slide12.txt", "content": "Now here‚Äôs a very important point ‚Äî and one that‚Äôs often overlooked.\u000bA basis is not unique.\n\nLet‚Äôs begin with a simple analogy in two-dimensional space.\u000bIn most problems, we use the standard coordinate system ‚Äî with one unit vector along the x-axis and one along the y-axis.\u000bIn that system, the basis vectors are: 1, 0 for the x-direction, and 0, 1 for the y-direction.\nBut now imagine we rotate the coordinate system by some angle ‚Äî let‚Äôs call it phi.\u000b\nAfter this rotation, we have a new set of axes ‚Äî shown in red on the diagram.\u000bThese new axes define a new basis, one that‚Äôs rotated by the angle phi.\u000bAnd here‚Äôs the key: this rotated basis is just as valid as the original one.\nEvery point in the plane can still be represented using the new basis.\u000bThe point hasn‚Äôt moved ‚Äî only the coordinate system has changed.\u000bSo the coordinates of the point relative to the new axes will be different, but the underlying geometry is the same.\nMathematically, the new coordinates, which we‚Äôll call x-prime and y-prime, are computed from the original coordinates x and y using this rotation:\u000bx-prime equals x times cosine phi, plus y times sine phi.\u000by-prime equals negative x times sine phi, plus y times cosine phi.\n\nIn matrix form, this looks like:\u000bopcosine phi, sine phi; negative sine phi, cosine phi,\nThis is a classic rotation matrix ‚Äî it rotates vectors by an angle phi in the plane.\nAnd this concept generalizes beautifully.\u000bIn three dimensions, you can rotate about the x-axis, the y-axis, or the z-axis, producing new orthogonal bases in each case.\u000bYou can also use combinations of these rotations ‚Äî just like the lower diagram shows ‚Äî rotating around alpha, beta, and gamma angles.\nAnd in n-dimensional space, the same principle applies.\u000b\nSo what‚Äôs the takeaway?\u000bThe basis you choose to describe vectors is not fixed.\u000bYou can rotate it, change it, or switch to one that better fits the problem.\u000bThis is especially useful in Fourier analysis, where we don‚Äôt just rotate axes ‚Äî we build entirely new bases, like sine and cosine functions, to describe signals more effectively.\nSo while the geometry remains unchanged, the coordinate description of a point ‚Äî or a signal ‚Äî depends on the basis you choose.\u000bThat‚Äôs the power of flexible, non-unique bases in linear algebra and signal processing.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 5 - Slide13.txt", "file_path": "Lecture 5\\Texts\\Slide13.txt", "content": "Now let‚Äôs step back and summarize what we‚Äôve built so far ‚Äî the formal structure of a vector space, especially in R-n, that is, n-dimensional real space.\u000bA vector in this space is just an ordered list of n numbers.\n\u000bFor example, we could define vector v as: v equals v1, v2, and so on, up to vn.\u000bAnd similarly, another vector w would be: w equals w1, w2, all the way to wn.\nNow, the inner product of these two vectors is written as:\u000bv dot w equals v1 times w1, plus v2 times w2, plus all the way up to vn times wn.\n\nThis formula isn‚Äôt arbitrary. It‚Äôs grounded in both geometric intuition and practical applications ‚Äî including signal detection and system analysis. But geometrically, it gives us the clearest picture of how vectors interact.\nFor example, if we take the inner product of a vector with itself, we get: v dot v equals the sum of the squares of its components. That is: v1 squared plus v2 squared, and so on, up to vn squared. Which is simply the squared length of the vector ‚Äî or the norm squared. \n\nSo we write: v dot v equals double bar v double bar squared.\nThis gives us the magnitude of a vector, which is fundamental to defining distance, angle, and orthogonality ‚Äî even in very high-dimensional spaces.\nNow, let‚Äôs talk about basis.\u000bIn R-n, we use what‚Äôs called the natural basis. These are vectors of length one that point along each coordinate direction.\u000bSo we define:\u000be1 as 1, 0, 0, and so on,\u000be2 as 0, 1, 0, and so on,\u000band so on, up to en, which is 0, 0, and finally 1.\n\nEach of these vectors is orthogonal to the others, meaning their inner product is zero unless you're comparing a vector with itself.\nSo any vector v can be written as a linear combination of these basis vectors. That is: v equals v1 times e1, plus v2 times e2, all the way up to vn times en.\nAnd how do we get each of these coefficients?\n\u000bSimple ‚Äî by taking the inner product of v with the corresponding basis vector. So we say: v dot ek equals vk.\nThis structure ‚Äî combining orthogonality, projection, and basis expansion ‚Äî gives us a clean and consistent framework for working with any dimensional space. Whether we‚Äôre dealing with 3D vectors in physics, or a hundred-dimensional space in data analysis, the math works the same way.\nAnd beneath it all is a visual, geometric idea: projecting, aligning, and reconstructing using components along axes.\u000bThat‚Äôs the foundation of vector spaces.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 5 - Slide14.txt", "file_path": "Lecture 5\\Texts\\Slide14.txt", "content": "Let‚Äôs close this section with a powerful and elegant idea ‚Äî something that ties everything we‚Äôve learned so far into a single, intuitive picture.\u000bEvery vector is a point.\u000bThat‚Äôs right ‚Äî a vector isn‚Äôt just a list of numbers. It represents a location in space.\n\nFor example, in two-dimensional space, a vector with components x and y ‚Äî written as x, y ‚Äî can be seen as a point on the plane.\u000bIf this vector has unit length ‚Äî meaning its magnitude is one ‚Äî then it lies on the unit circle.\u000bAs you move around that circle, each position corresponds to a different vector, but all with the same length.\nNow in three dimensions, the same idea holds.\u000bA unit-length vector becomes a point on the surface of a unit sphere.\u000bAgain, the magnitude is one, but the direction can vary.\u000bAnd every point on the sphere represents a different vector of unit length.\nThis concept scales beautifully to higher dimensions.\n\u000bIn general, if we‚Äôre in n-dimensional space, the collection of all vectors of the same length ‚Äî say, length r ‚Äî forms an n-sphere.\nMathematically, we write this as: S n equals open curly brace x in R n plus 1, such that double bar x double bar equals r close curly brace.\nThis simply means: we‚Äôre gathering all the vectors that are at a fixed distance r from the origin.\u000bEach of these vectors is also a point in space.\u000bThey all have the same magnitude ‚Äî r ‚Äî but different directions.\nSo geometrically, a vector is just a point, and when we fix the length of these vectors, they trace out meaningful shapes like circles in 2D, spheres in 3D, or hyperspheres in higher dimensions.\n\nThis perspective becomes especially important as we move forward into signal processing and functional spaces, where even functions can be treated like vectors ‚Äî and those vectors live in infinite-dimensional spaces.\nBut the core idea doesn‚Äôt change.\u000bA vector is a point. A point is a vector.\u000bAnd the same geometry that applies to physical space applies to abstract spaces as well ‚Äî including the ones we‚Äôll encounter next in our study of Fourier series.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 5 - Slide15.txt", "file_path": "Lecture 5\\Texts\\Slide15.txt", "content": "Let‚Äôs now take the next big step in our journey ‚Äî\u000ba function can actually be treated like a vector.\n\nNow, when you first think about a function ‚Äî for example, y equals f of x ‚Äî\u000bYou can imagine a smooth curve on a graph.\u000bAs x changes, you get a continuous stream of y-values,\u000band these values trace out a flowing shape across the domain.\nThat‚Äôs the classical view ‚Äî a function as a curve.\n\u000bBut here‚Äôs a shift in perspective that changes everything.\nSuppose you discretize the function.\u000bThat means instead of considering every possible x,\u000byou just pick a few specific values:\u000bx 1, x 2, all the way to x n.\nAt each of these points, you compute the function:\u000bf of x 1, f of x 2, and so on, up to f of x n.\nAnd now you have a finite list of numbers.\u000bThose function values ‚Äî can be written as a vector.\u000bEach value becomes one component of that vector.\n\nIn the diagram here, you see a red curve ‚Äî that‚Äôs the original function ‚Äî\u000band below it, green bars showing sample values.\u000bIf we sample the function at 7 locations,\u000bwe can think of those 7 values as a 7-dimensional vector.\nTake more samples ‚Äî say 700, or 7 million ‚Äî\u000band the dimension of the vector simply increases.\u000bBut conceptually, it‚Äôs still just a point in some high-dimensional space.\n\nSo here‚Äôs the key idea:\u000ba function is just a vector ‚Äî possibly a very high-dimensional one.\u000bAnd that means we can apply all the tools of vector analysis to functions.\u000bWe can talk about the length of a function,\u000bwe can project one function onto another,\u000band we can compute angles between functions,\u000bjust like we did with ordinary vectors in R-n.\nThis insight is exactly what powers Fourier analysis.\u000bIn a Fourier series, we project a function onto a set of orthogonal basis functions ‚Äî\u000bfunctions like sine and cosine ‚Äî\u000bjust as we projected ordinary vectors onto coordinate axes.\n\nSo let me leave you with this big takeaway:\nIf a vector is a point,\u000band a function is a vector,\u000bthen a function is also a point ‚Äî\u000ba point living in a high-dimensional function space.\nThat‚Äôs the foundation of what we‚Äôll build next ‚Äî\u000brepresenting and analyzing functions using the powerful language of vectors.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 5 - Slide16.txt", "file_path": "Lecture 5\\Texts\\Slide16.txt", "content": "We‚Äôve talked a lot about vectors ‚Äî and how to compute their inner product\u000bby multiplying corresponding components and then adding them up.\u000bThat‚Äôs the basic operation in vector space.\n\nMathematically, for vectors A and B, the inner product is written as:\u000bA dot B equals the sum from i equals minus infinity to infinity\u000bof a i times b i.\n\nNow, let‚Äôs extend this idea to functions.\u000bIn functional space, instead of dealing with discrete components,\u000bwe‚Äôre dealing with continuous values defined over time or space.\nSo, instead of a summation, we use an integral.\u000bThe inner product of two functions, A of t and B of t, is defined as the integral from minus infinity to infinity\u000bof A of t times B of t, d t.\nThat‚Äôs our functional equivalent of the vector dot product.\u000bThe interpretation is exactly the same:\u000bwe‚Äôre measuring how much two signals align ‚Äî\u000bwhether they point in similar directions in an infinite-dimensional space.\n\nNow here comes the big question:\u000bCan we find a basis for functions, just like we do in vector spaces?\nAnd the answer is yes.\u000bOne powerful basis uses something called the Dirac delta function ‚Äî\u000bwritten as delta of x minus a.\n\nLet me explain.\nThere‚Äôs a key identity in signal processing that says:\u000bf of t equals the integral from minus infinity to infinity\u000bof delta of tau minus t, times f of tau, d tau.\n\nWhat does this mean?\nWell, the delta function is like a mathematical spike ‚Äî\u000bIt‚Äôs zero everywhere, except at a single point.\u000bSo when we multiply f of tau by delta of tau minus t,\u000bWe‚Äôre isolating the value of f right at tau equals t.\u000bAnd by integrating over all tau, we‚Äôre reconstructing the full function\u000bby sweeping this spike across the domain.\nThink of it this way:\u000beach delta function ‚Äî centered at a point a ‚Äî\u000bacts like a basis vector.\u000bAnd each one is scaled by f of a ‚Äî\u000bthe value of the function at that location.\nThis is just like saying:\u000bv equals v one times e one plus v two times e two, and so on.\nSo yes, even in this continuous, infinite-dimensional space,\u000bwe‚Äôre still using the same principle:\u000bBasis functions times coefficients gives you the full object.\nAnd this paves the way for what‚Äôs coming next ‚Äî\u000bFourier series, where instead of spikes,\u000bwe‚Äôll use smooth, oscillating waves as our basis functions.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 5 - Slide17.txt", "file_path": "Lecture 5\\Texts\\Slide17.txt", "content": "Let‚Äôs now revisit a powerful concept ‚Äî\u000brepresenting a function as a sum of impulses.\nHere‚Äôs the idea:\u000bInstead of describing a function as one smooth continuous curve,\u000bwe break it down into a series of impulses, or spikes.\nEach impulse is a shifted delta function ‚Äî\u000bmeaning, a very narrow spike placed at a specific time location.\n\u000bWe scale each spike by the height of the function at that point.\u000bIn other words, the amplitude of the spike equals the value of f of t\u000bat that specific time t.\nSo we‚Äôre essentially saying:\u000bf of t equals the sum of delta functions,\u000beach multiplied by f at that location.\n\nLook at the diagram.\u000bThe smooth gray curve represents the original function,\u000band the red arrows show the delta spikes.\u000bEach spike points straight up, and its height\u000bmatches the value of the function at that moment.\n\nNow, if you add all of these weighted impulses together ‚Äî\u000byou recover the entire shape of the original function.\nThis construction gives us a complete basis in one dimension.\u000bAnd we can take this idea further.\nIn two dimensions, we use impulses defined over x and y.\u000bThink of image pixels ‚Äî each pixel acts like a 2D delta function,\u000bplaced at a specific location and scaled by intensity.\nIn three dimensions ‚Äî like with medical images or 3D scans ‚Äî\u000bwe build the signal from tiny volume elements, or voxels.\u000bEach voxel is just an impulse located at some point in 3D space,\u000bscaled by the value of the function there.\n\nSo no matter the dimensionality ‚Äî\u000bone D, two D, or three D ‚Äî\u000byou can think of a signal, image, or volume\u000bas being built from a grid of impulses.\nEach impulse contributes a small piece of the whole.\u000bAnd together, they form the complete function.\nThat‚Äôs the power of thinking in terms of delta functions ‚Äî\u000bthey give us a clean, mathematical way to build up\u000bany function from the ground up, piece by piece.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 5 - Slide18.txt", "file_path": "Lecture 5\\Texts\\Slide18.txt", "content": "This impulse-based thinking becomes especially useful when we move into the world of digital imaging.\nWhen you capture a picture, what you really get is a grid of pixels ‚Äî discrete samples of brightness or intensity at specific locations. Each pixel can be thought of as a weighted impulse. A brighter pixel contributes more to the image, a darker pixel contributes less. And when the pixel size gets smaller and smaller ‚Äî approaching zero ‚Äî you begin to approach the continuous version: the delta function.\n\nSo in this view, an entire image is just a sum of impulses, one per pixel, weighted by intensity.\nAnd this isn‚Äôt limited to 2D images.\n\nIn 3D medical imaging, for example, we deal with voxels ‚Äî volume elements ‚Äî which are essentially 3D pixels. Again, each voxel corresponds to an impulse in space.\nWhether it‚Äôs a 1D signal, a 2D image, or a 3D volume, we can represent the data using impulse-like basis functions. That‚Äôs one perspective.\nBut there‚Äôs another view ‚Äî just as powerful ‚Äî and that involves sinusoidal basis functions.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 5 - Slide19.txt", "file_path": "Lecture 5\\Texts\\Slide19.txt", "content": "Sine and cosine waves show up everywhere in nature ‚Äî\u000band there's a beautiful reason for that.\u000bThey arise naturally from circular motion.\nLet‚Äôs take a simple example.\n\u000bImagine a point moving in a perfect circle ‚Äî\u000blike the moon orbiting the Earth at constant speed.\n\nNow, if you shine a light on that motion\u000band watch the shadow it casts onto a flat surface ‚Äî\u000bsay, the x-axis ‚Äî\u000bwhat do you get?\nYou get a sine wave.\nThat‚Äôs right ‚Äî\u000bas the point moves around the circle,\u000bits horizontal position follows a smooth wave.\u000bThat‚Äôs sine.\nAnd if you project onto the y-axis instead,\u000byou get a cosine wave.\n\nSo these waveforms ‚Äî sine and cosine ‚Äî\u000bare really just the shadows of circular motion.\nThat‚Äôs a profound idea.\n\u000bIt tells us that sine and cosine functions\u000baren‚Äôt just mathematical inventions ‚Äî\u000bthey‚Äôre embedded in the geometry and physics of the real world.\nThey describe how things rotate, how waves travel,\u000bhow springs vibrate, how light and sound move.\nAnd because of this,\u000bsine and cosine waves form the foundation of something\u000bwe‚Äôll explore in depth ‚Äî the Fourier series.\n\nSo far, we‚Äôve seen two major ways to represent functions:\nFirst, as a sum of impulses ‚Äî\u000blike tiny samples or pixel points in space.\nAnd now, as a sum of sinusoids ‚Äî\u000bwaves that capture how a signal behaves across frequencies.\nBoth perspectives are incredibly powerful.\u000bThey help us describe, analyze, and reconstruct data\u000bin ways that are both mathematically precise and physically meaningful.\n\nIn the next section, we‚Äôll dive into how\u000bthese sine and cosine functions can be used\u000bas a basis to build any function you like ‚Äî\u000bjust like we built vectors from orthogonal components.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 5 - Slide20.txt", "file_path": "Lecture 5\\Texts\\Slide20.txt", "content": "Let‚Äôs now take a closer look at the sinusoidal basis ‚Äî especially the sine and cosine functions.\n\nIn the plot, the red curve represents the sine of x ‚Äî written as sine x ‚Äî and the green curve shows the cosine of x.\nThere‚Äôs a key difference in their symmetry. Cosine is an even function. That means if you reflect it across the vertical axis ‚Äî the y-axis ‚Äî it stays the same. In other words, the cosine of negative x equals the cosine of x.\n\nSine, on the other hand, is an odd function. That means it‚Äôs symmetric about the origin. So the sine of negative x equals the negative sine of x.\nBut here‚Äôs something subtle. Whether a function is ‚Äúeven‚Äù or ‚Äúodd‚Äù actually depends on where you place the origin. If you shift your coordinate system, the symmetry can change. So this classification is based on our point of view ‚Äî not an absolute property of the function.\n\nNow here‚Äôs the deeper idea: sine and cosine functions together form an orthogonal basis. That means you can use them to represent any reasonable function ‚Äî just like we previously used delta functions or unit vectors.\nThis sets the stage for a powerful idea: instead of expressing a function as a sum of impulses, like a series of spikes, we can express it as a sum of waves ‚Äî smooth, continuous, sinusoidal components.\nAnd that‚Äôs exactly what we‚Äôll explore next through the lens of the Fourier series.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 5 - Slide21.txt", "file_path": "Lecture 5\\Texts\\Slide21.txt", "content": "This representation makes that idea visual and intuitive.\n\nHere, each person is holding a different sine wave ‚Äî each wave has a different frequency or amplitude. When we add these waves together, we get a more complex waveform ‚Äî something that might look jagged, or smooth, or anything in between.\n\nAnd this is the foundation of the Fourier series. It tells us that any reasonable function ‚Äî smooth or sharp ‚Äî can be represented as a weighted sum of sine and cosine waves. These sinusoidal components differ in frequency, phase, and amplitude. But together, they can recreate signals of almost any shape.\n\nSo, just like we saw earlier that a function could be built from impulses, here we‚Äôre seeing that a function can also be built from waves. These are two complementary viewpoints ‚Äî both valid, both powerful.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 5 - Slide22.txt", "file_path": "Lecture 5\\Texts\\Slide22.txt", "content": "Let‚Äôs now bring this idea to life with a demonstration.\n\nWhat you‚Äôre seeing here is called spectral synthesis ‚Äî it‚Äôs the process of reconstructing a function by adding together sine and cosine waves.\nThe red curve represents the signal or shape we want to recreate. Under the hood, we‚Äôre combining multiple sinusoidal components ‚Äî each one a wave with its own frequency and amplitude.\nAs we begin, the approximation looks rough. But as we add more and more sine and cosine terms ‚Äî especially ones with higher frequencies ‚Äî the red curve starts to take shape. We can even mimic sharp edges or jumps, like you see in square waves, just by stacking enough high-frequency components together.\nHere‚Äôs what‚Äôs important: each sine or cosine wave contributes a specific amount to the final signal. That amount is determined by its amplitude. If we were to graph those contributions ‚Äî with frequency on the x-axis and amplitude on the y-axis ‚Äî we‚Äôd get what‚Äôs known as the Fourier spectrum.\n\nThink of it as the signal‚Äôs fingerprint. It tells us exactly how much of each frequency is present in the original function.\n\nThis is the core idea behind one-dimensional Fourier analysis. But it doesn‚Äôt stop there. We can extend the same concept to two-dimensional signals, like images ‚Äî and even to three dimensions, when we‚Äôre working with volumes.\nSo what begins as a sum of simple waves turns out to be a powerful way of analyzing and reconstructing complex signals in any dimension.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 5 - Slide23.txt", "file_path": "Lecture 5\\Texts\\Slide23.txt", "content": "Let‚Äôs now see what happens when we take everything we‚Äôve learned and apply it to two-dimensional signals ‚Äî like images.\n\nHere we have a famous example ‚Äî a grayscale portrait of Albert Einstein. At first glance, it seems complex and highly detailed. But if we perform 2D Fourier analysis, something remarkable happens: we can decompose this image into a collection of smooth, sinusoidal wave patterns.\n\nIn the middle panel, you see the amplitude spectrum ‚Äî this tells us how much of each frequency is present in the image. Frequencies closer to the center are low ‚Äî representing broad, smooth changes. As you move outward, the frequencies get higher ‚Äî capturing fine details and edges.\nOn the right, you see an example of one such component: a 2D sinusoidal wave. These are like ripples in water, traveling in various directions ‚Äî horizontal, vertical, diagonal ‚Äî and at different frequencies.\n\nNow if we zoom out, we can think of an image as a sum of many such 2D waves. That‚Äôs what the grid of small tiles on the bottom left represents ‚Äî each one is a different sinusoidal basis function. Some capture coarse patterns; others capture fine textures.\nAs we adjust the amplitudes and phases of these sinusoidal components just right ‚Äî and add them all together ‚Äî we reconstruct the original image.\nThis is the essence of Fourier analysis in two dimensions:\u000b\nWe take a complex image, and represent it using simple, smooth building blocks. The result is a completely new way to think about images ‚Äî not as pixels, but as combinations of spatial frequencies.\nAnd this approach is not limited to images of Einstein. It works for any image ‚Äî medical scans, photographs, microscopy, and beyond. That‚Äôs why Fourier methods are so foundational in signal processing, image analysis, and modern AI.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 5 - Slide24.txt", "file_path": "Lecture 5\\Texts\\Slide24.txt", "content": "Let‚Äôs take a moment to reflect on something deeper ‚Äî a principle that reaches beyond signal processing and touches the foundations of physics and information itself.\n\nIn physics, we talk about the wave-particle duality ‚Äî the idea that entities like photons and electrons exhibit both particle-like and wave-like behavior. Depending on how you observe them, they might act like tiny particles... or spread out like waves.\nInterestingly, we see a similar duality in information science.\n\nOn one side, we have the particle view: information as discrete units ‚Äî like pixels in an image, impulses in a signal, or delta functions in a mathematical model. This is the pixel-by-pixel, sample-by-sample perspective. It's granular and local.\nOn the other side, we have the wave view: information described in terms of global, smooth, continuous oscillations ‚Äî sinusoidal waves of varying frequency, amplitude, and phase. This perspective captures structure across space or time ‚Äî the big picture.\n\nSo when we represent a function ‚Äî or an image, or a signal ‚Äî we can choose either perspective.\nUsing delta functions or impulses, we build the signal by assembling localized pieces ‚Äî one sharp component at a time.\nUsing sinusoidal functions, we build it using smooth, periodic waveforms that span the domain.\nBoth approaches are valid. In fact, they are mathematically equivalent when used with the right basis. \nAnd this is the central idea of today‚Äôs lecture:\u000b\nA function can be viewed as a vector in a high-dimensional space ‚Äî and depending on our basis, we can represent that function in many different ways.\nYou can think of a delta basis: where each basis function is an impulse at a specific location.\nOr a sinusoidal basis: where each basis function is a sine or cosine wave with a different frequency.\nThese are just two different coordinate systems for the same space ‚Äî and by projecting the function onto the basis elements, we retrieve coefficients that let us reconstruct the original signal as a linear combination.\n\nSo in summary:\u000bThe delta basis gives us a particle-like view.\u000bThe sinusoidal basis gives us a wave-like view.\u000bAnd both help us understand information from complementary perspectives.\n\nThis concludes the first part of our discussion ‚Äî building intuition through geometry and representation.\nIn the second half, we‚Äôll go deeper into Fourier series, where we work with periodic functions, and learn how to express them precisely in terms of sine and cosine components. We‚Äôll take a short break now ‚Äî about ten minutes ‚Äî and then dive right into the math.\nSee you shortly.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 5 - Slide25.txt", "file_path": "Lecture 5\\Texts\\Slide25.txt", "content": "Let‚Äôs now begin our exploration of Fourier series by focusing on a special class of functions: periodic functions.\nA periodic function is one that repeats itself over and over again at regular intervals. Formally, we say that a function f of x is periodic if there exists a value ‚Äî which we call the period, and usually denote with capital P ‚Äî such that:\nf of x plus P equals f of x, for all values of x.\nThis repeating behavior is what allows us to use the Fourier series to represent such functions.\nTake a look at the top plot in this slide. It shows a one-dimensional example of a periodic function. It doesn't need to be a perfect sine or cosine wave ‚Äî the key idea is that it repeats exactly after a fixed horizontal interval. In this case, that interval is P units.\n\nNow look at the image below it. That‚Äôs a real-world example ‚Äî a two-dimensional periodic pattern. Patterns like this are common in materials science, digital imaging, or even textures. You can see how the structure repeats itself in both the horizontal and vertical directions ‚Äî that‚Äôs spatial periodicity in two dimensions.\nSo what is our goal?\n\nWe want to understand how any periodic function ‚Äî no matter how smooth, jagged, or irregular ‚Äî can be represented as a sum of sine and cosine waves.\nThis is the heart of the Fourier series. Even if a function looks complicated, we can rebuild it precisely by combining a collection of smooth sinusoidal waves ‚Äî each with its own frequency, amplitude, and phase.\n\nAnd here‚Äôs the beautiful part:\u000bIf we get just one period of the function right, the rest of the function falls into place, because it's simply repetition.\nSo moving forward, we‚Äôll first focus on periodic functions in one dimension. We‚Äôll go through the math, develop the intuition, and build the full Fourier series representation. Then, once we understand that, we‚Äôll extend the same ideas to two-dimensional cases, and even three dimensions ‚Äî which is especially useful for signals, images, and medical data.\nLet‚Äôs now dive into the mathematics and see how it all comes together.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 5 - Slide26.txt", "file_path": "Lecture 5\\Texts\\Slide26.txt", "content": "Now let‚Äôs take the next step and define the Fourier series more precisely.\n\nTo keep things simple ‚Äî and without losing any generality ‚Äî we‚Äôll start with a function defined over the interval from 0 to 1. We call this the unit period. The key assumption here is that the function repeats every unit interval. So if we understand what the function does between 0 and 1, we automatically understand it from 1 to 2, 2 to 3, and even from negative 1 to 0.\n\nLater, we‚Äôll generalize to other periods ‚Äî like from 0 to capital T, or from negative T over 2 to positive T over 2. But for now, let‚Äôs build our intuition using this simpler interval.\n\nSo, what kinds of functions are we considering here?\nWe‚Äôre working with real-valued functions that are square-integrable ‚Äî that means when you square the function and integrate it over the interval from 0 to 1, the result is finite. If that integral blows up to infinity, the function is too wild, and we can‚Äôt represent it with a Fourier series. But if it‚Äôs finite, then the function behaves well enough for our purposes.\n\nMathematically, we say such functions belong to the space L square of zero to one ‚Äî that‚Äôs the set of square-integrable functions over the interval from 0 to 1. This is a Hilbert space, meaning we can treat functions like vectors in an infinite-dimensional space.\nAnd just like vectors have a basis, functions in this space also have an orthonormal basis.\nWhat does that basis look like?\n\nIt turns out the basis consists of the constant function 1, plus an infinite family of sine and cosine functions:\nSquare root of 2 times cosine of 2 pi n t\nSquare root of 2 times sine of 2 pi n t\u000bfor n equals 1, 2, 3, and so on.\nThe factor of square root 2 ensures that each function has unit length ‚Äî meaning, its inner product with itself equals 1. That‚Äôs what makes the basis orthonormal.\n\nSo here‚Äôs the big idea:\nAny square-integrable function over the interval 0 to 1 can be expressed as a sum of these basis functions.\u000bAnd this sum is called the Fourier series.\nMathematically, we write:\nf of t equals a naught over two,\u000bplus the summation over n of a n times cosine of 2 pi n t,\u000bplus the summation over n of b n times sine of 2 pi n t.\n\nSo, what do all these parts mean?\nFirst, a naught is called the DC component. It represents the average value of the function over one period.\nThe a n coefficients are multiplied by cosine terms. These describe the even, symmetric parts of the function.\nAnd the b n coefficients go with the sine terms. They capture the odd, asymmetric parts of the function.\nIn essence, what we‚Äôre doing is splitting the function into symmetrical and asymmetrical patterns, using sines and cosines as building blocks. This is what makes the Fourier series so elegant ‚Äî it expresses any periodic function as a mix of smooth, familiar waveforms.\n\nNow, each of these coefficients can be computed using inner products, which we'll define using integrals in the next slide.\nBut conceptually, this is just like decomposing a vector into components ‚Äî except now we're working with continuous functions in an infinite-dimensional space, using smooth sine and cosine waves as our basis.\nAnd that‚Äôs the beauty of the Fourier series:\u000bRather than reconstructing a function using spikes or samples, we rebuild it by layering together simple waveforms.\nThis is the core principle of Fourier analysis:\u000bBreak down the complex using the simple.\u000bUse a family of smooth waves to represent arbitrary structure.\nNext, let‚Äôs see how to actually compute those coefficients.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 5 - Slide27.txt", "file_path": "Lecture 5\\Texts\\Slide27.txt", "content": "Let‚Äôs now take a closer look at something we claimed earlier ‚Äî that the set of sine and cosine functions, together with the constant function, form what we call an orthonormal basis.\n\nNow, this idea might sound a bit abstract at first. But if we break it down, it‚Äôs very intuitive.\nIn the world of regular vectors ‚Äî say, in two or three dimensions ‚Äî we say vectors are orthogonal if they‚Äôre at right angles to each other. And if each one also has a length of one, we call them orthonormal.\nThe same idea applies in function space. But instead of using a dot product like we do for regular vectors, we use something called an inner product ‚Äî which is defined using an integral. This inner product helps us measure both length and angle between functions.\n\nLet‚Äôs walk through the specific relationships shown here.\nFirst, the constant function ‚Äî just the number one ‚Äî has unit length. That means if we take its inner product with itself, we get one. So it‚Äôs normalized.\nNow let‚Äôs see what happens when we compare the constant function with a cosine or a sine function. If we take the inner product of the constant with cosine of two pi times n times t, we get zero ‚Äî for any positive integer n. The same is true for sine.\n\nWhy does this happen? Well, cosine and sine both oscillate above and below the horizontal axis, and over a full cycle, the positive and negative areas cancel out. So when you compare either of them with the constant, the result is zero ‚Äî meaning they‚Äôre orthogonal.\n\nNext, let‚Äôs compare sine and cosine functions with each other. Say, cosine of two pi times m times t and sine of two pi times n times t. Regardless of which frequencies m and n we choose, their inner product is zero. So sine and cosine are always orthogonal to each other.\n\nNow here‚Äôs where it gets a bit more subtle.\nIf we compare cosine functions with other cosine functions ‚Äî or sine with sine ‚Äî the result depends on the frequencies.\nIf the frequencies are different, meaning m is not equal to n, the inner product is zero. They‚Äôre orthogonal.\nBut if the frequencies match ‚Äî so m equals n ‚Äî the inner product is one-half. That means the functions are not quite unit-length yet. They‚Äôre orthogonal but not normalized.\n\nTo fix that, we just scale the sine and cosine functions by the square root of two. Then, their inner product with themselves becomes exactly one, making the whole set orthonormal.\nSo here‚Äôs the big picture: we now have a set of functions ‚Äî the constant, sine waves, and cosine waves ‚Äî that are all mutually perpendicular and have unit length. Together, they form a complete orthonormal basis.\n\nAnd once we have this basis, we can represent any periodic function as a combination of these building blocks. That combination is what we call the Fourier series, and the weights we use ‚Äî the Fourier coefficients ‚Äî tell us how much of each wave is present in the function.\nNext, we‚Äôll explore how to actually compute those coefficients using integrals.\nLet‚Äôs keep going.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 5 - Slide28.txt", "file_path": "Lecture 5\\Texts\\Slide28.txt", "content": "Before that, let‚Äôs talk about harmonics ‚Äî a term you‚Äôve probably heard in music, physics, or signal processing. In the context of Fourier series, harmonics refer to these beautifully smooth sinusoidal components, each oscillating at a different frequency.\nLet‚Äôs unpack that.\n\nThe red curve you see here is the first harmonic, or the fundamental frequency. It completes exactly one full oscillation over the interval from 0 to 1. This is what we call the n equals 1 term in the Fourier series.\nNow move down to the blue curve ‚Äî that‚Äôs the second harmonic, where n equals 2. It oscillates twice as fast ‚Äî so you get two full cycles in the same interval.\nThen the green one, with n equals 3, completes three full oscillations, and so on. As you move further down the harmonic ladder ‚Äî to n equals 4, n equal 5, and beyond ‚Äî the frequency keeps increasing, and the wave becomes more and more tightly packed.\n\nThis is the key idea behind the Fourier series: you build complex functions using simple, smooth building blocks. And those blocks are just sine and cosine waves with integer frequencies.\nHere‚Äôs something subtle but very important: look at how all these harmonic components start and end at the same value ‚Äî specifically, at zero. Not only that, but their rate of change ‚Äî or slope ‚Äî at the beginning and end is also smoothly matched. That‚Äôs what allows them to blend so seamlessly when we stack them.\n\nWhy does this matter?\nBecause when you‚Äôre representing a periodic function, you want the end of one period to flow naturally into the start of the next. If there‚Äôs a mismatch ‚Äî either in value or in slope ‚Äî you get a jump, or a kink, which breaks the smoothness of the function. The harmonics are special because they ensure everything lines up perfectly ‚Äî not just in position, but in motion.\n\nThat‚Äôs why we call these functions harmonic. They don‚Äôt just oscillate ‚Äî they cooperate. And together, they can mimic anything from a square wave to the shape of a human voice, as long as you choose the right amplitudes and phases.\nThis is the genius of Fourier‚Äôs insight: no matter how jagged or irregular a periodic signal may appear, you can always think of it as being composed of pure tones ‚Äî harmonics ‚Äî each contributing its rhythm to the total pattern.\nAlright, now that we've built some intuition about harmonics, let's move on to the mathematical tools that allow us to compute their coefficients precisely.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 5 - Slide29.txt", "file_path": "Lecture 5\\Texts\\Slide29.txt", "content": "Alright, let‚Äôs take a closer look at how the Fourier series works over one complete cycle ‚Äî that is, over the interval from zero to one.\nAs we‚Äôve discussed, any well-behaved function on this interval ‚Äî meaning it‚Äôs square-integrable ‚Äî can be broken down into three types of components.\nFirst, we have what‚Äôs called the DC component ‚Äî written as ‚Äúa naught divided by two.‚Äù\n\u000bThis term represents the average value of the function across the interval. It‚Äôs constant ‚Äî it doesn‚Äôt vary or oscillate. You can think of it as the baseline level. For example, in an image, this might represent the overall brightness. In audio, it could be the steady background hum.\n\nThen we have the even parts ‚Äî these are built using cosine waves.\u000bEach term looks like ‚Äúa n times cosine of two pi n t,‚Äù where n is a positive integer: one, two, three, and so on.\u000bCosine is symmetric ‚Äî if you flip it left to right, it looks exactly the same. That‚Äôs why we call these even components ‚Äî they preserve that mirror-like symmetry.\nNext come the odd parts ‚Äî and these use sine waves.\u000bEach term is of the form ‚Äúb n times sine of two pi n t.‚Äù\u000bUnlike cosine, sine flips sign when you reflect it, so it captures the antisymmetric ‚Äî or odd ‚Äî behavior of the function.\nSo to sum up:\u000bWe build the full function using a constant term, plus a sum of cosine waves for the even part, and a sum of sine waves for the odd part. All of these components are weighted by their respective coefficients ‚Äî a naught, a n, and b n.\n\nAnd here‚Äôs the elegant part:\u000bBecause we‚Äôre working with periodic functions, once you‚Äôve reconstructed the signal over that one interval ‚Äî from zero to one ‚Äî the rest of the function just repeats automatically. It‚Äôs like tiling the pattern across the entire line.\n\nYou can see that idea illustrated in the image at the bottom of the slide.\u000bThe red waveform shows the reconstructed signal, and the lighter blue curves are the sine and cosine waves that contribute to it. When combined properly, they match the shape of the original function ‚Äî and then extend it periodically to the left and right.\nThis kind of decomposition isn‚Äôt just theoretical ‚Äî it has real meaning.\nThat constant term may represent a background level.\nThe even parts can capture symmetric patterns in your data.\nAnd the odd parts can highlight asymmetries, like sharp edges or sudden transitions.\n\nSo the Fourier series gives us a powerful, structured way to describe a function using just smooth, well-behaved waves ‚Äî each one tied to a specific frequency and type of symmetry.\nAlright, now let‚Äôs move on and see how we can compute these coefficients ‚Äî the values of a naught, a n, and b n ‚Äî using projection. That‚Äôs coming next.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 30, "slide_filename": "Slide30.txt", "slide_annotation": "Lecture 5 - Slide30.txt", "file_path": "Lecture 5\\Texts\\Slide30.txt", "content": "Before that, here‚Äôs a really elegant and surprisingly powerful idea in mathematical analysis ‚Äî something that‚Äôs often overlooked, but incredibly useful in practice.\nNo matter what function you're dealing with ‚Äî call it f of x ‚Äî you can always split it into two parts:\u000ban even part, and an odd part.\n\nAnd here‚Äôs how that works.\nWe define the even part, denoted f e of x, as\u000b‚Äúf of x plus f of negative x, all divided by two.‚Äù\nMathematically, that‚Äôs:\u000bf e of x equals one-half times the sum of f of x and f of minus x.\nIf you plug in negative x into this expression, you‚Äôll get:\u000bf e of minus x equals one-half times f of minus x plus f of x ‚Äî\u000bwhich is exactly the same as f e of x.\u000bSo this part is symmetric ‚Äî it satisfies the condition of an even function.\n\nThen, we define the odd part, written as f o of x,\u000bby subtracting instead of adding. So you get:\u000bf o of x equals one-half times f of x minus f of negative x.\nNow, if you evaluate this at negative x, you‚Äôll find that:\u000bf o of negative x equals negative f o of x.\u000bThat‚Äôs the signature of an odd function ‚Äî it‚Äôs antisymmetric about the origin.\n\nSo here‚Äôs the beauty:\u000bEvery function, no matter how it looks, can be written as the sum of its even and odd parts.\nIn other words,\u000bf of x equals f e of x plus f o of x.\nThis is a universal decomposition ‚Äî and it's not just a neat identity.\u000b\nIt plays a critical role in understanding symmetry in signals, especially in Fourier analysis, where even and odd functions relate directly to cosine and sine components, respectively.\nSo remember this trick.\u000bWhenever you‚Äôre working with a function ‚Äî especially in signal processing or image analysis ‚Äî it‚Äôs often helpful to ask:\u000bWhat‚Äôs its even part? What‚Äôs its odd part?\u000bAnd how can we take advantage of that symmetry?\nWe‚Äôll put this idea to use shortly when we calculate the Fourier coefficients.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 31, "slide_filename": "Slide31.txt", "slide_annotation": "Lecture 5 - Slide31.txt", "file_path": "Lecture 5\\Texts\\Slide31.txt", "content": "Now that we understand the structure of the Fourier series ‚Äî with its constant term, cosine terms, and sine terms ‚Äî the next question is:\u000b\nHow do we actually calculate the coefficients?\nThese coefficients ‚Äî which we call a n and b n ‚Äî are essential.\u000bThey tell us exactly how much of each sine or cosine function we need in order to construct the original function.\u000bThink of them like weights ‚Äî how much of each frequency should be included in our series.\nSo, how do we find them?\n\nThe answer is beautifully simple ‚Äî we use inner products.\nRemember our earlier discussion: in vector spaces, you can isolate the contribution of a basis vector by projecting onto it.\u000bThe same logic applies here in the world of functions.\u000bBecause sine and cosine functions are orthogonal, we can project the function onto each basis element to extract the corresponding coefficient.\n\nLet‚Äôs look at this process in the context of the unit interval ‚Äî from zero to one.\nFirst, to get the a naught coefficient ‚Äî that‚Äôs the DC or constant component ‚Äî\u000bwe take the inner product of f of t with the constant function 1.\n\nThis gives us:\na naught over 2 equals the integral from 0 to 1 of f of t d t.\nSo, it‚Äôs just the average value of the function over that interval.\nNext, for the cosine coefficients, a n,\u000bwe multiply f of t by the cosine of 2 pi n t, and integrate over the interval from 0 to 1.\u000bThen, we multiply the result by 2.\n\nSo we have:\na n equals 2 times the integral from 0 to 1 of f of t times cosine of 2 pi n t, d t.\nIn the same way, to compute the sine coefficients, b n,\u000bwe do the same thing ‚Äî but using sine instead of cosine.\n\nSo the formula is:\nb n equals 2 times the integral from 0 to 1 of f of t times sine of 2 pi n t, d t.\nAnd that‚Äôs it.\nEach coefficient is computed by projecting the function onto its corresponding basis function ‚Äî either constant, cosine, or sine.\nThis is the beauty of orthogonality:\u000bEach projection is independent of the others. So we can break the function into clean, non-overlapping components.\nIt‚Äôs like using X, Y, and Z axes in 3D space to describe a vector ‚Äî but here we‚Äôre in an infinite-dimensional space, and our axes are made of smooth waveforms.\nNow that we know how to compute these coefficients, we‚Äôre ready to apply Fourier series to real examples ‚Äî and see how they reconstruct signals, even sharp or irregular ones, using only smooth sine and cosine waves.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 32, "slide_filename": "Slide32.txt", "slide_annotation": "Lecture 5 - Slide32.txt", "file_path": "Lecture 5\\Texts\\Slide32.txt", "content": "Let‚Äôs now walk through a concrete example to see how Fourier coefficients are actually computed in practice.\n\nSuppose you‚Äôre interested in finding a particular coefficient from the Fourier series ‚Äî let‚Äôs say the coefficient b 3.\u000bThis is the number that scales the sine term with frequency 3 ‚Äî in other words, sine of 2 pi times 3 t.\n\nSo the question is:\u000bHow much of this specific sine wave is present in the function f of t?\nTo answer this, we use a technique we've now seen several times ‚Äî the inner product.\n\nHere‚Äôs what we do:\u000bWe take the inner product of f of t with sine of 2 pi times 3 t.\nNow keep in mind ‚Äî this sine function is one of our basis elements.\u000bIt‚Äôs a fixed, known waveform. The function f of t is the one we‚Äôre analyzing.\u000bAnd because the basis functions are orthonormal, a wonderful thing happens:\n\nWhen we take this inner product, it acts like a filter.\nIt scans across the entire Fourier series of f of t, and picks out just the piece that exactly matches sine of 2 pi times 3 t.\nAll the other terms ‚Äî the constant term, the cosine terms, and the sine terms with different frequencies like\u000bsine of 2 pi t, or sine of 2 pi times 2 t, or sine of 2 pi times 4 t ‚Äî\u000ball of those vanish. Their inner products are zero, because they‚Äôre orthogonal to sine of 2 pi times 3 t.\nThe only term that survives is the one we care about ‚Äî the one that involves b 3.\nSo this inner product tells us, precisely and cleanly, what b 3 must be.\nAnd this isn‚Äôt just a lucky trick.\u000b\nIt‚Äôs a fundamental feature of the Fourier basis ‚Äî\u000bOrthogonality ensures that each coefficient can be extracted independently, with no interference from the others.\nIn short:\u000bTo find any Fourier coefficient ‚Äî whether it‚Äôs a n or b n ‚Äî\u000bYou simply project the function onto the corresponding sine or cosine basis function.\u000bThat‚Äôs it.\nAnd this is what makes the Fourier series so powerful.\u000bIt gives us a systematic, elegant way to peel apart the frequency content of a signal ‚Äî one layer at a time.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 33, "slide_filename": "Slide33.txt", "slide_annotation": "Lecture 5 - Slide33.txt", "file_path": "Lecture 5\\Texts\\Slide33.txt", "content": "Let‚Äôs now practically apply this idea ‚Äî focusing specifically on how we calculate individual Fourier coefficients.\n\nSuppose we want to compute the coefficient b3 from the Fourier series of some known function ‚Äî say f of t.\nWhat exactly do we do?\nWell, we take the sine function, sine of 2 pi times 3 t, which is one of our orthonormal basis functions, and we compute its inner product with f of t.\u000bThat means we integrate the product of f of t and sine of 2 pi times 3 t over the interval from 0 to 1.\nThat‚Äôs it.\u000b\nThe result of this integral is exactly the value of b 3.\u000bNo guessing. No curve fitting. Just a clean calculation using integration.\nAnd this same approach works for any other sine coefficient.\u000bTo compute b n, you use the sine of 2 pi n t as your basis function, multiply it by f of t, and integrate from 0 to 1.\nThe same logic applies to the cosine coefficients ‚Äî the a n terms.\u000bTo find those, you simply take f of t and compute its inner product with cosine of 2 pi n t.\u000bEach integral gives you one specific Fourier coefficient.\n\nNow why does this work so beautifully?\nIt‚Äôs all thanks to the orthonormality of the sine and cosine functions.\nWhen we take an inner product between f of t and any basis function ‚Äî say, sine of 2 pi times 3 t ‚Äî\u000bonly the matching sine term in the Fourier expansion contributes to the result.\u000b\nAll the other basis functions ‚Äî like the cosines, or sine terms with different frequencies ‚Äî are orthogonal to this one,\u000bso their inner products are zero and they vanish from the computation.\nThis is exactly like projecting a vector in three-dimensional space.\u000b\nIf you want the x-component of a vector, you project it onto the x-axis.\u000bFor the y-component, you project onto the y-axis.\u000bSame idea here ‚Äî but now, instead of axes, we have sine and cosine waves.\u000bAnd instead of finite dimensions, we‚Äôre working in an infinite-dimensional function space.\nSo again, orthonormality makes everything work cleanly.\u000bIt gives us a precise, mathematical way to extract each component of a function in the Fourier basis.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 34, "slide_filename": "Slide34.txt", "slide_annotation": "Lecture 5 - Slide34.txt", "file_path": "Lecture 5\\Texts\\Slide34.txt", "content": "Now we arrive at a full summary of the real form of the Fourier series.\nWhat we‚Äôve seen so far is that many well-behaved functions ‚Äî especially those that are continuous and square-integrable ‚Äî can be expressed as a weighted sum of sine and cosine waves.\u000bAnd this formula at the top captures that structure precisely.\n\nWe start with the function f of t.\u000bThen we write it as the sum of three components:\nFirst, the DC component, which is a naught over 2. That‚Äôs the constant or average value of the function ‚Äî the non-oscillating part.\nNext comes the cosine sum, where each term involves a coefficient a n times cosine of 2 pi n t.\nAnd finally, we have the sine sum, where each term includes b n times sine of 2 pi n t.\nThese coefficients ‚Äî a naught, a n, and b n ‚Äî are what determine how much of each sine or cosine wave contributes to the reconstruction of f of t.\nSo how do we find them?\n\nWell, we already know the answer from earlier in the lecture: we compute inner products.\u000bThat is, we take the integral of the product of our function f of t with each basis function, over the interval from 0 to 1.\nFor the constant term, a naught over 2, we simply integrate f of t from 0 to 1.\nTo get a n, we multiply f of t by the cosine of 2 pi n t and integrate.\nFor b n, we do the same with the sine of 2 pi n t.\n\nOnce you have these values, you plug them back into the series.\u000bAnd when you sum it all up ‚Äî the constants, the cosines, and the sines ‚Äî you recreate your original function, or at least approximate it very closely.\nIt‚Äôs almost magical.\u000bYou could be working with a smooth parabola, or a sharp triangular wave, or even a jagged, piecewise function ‚Äî and yet, you can reconstruct it entirely using just sine and cosine waves. That‚Äôs the power of the Fourier series.\n\nNow, remember earlier in the course we looked at functions in terms of discrete particles or impulses ‚Äî like pixel-based representations. That gave us a kind of particle view of functions.\nBut this approach gives us a wave-based view ‚Äî continuous, smooth, and grounded in frequency content.\nBoth are valid. And each is powerful in its own context.\n\nBut here‚Äôs a question worth asking:\u000bWhy would we ever want to use the complex form of the Fourier series?\nWell, look at this real version. It‚Äôs elegant, yes ‚Äî but it involves three separate types of terms: one for the DC component, one for cosine, and one for sine.\nBy switching to the complex form, we can merge all of this into a single sum using Euler‚Äôs formula, where complex exponentials capture both sine and cosine behavior simultaneously.\nThis makes the whole formulation more compact and symmetrical.\u000b\nThe math becomes cleaner. The algebra gets easier. And in engineering applications, it often leads to more efficient computations.\nSo while the real form gives us intuitive clarity, the complex form offers analytical elegance.\nThat‚Äôs exactly where we‚Äôre headed next.\nLet‚Äôs now explore how complex numbers help us take the Fourier series to the next level.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 35, "slide_filename": "Slide35.txt", "slide_annotation": "Lecture 5 - Slide35.txt", "file_path": "Lecture 5\\Texts\\Slide35.txt", "content": "Now let‚Äôs take a step into the world of complex numbers, because they offer a beautifully compact and powerful way to handle Fourier analysis.\n\nWe begin with something that might feel a little strange at first ‚Äî the definition of the imaginary unit, denoted by the letter i.\u000b\nBy definition, i squared equals negative one. That is:\ni squared equals minus one.\n\nNow, this seems counterintuitive, right? Because in the world of real numbers, squaring anything always gives you something non-negative. But here, we're stepping beyond the real line, into a new dimension ‚Äî one that allows us to define numbers that include this imaginary unit.\nAnd that opens the door to complex numbers.\nA complex number, typically written as z, is defined as:\nz = x plus i times y,\nwhere x is the real part, and y is the imaginary part.\u000bGeometrically, you can think of this as a point on the complex plane ‚Äî with x on the horizontal axis and y on the vertical axis.\nNow, for every complex number, we can define something called its conjugate.\u000bIf z is x plus i times y, then the conjugate of z, written with a bar over it, is:\nx minus I times y.\n\nThis is like flipping the point across the real axis ‚Äî mirroring it vertically.\nNow, let‚Äôs look at how we add and multiply complex numbers.\u000bThe rules are very similar to regular algebra ‚Äî you just have to remember that i squared is negative one. That‚Äôs the only twist.\nSo, for instance, if we add two complex numbers, we just add their real parts and their imaginary parts separately.\u000bIf we multiply them, we expand the product just like you would with binomials ‚Äî but we substitute i squared with minus one when it appears.\nFrom here, we can also extract the real and imaginary parts of a complex number using its conjugate:\nThe real part is the average of the number and its conjugate ‚Äî that‚Äôs z plus z bar divided by two.\nThe imaginary part is the difference between the number and its conjugate, divided by 2 i.\n\nNow, you might be wondering ‚Äî how does all this tie back to Fourier analysis?\nWell, just as we defined inner products for real-valued functions, we can do the same for complex-valued functions.\u000bAnd when we do, we often need to deal with conjugates inside integrals.\n\nHere‚Äôs the rule:\u000bIf you take the complex conjugate of an integral ‚Äî say, the integral of f of t times g of t ‚Äî that‚Äôs equal to the integral of f conjugate times g conjugate.\nThis is especially important when we compute energy, projection, or coefficients in complex signal spaces.\n\nAlso, in engineering and physics, we frequently use the star notation ‚Äî an asterisk ‚Äî to indicate conjugation. So if you see a star on the outside of an integral, that means we‚Äôre conjugating the whole result.\u000bLikewise, when it‚Äôs applied to the functions inside, it means we're conjugating those functions individually.\nSo why go through all of this?\nBecause working with complex numbers ‚Äî especially when combined with Euler‚Äôs formula, which we‚Äôll see shortly ‚Äî allows us to unify sine and cosine into a single, elegant expression. That‚Äôs a huge simplification.\n\nAnd that‚Äôs the real power of this approach:\u000bCleaner math, fewer terms, and deeper symmetry.\nSo with that, we‚Äôre now ready to look at the complex form of the Fourier series, which brings everything we've learned into one compact representation.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 36, "slide_filename": "Slide36.txt", "slide_annotation": "Lecture 5 - Slide36.txt", "file_path": "Lecture 5\\Texts\\Slide36.txt", "content": "When we work with complex numbers, there are two very common ways to describe them:\u000bthe Cartesian form and the polar form.\u000bBoth are fully equivalent, and they each offer a different way of thinking about the same quantity.\n\nLet‚Äôs begin with the Cartesian form. This is what we‚Äôve seen already:\u000bA complex number is written as x plus i times y, where x is the real part, and y is the imaginary part.\u000bSo we‚Äôre essentially giving the horizontal and vertical coordinates of a point in the complex plane ‚Äî just like specifying a location on a map.\n\nNow, in engineering, we often replace the symbol i with j to represent the imaginary unit. This is purely to avoid confusion ‚Äî because in electrical engineering, i is usually reserved for current.\u000bBut mathematically, i and j mean the same thing: they both satisfy the fundamental property that i squared equals negative one.\nThe other way to describe a complex number is through polar coordinates.\u000bInstead of saying how far right and up the point is, we describe the number by its magnitude and angle.\nSo what does that mean?\n\nWe draw a line from the origin to the point ‚Äî that‚Äôs the complex number ‚Äî and call its length r. That‚Äôs the magnitude, or modulus, and it‚Äôs calculated using the Pythagorean theorem:\u000br equals the square root of x squared plus y squared.\u000bThis is also written as the absolute value of z.\n\nNext, we describe the angle that line makes with the positive real axis.\u000bThat angle is called theta, and we compute it as the inverse tangent of y over x.\u000bSo now, we have two pieces: r and theta ‚Äî the magnitude and direction.\nWith that, we can now express the same complex number in polar form.\nIn place of x plus i y, we write:\nr times cosine theta plus i times r sine theta.\u000bOr, factoring out the r:\u000br times the quantity cosine theta plus i sine theta.\n\nThis is the foundation for something beautiful: Euler‚Äôs formula, which we‚Äôll talk about in the next slide.\u000bEuler discovered that cosine theta plus i sine theta is equal to e to the i theta ‚Äî giving us a powerful exponential representation of complex numbers.\nBefore we leave this slide, here‚Äôs one more important result:\u000bIf you multiply a complex number by its conjugate ‚Äî in other words, z times z bar ‚Äî you get a purely real number.\n\nLet‚Äôs walk through it:\u000bz is x plus i y, and its conjugate is x minus i y.\u000bWhen you multiply them together, you get x squared plus y squared, which is just the magnitude squared.\nThis shows that the conjugate cancels out the imaginary part, leaving only a real value ‚Äî something very useful in analysis and signal processing.\nSo just like vectors, complex numbers let us switch back and forth between rectangular and polar systems.\u000b\nEach form brings its own advantages, and we‚Äôll soon see how this flexibility pays off ‚Äî especially when we start rewriting Fourier series using complex exponentials.\nLet‚Äôs move to Euler‚Äôs formula and take this further.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 37, "slide_filename": "Slide37.txt", "slide_annotation": "Lecture 5 - Slide37.txt", "file_path": "Lecture 5\\Texts\\Slide37.txt", "content": "So far, we‚Äôve been casually treating the inner product as a simple dot product ‚Äî point-by-point multiplication followed by a sum. That works just fine when we‚Äôre dealing with real-valued vectors.\nBut when we step into complex vector spaces, things get a bit more subtle. In particular, we have to be careful with how we define the inner product. In the complex world, it‚Äôs not enough to just multiply and add ‚Äî we also need to take the complex conjugate of one of the vectors before doing so.\nLet‚Äôs break this down with a simple example.\n\nSuppose we have two complex vectors. Let‚Äôs call them X and Y.\nVector X has two components:\u000bthe first is a one plus b one times i,\u000band the second is a two plus b two times i.\nSimilarly, vector Y has components:\u000bc one plus d one times i,\u000band c two plus d two times i.\nNow, how do we compute the length of vector X?\n\u000bIn complex space, we use the sum of the squared magnitudes of each component. So the magnitude of X is the square root of\u000ba one squared plus b one squared plus a two squared plus b two squared.\nWe do the same for vector Y ‚Äî square the real and imaginary parts of each component, add them up, and then take the square root.\n\nNow, here‚Äôs the key idea.\nIf these two complex vectors ‚Äî X and Y ‚Äî are orthogonal, and we add them together to form a new vector Z, we expect the squared length of Z to equal the squared length of X plus the squared length of Y. That‚Äôs just the Pythagorean theorem in vector space.\nBut for that identity to hold, one critical condition must be met:\u000bthe cross-term ‚Äî the inner product between X and Y ‚Äî must vanish. In other words, the inner product between X and Y has to be zero.\nAnd this is where the complex conjugate becomes important.\n\nIn real vector spaces, we just take the dot product. But in complex spaces, we define the inner product as:\u000bX dot Y conjugate.\nThat means, we keep vector X as it is, and we take the complex conjugate of each component of Y before multiplying.\nThis adjustment ensures that everything works out mathematically:\u000bwe preserve orthogonality, we maintain proper lengths, and we uphold the geometry of the space.\nSo, whenever we‚Äôre working with complex vectors ‚Äî in signal processing, in quantum mechanics, or in Fourier analysis ‚Äî this conjugation step is absolutely essential.\n\nNow, don‚Äôt worry about memorizing the formulas. That‚Äôs why I added this green bubble here ‚Äî it‚Äôs just a reminder that this is more about understanding the concept than remembering every detail.\nOnce you internalize this idea ‚Äî that conjugation is needed to define meaningful inner products in complex space ‚Äî everything that follows, including the complex form of the Fourier series, will feel much more natural.\nLet‚Äôs now move forward and apply this idea in the Fourier domain.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 38, "slide_filename": "Slide38.txt", "slide_annotation": "Lecture 5 - Slide38.txt", "file_path": "Lecture 5\\Texts\\Slide38.txt", "content": "Now that we‚Äôve introduced the concept of inner products for complex vectors, let‚Äôs take a closer look at how the definition actually works ‚Äî and more importantly, why it includes a conjugate.\n\nIn a real-valued vector space, things are simple. You multiply corresponding components and sum the results. But in a complex space, if we follow that same rule ‚Äî just multiplying and summing ‚Äî we may end up with a complex number. And that‚Äôs a problem, because when we‚Äôre measuring things like length, or comparing angles between vectors, we need real values.\n\nSo here‚Äôs what we do instead.\nWe define the inner product of two complex-valued functions, let‚Äôs say f of t and g of t, by integrating the product of f of t and the complex conjugate of g of t.\nSymbolically, we write:\u000b‚ü®f of t, g of t‚ü© equals the integral from minus infinity to infinity of f of t times g star of t, d t.\n\nThat‚Äôs our general definition. But often, we work over a specific interval ‚Äî for example, from zero to one. In that case, we write:\u000b‚ü®f of t, g of t‚ü© equals the integral from 0 to 1 of f of t times g star of t, d t.\n\nNow, why is this conjugation so important?\nWell, imagine taking the inner product of a function with itself. We‚Äôd expect that result to be real and non-negative ‚Äî something that represents the square of the function‚Äôs length or its energy. But without the conjugate, we might get a complex number, which doesn‚Äôt make physical sense in that context.\nThe conjugate takes care of that. It ensures the phase ‚Äî or directional twist ‚Äî of the complex function is canceled out. What‚Äôs left is the amplitude ‚Äî the magnitude ‚Äî and that‚Äôs what we care about when computing lengths or measuring how much two functions align.\n\nAnother way to see this is: the conjugate helps us ignore the phase and focus purely on the size or strength of the signal.\nSo this isn‚Äôt just a technical tweak ‚Äî it‚Äôs what makes the geometry of complex spaces work. Thanks to the conjugate, we preserve all the essential properties:\u000borthogonality, projection, the norm ‚Äî and more.\nSo when we say two complex-valued functions are orthogonal, we mean that their inner product ‚Äî including the conjugate ‚Äî is zero. That‚Äôs the complex equivalent of vectors being at right angles.\nThis is why conjugates appear everywhere in Fourier analysis and other areas involving complex functions. They ensure consistency and meaning across the entire framework.\n\nSo, to summarize:\u000bIn complex function spaces, an inner product is defined using integration with conjugation.\u000bThat‚Äôs the key idea ‚Äî and it makes everything else fall neatly into place.\nUp next, we‚Äôll use this inner product to compute the complex Fourier coefficients.\nLet‚Äôs move forward.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 39, "slide_filename": "Slide39.txt", "slide_annotation": "Lecture 5 - Slide39.txt", "file_path": "Lecture 5\\Texts\\Slide39.txt", "content": "All right, now that we've got a solid understanding of complex numbers, let‚Äôs talk about one of the most beautiful and powerful results in mathematics‚ÄîEuler‚Äôs Formula.\n\nLet‚Äôs start with the exponential function. You might remember that the exponential of a number can be written as an infinite series. For any complex number z, we can write:\n‚Äúe to the z equals one, plus z over one factorial, plus z squared over two factorial, plus z cubed over three factorial, and so on.‚Äù\nIn math terms, that‚Äôs:\n‚ÄÉ‚ÄÉe to the z equals the sum from n equals zero to infinity of z to the n over n factorial.\nThis series works not only for real numbers, but also for complex numbers. And that‚Äôs important.\nHow do we know this infinite sum still makes sense when z is complex? Well, there‚Äôs something called the ratio test, which shows that this series converges no matter what complex number we use. In other words, it always gives a valid result. That‚Äôs why we can define e to the z for any complex number z. That‚Äôs a big deal.\n\nNow, here‚Äôs where things get really exciting.\nWhat if we plug in a purely imaginary number, like i times theta, into this exponential?\nIt turns out that:\n‚ÄÉ‚ÄÉe to the i theta equals cosine theta plus i times sine theta.\nThat‚Äôs Euler‚Äôs formula.\n\nThis equation is incredibly elegant. It shows how exponential functions and trigonometric functions‚Äîtwo ideas that seem totally different‚Äîare actually deeply connected.\nAnd here‚Äôs something even more useful. If you rearrange Euler‚Äôs formula a bit, you can write cosine and sine in terms of complex exponentials.\n\nSpecifically:\n‚ÄÉ‚ÄÉCosine theta equals e to the i theta plus e to the minus i theta, divided by two.\n‚ÄÉ‚ÄÉSine theta equals e to the i theta minus e to the minus i theta, divided by two i.\nThese two identities are incredibly helpful, especially in signal processing and Fourier analysis. They let us replace sines and cosines with exponentials, which makes the math much cleaner and often easier to compute.\n\nSo to sum up:\nEuler‚Äôs formula is a bridge between the world of waves‚Äîlike sine and cosine‚Äîand the world of exponentials. And as we‚Äôll see next, that bridge is exactly what we need to rewrite the Fourier series in a compact, exponential form.\nLet‚Äôs move forward and take a look at that next.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 40, "slide_filename": "Slide40.txt", "slide_annotation": "Lecture 5 - Slide40.txt", "file_path": "Lecture 5\\Texts\\Slide40.txt", "content": "Now that we‚Äôve built up our understanding of Euler‚Äôs formula, we‚Äôre ready to transition from the real form of the Fourier series to the complex form.\n\nLet me start by reminding you of the real version. For a periodic function, f of t, we can write it as:\n‚ÄÉ‚ÄÉ\"a zero divided by two, plus the sum over n of a n times cosine of two pi n t, plus b n times sine of two pi n t.\"\nThat‚Äôs the standard real Fourier series form. We‚Äôve seen that it works well, but it requires us to carry around both sine and cosine terms‚Äîand two separate sets of coefficients.\n\nNow here‚Äôs the trick: using Euler‚Äôs formula, we can rewrite sine and cosine using complex exponentials.\nFor example, cosine of two pi n t becomes:\n‚ÄÉ‚ÄÉ\"e to the power i times two pi n t, plus e to the power negative i times two pi n t, all divided by two.\"\nAnd sine of two pi n t becomes:\n‚ÄÉ‚ÄÉ\"e to the i two pi n t, minus e to the negative i two pi n t, all divided by two i.\"\n\nSo what‚Äôs the point of doing this?\nWell, once we‚Äôve written everything in terms of exponentials, we can merge the sine and cosine parts into one unified expression. Everything now becomes a sum of complex exponentials‚Äîno separate sine and cosine terms needed.\n\nEven more interesting, these exponential functions‚Äîlike e to the i two pi n t‚Äîwhere n is any integer, actually form an orthonormal basis.\nThat means: each function has unit length, and all the functions are mutually orthogonal. If you take the inner product of e to the i two pi m t and e to the i two pi n t, the result is:\n‚ÄÉ‚ÄÉ\"one, if m equals n; and zero, if m is not equal to n.\"\nThis elegant structure is one of the biggest reasons we prefer the complex form in many applications. It makes the math cleaner, the derivations simpler, and the overall representation more compact.\n\nSo in summary: by converting sine and cosine into exponentials using Euler‚Äôs identity, we arrive at a cleaner and more powerful way to express Fourier series. And this sets the stage for everything that follows in frequency analysis and Fourier transforms.\nComing up next, we‚Äôll write down the full complex form of the Fourier series.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 41, "slide_filename": "Slide41.txt", "slide_annotation": "Lecture 5 - Slide41.txt", "file_path": "Lecture 5\\Texts\\Slide41.txt", "content": "Let‚Äôs take a moment now to fully understand what we mean by an orthonormal basis, especially when we‚Äôre working with complex exponentials.\n\nWe define a family of functions‚Äîdenoted e n of t‚Äîwhere each function is written as:\u000b‚ÄÉ‚ÄÉ\"e to the power two pi i n t.\"\u000bHere, n is any integer, and t is in the interval from zero to one.\u000b\nNow, to confirm that these functions actually form an orthonormal basis, we‚Äôll compute the inner product between two of them. Let‚Äôs say we have e n t and e m t, and n is not equal to m.\nSince we‚Äôre in a complex function space, remember that we have to conjugate the second function when we compute the inner product.\u000bSo, the inner product between e n and e m is:\u000b‚ÄÉ‚ÄÉ\"The integral from zero to one of e to the two pi i n t, times the complex conjugate of e to the two pi i m t.\"\u000b\nNow, taking the conjugate of an exponential just flips the sign in the exponent. So this product becomes:\u000b‚ÄÉ‚ÄÉ\"e to the power two pi i times the quantity n minus m, all times t.\"\u000bWe‚Äôre now integrating this from zero to one.\u000bWhat happens?\nWell, if n is not equal to m, this exponential is rotating in the complex plane over the entire interval, and the total contribution over one period averages out to zero.\u000bSo, in that case, the inner product is zero. This confirms that the functions are orthogonal when n and m are different.\u000b\n\nNow, what if n equals m?\u000bThen the exponent becomes zero, so we‚Äôre simply integrating the constant function one, from zero to one. That gives us a value of one.\u000bSo this confirms that the functions are normalized when n equals m.\u000bPutting both of these observations together, we have the classic orthonormality condition:\u000b‚ÄÉ‚ÄÉ\"The inner product of e n and e m is equal to one if n equals m, and zero otherwise.\"\u000bThat‚Äôs exactly what we expect from an orthonormal basis.\n\nThese complex exponential functions form the mathematical backbone of the Fourier series. Each function is like a ‚Äúbuilding block‚Äù that carries a particular frequency, and together they allow us to represent almost any function on the interval from zero to one.\u000bThis structure is elegant, efficient, and deeply powerful.\u000bAnd in the next step, we‚Äôll use these building blocks to express the complex form of the Fourier series.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 42, "slide_filename": "Slide42.txt", "slide_annotation": "Lecture 5 - Slide42.txt", "file_path": "Lecture 5\\Texts\\Slide42.txt", "content": "Now that we‚Äôve built the full foundation, we‚Äôre finally ready to write down the complex form of the Fourier series.\u000bInstead of expressing our function f of t using separate sine and cosine terms, we now use complex exponentials as the basis functions. And the result is both beautiful and powerful.\u000bWe write:\u000b\"f of t equals the sum, from n equals negative infinity to positive infinity, of c n times e to the power i two pi n t.\"\u000bSo here, each term in the sum is a complex sinusoid at frequency n. And the corresponding coefficient, c n, tells us how much of that frequency is present in the signal.\u000b\nNow, how do we find each coefficient?\u000bBecause our basis functions are orthonormal, we can isolate any one coefficient simply by taking the inner product of f with the exponential function e to the i two pi n t. That gives us:\u000b\"c n equals the inner product of f of t and e to the i two pi n t,\"\u000b\"which equals the integral from zero to one of f of t times e to the negative i two pi n t, with respect to t.\"\u000bNotice here that minus sign in the exponent. That comes from taking the complex conjugate inside the inner product.\u000bThis formula is incredibly useful‚Äîit gives us a direct way to compute each coefficient without interference from the others.\u000b\nNow, if you recall the real Fourier series with cosine and sine terms, and if you rewrite cosine and sine using Euler‚Äôs formulas, and substitute them back into the real series, you‚Äôll get the same expression‚Äîbut now in terms of complex exponentials.\u000bAnd the coefficients come out as:\u000bFor non-zero n,\u000b\"c n equals a n over two, minus i times b n over two.\"\u000bFor n equals zero,\u000b\"c zero equals a zero over two.\"\u000b\nSo the complex coefficients still capture the exact same information as a n, b n, and a naught‚Äîbut now in a more compact and unified form.\u000bAnd finally, if your function f of t is real-valued, then the Fourier coefficients have a special property called conjugate symmetry. That is:\u000b\"The complex conjugate of c n equals c negative n.\"\u000bThis symmetry ensures that when you sum the exponentials‚Äîboth positive and negative frequencies‚Äîthe imaginary parts cancel out, and you‚Äôre left with a real-valued function.\u000b\nSo to summarize, this complex form of the Fourier series gives us a cleaner expression, easier computation, and a deeper insight into the structure of signals. It‚Äôs no surprise this form is central in many areas, from signal processing to quantum mechanics.\u000bNext, we‚Äôll build on this to explore how these complex exponentials serve as building blocks for signal decomposition and frequency-domain analysis.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 43, "slide_filename": "Slide43.txt", "slide_annotation": "Lecture 5 - Slide43.txt", "file_path": "Lecture 5\\Texts\\Slide43.txt", "content": "Let‚Äôs take a step back now and try to visualize everything we've been talking about.\nWhen we say we‚Äôre representing a function using a Fourier series, what does that mean? \n\nWell, it means we‚Äôre placing that function inside a kind of infinite-dimensional space. Each dimension in this space corresponds to a different basis function‚Äîjust like in three-dimensional space, where we have the x, y, and z axes.\nOn the left side of this slide, you see the real basis. These are made up of the constant function 1, the cosine functions like cosine of 2 pi m t, and the sine functions like sine of 2 pi n t. These real functions are all orthonormal, and together, they span the space of all square-integrable periodic functions.\n\nNow notice the red arrow labeled f of t. That red arrow represents a function. And just like any vector in 3D space, we can express this function as a sum of its components in each direction. But in our case, the directions are sine, cosine, and the constant function.\nTo figure out how much of each component is present, we project the function onto each basis function. That‚Äôs exactly what we‚Äôre doing when we compute the Fourier coefficients. We're just asking: how much of cosine is in this signal? How much of sine? And so on.\n\nNow look at the right-hand diagram. It shows the same idea, but using a complex basis instead. Here, the directions are complex exponentials like e to the i 2 pi n t. These complex exponentials are also orthonormal and form a complete basis in the complex function space.\nThe red arrow here, labeled g of t, could represent the same function as f of t. But now, it‚Äôs decomposed using complex components instead of sines and cosines.\nSo whether we use real functions or complex exponentials, the concept is the same: we're decomposing a function into its ‚Äúdirectional‚Äù components in function space‚Äîjust like how you decompose a 3D vector into its x, y, and z parts.\n\nThis visualization helps us see Fourier series not just as a bunch of formulas‚Äîbut as a geometric idea. A function is a vector, and we‚Äôre expressing it as a sum of basis vectors.\nThis is the heart of Fourier analysis.\nNext, let‚Äôs take a look at some concrete examples to see how this actually plays out in practice.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 44, "slide_filename": "Slide44.txt", "slide_annotation": "Lecture 5 - Slide44.txt", "file_path": "Lecture 5\\Texts\\Slide44.txt", "content": "Let‚Äôs wrap up this lecture by exploring a concrete and classic example: the square wave.\nWhat you see here is a periodic function, one that flips back and forth between two constant values‚Äîa positive one and a negative one.\n\nLet‚Äôs walk through the structure.\nFrom time t equals zero up to one-half, the function holds steady at plus one. Then, from one-half to one, it suddenly drops to minus one. After that, the pattern repeats. And again. And again. This continues infinitely in both directions along the time axis.\nSo overall, this is a piecewise function. Within each period, it's constant in two segments, with an abrupt jump at the halfway point. And because it repeats every unit of time, we say it has a period of one.\n\nNow, at first glance, you might think, ‚ÄúThis function is too rough for sine and cosine to handle.‚Äù It has sharp edges, and it jumps suddenly. But that‚Äôs where the power of Fourier analysis shines.\nEven though the square wave is not smooth, we can still represent it as a sum of smooth, continuous sinusoids. It‚Äôs exactly this kind of jumpy, discontinuous behavior that makes the square wave such a great test case for the Fourier series.\n\nYou‚Äôll soon see that we can approximate the square wave by adding up enough sine components. And the more terms we include, the closer the sum will come to capturing that sharp transition from plus one to minus one.\nThis example will also reveal an interesting phenomenon known as the Gibbs effect, which we‚Äôll talk about next.\nBut for now, just keep in mind: even for a simple-looking function like this square wave, Fourier series gives us a powerful and systematic way to build it up‚Äîusing only smooth building blocks.\n\nSo now let‚Äôs take the next step and compute the Fourier coefficients for our square wave.\n\nAs we saw before, the square wave alternates between plus one and minus one over each period. From zero to one-half, it‚Äôs plus one. From one-half to one, it drops to minus one. And this pattern repeats every unit interval.\n\nNow, we begin with the zeroth Fourier coefficient. This represents the average value of the function over one period. But notice: for every positive bump, there‚Äôs a negative one of equal size. So the total area above the axis cancels out the area below. That means the average value is zero.\nSo, the zeroth term disappears.\n\nNext, we compute the other Fourier coefficients, often denoted by f-hat of n. These are obtained by taking the inner product of the function with the complex exponential e to the negative 2 pi i n t.\nSince our square wave has two constant segments, we split the integral into two parts: one from 0 to one-half, where the function equals plus one, and one from one-half to 1, where the function equals minus one.\nWe integrate e to the negative 2 pi i n t over each of these intervals, then subtract them.\n\nAfter doing the math, we arrive at this compact expression:\nF hat n equals to 1 over pi i n times the quantity 1 minus e to the negative pi i n.\n\nNow this is an elegant formula. It tells us exactly how strong each frequency component is. For each integer value of n‚Äînot equal to zero‚Äîwe get a corresponding complex exponential that contributes to building up the square wave.\n\nAnd so, we write the full Fourier series as an infinite sum of these terms. That is, summing over all non-zero values of n:\nThe sum of 1 over pi i n times 1 minus e to the negative pi i n times e to the 2 pi i n t\nEach of these exponential functions corresponds to a specific frequency, and each coefficient tells us how much of that frequency is present in the signal.\nOn the next slide, we‚Äôll further simplify this result and begin to see the pattern more clearly‚Äîespecially for odd harmonics.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 45, "slide_filename": "Slide45.txt", "slide_annotation": "Lecture 5 - Slide45.txt", "file_path": "Lecture 5\\Texts\\Slide45.txt", "content": "Now let‚Äôs simplify the Fourier series we derived for the square wave.\nEarlier, we saw that the Fourier coefficients involved a term like ‚Äú1 minus e to the negative pi i n.‚Äù Let‚Äôs think carefully about that. What happens when n is even?\n\nWell, if you plug in an even number, this expression becomes zero. That means: all even-numbered terms in the series disappear completely. On the other hand, when n is odd, this term evaluates to two.\nSo what does this tell us? It tells us that the square wave is made up only of odd harmonics. That is, only the sine waves whose frequencies are odd multiples of the base frequency will appear in the final expression.\nWith this insight, we rewrite the series, summing over only the odd values of n. And since the numerator is just 2 now, the coefficient simplifies to 2 over pi i n, multiplied by e to the 2 pi i n t.\n\nNow we go one step further. We combine the terms for plus n and minus n. When we do that and apply Euler‚Äôs identity again, something beautiful happens. The sum of e to the i theta and e to the negative i theta becomes a sine function. Specifically:\ne to the 2 pi i n t minus e to the minus 2 pi i n t equals 2 i times sine of 2 pi n t\nSo now, our series, which originally looked complex and full of exponentials, becomes a clean sine series.\nWe make one last substitution. Since we‚Äôre only keeping odd values of n, we can write n as 2 k plus 1, where k runs from zero to infinity.\n\nThis gives us the final, elegant result:\nf of t equals 4 over pi, times the sum from k equals 0 to infinity, of 1 over 2 k plus 1, times sine of 2 pi times 2 k plus 1 t\nThis is the Fourier series for the square wave.\n\nNotice how only sine terms appear. That makes perfect sense, because the square wave is an odd function‚Äîit‚Äôs symmetric around the origin. And in Fourier analysis, odd functions naturally expand into sine terms, just like even functions expand into cosines.\nThis result is not only beautiful but also extremely useful. It tells us exactly how to build a square wave by layering together the right sine waves, each scaled just right.\nLet‚Äôs go on and see what this looks like when we actually approximate the square wave with a few of these terms.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 46, "slide_filename": "Slide46.txt", "slide_annotation": "Lecture 5 - Slide46.txt", "file_path": "Lecture 5\\Texts\\Slide46.txt", "content": "Let‚Äôs take a moment now to understand what happens when we approximate a function using only a finite number of Fourier terms.\nIn theory, the Fourier series uses infinitely many sine or complex exponential terms to exactly reconstruct the original function. But in practice, we can only sum up a limited number of them. So what does that mean?\n\nWell, in regions where the function is smooth and continuous, like in the middle of a flat section of the square wave, the approximation is excellent. The series follows the shape of the function very closely, and the error becomes negligible as we include more terms.\nBut notice what happens at the discontinuities‚Äîthose sudden jumps where the function goes from 1 to 1. That‚Äôs where the series begins to struggle.\n\nIf you look at the plot here, you‚Äôll see a ripple near each jump. These ripples don‚Äôt go away even if we add more and more terms. This phenomenon is known as the Gibbs phenomenon, and we‚Äôll explore it in detail shortly.\nFor now, focus on what the formula in the blue box is telling us. It describes the value that the Fourier series converges to at each point x.\nIf the function f of x is continuous at that point, then the Fourier series converges to f of x. That‚Äôs simple enough.\nBut if the function is discontinuous, then the series converges to the average of the left-hand and right-hand limits. Mathematically, that means:\nS of x equals one-half times f of x from the left, plus f of x from the right\n\nSo the series doesn‚Äôt ‚Äúmiss‚Äù the function entirely at the jump‚Äîit just lands at the midpoint. This is a subtle but important point: the Fourier series always converges to something, and that something is precisely defined.\nIn summary, finite Fourier series work very well across most of the domain, but they introduce oscillations and errors around jumps. That‚Äôs an inherent limitation of this type of approximation‚Äîand next, we‚Äôll give this behavior a name and look more closely at its mathematical structure.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 47, "slide_filename": "Slide47.txt", "slide_annotation": "Lecture 5 - Slide47.txt", "file_path": "Lecture 5\\Texts\\Slide47.txt", "content": "Now here‚Äôs something truly fascinating‚Äîand unavoidable‚Äîwhen working with Fourier series. It‚Äôs known as the Gibbs effect.\n\nLet‚Äôs revisit what we saw on the previous slide. When we try to approximate a function with a sudden jump, like our square wave, the Fourier series doesn‚Äôt quite nail the jump. Instead, it creates an oscillatory overshoot near the point of discontinuity. You can see these ripples in the red-highlighted regions on this plot.\n\nWhat‚Äôs important to understand here is that this overshoot doesn‚Äôt disappear‚Äîeven if we use hundreds or thousands of terms. Yes, the oscillations become more tightly packed, clustering closer to the jump. But the height of the overshoot‚Äîthe peak error‚Äîdoes not go away.\nIt approaches a fixed value‚Äîabout 9 percent of the jump in the function. That‚Äôs the defining feature of the Gibbs effect.\n\nSo why does this happen? Well, the Fourier series is built from smooth, continuous waves‚Äîlike sines and cosines or complex exponentials. But when we try to represent a sharp edge‚Äîa discontinuity‚Äîusing only smooth components, we hit a fundamental limit. There‚Äôs no perfect way to form a step out of waves.\nThis mismatch creates those persistent ripples. It‚Äôs not a bug in the method‚Äîit‚Äôs a deep mathematical truth.\n\nSo even though the Fourier approximation converges pointwise almost everywhere, and even in the sense of least squares, it never perfectly resolves a jump. This is especially important in applications like signal processing or image reconstruction, where sharp edges and sudden transitions often appear.\nThe takeaway is: when you see these ringing effects around edges in a Fourier approximation, you're witnessing the Gibbs effect in action.\nLet‚Äôs now bring all this together with a final reflection on what we‚Äôve learned from the square wave example.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 48, "slide_filename": "Slide48.txt", "slide_annotation": "Lecture 5 - Slide48.txt", "file_path": "Lecture 5\\Texts\\Slide48.txt", "content": "So far, we‚Äôve assumed that the function we‚Äôre analyzing is periodic with a period of one. But in real applications, that‚Äôs not always the case. The period might be two, five, or even some irrational number like pi. So how do we handle that?\n\nIt turns out the Fourier series still works perfectly‚Äîwe just need to adjust our formula to account for the actual period, which we‚Äôll call capital T.\nThe idea is simple: we scale the time variable by dividing it by T. That way, we bring the problem back into a familiar form‚Äîsimilar to what we did when the period was one.\n\nSo the Fourier series becomes:\nf of t equals the sum over all n from negative infinity to infinity of c n times e to the power i 2 pi n t over T.\n\nLet me say that again more slowly:\nWe write the function f of t as the sum of c n times e raised to the power i times two pi times n times t divided by T.\nNow what about the coefficients?\nWe compute c n using an integral over one full period. And we have two equivalent options for that:\n\nFirst option:\nc n equals one over T times the integral from zero to capital T of f of t times e to the power negative i 2 pi n t over T, integrated with respect to t.\nOr, second option:\nc n equals one over T times the integral from negative T over 2 to positive T over 2 of f of t times e to the power negative i 2 pi n t over T, again integrated with respect to t.\n\nEither form works. The only difference is the range of integration‚Äîwhether you go from zero to T or from minus T over 2 to plus T over 2.\n\nSo what‚Äôs the takeaway?\nThis is just a change of variable. We're scaling time so that the period becomes one, applying all our previous results, and then scaling back.\n\nThis gives us a general version of the Fourier series that works for any period‚Äînot just the special case where the period is one.\nAnd this flexibility is exactly what makes the Fourier series so powerful. It adapts easily to the structure of whatever signal you're analyzing.\nUp next, we‚Äôll take this idea even further and connect it to the Fourier transform, which is what happens when the period becomes infinitely large.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 49, "slide_filename": "Slide49.txt", "slide_annotation": "Lecture 5 - Slide49.txt", "file_path": "Lecture 5\\Texts\\Slide49.txt", "content": "Up to this point, everything we‚Äôve talked about has been in one dimension. But in real-world applications‚Äîespecially in imaging‚Äîwe usually work with two-dimensional data. Think about a photograph, a CT slice, or an MRI frame. These are functions of two variables, not just one.\nSo, does the Fourier series still apply? Absolutely. In fact, the transition to two dimensions is very smooth and intuitive.\n\nLet‚Äôs start with the formula at the top. This is the 2D Fourier transform, or more precisely, the 2D discrete Fourier series. It converts a spatial domain function into a frequency domain representation.\n\nWe begin with a function f of m and n. Here, m and n are the spatial coordinates, like pixel row and column indices.\nThen we compute the capital F of x and y, which is the frequency-domain representation of the image. We get it using a double summation‚Äîfrom m equals 0 to M minus 1, and from n equals 0 to N minus 1.\nEach term in the sum includes f of m and n, multiplied by a complex exponential:\u000be to the power negative j 2 pi times x times m over M plus y times n over N.\nThis is just the 2D extension of what we did before in 1D‚Äînow applied along both spatial directions.\n\nNext, let‚Äôs look at the inverse transform‚Äîthe formula at the bottom.\nHere, we reconstruct the original image, f of m and n, from its frequency components, F of x and y. Again, we use a double summation ranging from 0 to M minus 1 and N minus 1.\nAnd we add a normalization factor of 1 divided by M times N.\nThe exponential now becomes:\u000be to the power positive j 2 pi times x times m over M plus y times n over N.\n\nSo once again, we‚Äôre projecting onto a set of complex exponential basis functions‚Äîjust like in the 1D case, but now expanded to two dimensions.\nThis idea even extends naturally to three dimensions, which is useful for things like volumetric imaging, 3D scans, or dynamic time-sequence data in medical imaging.\n\nSo to summarize: the 2D Fourier series gives us a powerful way to analyze images in terms of spatial frequencies. It's conceptually the same as the 1D case‚Äîjust applied in two directions simultaneously.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 5", "slide_number": 50, "slide_filename": "Slide50.txt", "slide_annotation": "Lecture 5 - Slide50.txt", "file_path": "Lecture 5\\Texts\\Slide50.txt", "content": "And this brings us to your homework assignment.\n\nFirst, I‚Äôd like you to derive the formulas for the Fourier series coefficients using real-valued notation‚Äîjust like we discussed today. Go through the reasoning step by step, and try to develop some familiarity with the structure of these expressions. Think of it as reinforcing what we‚Äôve covered, not just repeating it.\nSecond, watch the YouTube video linked here, and use it as a guide to implement the square wave example we worked through. This will give you hands-on experience with how the Fourier series behaves‚Äîespecially when approximating functions with sharp transitions. \n\nThis topic is a bit more involved than convolution, so I recommend reviewing the lecture carefully. But if you get the hang of it, you‚Äôll find the transition to the Fourier transform much smoother in the next lecture.\nGood luck‚Äîand see you next time.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 6", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 6 - Slide1.txt", "file_path": "Lecture 6\\Texts\\Slide1.txt", "content": "Welcome back, everyone.\n\nToday, we move one step further in our journey ‚Äî from the Fourier series to the Fourier transform. This is a big transition, and it‚Äôs a powerful one. So, what exactly changes when we move from a series to a transform? That‚Äôs what we‚Äôll uncover together in this lecture.\n\nBefore we begin, I want to emphasize something important. You need to have a solid understanding of the Fourier series first. The transform builds directly on those ideas. If you‚Äôre still feeling confused about Fourier series ‚Äî don‚Äôt worry ‚Äî but do take action. You can interact with ChatGPT, discuss with your classmates, review the lecture materials, and review the chapter I wrote. You can also search online for different explanations or examples that make more sense to you. Group discussions can be constructive.\n\nFourier analysis is fundamental ‚Äî not just for this course, but for understanding medical imaging technologies down the line. I want to make sure no one gets left behind here. Because if you miss this foundation, the rest of the material will feel considerably harder.\nAlright ‚Äî let‚Äôs dive in.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 6 - Slide2.txt", "file_path": "Lecture 6\\Texts\\Slide2.txt", "content": "Again, so this is our schedule. We are on schedule, so no problem.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 6 - Slide3.txt", "file_path": "Lecture 6\\Texts\\Slide3.txt", "content": "Now, let‚Äôs break things down to a very basic and powerful idea.\n\nSuppose we have a function ‚Äî let‚Äôs call it f of t ‚Äî a one-dimensional function that varies over time. You can think of it as a smooth, continuous curve. That‚Äôs the usual way we look at functions.\nBut here‚Äôs a different perspective ‚Äî instead of seeing f of t as one continuous piece, imagine it as a sum of impulses. Yes ‚Äî a collection of sharp, narrow spikes, each carrying a little bit of information about the function at a specific time.\n\nThis is where the Dirac delta function, or delta for short, comes in. From linear systems theory, we know that any continuous function can be viewed as the convolution of that function with a train of delta functions.\n\nWhat does this mean, intuitively? It means you can imagine slicing the original function into many tiny segments. Each segment becomes a small impulse ‚Äî and when you add up all those impulses, you reconstruct the full function.\nSo, this is one way to represent a function ‚Äî not as a smooth line, but as a weighted sum of sharp impulses. And this idea is going to be very helpful as we move toward the Fourier transform.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 6 - Slide4.txt", "file_path": "Lecture 6\\Texts\\Slide4.txt", "content": "Now let‚Äôs look at the same function from a completely different angle ‚Äî instead of seeing it as a sum of impulses, imagine it as a sum of waves.\n\nHere, we‚Äôre working with a periodic function ‚Äî meaning it repeats itself over time. Let‚Äôs just focus on one complete cycle of that function. The rest are just copies.\nThe key idea is this: a periodic function like this can be broken down into many sine and cosine waves ‚Äî we call these sinusoidal components. These waves can vary in three ways: their frequency, which tells us how fast they oscillate; their amplitude, which tells us how tall they are; and their phase, which tells us where each wave starts.\n\nSo what‚Äôs the trick? We want to find the right combination of amplitudes, frequencies, and phases ‚Äî so that when we add up all these sine and cosine waves, we recover the original function.\nAt first, this might sound abstract, but conceptually it‚Äôs not that difficult. You‚Äôre just looking at the same function in a different way.\nEarlier, we saw a kind of particle view ‚Äî slicing the function into sharp impulses. Now we‚Äôre seeing a wave view ‚Äî smoothing it into oscillating signals.\nAnd this wave-based view is the foundation for Fourier analysis.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 6 - Slide5.txt", "file_path": "Lecture 6\\Texts\\Slide5.txt", "content": "So now that we've talked about breaking a function into a sum of waves, let's formalize that idea using the real form of the Fourier series.\n\nWhat we‚Äôre looking at here is just a mathematical way of saying: any periodic function f of t can be written as a combination of three types of components:\nA constant term, called the DC component,\nA sum of cosine terms,\nAnd a sum of sine terms.\nEach term has a different frequency and amplitude, depending on the value of n. The higher the value of n, the higher the frequency of that term.\n\nSo the general formula goes like this:\nf of t equals a-zero over 2, plus the sum from n equals 1 to N of a-n times cosine of 2 pi n t, plus b-n times sine of 2 pi n t.\nNow, where do these coefficients come from?\nWell, we have formulas to compute them. That‚Äôs the trick!\nTo find a zero, you integrate f of t from 0 to 1.\nTo find a-n, you multiply f of t with cosine of 2 pi n t, and integrate.\nAnd for b-n, you do the same, but with sine of 2 pi n t.\n\nSo if I give you any function f of t, you just plug it into these formulas, do the math, and you get a set of numbers ‚Äî the a‚Äôs and b‚Äôs. With those coefficients, you can then reconstruct the original function by summing all the sine and cosine waves.\nAs N gets larger ‚Äî meaning you include more wave components ‚Äî the reconstructed function becomes more and more accurate. In fact, when N approaches infinity, this series can represent the function exactly, under reasonable conditions.\n\nNow here‚Äôs something deeper: These sine and cosine functions form an orthonormal basis ‚Äî that‚Äôs just a fancy way of saying they‚Äôre like the X, Y, and Z axes in 3D space, but in an infinite-dimensional space of functions.\nSo geometrically, what you're doing is projecting the function f of t onto each basis function ‚Äî kind of like breaking a 3D vector into its X, Y, and Z components. Except here, we‚Äôre breaking a function into sine, cosine, and constant components.\n\nIf this all feels a bit abstract, don‚Äôt worry. The key takeaway is:\u000bWe‚Äôre representing a function as a sum of waves.\u000bAnd we have formulas that let us find the right weights ‚Äî or coefficients ‚Äî for those waves.\nThat‚Äôs what the Fourier series, in its real form, is all about.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 6 - Slide6.txt", "file_path": "Lecture 6\\Texts\\Slide6.txt", "content": "So far, we've talked about the real form of the Fourier series, which uses sine and cosine functions. But there's another way to write the same idea ‚Äî in what we call the complex form.\n\nNow don‚Äôt be alarmed by the word ‚Äúcomplex.‚Äù This version is mathematically equivalent to the real form ‚Äî it's just more compact and elegant.\nInstead of separating things into sines and cosines, we use complex exponentials ‚Äî specifically, e to the power of 2 pi i n t. That‚Äôs Euler‚Äôs formula at work.\nIn this form, the function f of t is expressed as a sum ‚Äî from n equals minus N to N ‚Äî of coefficients c n multiplied by e to the 2 pi i n t.\nAnd just like before, we need to figure out those coefficients. So how do we compute c n?\n\nWe use this formula:\u000bc n equals the integral from 0 to 1 of e to the power minus 2 pi i n t times f of t, with respect to t.\nThis is essentially an inner product ‚Äî projecting your function onto the exponential basis function e to the power minus 2 pi i n t. \n\nIn my book, I like to write the imaginary unit i in front ‚Äî as in minus i 2 pi n t ‚Äî but it‚Äôs just a matter of notation. The meaning stays the same.\nAt the bottom here, you see the same formula written slightly differently ‚Äî as f-hat of n ‚Äî which is just another way of naming the coefficient.\n\nNow, throughout this slide, we‚Äôre assuming that the function is periodic with unit period ‚Äî meaning it repeats every interval from 0 to 1. If the function instead repeats from, say, 100 to 101, it‚Äôs still the same picture ‚Äî just shifted.\nSo the complex form doesn't change the logic ‚Äî it simply gives us a cleaner, more powerful way to write and manipulate Fourier series, especially when we move into Fourier transforms.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 6 - Slide7.txt", "file_path": "Lecture 6\\Texts\\Slide7.txt", "content": "So far, we‚Äôve been working under a simple assumption ‚Äî that the function we‚Äôre analyzing has a unit period. That means the function repeats every one unit of time, say, from 0 to 1.\n\nBut what if the period is something else ‚Äî like 5, or 100? That‚Äôs what we‚Äôre covering here.\nThis slide shows how to handle a function with an arbitrary period, which we‚Äôll call capital T. The good news is: the concept stays exactly the same ‚Äî we‚Äôre still breaking the function into complex exponential components ‚Äî but we just adjust the formulas a bit.\n\nHere‚Äôs how the function f of t is now written:\nf of t equals the sum from n equals minus infinity to infinity of c n times e to the power i 2 pi n t over T.\nSo you can think of this as a generalized version of the complex Fourier series.\n\nAnd to compute the coefficient c n, we use the formula:\nc n equals 1 over capital T, times the integral from minus T over 2 to T over 2 of e to the power minus i 2 pi n t over T, which is multiplied by f of t, with respect to t.\n\nNow let me break that down for you:\nThe exponential term ‚Äî that‚Äôs your harmonic basis function, adjusted for the new period.\nt over T is just a way to normalize time, so that we‚Äôre still operating over a standard interval.\nThe 1 over T in front is a scaling factor that makes everything work out correctly.\nAnd the range of integration ‚Äî from minus T over 2 to plus T over 2 ‚Äî gives us a symmetric interval, which is often more convenient for analysis.\n\nSo, whether your period is 1, 10, or any other positive number, you simply use T in place of 1 and apply this formula. You‚Äôre still summing wave-like components ‚Äî just stretched or compressed in time depending on the period.\nNow here‚Äôs something helpful:\u000bIf you set T equals 1, this formula reduces back to the unit-period version we saw before. So you only need to remember this general form ‚Äî and adjust T as needed.\n\nIn practice, the real form of the Fourier series is nice because it gives you a clear geometric picture ‚Äî sines, cosines, and their amplitudes.\u000bThe complex form, like the one we see here, is more compact and elegant, especially when we move into transforms. It just takes a little abstract thinking in complex space.\nSo this wraps up our review of Fourier series ‚Äî both real and complex ‚Äî and now we‚Äôre ready to move on to the Fourier transform itself.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 6 - Slide8.txt", "file_path": "Lecture 6\\Texts\\Slide8.txt", "content": "By now, you might be wondering ‚Äî why go through all this? Why even bother with the Fourier transform?\nWell, let‚Äôs take a step back and think with common sense.\n\nWe started with a function ‚Äî f of t ‚Äî and we‚Äôve looked at several ways to represent it. First, we saw it as a sum of impulses. Then, as a sum of waves ‚Äî sines and cosines. Each of these views gives us a different lens to understand the same signal.\nAnd that‚Äôs the point. Multiple perspectives give us more flexibility and smarter strategies for solving problems.\n\nIn real life, when you're facing a tough task, what do you usually do? You don‚Äôt try to tackle it head-on in the hardest way possible. You try to simplify it. You look for shortcuts. You use tools that make the problem easier.\nThat‚Äôs exactly what we‚Äôre doing with Fourier analysis.\nSometimes a problem looks really messy in the time domain ‚Äî but if we switch to the Fourier domain, that same problem might become simple and elegant.\n\nHere‚Äôs an analogy: imagine trying to do multiplication using Roman numerals. It‚Äôs a nightmare! But switch to Arabic numerals ‚Äî or even binary ‚Äî and suddenly, multiplication becomes easy. Especially for computers.\nSo choosing the right representation can make all the difference.\nThat‚Äôs what the Fourier transform is all about. It gives us a new way to look at a function ‚Äî in terms of its frequency content ‚Äî and often, that view makes analysis or computation much simpler.\n\nThere‚Äôs also a deeper strategy here:\u000bSometimes a function is complicated as a whole, but if we break it down into smaller, simpler parts, we can understand it better.\nThis is the classic divide and conquer approach.\nWe‚Äôve already seen this with impulse decomposition. If you know how a system responds to a single impulse, then you can figure out how it will respond to a complicated signal ‚Äî just by adding up the responses to each impulse.\nThe same logic applies to sine waves ‚Äî if you understand how a system reacts to one sine wave, and you can express a signal as a sum of sine waves, then you‚Äôve got the whole picture.\n\nSo, in short, Fourier analysis follows two powerful principles:\nUse simple tools to solve complex problems.\nDivide big problems into small, manageable pieces.\nAnd that‚Äôs what we‚Äôre building toward with the Fourier transform.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 6 - Slide9.txt", "file_path": "Lecture 6\\Texts\\Slide9.txt", "content": "So now that we‚Äôve covered the Fourier series ‚Äî and we‚Äôve seen why changing representations can make problems easier ‚Äî it‚Äôs time to dive into the main topic of today‚Äôs lecture: the Fourier transform.\n\nThis corresponds to Chapter 4 in the book draft I shared with you. I do plan to revise it soon ‚Äî polish up some explanations and fix a few typos ‚Äî and I‚Äôll send you the updated version once it‚Äôs ready.\n\nHere‚Äôs how we‚Äôll structure today‚Äôs lecture:\nFirst, in Section 1, we‚Äôll talk about how the Fourier transform is derived from the Fourier series. This transition is logical and elegant, and it gives us a solid foundation for everything that follows.\n\nThen, in Section 2, we‚Äôll explore some of the most important properties of the Fourier transform. These properties aren‚Äôt just mathematical curiosities ‚Äî they‚Äôre practical tools used in signal processing, imaging, and many other fields.\n\nAfter that, in Section 3, we‚Äôll extend our thinking to higher dimensions. I‚Äôll show you how the Fourier transform applies not just to one-dimensional signals, but also to images ‚Äî and beyond. I‚Äôll even walk you through an example where converting an image to the Fourier domain makes it easy to remove noise.\n\nFinally, in Section 4, we‚Äôll wrap things up with a few remarks and reflections.\n\nSo by the end of this lecture, you should not only understand the mathematical structure of the Fourier transform, but also see its real-world value, especially in areas like medical imaging.\nLet‚Äôs get started with the first section: the derivation of the Fourier transform.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 6 - Slide10.txt", "file_path": "Lecture 6\\Texts\\Slide10.txt", "content": "You‚Äôve actually seen this slide before ‚Äî it‚Äôs a summary of what we covered in the last lecture, and now it serves as our starting point for deriving the Fourier transform.\n\nLet‚Äôs quickly revisit the main idea:\nWe said that any arbitrary function can be treated as if it were periodic, defined over a fixed interval. In this case, we‚Äôre looking at a symmetric interval ‚Äî from minus capital T over 2 to plus capital T over 2.\n\nWithin this interval, we can represent the function f of t as a sum of sinusoidal components ‚Äî specifically, complex exponentials of the form:\ne to the power i times 2 pi n, times t over T.\nThis is a compact way to write sine and cosine waves using Euler‚Äôs formula. So even though the expression looks complex, it represents a mix of sine and cosine waves at various frequencies, where n runs from minus infinity to plus infinity.\nAnd just like before, each component has a corresponding coefficient, which tells us how much of that wave appears in the overall function.\n\nTo compute the coefficient c n, we use the formula at the bottom:\nc n equals 1 over capital T, times the integral from minus T over 2 to T over 2, of e to the minus i 2 pi n t over T, times f of t, with respect to t.\nThis is just an inner product ‚Äî projecting f of t onto a complex exponential basis function. That projection gives us the weight, or the contribution, of that particular frequency.\n\nSo, this slide wraps up all the key ideas from the Fourier series ‚Äî including:\nTreating functions as periodic,\nExpressing them with complex exponentials,\nAnd computing coefficients through integration.\nWith this solid foundation in place, we‚Äôre now ready to take the next step ‚Äî and derive the Fourier transform, which generalizes everything we‚Äôve done so far.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 6 - Slide11.txt", "file_path": "Lecture 6\\Texts\\Slide11.txt", "content": "Alright ‚Äî now let‚Äôs take the next step together.\nEarlier, we learned how to calculate the Fourier coefficient c n for a function with arbitrary period T. Now, we‚Äôre going to insert that expression for c n directly into the Fourier series formula.\n\nWhat happens when we do that?\nWell, here‚Äôs what we get: we take the formula for c n ‚Äî which is an integral ‚Äî and plug it into the sum. So now, each term in the series contains an inner product multiplied by a complex exponential.\n\nNotice this part out front: 1 over capital T. This shows up because we‚Äôre working over a general period T, not the unit interval. So we need to normalize ‚Äî or average ‚Äî over the full interval from minus T over 2 to plus T over 2.\nThis 1 over T acts like a scaling factor ‚Äî a way of balancing the sum so that everything works out correctly.\nNow look inside the brackets. What we‚Äôre really doing here is taking the inner product of two functions:\nOne is f of t,\nAnd the other is the complex exponential e to the power minus i 2 pi n t over T.\n\nYou multiply them together point by point, and then integrate over the interval. That‚Äôs what we mean by an inner product in this context.\nAnd we‚Äôre doing this for many values of n, both negative and positive. So you can imagine we‚Äôre computing inner products at a whole series of frequency points ‚Äî where each frequency is n divided by T.\n\nAs T becomes large, the spacing between these frequencies ‚Äî that is, the difference between one n over T and the next ‚Äî becomes smaller and smaller. Eventually, this sum will start to resemble an integral over a continuous range of frequencies, and that‚Äôs the key idea behind the Fourier transform.\n\nSo, to summarize:\u000bWe‚Äôre inserting the formula for c n, and that gives us a series of inner products across many frequencies, with a spacing of delta u equals 1 over T. This step sets us up perfectly to move from a discrete sum to a continuous integral ‚Äî which is what the Fourier transform is all about.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 6 - Slide12.txt", "file_path": "Lecture 6\\Texts\\Slide12.txt", "content": "Now to understand that last idea more clearly, let‚Äôs take a look at this picture.\n\nWhat we‚Äôre doing here is sampling inner products at many frequency points. Each of these points corresponds to a value of n divided by capital T ‚Äî that is, u equals n over T. As n varies from negative to positive integers, you get a collection of frequency points spread along the u-axis, which represents frequency.\n\nWhen n equals 0, you‚Äôre at the DC component ‚Äî the zero-frequency term. When n equals 1, you get one frequency point. When n equals minus 1, you get another. And so on.\nSo you can imagine each vertical line here as one of those frequency points ‚Äî u equals n over T ‚Äî spread across the axis.\n\nNow here‚Äôs the key idea:\u000bIf we let the period T become very large, then the range from minus T over 2 to plus T over 2 will grow wider and wider. In fact, in the limit as T goes to infinity, that interval covers the entire frequency axis ‚Äî from minus infinity to plus infinity.\n\nThat‚Äôs what we mean by dense sampling of frequency.\nAs T increases, the spacing between neighboring frequencies gets smaller and smaller. Mathematically, the spacing ‚Äî which we call delta u ‚Äî becomes 1 over capital T. So the gap between u equals to n over T and u equals to (n plus 1) over T shrinks.\nEventually, these discrete frequency samples get so close together that they begin to form a continuous spectrum.\nAnd that‚Äôs the magic.\n\nWe started with a periodic function, which gives us discrete frequencies. But as the period stretches toward infinity, the function becomes non-periodic, and the discrete frequency points turn into a continuous frequency range.\nSo, what is a non-periodic function in this view?\nIt‚Äôs just a periodic function with an infinitely long period. Anything that happens beyond that infinite window doesn‚Äôt affect what we see ‚Äî and that‚Äôs how we bridge from the Fourier series to the Fourier transform.\n\nThis slide captures the big picture:\nDiscrete frequencies spaced by 1 over T,\nBecoming densely packed as T increases,\nAnd ultimately forming a continuous frequency axis.\nThat‚Äôs our gateway into the Fourier transform.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 6 - Slide13.txt", "file_path": "Lecture 6\\Texts\\Slide13.txt", "content": "Alright ‚Äî this slide is the heart of the lecture. Take a moment to follow this carefully, because here is where we formally derive the Fourier transform ‚Äî and also see how we can recover the original function using the inverse transform.\n\nLet‚Äôs begin at the top.\nWe‚Äôre starting with our expression for f of t, written as a Fourier series. We substitute the expression for c over n ‚Äî the Fourier coefficient ‚Äî right into this formula.\n\nNow remember, c over n contains an integral ‚Äî and it includes a 1 over T term out front. Since T is constant, we pull it out of the sum.\nAt this point, we apply a key idea we introduced earlier:\u000bLet T become very large, so large that we‚Äôre effectively dealing with a non-periodic function. When T approaches infinity, the spacing between the frequency samples, delta u equals to 1 over T, becomes tiny ‚Äî almost zero.\n\nSo what happens?\nInstead of summing over discrete frequencies, we move to a continuous frequency variable, which we call u ‚Äî where u equals n divided by T.\nThis transition lets us replace the sum with an integral.\nSo now we‚Äôve gone from a sum of discrete terms to an integral over a continuous range of frequencies. The inner product becomes f-hat of u, the Fourier transform of f of t. And the exponential term becomes e to the power i 2 pi u t ‚Äî our new kernel in the transform.\n\nLet‚Äôs pause here and look at the final expression.\nYou‚Äôll notice two things:\nFirst, to compute f-hat of u, the Fourier transform, we integrate f of t multiplied by e to the minus i 2 pi u t. That‚Äôs the forward transform ‚Äî moving from the time domain to the frequency domain.\nSecond, to reconstruct f of t, we take f-hat of u, multiply it by e to the power i 2 pi u t, and integrate over all u. That‚Äôs the inverse transform ‚Äî bringing us back from frequency to time.\nSo these two formulas ‚Äî the forward and inverse transforms ‚Äî are the foundation of the Fourier transform.\n\nThey let us take a non-periodic function and decompose it into a continuous spectrum of frequencies. And then, using that spectrum, we can reconstruct the original function with complete accuracy.\nThis is what makes the Fourier transform so powerful ‚Äî not just mathematically, but also practically ‚Äî in fields like signal processing, image analysis, and, of course, medical imaging.\nFrom this point forward, we‚Äôre now working with non-periodic functions, and we‚Äôve fully transitioned from Fourier series to the Fourier transform.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 6 - Slide14.txt", "file_path": "Lecture 6\\Texts\\Slide14.txt", "content": "Now let‚Äôs bring things down to earth a bit and look at a concrete example ‚Äî something visual, something simple.\nThis function right here is called the rectangular function, or sometimes the gate function. You‚Äôll also see it written as pi of t.\nAnd as you can see from the graph ‚Äî it really does look like a gate.\n\nLet‚Äôs break it down:\nThe value of the function is 1 when the absolute value of t is less than one-half ‚Äî that is, between minus one-half and plus one-half.\nOutside that interval, the function drops to zero.\nSo, the entire ‚Äúon‚Äù region is one unit wide, and the height is 1, which means the area under the curve is also 1.\nThis function is simple, but it‚Äôs also very important ‚Äî and we‚Äôre going to use it often in later examples.\n\nNow here‚Äôs the key point:\u000bThis is a non-periodic function.\nAnd back when we only had Fourier series ‚Äî which apply to periodic functions ‚Äî we couldn‚Äôt directly express this kind of shape using sines and cosines.\nBut now, thanks to the Fourier transform, we can.\n\nWe‚Äôre no longer limited to repeating signals. We can now handle functions like this ‚Äî compact, finite, non-repeating ‚Äî and still express them in terms of sinusoidal components, just over a continuous range of frequencies.\nSo this rectangular function ‚Äî although it‚Äôs simple ‚Äî gives us a great opportunity to visualize the power of the Fourier transform.\nLet‚Äôs keep going and see what it looks like in the frequency domain.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 6 - Slide15.txt", "file_path": "Lecture 6\\Texts\\Slide15.txt", "content": "Now let‚Äôs do something clever.\n\nWe just looked at the gate function, or rectangular function, which is not periodic. And because it‚Äôs non-periodic, we can't represent it using a Fourier series ‚Äî only the Fourier transform works in that case.\nBut what if we wanted to use Fourier series?\u000bWell ‚Äî we can! We just need to make the function periodic.\nAnd here‚Äôs how we do it: we take that single gate function ‚Äî and simply repeat it over and over again, at regular intervals. This is called periodization.\n\nIn this slide, the original gate function is being repeated with a period of 16. So now you‚Äôve got a train of rectangular pulses, equally spaced along the time axis.\nVisually, you can think of it like flipping a light switch on for a second ‚Äî and then leaving it off for a long time ‚Äî then flipping it on again ‚Äî and repeating that process over and over.\nThe key is: now we‚Äôve created a periodic signal.\nAnd because it‚Äôs periodic, we can now apply Fourier series to analyze it ‚Äî just like we did before.\n\nAll you need to do is plug this periodic function into the Fourier series formulas, and you‚Äôll get a representation as a sum of sine and cosine waves ‚Äî or complex exponentials.\nThis kind of construction is very useful. In fact, it shows up in engineering and physics quite a bit ‚Äî especially in systems that switch on and off repeatedly. Sometimes you‚Äôll hear people talk about the duty cycle, which refers to how much of each period the function is ‚Äúon‚Äù versus ‚Äúoff.‚Äù\n\nSo again ‚Äî by making the function periodic, we unlock the power of the Fourier series.\nAnd soon, we‚Äôll compare this with what happens when we go back to the non-periodic version, and apply the Fourier transform instead.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 6 - Slide16.txt", "file_path": "Lecture 6\\Texts\\Slide16.txt", "content": "Now, let‚Äôs visualize the powerful idea we‚Äôve been building toward ‚Äî how we move from Fourier series to the Fourier transform.\nWhat you see here is the frequency-domain representation of the rectangular function as we increase the period of its periodic extension.\nLet‚Äôs start at the top left.\nThese vertical lines represent the Fourier coefficients ‚Äî discrete spikes at different frequency points. These are the outputs of the Fourier series.\u000b\nYou‚Äôve got:\nA DC component at zero frequency,\nThe first harmonic,\nThe second harmonic, and so on.\nEach coefficient is calculated using the formulas we saw earlier. And because the original function is real-valued, these coefficients appear symmetrically around zero when using the complex form.\n\nNow ‚Äî here‚Äôs the trick:\u000bWe begin to increase the period T of the function.\nThat means we‚Äôre spacing the rectangular pulses farther apart in the time domain. And in the frequency domain, this has a very important effect:\u000bThe spacing between the Fourier components, which is 1 over T, becomes smaller and smaller.\nYou can see this happening in the middle image ‚Äî the spikes begin to cluster more tightly. The frequency axis, labeled u equals n divided by T, gets more densely sampled.\nAnd as we continue increasing T ‚Äî moving to the bottom image ‚Äî the spacing becomes infinitesimally small. At this point, we‚Äôre no longer looking at a discrete spectrum. We now have a continuous curve.\nThat‚Äôs the moment when we‚Äôve transitioned from the Fourier series to the Fourier transform.\n\nSo here‚Äôs the big picture:\nWhen a function is periodic, we get a discrete Fourier spectrum.\nBut when the function becomes non-periodic, by letting the period go to infinity, we end up with a continuous Fourier spectrum.\nThis spectrum is smooth and continuous ‚Äî and it tells us how much of each frequency exists in the original signal.\nSo this slide gives us the geometrical insight ‚Äî the visual transition ‚Äî from the world of periodic signals and discrete spectra, to the world of non-periodic signals and continuous spectra.\nAnd this is what the Fourier transform captures so beautifully.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 6 - Slide17.txt", "file_path": "Lecture 6\\Texts\\Slide17.txt", "content": "Now that we‚Äôve built up all the ideas step by step, let‚Äôs bring it all together into one elegant concept ‚Äî the Fourier transform pair.\nWhat you see on this slide captures the very heart of Fourier analysis.\n\nLet‚Äôs begin with the first equation, which is the forward Fourier transform.\u000bSuppose you have a signal in the time domain, which we‚Äôll call f of t. The goal is to understand how much of each frequency is present in that signal.\n\nSo how do we do that?\nWe project f of t onto a family of complex exponential functions. These functions look like e to the power minus i, 2 pi, u, t, where u is the frequency. This projection acts like an inner product, telling us how strongly f of t resonates with each frequency component.\nWhen we integrate this product across all time, we obtain f hat of u, also called the frequency spectrum or the Fourier transform of f of t.\nSo this first equation maps the signal from the time domain to the frequency domain.\n\nNow look at the second equation. This is the inverse Fourier transform.\nHere, we take all those frequency components ‚Äî that is, each f hat of u ‚Äî and multiply them by their corresponding complex exponential, this time e to the power i, two pi, u, t. Then, we integrate over all frequencies.\nAnd what do we get?\u000bWe reconstruct the original signal ‚Äî f of t ‚Äî exactly.\nSo in summary, the forward transform takes you from time to frequency, and the inverse transform brings you back from frequency to time.\u000bThat‚Äôs the two-way mapping shown symbolically at the bottom:\u000bf of t goes to f hat of u, and back again.\n\nNow, a few mathematical notes to make this complete:\nFirst ‚Äî this transform assumes that f of t is square-integrable, meaning if you square the function and integrate it over all time, the total is finite. This ensures convergence and makes the math work properly.\n\nSecond ‚Äî even though we‚Äôre dealing with a non-periodic function here, the idea still grows out of the Fourier series. Remember what happens when the period of a function becomes very large ‚Äî the discrete set of frequency components becomes a continuous spectrum. The Fourier transform is essentially the limit of the Fourier series as the period tends to infinity.\nAnd finally ‚Äî here‚Äôs a nice geometric interpretation.\nThink of projecting a 3D vector onto the x, y, and z axes.\u000b\nIn the same way, when we apply the Fourier transform, we‚Äôre projecting our function onto an infinite set of sine and cosine waves ‚Äî or more precisely, complex exponentials. Each frequency contributes a tiny slice of the original signal.\u000bAnd by adding all those slices back together ‚Äî using integration ‚Äî we reconstruct the full signal.\n\nThis isn‚Äôt just a clever trick.\u000bIt‚Äôs one of the most powerful tools in all of engineering, physics, and mathematics.\u000bIt lets us analyze and manipulate signals in both time and frequency ‚Äî without losing any information.\nWith this foundational concept in place, we‚Äôre ready to explore some practical examples and dive deeper into the properties of the Fourier transform.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 6 - Slide18.txt", "file_path": "Lecture 6\\Texts\\Slide18.txt", "content": "Let‚Äôs go through a concrete example ‚Äî the Fourier transform of the gate function, also called the rectangular function.\nWe‚Äôve already seen this function earlier. It‚Äôs a flat function that equals 1 when the time variable t is between minus one-half and plus one-half, and it‚Äôs zero everywhere else. So it looks like a rectangle centered at zero.\n\nNow, we want to find its Fourier transform. To do that, we apply the definition.\nWe take the integral of the original function multiplied by a complex exponential ‚Äî that‚Äôs e to the power negative i 2 pi u t ‚Äî and we integrate over all time.\n\nBut since the gate function is zero outside the interval from minus one-half to plus one-half, we only need to integrate over that range.\nSo the integral becomes: from minus one-half to plus one-half of e to the negative i 2 pi u t, with respect to t.\nThis is a standard exponential integral. When you solve it, you get the function sine of pi u, divided by pi u.\nThis result is known as the sinc function and it shows up everywhere in signal processing and imaging.\nSo here‚Äôs what we‚Äôve found: a rectangle in time turns into a sinc wave in frequency. That‚Äôs a beautiful and very useful result.\n\nNow, what if we stretch the gate to make it wider?\nLet‚Äôs say the gate equals 1 from minus capital T over 2 to plus T over 2, and zero elsewhere. Then the Fourier transform becomes a scaled version of the same sinc function ‚Äî it just shrinks or stretches based on the value of T.\nSo you could try computing it directly, or you could use a shortcut ‚Äî the scaling property of the Fourier transform, which we‚Äôll talk about soon.\n\nBut the key point is this: the rectangular function in time gives us a sinc function in frequency.\nAnd here‚Äôs one final note. You might wonder what happens when u equals zero, because both the top and bottom become zero. But don‚Äôt worry ‚Äî if you take the limit using calculus, it turns out the value is exactly 1. So the function is smooth at the center.\nWe‚Äôll use this gate-to-sinc pair often, so keep it in mind as we move forward.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 6 - Slide19.txt", "file_path": "Lecture 6\\Texts\\Slide19.txt", "content": "Now let‚Äôs take a look at the actual shape of the sinc function ‚Äî the one we just derived from the rectangular gate.\n\nWhat you see here is the graph of that function ‚Äî sinc of u ‚Äî which is defined as sine of pi times u, divided by pi times u.\nIt has a smooth, wave-like shape. The peak is right at the center ‚Äî at u equals zero ‚Äî and the height there is exactly 1.\nAs you move away from the center in either direction, the function oscillates ‚Äî it goes up and down ‚Äî but the peaks get smaller and smaller.\nThat‚Äôs because the sine wave in the numerator keeps swinging, but the denominator grows, so the overall value shrinks.\nThis is the signature look of the sinc function ‚Äî one strong central lobe and then smaller and smaller ripples on the sides.\n\nSo this is the frequency domain representation of a simple gate in time.\nAnd just to recap ‚Äî this sinc function came from a rectangular gate function of width 1.\nIf we use a wider gate ‚Äî let‚Äôs say the width is capital T instead of 1 ‚Äî then the sinc function would stretch out horizontally. That is, the ripples would become narrower in frequency, because time and frequency are inversely related.\n\nWe‚Äôll explore that in more detail when we talk about the scaling property. But for now, just remember: a narrow gate in time gives a wide sinc in frequency. A wide gate in time gives a narrow sinc.\nThis is a key idea we‚Äôll return to again and again.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 6 - Slide20.txt", "file_path": "Lecture 6\\Texts\\Slide20.txt", "content": "Now let‚Äôs move on to our second example ‚Äî the triangle function.\n\nThis function, often called the triangular function, is shaped just like its name suggests ‚Äî a triangle. It peaks at 1 when x equals 0, and then it decreases linearly to zero as x moves out to minus 1 and plus 1. Outside of that range, the value is zero.\n\nMathematically, it‚Äôs defined like this: one minus the absolute value of x when x is between negative 1 and 1, and zero otherwise.\nSo what we want to do now is find its Fourier transform.\n\nIn other words, we want to express this smooth, non-periodic triangular function as a combination of infinitely many sinusoidal waves ‚Äî just like we did with the gate function.\n\nWe‚Äôll add up all these waves ‚Äî each with a different frequency and amplitude ‚Äî and by doing so, we‚Äôll be able to reconstruct this triangle shape exactly.\nThat‚Äôs the idea behind Fourier transform: take a function, no matter how it looks, and rewrite it as a sum of wave components. Each component carries part of the shape ‚Äî and when you combine them all, you get the original back.\nLet‚Äôs see what that looks like in this case.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 6 - Slide21.txt", "file_path": "Lecture 6\\Texts\\Slide21.txt", "content": "Let‚Äôs now take a look at the result of the Fourier transform of the triangle function.\n\nIf we go through the steps ‚Äî plugging the triangle function into the Fourier transform formula and carrying out the integration ‚Äî we end up with a very elegant result. The Fourier transform turns out to be the square of the sinc function.\nThat is, we get sinc squared.\n\nYou might remember that the Fourier transform of the gate function gave us a sinc function. So this result ‚Äî sinc squared ‚Äî is not just a coincidence.\nThere‚Äôs actually a deeper reason behind it.\nIt‚Äôs related to an operation called convolution. We saw this in the context of the Fourier series, and we‚Äôll explore it again in more depth when we talk about properties of the Fourier transform.\n\nBut the key takeaway here is this: the triangle function is, in a way, the convolution of two gate functions. And in the frequency domain, convolution in time corresponds to multiplication in frequency. That‚Äôs why the sinc function ‚Äî from the gate ‚Äî becomes sinc squared for the triangle.\n\nAnd down at the bottom, we can see the graph of sinc squared. It has the same overall shape as sinc ‚Äî a central peak and decaying ripples ‚Äî but now the oscillations are all positive and fall off more smoothly.\nSo, this example shows another beautiful connection between shapes in the time domain and their patterns in the frequency domain.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 6 - Slide22.txt", "file_path": "Lecture 6\\Texts\\Slide22.txt", "content": "Let‚Äôs now take a look at a few more examples of Fourier transform pairs ‚Äî some of them elegant, some of them a bit more abstract.\n\nFirst, we have the Gaussian function. In the time domain, it‚Äôs written as e to the power negative pi t squared. This is the familiar bell-shaped curve ‚Äî smooth, centered, and decaying quickly. What‚Äôs remarkable is that its Fourier transform is also a Gaussian ‚Äî e to the power negative pi u squared. The shape stays the same, just transformed into the frequency domain. The Gaussian is one of the few functions that‚Äôs unchanged, except for scaling ‚Äî we say it‚Äôs self-Fourier.\n\nNext, let‚Äôs consider a constant function ‚Äî just a flat value c across time. Now, technically this function is not square-integrable, meaning we can‚Äôt just apply the Fourier transform in the usual way. But in a generalized sense, we can still assign it a meaning. And it turns out the Fourier transform of a constant is a scaled delta function ‚Äî specifically, c times delta of u. This makes intuitive sense: a constant signal has no frequency variation, so all its energy is concentrated at zero frequency.\n\nAnd finally, look at this interesting example: the shifted delta function delta of t minus a. If you perform the Fourier transform of this, you get a complex exponential ‚Äî e to the power minus i 2 pi a u. This tells us that shifting a delta function in time introduces a phase shift in frequency. And that phase shift depends on the amount of translation ‚Äî the value a ‚Äî and also on the frequency u.\n\nNow, I should point out ‚Äî when you‚Äôre working with generalized functions like constants and delta functions, the usual rules don‚Äôt always apply directly. These are not square-integrable functions. So we use the tools of distribution theory to handle them more rigorously. If you‚Äôre curious, I explain this in more detail in the book chapter.\n\nBut for now, just keep in mind ‚Äî even with these edge cases, the idea of expressing a signal in terms of wave components still holds.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 6 - Slide23.txt", "file_path": "Lecture 6\\Texts\\Slide23.txt", "content": "Now that we‚Äôve gone through several examples, let‚Äôs take a moment to highlight some core properties of the Fourier transform ‚Äî the features that make it such a powerful and elegant analytical tool.\n\nWe begin with linearity. If you combine two functions, say f and g, using constants a and b ‚Äî meaning you form \"a times f plus b times g\" ‚Äî then the Fourier transform of that sum is simply \"a times the transform of f plus b times the transform of g.\" This reflects the principle of superposition, which is central to all linear systems.\n\nNext is the time-shifting property. If you shift a function in time ‚Äî let‚Äôs say you move it by x naught ‚Äî then in the frequency domain, the transform stays the same in shape but gets multiplied by a complex exponential factor. Specifically, it‚Äôs multiplied by e to the power minus i 2 pi x naught u. So shifting in time introduces a phase shift in frequency.\n\nNow let‚Äôs flip that idea ‚Äî this is called modulation. If you multiply a function in the time domain by a complex exponential, then you shift its frequency content. So time-domain modulation causes a translation in the frequency domain.\n\nAnother important concept is scaling. If you compress or stretch a function in time ‚Äî say, you use f of a t ‚Äî then the frequency representation stretches or compresses in the opposite way. And the result is scaled by one over the absolute value of a. So if the time signal gets narrower, the spectrum spreads out.\n\nWe also have conjugation. If you take the complex conjugate of a function, then its Fourier transform reflects across the frequency axis ‚Äî that is, the transform at minus u becomes the conjugate of the transform at u.\nAnd finally, if the original function is real-valued, its Fourier transform has Hermitian symmetry. That means the spectrum is symmetric in a complex-conjugate sense.\n\nEach of these properties gives us deeper insight into how signals behave across time and frequency. We‚Äôll explore them in more detail as we continue ‚Äî but for now, keep these tools in mind. They‚Äôll help you decode and design systems with much greater clarity.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 6 - Slide24.txt", "file_path": "Lecture 6\\Texts\\Slide24.txt", "content": "Let's look into more detail. Okay, the first linearity. So this is just, again, this part copied from the Stanford textbook. So you have a function f and g. \n\nThere are Fourier transformations. So you have Fourier transformation. You have this summation, the summation of the original function. \nThen you perform Fourier transformation. The result is the same as the summation of the Fourier transformation of f and the Fourier transformation of g. So this is additivity. So in the system, linear system lecture, we explain that. \n\nHow about the scaling or homogeneity? So if f is scaled by alpha, then we perform Fourier transform. That's the same as performing the Fourier transform of f. Then you scale the result by the same scaling factor. So you can verify these two properties according to the formula by defining the Fourier transform. So you can just see here, just as an example, you show additivity. And you can similarly show the scaling property.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 6 - Slide25.txt", "file_path": "Lecture 6\\Texts\\Slide25.txt", "content": "Now let‚Äôs look at another important idea in Fourier analysis ‚Äî the shift property, also called the translation property.\n\nThis property explains what happens when we shift a function in time. Suppose we have a function, f of t, and we shift it by some constant b. That means our new function is f of t minus b. So what happens to its Fourier transform?\nThe answer is: the shape of the spectrum stays the same, but we multiply it by a phase factor. Specifically, we multiply it by e to the power minus 2 pi i s b. In other words, shifting the function in time causes a rotation in the phase of the frequency spectrum.\n\nLet‚Äôs go through why that‚Äôs true.\nWe start with the definition of the Fourier transform of f of t minus b. Inside the integral, we make a substitution: we let u equal t minus b. That‚Äôs just a change of variable ‚Äî and it doesn‚Äôt affect the limits of the integral, which still go from negative infinity to positive infinity.\n\nNow, when we rewrite the integral in terms of u, something interesting happens. The exponential term splits into two parts. One part depends on u, and the other depends on b. The part with b is just a constant, so we can pull it outside the integral.\nWhat remains inside is the same integral we started with ‚Äî the Fourier transform of the original function, f of t.\nSo in the end, we have the original spectrum, multiplied by this exponential phase term.\n\nThat‚Äôs the core idea of the shift property:\u000bWhen you shift a function in time, it introduces a phase shift in the frequency domain.\nThis is an extremely useful result when working with signals that are delayed or moved in time ‚Äî it tells you exactly how the spectrum changes, and helps preserve the full mathematical relationship between time and frequency.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 6 - Slide26.txt", "file_path": "Lecture 6\\Texts\\Slide26.txt", "content": "Let‚Äôs take a closer look at the scaling property of the Fourier transform.\nHere‚Äôs the idea. Suppose we have a function, f of t. And now, instead of f of t, we look at f of a times t, where a is just some constant ‚Äî not zero.\n\nWe‚Äôre changing the time scale. The question is: what happens to its Fourier transform?\nWell, this depends on whether a is positive or negative. But in both cases, something interesting happens.\nFirst, let‚Äôs say a is greater than zero.\nWe plug f of a t into the Fourier transform formula. If you do the math ‚Äî which involves changing variables and rearranging terms ‚Äî you end up with this:\nThe Fourier transform of f of a t becomes\u000bone divided by a, times F of s divided by a.\n\nSo what does that mean?\nIf we stretch the function in time ‚Äî that is, make it slower ‚Äî its frequency content gets compressed.\u000bAnd if we squeeze it in time ‚Äî make it faster ‚Äî the frequency content stretches out.\n\nNow, what if a is less than zero?\nIn that case, the same formula still works, but with a minus sign that comes from flipping the limits of integration.\n\nSo we just take the absolute value of a, and the final result becomes:\nThe Fourier transform of f of a t equals\u000bone over the absolute value of a, times F of s over a.\n\nSo in short:\u000bScaling in time causes the opposite effect in frequency.\u000bThat‚Äôs the scaling property.\nAnd this is one more way to see how time and frequency are tightly connected in the Fourier world.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 6 - Slide27.txt", "file_path": "Lecture 6\\Texts\\Slide27.txt", "content": "So now, let‚Äôs bring this scaling idea to life with a visual example.\n\nTake a look at the top plot. This shows different versions of a triangular function ‚Äî blue, orange, and green. They all have the same shape, but different widths.\nThe blue triangle is the narrowest.\u000bThe orange one is a little wider.\u000bAnd the green one is the widest of all.\nNow, let‚Äôs look at what happens in the frequency domain ‚Äî shown in the bottom plot.\n\nHere‚Äôs the key idea:\u000bWhen the function gets wider in time, its frequency content gets narrower.\nYou can see that clearly:\nThe narrow blue triangle has the widest Fourier transform ‚Äî that‚Äôs the blue curve in the bottom plot.\nThe medium-width orange triangle gives a more focused frequency response ‚Äî that‚Äôs the orange curve.\nAnd the wide green triangle gives a sharp, tall peak in the frequency domain ‚Äî the green curve below.\nThis is the heart of the scaling property.\u000bWider in time means tighter in frequency, and vice versa.\n\nIt‚Äôs a kind of duality ‚Äî as if time and frequency are pulling on opposite ends of a rope.\nAnd as a special case, if you use a delta function ‚Äî which is infinitely narrow in time ‚Äî its Fourier transform becomes perfectly flat, spread across all frequencies.\nOn the other hand, a Gaussian function is balanced ‚Äî it‚Äôs the only shape where both the time and frequency profiles remain Gaussian.\nSo visually, scaling is not just a math trick ‚Äî it‚Äôs a powerful way to understand how functions behave in both domains.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 6 - Slide28.txt", "file_path": "Lecture 6\\Texts\\Slide28.txt", "content": "Now let‚Äôs talk about a beautiful and surprising property of the Fourier transform ‚Äî its connection to derivatives.\nSuppose we start with a function, f of t, and its Fourier transform is capital F of s.\n\nNow you might ask:\u000bWhat happens if we take the derivative of f ‚Äî that is, f prime of t ‚Äî and then perform the Fourier transform?\nThe answer is elegant:\u000bTaking a derivative in time becomes multiplication in the frequency domain.\n\nMore precisely,\u000bThe Fourier transform of f ' of t is just 2 pi i s times the Fourier transform of f.\u000bSo again, taking a derivative becomes a simple multiplication ‚Äî and the factor depends on frequency.\nThere‚Äôs also a second version of this rule:\u000bIf you take the derivative of the Fourier transform itself with respect to s,\u000bThat‚Äôs the same as applying -2 pi i t to f of t ‚Äî and then transforming it.\n\nSo in summary:\nDerivatives in one domain correspond to multiplications in the other domain.\nAnd this duality works both ways ‚Äî from time to frequency, or frequency to time.\nThis relationship is incredibly useful in practice, especially in physics and engineering, where differentiation often pops up in systems and signals.\n\nAnd best of all, it‚Äôs not magic ‚Äî you can prove this directly from the Fourier transform formula, just by carefully taking the derivative under the integral.\nSo this derivative property really adds to our collection of powerful tools in the Fourier toolbox.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 6 - Slide29.txt", "file_path": "Lecture 6\\Texts\\Slide29.txt", "content": "Now here's one of the most important Fourier transform pairs ‚Äî the impulse train, also called the comb function.\nThink of it this way:\u000b\nWe place delta functions at evenly spaced intervals ‚Äî for example, every delta t along the time axis.\u000bThis gives us a repeating sequence of sharp spikes ‚Äî like the teeth of a comb ‚Äî so we call it a comb function in time.\nMathematically, we write this as the sum of delta of t minus n delta t, summed over all integers n.\u000bThat just means we have delta functions at 0, at delta t, at 2 delta t, and so on, going both directions.\n\nNow here‚Äôs the key:\u000bWhen we take the Fourier transform of this comb in time, what do we get?\nSurprisingly, we get another comb ‚Äî this time in the frequency domain.\u000bBut the spacing changes ‚Äî instead of delta t, the spikes are now spaced by 1 over delta t.\nSo if we sample more tightly in time, the frequency comb spreads out.\u000bAnd if we stretch out the spacing in time, the frequency spikes get closer together.\nThis is exactly the scaling property we saw earlier ‚Äî a tight structure in one domain leads to a broad structure in the other.\n\nNow you might be wondering ‚Äî how do we handle all these delta functions?\u000bThey‚Äôre not ordinary functions ‚Äî they‚Äôre distributions or generalized functions.\u000bAnd we‚Äôre dealing with an infinite sum of them.\nThis touches on deeper mathematics ‚Äî involving convergence and rigor ‚Äî but for our purposes, this formula holds and is extremely useful.\u000bWe‚Äôll come back to this paired comb concept again in the next lecture, especially when we talk about sampling theory and the Fourier series.\n\nSo just remember:\u000bA comb in time transforms to a comb in frequency.\u000bAnd their spacing is inversely related ‚Äî one over the other.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 30, "slide_filename": "Slide30.txt", "slide_annotation": "Lecture 6 - Slide30.txt", "file_path": "Lecture 6\\Texts\\Slide30.txt", "content": "Let‚Äôs now talk about one of the most powerful tools in the Fourier world ‚Äî the convolution theorem.\n\nIn plain terms, the convolution theorem says this:\nConvolution in the time domain becomes multiplication in the frequency domain.\n\nLet me say that again, but clearly:\u000bIf you have two functions ‚Äî let‚Äôs call them f of t and g of t ‚Äî and you convolve them together, then their Fourier transform is simply the product of their individual transforms.\nSo:\u000bIf the Fourier transform of f of t is capital F of s,\u000band the transform of g of t is capital G of s,\u000bthen the transform of f convolved with g is F of s times G of s.\nThat‚Äôs the core idea.\n\nNow why does this matter?\nBecause convolution ‚Äî though useful ‚Äî is often hard to compute directly.\u000bIt involves flipping one function, shifting it, multiplying, and integrating. That‚Äôs a lot of work!\nBut thanks to this theorem, we can skip all that.\u000bInstead, we go to the frequency domain, multiply two functions ‚Äî much simpler ‚Äî and then just come back to the time domain using an inverse transform.\n\nLet‚Äôs walk through a concrete example.\nThink about a gate function ‚Äî also called a rectangular pulse. Its Fourier transform is a sinc function ‚Äî which looks like a smooth wave with side ripples.\nNow, if we take two gate functions and convolve them in time, we get a triangle function.\n\nSo what happens in the frequency domain?\nEach gate becomes a sinc function. When we multiply the two sinc functions together, we get sinc squared.\nThat is ‚Äî sinc of s times sinc of s gives you sinc squared of s.\n\nSo the triangle function ‚Äî which was a convolution of two gates ‚Äî has a Fourier transform equal to sinc squared.\nThat‚Äôs a beautiful result, and it all comes from the convolution theorem.\nIt also shows how Fourier analysis turns complicated operations into simpler ones, especially in engineering, signal processing, and physics.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 31, "slide_filename": "Slide31.txt", "slide_annotation": "Lecture 6 - Slide31.txt", "file_path": "Lecture 6\\Texts\\Slide31.txt", "content": "Now let‚Äôs take a moment to understand why the convolution theorem actually works. What‚Äôs the reasoning behind it?\nSo far, we've seen that the Fourier transform of the convolution of two functions ‚Äî say, f of t and g of t ‚Äî turns into a simple multiplication of their individual transforms. \n\nBut why is that true?\nLet‚Äôs walk through the logic behind it, one step at a time.\nWe start with this:\u000bMultiply the Fourier transform of f of t by the Fourier transform of g of t. That means you're multiplying two integrals. One for g of t times a complex exponential, and the other for f of x times the same kind of exponential ‚Äî but with a different variable.\nTo combine them, we use different dummy variables inside the integrals ‚Äî like t in one and x in the other ‚Äî so they don't interfere. Then we combine them into a double integral.\n\nNow we group the exponential terms. The two exponentials become a single exponential with t plus x in the exponent.\nThis sets up our key trick: we change variables. In the inner integral, let‚Äôs say\u000bu equals t plus x. That means t equals u minus x, and since we're just changing variables inside an integral, the limits stay the same.\nSo now, what used to be g of t becomes g of u minus x, and the exponential becomes a function of u. This lets us express the entire inside of the integral in terms of u, and f of x stays outside.\nAt this point, we switch the order of integration ‚Äî that‚Äôs allowed because the functions we‚Äôre working with are well-behaved. So now we‚Äôre integrating over u, with x nested inside.\n\nWhat you get in the end is this:\u000bYou have e to the power minus 2 pi i s  u ‚Äî that‚Äôs the same complex exponential from the Fourier formula ‚Äî and it‚Äôs multiplied by the convolution of g and f at position u.\nThat‚Äôs the key. The inner integral has now become the convolution of g and f, evaluated at u.\nSo finally, this whole thing is just the Fourier transform of that convolution.\nThat‚Äôs the magic of it.\u000b\nMultiplication in the Fourier domain really does correspond to convolution in the time domain ‚Äî and now you‚Äôve seen where it comes from.\nOf course, these steps are dense, and I encourage you to go over them again slowly. Grab a coffee, sit back, and follow the substitutions. Each step follows cleanly from the last ‚Äî and the full picture is quite elegant.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 32, "slide_filename": "Slide32.txt", "slide_annotation": "Lecture 6 - Slide32.txt", "file_path": "Lecture 6\\Texts\\Slide32.txt", "content": "So let‚Äôs take a step back and ask‚Äîwhy is convolution in the time domain equivalent to multiplication in the frequency domain?\n\nTo understand this, let‚Äôs think about a shift-invariant linear system. If you feed a sinusoidal signal into such a system, the output will also be sinusoidal. And‚Äîhere‚Äôs the key‚Äîit will be at the same frequency. The system might change the amplitude or the phase, but it doesn‚Äôt generate new frequencies. That‚Äôs the hallmark of a shift-invariant, or time-invariant, linear system.\nThis leads to a deep and beautiful conclusion. If sinusoidal signals are preserved in shape and frequency, then the effect of the system on each frequency component can be described by a single number‚Äîjust a scaling factor that depends on frequency.\n\nSo, imagine you take any general signal, and you decompose it into a sum of sinusoidal components. The system acts on each component individually, scaling each one by a different amount. That‚Äôs exactly what multiplication looks like in the Fourier domain. Each frequency is being multiplied by its own gain factor.\nThat‚Äôs why convolution in the time domain turns into multiplication in the frequency domain.\n\nNow, here's something important: this only works because of that special property of sinusoids. Among all functions, only sinusoids maintain their shape when passing through a shift-invariant linear system. Delta functions, for instance, don‚Äôt behave that way‚Äîthey get transformed into something entirely different. But a cosine remains a cosine. A sine remains a sine. Just possibly scaled or phase-shifted.\nAnd this is what makes the Fourier transform so unique. It‚Äôs the only transform for which the convolution theorem holds‚Äîbecause only sinusoids have this invariance.\n\nYou might wonder‚Äîcould we define a similar convolution theorem using other transforms, like wavelet transforms or Hadamard transforms? The answer is no. Those basis functions don‚Äôt have this shape-preserving property under linear shift-invariant systems. So the convolution theorem doesn‚Äôt hold for them.\n\nNow if you‚Äôre curious, you can actually find some advanced papers on this. Try searching for ‚Äúcharacterization of the convolution theorem.‚Äù You‚Äôll find rigorous mathematical proofs that ultimately say the same thing: the convolution theorem is special to the Fourier transform.\nBut you don‚Äôt need pages of math to get the intuition. If a function keeps its shape through a system, and we can describe the system‚Äôs effect with a frequency-dependent multiplier, then that‚Äôs multiplication in the Fourier domain.\nAnd if you're passionate about the math behind this, you could even write a short paper exploring this idea. I'd be happy to discuss it with you.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 33, "slide_filename": "Slide33.txt", "slide_annotation": "Lecture 6 - Slide33.txt", "file_path": "Lecture 6\\Texts\\Slide33.txt", "content": "Now let‚Äôs talk about another important result in Fourier analysis ‚Äî Parseval‚Äôs Identity.\n\nThis identity says that the total energy of a signal, when measured in the time domain, is exactly equal to the total energy in the frequency domain. Mathematically, it's written as:\u000bthe integral of the square of the function f of t over all time, equals the integral of the square of its Fourier transform ‚Äî f hat of s ‚Äî over all frequencies.\nLet me give you some intuition here.\n\nSuppose you have a function, maybe it‚Äôs an electric signal or a sound wave. In the time domain, the quantity f of t squared represents the power of that signal at each instant. When you integrate this over time, you‚Äôre summing up all that power ‚Äî you get the total energy.\nNow, that same signal can be broken down into its frequency components using the Fourier transform. In that domain, each component has an amplitude, and squaring that amplitude gives you the energy associated with that frequency. Integrating over all frequencies gives you the total energy ‚Äî again.\n\nSo Parseval‚Äôs Identity tells us something very beautiful: energy is conserved between the time domain and the frequency domain.\nThis has a strong connection to what you learned in physics. Think about alternating current. If you have a sinusoidal current flowing through a resistor, the instantaneous power is proportional to the current squared. And when you integrate that over time, you get the total energy consumed by the resistor.\n\nNow, from a geometric perspective, you can think of f of t as a vector in an infinite-dimensional space. That sounds abstract, but it‚Äôs just like a regular vector ‚Äî except with infinitely many components. And the length of this vector is found by summing up the square of each component ‚Äî just like in ordinary geometry.\n\nThe Fourier transform is a kind of coordinate transformation ‚Äî like a rotation of the space. So when we go from the time domain to the frequency domain, we‚Äôre rotating the vector. But that doesn‚Äôt change its length. So geometrically, Parseval‚Äôs Identity simply says: the length of the signal vector is preserved under the Fourier transform.\nYou can also prove this identity using standard calculus and the definition of the Fourier transform. And yes, that‚Äôs a good exercise. But I want you to also grasp the meaning behind it.\n\nSo next time you see this identity, remember ‚Äî it‚Äôs not just a formula. It tells you that energy, or vector length, is preserved when moving between the time and frequency domains. And that is both physically and mathematically very powerful.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 34, "slide_filename": "Slide34.txt", "slide_annotation": "Lecture 6 - Slide34.txt", "file_path": "Lecture 6\\Texts\\Slide34.txt", "content": "So far, we‚Äôve been talking about transformations in one dimension ‚Äî mainly between time and frequency using the Fourier transform. But to truly understand what's going on, it helps to think geometrically.\nLet‚Äôs start with something more familiar ‚Äî a simple two-dimensional rotation.\n\nImagine we have a point in the 2D plane, described by coordinates x and y in a standard X Y coordinate system. Now, suppose we want to express this same point in a new coordinate system, X-prime and Y-prime, which is rotated by some angle phi from the original axes.\nAs the diagram shows, the new coordinates ‚Äî x-prime and y-prime ‚Äî can be computed from the original ones using the rotation formulas.\n\nSo x-prime equals x times cosine phi plus y times sine phi,\u000band y-prime equals negative x times sine phi plus y times cosine phi.\nYou can also write this transformation in matrix form. It‚Äôs a simple 2-by-2 rotation matrix.\nNow why is this relevant?\n\nBecause this kind of rotation is a basic example of an orthonormal transformation, just like the Fourier transform. When we rotate the coordinate system, we‚Äôre not changing the length of the vector ‚Äî we‚Äôre just viewing it from a different angle. The structure stays the same, just expressed differently.\n\nThis gives us a visual and intuitive way to think about more abstract transformations. In higher dimensions ‚Äî or even infinite dimensions ‚Äî the concept is similar. We‚Äôre taking a function, representing it in a new coordinate system, and the transformation preserves important properties like length and energy.\n\nThat‚Äôs why understanding this simple 2D rotation helps us better appreciate what the Fourier transform is doing. It‚Äôs not magic ‚Äî it‚Äôs geometry, just in a much bigger space.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 35, "slide_filename": "Slide35.txt", "slide_annotation": "Lecture 6 - Slide35.txt", "file_path": "Lecture 6\\Texts\\Slide35.txt", "content": "Now that we understand the Fourier transform in one dimension, let‚Äôs take it a step further ‚Äî to two dimensions.\n\nJust like in the 1D case, the idea is to take a function ‚Äî in this case, a function of two variables, x and y ‚Äî and express it in terms of its frequency content. But instead of waves traveling along a line, now we‚Äôre dealing with wave patterns that extend in all directions across a plane.\n\nThe 2D Fourier transform lets us analyze how these wave components ‚Äî traveling in the x direction, the y direction, or even diagonally ‚Äî contribute to the overall structure of the signal or image.\nThe formula here tells us how to compute the 2D Fourier transform. The function f of x and y is transformed into capital F of u and v, where u and v represent the spatial frequencies in the horizontal and vertical directions.\nAnd just like before, there‚Äôs an inverse formula that lets us go back from the frequency domain to the original spatial domain.\nNow, why is this so useful?\n\nBecause in many real-world applications ‚Äî like medical imaging, computer vision, or signal processing ‚Äî we deal with 2D data. Think of an image, for example. Each pixel represents a value at some x and y location. When we apply the 2D Fourier transform, we can analyze the texture, orientation, and frequency content of that image.\n\nOn the left side of the slide, we see a simple geometric object ‚Äî like a bar. On the right is its 2D Fourier spectrum. Notice how the orientation of the object affects the direction of the frequency response.\nSo just like in one dimension, we‚Äôre decomposing a complex signal into simpler, sinusoidal components ‚Äî but now in two dimensions. And that opens up a whole new world of possibilities.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 36, "slide_filename": "Slide36.txt", "slide_annotation": "Lecture 6 - Slide36.txt", "file_path": "Lecture 6\\Texts\\Slide36.txt", "content": "Let‚Äôs now look at a powerful application of the 2D Fourier transform ‚Äî noise suppression.\nTake a look at the image on the left. It‚Äôs a noisy version of a familiar test image ‚Äî lots of random graininess, especially in the background and darker areas.\n\nNow, if we perform a 2D Fourier transform ‚Äî which we indicate here by \"F T\" ‚Äî we move from the spatial domain to the frequency domain. The result is this bottom-left image, which shows the frequency spectrum of the noisy image.\nNotice something important: most of the meaningful image information is concentrated around the center, which corresponds to the low-frequency components. But the noise is spread out ‚Äî especially toward the edges ‚Äî as high-frequency speckles.\nAnd this gives us an idea.\n\nWhat if we simply suppress or remove those high-frequency components? That‚Äôs what we‚Äôre doing on the bottom-right: we apply a mask ‚Äî setting the noisy high-frequency regions to zero, while keeping only the central, low-frequency region.\n\nThen we perform the inverse Fourier transform ‚Äî marked here as \"I F T\" ‚Äî to convert back to the spatial domain.\nAnd just like that, we get a much cleaner version of the original image. Most of the noise is gone, and the essential structure is preserved.\nThis is one of the key strengths of working in the frequency domain. Certain operations ‚Äî like filtering or noise removal ‚Äî can be done more easily, more effectively, and sometimes more intuitively after transformation.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 37, "slide_filename": "Slide37.txt", "slide_annotation": "Lecture 6 - Slide37.txt", "file_path": "Lecture 6\\Texts\\Slide37.txt", "content": "Let‚Äôs take this a step further and look at low-pass and high-pass filtering using the Fourier transform.\n\nStart with the image on the far left ‚Äî this is our original image of a building. Below it, you see the corresponding frequency spectrum. It contains both low-frequency components, which encode smooth variations like brightness and shading, and high-frequency components, which capture edges and fine details.\n\nNow look at the middle column.\nHere, we‚Äôve applied a low-pass filter. That means we kept only the low-frequency components ‚Äî those near the center of the Fourier spectrum ‚Äî and removed the rest. You can see the filtered spectrum just below. The result, shown in the middle image above, is a smoothed version of the original ‚Äî the fine details are gone, and only the broad, soft structures remain.\n\nNext, we move to the rightmost column.\nThis time we‚Äôve done the opposite. We removed the low frequencies and kept only a selected band of high-frequency components. You can see that in the frequency plot ‚Äî a ring where the center is zeroed out. The corresponding spatial image above now reveals only the edges ‚Äî the sharp transitions in intensity.\nSo by simply choosing which frequency bands to preserve or remove, we can control the kind of information we keep ‚Äî soft versus sharp, background versus boundary.\n\nThis is another powerful reason why we often work in the frequency domain ‚Äî filtering becomes intuitive and flexible.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 38, "slide_filename": "Slide38.txt", "slide_annotation": "Lecture 6 - Slide38.txt", "file_path": "Lecture 6\\Texts\\Slide38.txt", "content": "Let‚Äôs now walk through an important example: the two-dimensional rectangle function, centered at the origin, with side lengths X and Y.\n\nThis function is quite simple in the spatial domain ‚Äî it's just a bright rectangular block surrounded by zeros. But when we take its 2D Fourier transform, something fascinating happens.\nThe resulting function in the frequency domain is a product of two sinc functions ‚Äî one in the u direction and one in the v direction. And remember, sinc functions come from the Fourier transform of a box function in one dimension. So when we go to two dimensions, the result is just a multiplication of two of them ‚Äî one along each axis.\n\nYou can see this result visualized in the bottom right. There's a sharp peak in the center ‚Äî that‚Äôs the low-frequency content ‚Äî and then it decays with oscillations outward, characteristic of sinc behavior.\n\nNow here‚Äôs something more interesting: when we move into higher dimensions, something new becomes possible ‚Äî rotation. In 1D, you can only flip a signal left or right, but there's no true concept of rotation. However, in 2D or 3D, you can rotate the object, and that rotation affects the frequency domain in a meaningful way.\nThis rotational property is one of the key advantages of analyzing signals in higher dimensions. It allows us to understand and manipulate how orientation in space translates into patterns in frequency.\n\nSo this simple example ‚Äî a rectangular function ‚Äî gives us a powerful insight into the structure of Fourier transforms in two dimensions.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 39, "slide_filename": "Slide39.txt", "slide_annotation": "Lecture 6 - Slide39.txt", "file_path": "Lecture 6\\Texts\\Slide39.txt", "content": "Let‚Äôs talk about something elegant and powerful ‚Äî the rotation property of the Fourier transform.\n\nHere‚Äôs the idea: if you rotate a two-dimensional function in space, then its Fourier transform also rotates ‚Äî by the exact same angle and in the same direction. That means the structure of the frequency content preserves the orientation of the original signal.\n\nFor instance, if you rotate an image by 30 degrees counterclockwise, the entire frequency spectrum also rotates by 30 degrees counterclockwise. The shape and the distribution of frequencies remain the same, just oriented differently. This behavior is not just limited to 2D ‚Äî it holds in any number of dimensions.\n\nWhy does this happen? Well, it comes from the mathematics of how the Fourier transform is defined. If we apply a rotation matrix ‚Äî let‚Äôs call it R theta ‚Äî to the spatial variable, we see that the frequency variable ends up rotated in exactly the same way. It‚Äôs a symmetry property built into the transform itself.\n\nNow, the proof is shown here, and you‚Äôre welcome to follow through it if you like. It involves a change of variables and some linear algebra, but the takeaway is beautifully simple: rotate the function, and its spectrum rotates too.\nThis rotational invariance is one of the reasons Fourier analysis is so useful in imaging, especially in applications like object detection, texture analysis, and pattern recognition ‚Äî where orientation should not change the essential features.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 40, "slide_filename": "Slide40.txt", "slide_annotation": "Lecture 6 - Slide40.txt", "file_path": "Lecture 6\\Texts\\Slide40.txt", "content": "Let me now give you a more intuitive, geometric explanation for the rotation property of the Fourier transform.\nSuppose you have a 2D function ‚Äî like the white rectangular bar you see on the top-left image. This function has a certain frequency structure, and that‚Äôs captured in its Fourier transform, shown to the right.\n\nNow, here‚Äôs the key idea: each point in the Fourier spectrum corresponds to a sinusoidal wave component ‚Äî a wave moving in a specific direction and with a certain frequency. When you sum up all these wave components, you reconstruct the original image.\n\nSo what happens when we rotate the original function? Say we rotate it 45 degrees counterclockwise. Well, to reconstruct this rotated function using the same wave-based approach, each of those original wave components must also rotate by the same 45 degrees. That‚Äôs the only way they‚Äôll still combine to match the rotated shape.\n\nAs you can see in the second row, the rotated spatial function leads to a rotated frequency spectrum. Both are rotated by the same angle, preserving the overall structure. This is what we mean by the rotation property of the Fourier transform.\nYou can verify this rigorously with mathematics, but this geometric reasoning gives us an intuitive, visual understanding of what‚Äôs happening.\n\nThis is part of a much deeper concept: the duality of information. You can think of an image as made of individual points ‚Äî pixels or voxels ‚Äî or as made of sinusoids, each with different directions and frequencies. These two views are completely interchangeable, and Fourier analysis is the bridge between them.\n\nThat‚Äôs why this topic is so foundational. It‚Äôs not just about imaging. Fourier analysis underlies many areas ‚Äî in physics, engineering, and even mathematics ‚Äî wherever we want to understand structure in terms of frequency.\nNow let‚Äôs take a moment to summarize what we‚Äôve learned so far.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 41, "slide_filename": "Slide41.txt", "slide_annotation": "Lecture 6 - Slide41.txt", "file_path": "Lecture 6\\Texts\\Slide41.txt", "content": "Maybe you'll enjoy this little logo ‚Äî it‚Äôs a compact way to capture the spirit of what we've been discussing.\nOn the left, we see the Greek letter delta. That symbolizes the delta function, which represents the particle-like, pointwise nature of signals ‚Äî sharp, localized, discrete. It's our way of representing structure in the spatial or time domain.\n\nNext to it is e to the power i-theta, a complex exponential. This captures the wave nature ‚Äî smooth, oscillating, continuous. It represents sinusoids, and it's central to how we describe frequency, phase, rotation, and oscilation in the complex plane, describing wave propagation.\n\nSo together, delta and e to the i-theta ‚Äî one representing particles, the other waves ‚Äî form a kind of duality. And this duality runs throughout everything we‚Äôve covered: convolution, linear systems, Fourier series, and Fourier transforms.\nThe real power of Fourier analysis is in how it unifies these two views. A signal can be described by where things happen ‚Äî using delta functions ‚Äî or by how things oscillate ‚Äî using sinusoids. This interplay is what makes the field so rich and so widely applicable.\n\nThat‚Äôs why I like this little symbol. It‚Äôs simple, but it reflects deep ideas that are at the heart of signal processing and medical imaging.  We will understand this logo more and more as we unravel its meaning in this and next several lectures.", "total_slides_in_lecture": 42}
{"lecture": "Lecture 6", "slide_number": 42, "slide_filename": "Slide42.txt", "slide_annotation": "Lecture 6 - Slide42.txt", "file_path": "Lecture 6\\Texts\\Slide42.txt", "content": "Alright, to wrap up today‚Äôs session, here‚Äôs your homework assignment.\n\nFirst, I‚Äôd like you to read about the uncertainty principle of the Fourier transform. This is a fascinating and important concept. Summarize it in your own words, but keep it concise ‚Äî no more than three sentences.\n\nSecond, I want you to analytically compute the Fourier transform of this function: the exponential of b times t, multiplied by the step function u of negative t. Here, b is a positive constant, and u of t is defined as 1 for positive time and 0 otherwise.\nThe due date is one week from now, by midnight next Friday. Please make sure to upload your report to MLS.\n\nAnd before we end, I‚Äôve heard from a few students that the Fourier series material was a bit confusing. If that‚Äôs you, don‚Äôt hesitate ‚Äî come talk to me now or reach out later. I‚Äôm happy to help clarify anything.\nThat‚Äôs all for today ‚Äî thank you!", "total_slides_in_lecture": 42}
{"lecture": "Lecture 7", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 7 - Slide1.txt", "file_path": "Lecture 7\\Texts\\Slide1.txt", "content": "Welcome, everyone. Today, we‚Äôre diving into the topic of signal processing, which is a key concept in this course and incredibly important in the field of biomedical imaging.\n\nSome of you may have already gone through the chapter in the textbook ‚Äî and that‚Äôs a great start. But as I‚Äôve mentioned before, the draft versions of these chapters may still contain a few typos or unclear steps. That‚Äôs why it‚Äôs important not just to read, but also to listen carefully during the lecture.\n\nI‚Äôve reviewed and corrected the derivations we‚Äôll go over today, so what you see here should be more accurate and easier to follow than the original draft. This is also a good example of why attending the lecture ‚Äî or in this case, watching this video ‚Äî is still valuable, even if you‚Äôve read the chapter. I‚Äôll walk you through the ideas step by step, explain the reasoning behind each move, and help you build a stronger intuition than what the written text alone might offer.\n\nSo after this session, I encourage you to review both the lecture and the textbook. With both perspectives, your understanding will be much deeper ‚Äî and that‚Äôs the goal.\nLet‚Äôs get started.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 7 - Slide2.txt", "file_path": "Lecture 7\\Texts\\Slide2.txt", "content": "And again, we are on schedule. And so far, we have learned the linear systems and learned the Fourier analysis.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 7 - Slide3.txt", "file_path": "Lecture 7\\Texts\\Slide3.txt", "content": "Let me show you a logo I designed to help you understand the foundation of signal processing in a visual way.\nInside the circle, you‚Äôll see the expression: delta multiplied by e to the power i theta.\u000bThat‚Äôs delta times e to the i theta.\nThis might remind you of Euler‚Äôs formula, where e to the power i theta equals cosine theta plus i times sine theta.\n\u000bSo this single term includes both a real part ‚Äî the cosine ‚Äî and an imaginary part ‚Äî the sine.\nNow, the delta function here stands for an impulse ‚Äî something that happens instantly, at a single point in time.\nTogether, the delta and the exponential show us two building blocks we‚Äôll come back to again and again: sharp spikes and smooth waves.\nBut that‚Äôs not enough to build real-world signals.\n\nTo represent a general signal, we also need to be able to shift and scale these building blocks.\nShifting means moving a function left or right in time.\u000bScaling means stretching or compressing it, or changing its amplitude.\nFor example, with sine waves, we can change the frequency ‚Äî making them faster or slower.\u000bWe can also shift them in time ‚Äî or scale the height ‚Äî to make them louder or softer.\n\nSo when we say ‚Äúoperators need to shift and scale,‚Äù we mean we need to move and adjust these basic functions ‚Äî the delta and the sine wave ‚Äî to create more complex signals.\nAnd with that power, we can build up any continuous or piecewise continuous function.\nThat‚Äôs the big idea behind this logo ‚Äî a visual summary of how signal processing works at its core.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 7 - Slide4.txt", "file_path": "Lecture 7\\Texts\\Slide4.txt", "content": "Now let‚Äôs talk more specifically about how a function can be represented using the Fourier series or the Fourier transform.\n\nAt the top, we see a Fourier series representation. It tells us that a function of t ‚Äî written as f of t ‚Äî can be expressed as a sum over many terms.\nMathematically, this is written as:\nf of t equals the sum from n equals negative infinity to positive infinity, of c  n times e to the power 2 pi i n t over capital T.\nEach term in the sum is a complex exponential ‚Äî and each coefficient, c  n, tells us how much of that particular frequency component is present in the signal.\n\nNow how do we calculate these coefficients?\nWe use the formula:\nc  n equals 1 over capital T, times the integral from 0 to capital T, of f of t multiplied by e to the power negative 2 pi i n t over capital T, d t.\nSo what we‚Äôre doing here is projecting the function f of t onto a basis function ‚Äî that basis is the complex exponential. This is very much like taking an inner product between vectors.\n\nYou can think of the function as living in a high-dimensional space ‚Äî in fact, infinitely many dimensions. Each basis function spans one of those directions. So when we compute c  n, we‚Äôre measuring how much f of t points in the direction of that basis function.\nNow, what if the function isn‚Äôt periodic, or the period becomes very large?\nThat‚Äôs where the Fourier transform comes in.\n\nInstead of summing over discrete frequencies, we move to a continuous frequency variable, often called s.\nThe Fourier transform of f of t is written as f hat of s, and it‚Äôs defined as:\nThe integral from negative infinity to positive infinity, of f of t times e to the power negative 2 pi i s t, d t.\nTo recover the original function, we use the inverse Fourier transform, which is:\nf of t equals the integral from negative infinity to positive infinity, of f hat of s times e to the power 2 pi i s t, d s.\n\nSo you can see ‚Äî when we move from a periodic function with period capital T, to a general function as T approaches infinity, the Fourier series becomes the Fourier transform.\nThis is a very elegant transition ‚Äî from a discrete sum to a continuous integral ‚Äî and it forms the basis of much of signal processing.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 7 - Slide5.txt", "file_path": "Lecture 7\\Texts\\Slide5.txt", "content": "Now that we‚Äôve covered the basics of the Fourier series and transform, let‚Äôs talk about a very important result: the Convolution Theorem.\nThis theorem creates a beautiful bridge between two worlds ‚Äî the time domain and the frequency domain.\n\nHere‚Äôs what it says:\u000bConvolution in the time domain becomes multiplication in the frequency domain.\nIn simple terms, if you have a linear, time-invariant system, the output is the convolution of the input signal with the system‚Äôs impulse response.\u000bBut instead of computing that convolution directly ‚Äî which can be messy ‚Äî you can switch to the frequency domain, where it becomes much easier.\n\nSo, let‚Äôs say:\nf of t is your input signal,\ng of t is the system‚Äôs impulse response,\nand f star g means the convolution of f and g.\nThen, in the frequency domain, their Fourier transforms ‚Äî call them F of s and G of s ‚Äî satisfy this simple rule:\nThe Fourier transform of the convolution equals the product of the individual Fourier transforms.\n\nOr in math:\u000bFourier of f star g equals F of s times G of s.\nThat means you now have two ways to compute the system‚Äôs output:\nYou can do convolution in the time domain.\nOr ‚Äî you can transform both the input and the impulse response into the frequency domain, multiply them there, and then take the inverse Fourier transform to go back to the time domain.\nNow, let‚Äôs look at a simple example.\n\nSuppose you convolve two rectangular functions, also called gate functions. Each one looks like a block or a box.\nTo compute the convolution, you flip one function, shift it, multiply the overlapping parts, and add the result.\nWhen the two rectangles perfectly overlap, the area under the curve reaches its maximum. As they move apart, the overlapping area decreases linearly ‚Äî and what you get is a triangle-shaped function.\nSo, two rectangles convolved give you a triangle.\n\nNow let‚Äôs look at this in the frequency domain.\nWe know the Fourier transform of a rectangular function is a sinc function ‚Äî that‚Äôs sinc of s, which comes from sine of pi s over pi s.\nSo when we convolve two rectangles in the time domain, we get a triangle. That means, in the frequency domain, we multiply two sinc functions ‚Äî and we get sinc squared.\nSo: Triangle in time domain means sinc squared in frequency domain.\n\nThis is a powerful example that helps you visualize the convolution theorem in action. It shows how the shapes in time relate to the shapes in frequency ‚Äî and why this theorem is such a key tool for signal processing and systems analysis.\nKeep this visual and mathematical link in mind ‚Äî it will come up again and again.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 7 - Slide6.txt", "file_path": "Lecture 7\\Texts\\Slide6.txt", "content": "Now, let‚Äôs step back for a moment and ask a deeper question: Why does the convolution theorem work the way it does? What makes it so special?\n\nIn the last lecture, I mentioned something that gives us a clue.\u000bIf you have a shift-invariant linear system, and you input a sinusoidal signal, what comes out is also a sinusoid, and ‚Äî importantly ‚Äî at the same frequency.\nThat‚Äôs a big deal.\nIt means the system doesn't generate any new frequencies. It doesn‚Äôt distort the sinusoid into some other shape. It simply scales it or shifts its phase ‚Äî but the frequency stays the same.\nThis is a unique and powerful property of sinusoids.\n\nNow let‚Äôs think about what that means for convolution and Fourier analysis.\nIn time-domain analysis, convolution is the operation that describes how a system responds to an input.\nBut if your input and your system are both described using sinusoids, and those sinusoids don‚Äôt change frequency, then it turns out convolution becomes multiplication in the Fourier domain.\n\nWhy?\nBecause in the Fourier domain, every signal is broken into sinusoids. And if each one stays intact ‚Äî same frequency in, same frequency out ‚Äî then multiplying their responses is all you need to compute the system output.\nThat‚Äôs exactly what the convolution theorem says:\u000bConvolution in time equals multiplication in frequency.\nBut ‚Äî and this is important ‚Äî this beautiful relationship only works because of the special behavior of sinusoids.\nIf your input signal isn‚Äôt made of sinusoids, or your system doesn‚Äôt treat them nicely, this property breaks down.\nThat‚Äôs why this theorem only exists for the Fourier transform, which is based entirely on sinusoids.\u000bOther transforms ‚Äî like wavelets or polynomials ‚Äî don‚Äôt have this same \"in equals out\" behavior, and so they don‚Äôt give us a convolution theorem in the same way.\n\nIf you‚Äôre curious, I actually encourage you to think about this more. You could even write a short paper or a reflection on it.\nHere‚Äôs something to try:\nTry proving that if a function goes into a system and the same form comes out ‚Äî unchanged except for amplitude and phase ‚Äî then that function must be a sinusoid.\n\nAnd also prove the related idea:\u000bIf sinusoids go in and come out with the same frequency, then convolution in time must match multiplication in frequency.\nThese are two different ideas ‚Äî but together, they give us the foundation for the convolution theorem and explain why Fourier analysis is so powerful for linear systems.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 7 - Slide7.txt", "file_path": "Lecture 7\\Texts\\Slide7.txt", "content": "Let‚Äôs now walk through an example to help us better understand why the convolution theorem works ‚Äî especially when sinusoids are involved.\n\nWe‚Äôll use a one-dimensional, shift-invariant linear system.\u000bIn such a system, the output ‚Äî which we‚Äôll call o of t ‚Äî is the convolution of the system‚Äôs impulse response h of t with the input signal, i of t.\nSo we write:\no of t equals h of t convolved with i of t.\nAccording to the convolution theorem, when we take the Fourier transform of both sides, this convolution becomes multiplication.\nSo we have:\ncapital O of u equals capital H of u times capital I of u,\u000bwhere capital O, H, and I are the Fourier transforms of the output, impulse response, and input, respectively, and u is the frequency variable.\n\nNow, let‚Äôs suppose we feed the system a complex sinusoidal input.\u000bThat is:\u000bi of t equals e to the power i 2 pi u naught t,\u000bwhere u naught is the frequency of the sinusoid.\nAs we learned earlier, the Fourier transform of a pure sinusoidal signal is a delta function.\u000bSo, in the frequency domain:\ncapital I of u equals delta of u minus u naught.\nThis delta function tells us the signal contains only one frequency ‚Äî u naught ‚Äî and no others.\n\nNow, using the convolution theorem:\ncapital O of u equals capital H of u times delta of u minus u naught.\nMultiplying anything by a delta function simply picks out the value at that point.\u000bSo this simplifies to:\ncapital O of u equals H of u naught times delta of u minus u naught.\nThis tells us that the output, in the frequency domain, also consists of a single frequency ‚Äî the same one as the input ‚Äî and it‚Äôs just scaled by the system‚Äôs transfer function at that frequency, which is H of u naught.\nNow we go back to the time domain.\n\nIf the Fourier transform of the output is a scaled delta function in frequency, then the time-domain output must also be a sinusoid ‚Äî at the same frequency.\nSo, we have:\no of t equals H of u naught times e to the power i 2 pi u naught t.\nThis shows that the output is still a complex sinusoid ‚Äî same frequency as the input ‚Äî but scaled by a factor, H of u naught, which may include amplitude and phase changes.\nThe key idea here is:\nThe frequency doesn't change.\nNo new frequencies are generated.\nThe output stays sinusoidal if the input was sinusoidal.\n\nThis confirms that a shift-invariant linear system preserves frequency when the input is a pure sinusoid.\nAnd we were able to demonstrate this clearly and quickly ‚Äî thanks to the convolution theorem and the properties of the Fourier transform.\nIt‚Äôs a great example of how math gives us not just a result, but deep insight into how signals behave in systems.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 7 - Slide8.txt", "file_path": "Lecture 7\\Texts\\Slide8.txt", "content": "Let me now show you the other direction of this idea ‚Äî and this time, we‚Äôre not going to rely on the convolution theorem directly.\n\nWe‚Äôll start with a simple question:\u000bCan any function other than a sinusoid go into a system and come out unchanged in shape ‚Äî just scaled by a constant?\nLet‚Äôs think about that.\n\nSuppose the impulse response of the system is a delta function. Then, any signal you put into that system just passes through exactly as it is ‚Äî unchanged. The system does nothing.\u000bThis is an ideal system ‚Äî it responds instantly and perfectly.\n\nFor example, if a camera had a perfect delta-function impulse response, every point in the scene would be captured exactly ‚Äî with perfect sharpness, no blur at all.\nBut in real life, that doesn‚Äôt happen. No camera has a true delta response.\n\nIn practical systems, the impulse response is not a delta function ‚Äî it‚Äôs more spread out. That means the system blurs things.\u000bEven if you feed it a sharp impulse, the output will be a wider, smeared version ‚Äî not the same as the input.\nSo what does that mean in the frequency domain?\n\nWell, we know that the Fourier transform of a delta function is a constant ‚Äî it's just one everywhere.\u000bThat‚Äôs because a delta contains all frequencies equally.\nBut if your impulse response is not a delta ‚Äî if it‚Äôs more spread out ‚Äî then its Fourier transform won‚Äôt be constant.\u000bIt will vary in frequency. That‚Äôs key.\n\nNow, let‚Äôs suppose you have some input signal ‚Äî let‚Äôs call it i of t ‚Äî and this signal goes into the system.\u000bYou get an output o of t, and let‚Äôs say the output has the exact same shape as the input, just scaled by some number alpha.\n\nThat is:\no of t equals alpha times i of t.\nThat sounds simple, but let‚Äôs look at what it means in the frequency domain.\nIf we take the Fourier transform of both sides, we get:\nCapital O of u equals alpha times capital I of u.\nBut from the convolution theorem, we also know:\nCapital O of u equals capital H of u times capital I of u.\nIf we compare those two expressions, we find:\nCapital H of u must equal alpha ‚Äî a constant.\nBut as we just said, that would only be possible if h of t ‚Äî the impulse response ‚Äî is a delta function.\u000bAnd we assumed it's not.\nSo we have a contradiction.\n\nThat tells us:\u000bNo function other than a sinusoid can go through a general shift-invariant linear system and come out with the same shape ‚Äî unless the system is trivial, like an ideal delta.\nOnly sinusoids have this amazing property:\u000bSinusoid in, sinusoid out ‚Äî same frequency, just scaled and shifted in phase.\nThat‚Äôs what makes them so special in Fourier analysis.\u000bThat‚Äôs why convolution in the time domain becomes multiplication in the frequency domain ‚Äî because the system affects each sinusoidal component independently and proportionally.\n\nSo what you see here is not just a bunch of equations ‚Äî it‚Äôs a deeper logic behind the theorem.\u000bUnderstanding this helps you move beyond just memorizing formulas ‚Äî it helps you truly see why Fourier analysis works the way it does.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 7 - Slide9.txt", "file_path": "Lecture 7\\Texts\\Slide9.txt", "content": "Let‚Äôs now turn to a beautiful result in Fourier analysis called Parseval‚Äôs Identity.\nThis identity tells us that the total energy of a signal ‚Äî measured in the time domain ‚Äî is exactly equal to the total energy in the frequency domain.\n\nMathematically, it says:\nThe integral of the absolute value squared of f of t, over all time, equals the integral of the absolute value squared of f hat of s, over all frequencies.\n\nIn simpler words:\u000bIf you take a signal and square its amplitude at each point in time, then add up all those values ‚Äî you get the total energy of the signal in the time domain.\nNow do the same thing in the frequency domain:\u000bTake the Fourier transform of the signal, square the magnitude of each frequency component, and integrate ‚Äî and you get the same total.\n\nThis is not just a coincidence. It‚Äôs a direct consequence of the fact that the Fourier transform is a unitary operator ‚Äî it preserves the ‚Äúlength‚Äù of the function, just like a rotation in vector space.\nThink of f of t and f hat of s as infinite-dimensional vectors.\u000bThen, this identity is telling you that their norms ‚Äî or their lengths ‚Äî are the same.\u000bSo, the energy stays the same when you move from one domain to the other.\nThis is why we sometimes say that the Fourier transform conserves energy.\n\nThe bottom part of the slide walks through the proof. Let me summarize the key steps:\nWe start with the inner product:\nIntegral of f of t times the complex conjugate of g of t.\nThen, we apply the inverse Fourier transform to g of t.\u000bThis gives us an expression involving f of t and the Fourier transform of g, which we call F g of s.\nThen we do some algebra ‚Äî using properties of complex conjugation, and switching the order of integration ‚Äî until we end up with:\nThe integral of f of s times the complex conjugate of g of s.\nSo in the end, the inner product in the time domain equals the inner product in the frequency domain.\n\nThat‚Äôs Parseval‚Äôs Identity in action ‚Äî and it works for all functions in an inner product space, not just in signal processing.\nIt‚Äôs one more reason why the Fourier transform is not just a clever trick ‚Äî it‚Äôs a deep and powerful tool grounded in geometry and physics.\nIf you‚Äôre interested, you can think of it this way:\u000bEvery time you transform a signal into the frequency domain, you‚Äôre rotating it into a new basis ‚Äî and the energy stays exactly the same.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 7 - Slide10.txt", "file_path": "Lecture 7\\Texts\\Slide10.txt", "content": "Let‚Äôs now apply Parseval‚Äôs theorem to a practical example ‚Äî the average power of a periodic signal.\n\nHere we‚Äôre working with a power signal, which is a signal that keeps oscillating over time, like a sine wave.\nThe average power P of such a signal over one period ‚Äî from minus capital T over 2 to plus capital T over 2 ‚Äî is given by:\nP equals 1 over T, times the integral of the absolute value of x of t squared, d t.\nAccording to Parseval‚Äôs theorem for power signals, we can also express this using the Fourier series coefficients.\u000bSo, power equals the sum of the absolute value squared of all c  n ‚Äî the Fourier coefficients ‚Äî from n equals minus infinity to infinity.\n\nLet‚Äôs go through the example.\nSuppose we have this signal:\nx of t equals 2 times sine of 100 t.\nThat‚Äôs just a sinusoidal wave with amplitude 2 and frequency 100.\nNow, suppose this signal is the voltage across a unit resistor ‚Äî that is, a resistor with resistance equal to 1 ohm.\nThen the instantaneous power is just the voltage squared, and the average power is the integral of the squared signal over one period, divided by T.\n\nLet‚Äôs calculate it first in the time domain:\nWe square the signal:\u000b2 times sine of 100 t becomes 4 times sine squared of 100 t.\nWhen we compute the average of that over one period, we get:\nP equals 2.\n\nNow let‚Äôs do it in the frequency domain, using the Fourier series.\nWe rewrite the sine wave using Euler‚Äôs formula:\nsine of 100 t equals e to the j 100 t minus e to the minus j 100 t, over 2 j.\nSo our signal becomes:\nx of t equals 2 times that expression, which simplifies to:\nminus j e to the j 100 t, plus j e to the minus j 100 t.\nComparing this to the general form of a Fourier series:\nx of t equals the sum of c  n times e to the j n omega naught t,\u000bwe can identify the coefficients:\nc  1 equals minus j, and\u000bc  minus 1 equals plus j.\n\nNow we apply Parseval‚Äôs theorem:\nThe total power is the sum of the squares of the magnitudes of these coefficients.\nSo we compute:\nabsolute value of c one squared, which is 1,\u000bplus absolute value of c minus one squared, also 1.\nAdding them gives us 2 ‚Äî the same result we got in the time domain.\n\nThis example shows how Parseval‚Äôs theorem works in practice.\nYou can either:\nSquare the signal and average it over time,\u000bor Decompose the signal into its sinusoidal components, square their coefficients, and sum them up.\n\nEither way, you‚Äôll get the same result. That‚Äôs the beauty of Fourier analysis ‚Äî it gives you two consistent views of the same signal.\nAnd this works not just for simple sine waves ‚Äî it holds even when a signal is made up of many frequencies, including very high ones.\u000bJust square each coefficient, add them up, and you‚Äôll know the total power of the signal.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 7 - Slide11.txt", "file_path": "Lecture 7\\Texts\\Slide11.txt", "content": "Let‚Äôs take a moment to review this important concept ‚Äî how a continuous function can be represented using delta functions.\nYou may remember that the delta function plays a central role in signal processing.\u000b\nAnd here's the key idea:\u000bWhen you multiply a delta function with a continuous function f of t, and integrate over all time, the result is just the value of f at the point where the delta is centered.\n\nMathematically, we write:\nThe integral from minus infinity to infinity of delta of t minus t naught, times f of t, d t ‚Äî equals f of t naught.\nThis tells us that the delta function samples the value of f at t naught.\u000bIt picks out that one value and ignores everything else.\n\nNow, you might wonder how this works ‚Äî so here‚Äôs an intuitive explanation.\nThink of the delta function as the limit of a very narrow and very tall pulse ‚Äî getting narrower and taller, but always keeping the same area, which is 1.\n\nSo if we take that narrow pulse, center it at t naught, and multiply it with f of t, we get a tiny slice of the function near t naught.\nWhen we integrate over that narrow region, the result becomes:\nf of t star, where t star is some point very close to t naught.\nAnd in the limit ‚Äî as the width goes to zero ‚Äî that becomes exactly:\nf of t naught.\nSo this is how we record a sample.\n\nIt also leads us to a deeper interpretation:\nWe can reconstruct the entire function f of t by summing up all these infinitesimal slices ‚Äî each one represented by a delta function, placed at the right location, and scaled by the value of the function at that point.\n\nThat‚Äôs what this second formula is saying:\nf of t equals the integral from minus infinity to infinity of delta of tau minus t, times f of tau, d tau.\nIt‚Äôs a kind of weighted sum of deltas ‚Äî where the weights are just the values of the function itself.\n\nYou can think of this as a very fine-grained, particle-like view of a continuous function.\u000bEach point ‚Äî each ‚Äúparticle‚Äù ‚Äî contributes through a delta function, and together, they rebuild the original signal.\nThis is not just a mathematical trick ‚Äî it connects deeply with how we sample, record, and reconstruct signals in the real world.\nAnd once again, it highlights why the delta function is so important ‚Äî both in theory and in practice.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 7 - Slide12.txt", "file_path": "Lecture 7\\Texts\\Slide12.txt", "content": "Let‚Äôs revisit the convolution theorem ‚Äî this time with a concrete visual example to help reinforce the idea.\nThis theorem is incredibly important, and we‚Äôll keep coming back to it.\u000bSo it's essential to get comfortable with how it works ‚Äî both mathematically and visually.\n\nLet‚Äôs look at what we have here.\nIn the top row, we see three time-domain signals:\nOn the left is f of x, a simple rectangular function, also known as a gate function, centered at zero and extending from minus b over 2 to plus b over 2.\nIn the middle is h of x, which consists of two delta functions ‚Äî located symmetrically around zero at minus a over 2 and plus a over 2.\nOn the right is the result of the convolution of f and h ‚Äî which we‚Äôll call g of x.\n\nNow remember:\u000bWhen you convolve a function with a delta, the result is simply a shifted version of that function.\nSo here, convolving the gate function f of x with each of the two deltas in h of x gives us two shifted gate functions ‚Äî one shifted left and one shifted right.\u000b\nWhen we add them together, we get g of x ‚Äî a pair of rectangular pulses.\nNow let‚Äôs look at the bottom row ‚Äî this is the frequency domain.\nHere‚Äôs where the convolution theorem comes in.\n\nAccording to the theorem:\nConvolution in the time domain becomes multiplication in the frequency domain.\nSo we take the Fourier transforms of the three signals:\nf of x becomes capital F of k, which is a sinc-shaped curve ‚Äî smooth and localized in frequency.\nh of x, which had two deltas in time, becomes a cosine wave in the frequency domain ‚Äî that‚Äôs capital H of k, centered at zero, with oscillations depending on the spacing of the deltas.\n\nNow, to get capital G of k, we multiply the two frequency-domain functions:\nCapital F of k times capital H of k equals capital G of k.\nAnd the result, shown on the right, is a modulated sinc ‚Äî essentially the original sinc shape, but with oscillations introduced by the cosine multiplier.\n\nSo the key takeaway is this:\nIn time, the convolution added and shifted the pulses.\nIn frequency, the multiplication mixed the smooth sinc shape with oscillations.\nThis is a perfect visual illustration of what the convolution theorem does:\nIt transforms an operation that involves sliding and summing in time into a much simpler pointwise multiplication in the frequency domain.\nUnderstanding this duality will help you immensely when working with signals ‚Äî whether in analysis, filtering, or system design.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 7 - Slide13.txt", "file_path": "Lecture 7\\Texts\\Slide13.txt", "content": "We‚Äôve now come to a natural turning point in our discussion.\nSo far, everything we‚Äôve studied ‚Äî from Fourier series and Fourier transforms, to convolution and delta functions ‚Äî has been in the continuous domain.\n\nBut now we need to ask a very important question:\u000bWhy do we work with digital signals? Why go digital?\n\nWell, here are some good reasons.\nFirst ‚Äî digital signals are exact.\u000bThey‚Äôre stored as numbers, so they don‚Äôt degrade over time the way analog signals do.\n\nSecond ‚Äî errors can be detected and corrected.\u000bDigital systems can automatically check for mistakes during transmission or storage ‚Äî and even fix them.\n\nThird ‚Äî noise and interference can be filtered out more easily.\u000bIn a digital system, it's much simpler to distinguish real data from unwanted signals.\n\nFourth ‚Äî with digital transmission, we can send multiple types of information over the same line ‚Äî audio, video, text ‚Äî all bundled together efficiently.\n\nAnd finally ‚Äî digital systems support data compression, which lets us send more information using less bandwidth.\n\nAll of this is why most modern communication, computation, and imaging systems have moved to the digital domain.\nAnd that brings us to the next part of our journey.\nLet‚Äôs study how to process digital signals next.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 7 - Slide14.txt", "file_path": "Lecture 7\\Texts\\Slide14.txt", "content": "Now that we‚Äôve seen why digital signals are so useful, let‚Äôs take a look at how real-world signals actually make their way into a computer.\nThis diagram shows the full journey ‚Äî from a physical system to digital data inside your laptop.\n\nLet‚Äôs walk through it step by step.\nIt all starts with the physical world ‚Äî maybe we‚Äôre measuring temperature, pressure, light intensity, motion, or some other physical quantity.\n\nTo capture this, we need a sensor ‚Äî or more specifically, a transducer.\u000bA transducer converts the physical phenomenon into an electrical signal ‚Äî usually an analog one.\u000bThis analog signal may be noisy, or it may contain extra fluctuations we don‚Äôt want.\n\nSo the next step is called signal conditioning.\u000bHere we might filter the signal, smooth it, amplify it, or perform other adjustments to get it into a clean, usable form.\u000bThis step helps us reduce noise and make sure the signal is suitable for the next stage.\n\nNow comes the critical component ‚Äî the Analog-to-Digital Converter, or A-D converter for short.\nThis is where the continuous analog signal gets sampled and turned into a digital signal ‚Äî a sequence of numbers the computer can store and process.\n\nMore specifically, to get a analog signal into the computer, let‚Äôs break down what happens during digitization with an A/D converter ‚Äî using this visual example.\n\nHere you see an analog signal, represented here as a smooth, continuous wave ‚Äî like a sound wave moving across time.\u000bActually, a digital computer can‚Äôt store this continuous curve directly.\u000bInstead, it must sample the signal at specific time points.\n\nAt each of these time points, we ask:\u000bWhat is the value of the signal right here?\u000bAnd that gives us a sequence of numbers.\nIn the rightmost illustration, you can see vertical bars that ‚Äúcatch‚Äù the wave at regular intervals.\u000bThese bars represent sampling operations ‚Äî and the height of each bar shows the value of the signal at that point in time.\n\nNow, here's something important:\u000bThe computer can‚Äôt store every possible value ‚Äî like pi=3.14159 etc. or e=2.71828 etc. with infinite precision.\u000bInstead, it must round the values to the nearest available level.\nThis process is called quantization.\u000bIt means converting a smooth, continuous range of amplitudes into a set of fixed, discrete values ‚Äî often represented by whole numbers.\n\nSo here, we get a string of numbers like:\u000b3, 5, 6, 6, 4, 2, 1, 2.\nAnd these numbers are what the computer stores.\nBut even these whole numbers aren‚Äôt stored in decimal form.\u000bComputers use binary ‚Äî just zeros and ones to represent either an integral or a decimal number.\n\nSo each number gets translated into binary. For example:\n1 becomes zero one,\n2 becomes one zero,\n3 becomes one one,\nand so on.\nThis is how a continuous analog signal ‚Äî like sound or temperature ‚Äî becomes a digital signal inside a computer:\u000bFirst through sampling across time, then through quantization of amplitude, and finally through binary encoding.\n\nEach step introduces a tradeoff:\nMore sampling points give better resolution in time.\nMore quantization levels give better precision in amplitude.\nBut more of both means more data to store and process.\nThis is the foundation of digital signal processing ‚Äî and now you‚Äôve seen how it all begins.\n\nAnd finally, that digital data ‚Äî made up of zeros and ones ‚Äî enters the computer, where it can be displayed, analyzed, stored, or transmitted.\n\nSo again, the pipeline goes like this:\nPhysical system ‚Üí transducer ‚Üí signal conditioning ‚Üí analog-to-digital conversion ‚Üí computer.\nThis entire process happens in almost every modern system ‚Äî from medical devices to smart homes to autonomous vehicles.\nAnd the better we understand this chain, the better we can design systems that are accurate, efficient, and reliable.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 7 - Slide15.txt", "file_path": "Lecture 7\\Texts\\Slide15.txt", "content": "So now that we‚Äôve brought our signal into the computer, let‚Äôs take a closer look at what‚Äôs really happening during analog-to-digital conversion.\n\nAs you can see in the top plot, we start with a smooth, continuous signal ‚Äî x of t ‚Äî defined for all time and with continuous amplitude.\nBut the computer can‚Äôt store that continuous curve ‚Äî it needs discrete data.\nSo we begin with the first major step: sampling, also called discretization in time.\nThis means we only record the value of the signal at selected time points ‚Äî evenly spaced along the horizontal axis.\u000bAt each of these points, we ‚Äúcatch‚Äù the value of the signal, just like placing a pin at that instant.\n‚Äò\nNow, the second step is quantization ‚Äî converting the amplitude of each sample into a finite-precision value.\nWe can‚Äôt store irrational numbers like pi or e with infinite precision.\u000bInstead, we round them to a reasonable approximation ‚Äî say, 3.14 for pi.\u000bAnd for our purposes, we assume this rounding is accurate enough to not affect the result significantly.\nSo in our analysis, we mostly ignore the quantization error and focus on sampling ‚Äî the time discretization.\n\nThe bottom diagram shows how the sampled signal looks:\u000bIt‚Äôs now a series of impulses ‚Äî each one located at a sample time and scaled to the value of the original signal at that moment.\nThis sequence of impulses is what we work with in digital signal processing.\n\nBut here‚Äôs something important to remember:\nThe real signal ‚Äî the one that matters in the physical world ‚Äî is still continuous.\nWhat we do with computers is a second-best approximation, based on discrete samples.\n\nSo the key question becomes:\nCan we process these sampled values in a way that still lets us understand or recover the original continuous signal?\nThis is the challenge at the heart of signal processing ‚Äî bridging the gap between discrete computation and continuous reality.\nAnd that‚Äôs what we‚Äôll be exploring in the next steps of our journey.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 7 - Slide16.txt", "file_path": "Lecture 7\\Texts\\Slide16.txt", "content": "Now, let‚Äôs go back and revisit the question we just posed:\u000bIf we take a continuous signal and convert it into discrete samples, can we still reconstruct the original signal?\nTo explore that, let‚Äôs start with this example ‚Äî a pure continuous wave.\n\nHere, we‚Äôve plotted the function\u000b5 times sine of 2 pi 4 t\u000bThat‚Äôs a simple sine wave with amplitude 5 and frequency 4 cycles per second ‚Äî or 4 hertz.\nAnd this signal is completely continuous in both time and amplitude.\u000b\nThat means it's defined for every instant in time, not just selected points, and the values can take on any number, not just a limited set.\nThis is the kind of signal you might get from an ideal microphone or sensor measuring vibration, sound, or light ‚Äî in the real world, before any digital conversion takes place.\n\nBut here‚Äôs the key point:\nThe computer can‚Äôt store this curve as it is.\u000bSo we need to figure out: How often do we need to sample this signal to capture all the important information?\u000b\nHow dense should our sampling be, so that we don‚Äôt lose critical features?\nAnd that question brings us right to the Sampling Theorem ‚Äî the mathematical foundation for how we digitize continuous signals without losing information.\nThat‚Äôs exactly what we‚Äôll examine next.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 7 - Slide17.txt", "file_path": "Lecture 7\\Texts\\Slide17.txt", "content": "So here we have another view of that same example.\nThis wave has a frequency of 4 hertz ‚Äî meaning it completes 4 full cycles per second.\n\nNow we‚Äôre sampling it with a very small interval, or in other words, at a very high rate: 256 samples per second. That‚Äôs far more frequent than the wave itself changes.\nAnd the result is what we call well-sampled data.\n\nAs you can see, the discrete dots ‚Äî the sampling points ‚Äî follow the shape of the original wave very closely.\u000bThere‚Äôs no confusion. No ambiguity. No major information is lost.\nThis is exactly what we want when digitizing a signal ‚Äî the samples capture the behavior of the original continuous signal with high accuracy.\n\nHeuristically, you can just look at the plot and say: yes, these samples preserve the essence of the original wave.\nSo, in this case, we can confidently say: the sampling was successful.\nBut what if we sample too slowly?\nLet‚Äôs take a look at that next.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 7 - Slide18.txt", "file_path": "Lecture 7\\Texts\\Slide18.txt", "content": "Now here‚Äôs what happens when the sampling is too slow.\nThis plot shows a rapidly oscillating signal ‚Äî it's a sine wave with a high frequency. But look at those red dots. Those are the actual points we sampled, and as you can see, there are only a few of them.\nThis is a classic case of under-sampling.\n\nIn this case, the original signal ‚Äî that fine blue waveform ‚Äî is oscillating at a frequency of 8 hertz. But the sampling rate is just 8.5 samples per second, which is not high enough to capture the details of the wave.\n\nSo what‚Äôs the problem?\nWell, based on just the red dots, it might look like the wave is slowly rising and falling ‚Äî as if it‚Äôs a low-frequency signal. But that‚Äôs not true at all.\nThis is known as aliasing ‚Äî when a high-frequency signal gets misinterpreted as something much lower in frequency, simply because it was sampled too slowly.\nSo even though you're doing regular sampling ‚Äî evenly spaced in time ‚Äî the result can be very misleading if your sampling rate is too low.\n\nThis example shows why choosing the right sampling rate is critical.\u000bToo slow, and you could end up completely misrepresenting your signal.\nNext, let‚Äôs talk about the theoretical limit ‚Äî the Nyquist rate ‚Äî and how it protects us from this problem.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 7 - Slide19.txt", "file_path": "Lecture 7\\Texts\\Slide19.txt", "content": "Here we‚Äôre looking at the difference between a continuous signal and its discrete version.\n\nOn the left, we have a smooth, continuous function ‚Äî like something you might see in the physical world, such as a sound wave or a voltage signal. It's defined at every instant in time.\n\nBut to work with this signal on a computer, we can't use all those infinite points. Instead, we select only a finite number of values, spaced out at regular intervals. That's what's shown on the right.\nThis process is called sampling.\n\nIf we sample densely enough ‚Äî meaning, if the points are close together ‚Äî then the discrete version can represent the continuous signal fairly accurately. The more samples we take, the more detail we preserve from the original waveform.\n\nSo the idea is: start with a smooth, continuous signal and convert it into a sequence of numbers that still captures the shape and behavior of the original. That's what makes signal processing on computers possible.\nUp next, we‚Äôll explore how often we need to sample to preserve that accuracy ‚Äî and what happens if we don't.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 7 - Slide20.txt", "file_path": "Lecture 7\\Texts\\Slide20.txt", "content": "Now, here‚Äôs a really important idea ‚Äî something we call the aliasing problem.\n\nRemember, we just said that sampling needs to be dense enough? But what does that mean?\nTake a look at the curve on the left. It‚Äôs a complex, high-frequency signal ‚Äî lots of rapid changes and fine details. If we don‚Äôt sample it frequently enough ‚Äî meaning, we don‚Äôt take enough points per second ‚Äî we miss those small peaks and dips.\nIn the middle, you see the result of sparse sampling. We‚Äôre only picking up a few values. And when we try to reconstruct the signal, shown on the right, we get something much smoother and simpler than the original. It might even look okay at first glance, but it‚Äôs actually wrong.\n\nAll that detail ‚Äî all the fast oscillations ‚Äî is lost. Even worse, the reconstructed version can look like a completely different signal, with a false shape or even the wrong frequency.\nThis is what we call aliasing. It happens when the sampling rate is too low to capture the higher-frequency components. And once they‚Äôre lost, you can‚Äôt recover them from the sampled data.\nThat‚Äôs why choosing the right sampling rate is so critical in signal processing.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 7 - Slide21.txt", "file_path": "Lecture 7\\Texts\\Slide21.txt", "content": "Let‚Äôs now look at what‚Äôs happening in the spatial domain when we sample a signal.\n\nImagine you have a continuous signal ‚Äî shown on the left. To sample this signal, what we‚Äôre really doing is multiplying it by a train of impulses. That‚Äôs the middle figure.\nThese impulses are spaced evenly along the axis, representing fixed sampling intervals. And when you multiply the continuous signal by this impulse train, the result is a set of discrete values. That‚Äôs what you see on the right: only the values of the signal at those impulse positions are preserved ‚Äî the rest are discarded.\n\nIn the spatial domain, sampling is essentially a point-wise multiplication of the signal with an impulse train.\nAnd here‚Äôs the key idea: this operation in the spatial domain has a direct consequence in the frequency domain. Multiplying by an impulse train in space corresponds to replicating the spectrum of the signal ‚Äî creating a periodic train of delta functions in the frequency domain.\nJust as we observe periodicity in time or space due to regular sampling, we also obtain periodicity in frequency. This connection ‚Äî between operations in the spatial domain and their effects in the frequency domain ‚Äî is fundamental in signal processing.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 7 - Slide22.txt", "file_path": "Lecture 7\\Texts\\Slide22.txt", "content": "Now let‚Äôs move to the frequency domain and see what sampling looks like there.\nEarlier, we saw that sampling in the spatial or time domain means multiplying the signal by an impulse train. In the frequency domain, that multiplication becomes a convolution.\n\nHere‚Äôs what happens:\nOn the left, we have the original signal‚Äôs spectrum ‚Äî a smooth curve that shows how the energy is spread across different frequencies.\nIn the center, we see a train of delta functions. These represent the periodic sampling pattern in the time domain.\nWhen we convolve the original spectrum with this delta train, what we get is shown on the right: multiple shifted copies of the original spectrum. One in the center, and others repeated at regular intervals to the left and right. These are called spectral replicas.\n\nNow, this setup is fine if the copies don‚Äôt overlap. But in the case shown here, the original spectrum is wide ‚Äî it spreads across many frequencies ‚Äî and the spacing between these delta peaks is not large enough. So the replicas overlap, and this overlap is what causes aliasing.\n\nAliasing is when different frequency components get mixed together. It‚Äôs like solving the equation x plus y equals 10,  you can't tell how much is x and how much is y. Similarly, when spectra overlap, you lose the ability to tell which part of the signal came from which frequency. That‚Äôs a problem.\n\nSo to avoid aliasing, we need to space those spectral copies far enough apart ‚Äî which means we need a high enough sampling rate in the time domain. That ensures each frequency copy stays separate, and the original signal can be recovered accurately.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 7 - Slide23.txt", "file_path": "Lecture 7\\Texts\\Slide23.txt", "content": "Now let‚Äôs shift gears and talk about conditioning in the spatial domain.\n\nSuppose you start with a signal that has a lot of sharp changes ‚Äî a noisy or high-frequency function, like the one shown on the left. Before sampling or further processing, we often want to smooth out the extremes. This helps avoid issues like aliasing or instability in reconstruction.\n\nSo what do we do?\nWe multiply the signal by a smooth, bell-shaped function ‚Äî something like a Gaussian. That‚Äôs the curve shown in the middle. This operation acts like a soft window. It suppresses the values at the edges and emphasizes the center part of the signal.\nThe result, shown on the right, is a conditioned signal. It‚Äôs still based on the original data, but now it‚Äôs more concentrated, more stable, and less prone to causing problems downstream.\n\nThis process is called conditioning ‚Äî and in the spatial domain, it‚Äôs done by pointwise multiplication. We‚Äôre not changing the signal globally ‚Äî just shaping it locally to make it behave better.\nThis idea becomes even more powerful when we look at its effect in the frequency domain ‚Äî and that‚Äôs where we‚Äôre headed next.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 7 - Slide24.txt", "file_path": "Lecture 7\\Texts\\Slide24.txt", "content": "Now let‚Äôs see why we‚Äôre often better off working in the frequency domain.\nSuppose we start with a nicely concentrated spectrum ‚Äî something like the narrow peak shown on the left. This means our signal in the spatial domain is smooth and well-behaved.\n\nNow, when we sample the signal ‚Äî which, in the frequency domain, corresponds to a convolution with a delta train ‚Äî we generate copies of this spectrum. These appear periodically across the frequency axis. You can see a central copy and two others on either side.\nIf the original spectrum is narrow enough, these copies won‚Äôt overlap ‚Äî and that‚Äôs a key idea. No overlap means no aliasing. That‚Äôs good news.\n\nIn this case, we can use a low-pass filter to isolate the central copy and discard the rest. This makes it possible to perfectly reconstruct the original signal from its samples.\n\nThat‚Äôs why frequency domain analysis is so powerful ‚Äî it gives us a clear way to understand, diagnose, and even fix sampling-related problems. And it‚Äôs where the idea of an ideal sampling filter comes in. This filter selects only what we need and suppresses what we don‚Äôt.\nWe‚Äôll explore this concept even further as we continue.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 7 - Slide25.txt", "file_path": "Lecture 7\\Texts\\Slide25.txt", "content": "So what does an ideal sampling filter look like?\n\nLet‚Äôs start in the frequency domain. If we want to keep only the part of the spectrum we care about, we can apply a rectangular window ‚Äî often called a gate function. This simply means we multiply the spectrum by a box that passes certain frequencies and zeros out the rest. Everything outside the band is eliminated.\n\nNow here‚Äôs the important part: multiplication in the frequency domain corresponds to convolution in the spatial or time domain. So when we apply this gate in Fourier space, the corresponding operation in the spatial domain is convolution with the inverse Fourier transform of the rectangle. And that inverse transform is something we‚Äôve seen before ‚Äî it‚Äôs the sinc function.\n\nThis sinc function has a sharp central peak and extends out forever with oscillating ripples. We say it has infinite support, and those ripples are often called ringing. So while the ideal filter works perfectly in theory, it‚Äôs not so practical in real applications. Why? Because we can‚Äôt build something with infinite extent. But conceptually, it‚Äôs extremely valuable ‚Äî it helps us understand the limits of what‚Äôs possible in signal reconstruction.\n\nWe‚Äôll next look at what happens when we approximate this ideal.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 7 - Slide26.txt", "file_path": "Lecture 7\\Texts\\Slide26.txt", "content": "Now let‚Äôs talk about a more practical ‚Äî but less perfect ‚Äî alternative: the cheap sampling filter.\n\nSuppose we switch things around. Instead of applying a gate function in the frequency domain, we use a sinc function there. In that case, its counterpart in the time domain is a rectangular function.\n\nThis rectangle acts like a local averaging window ‚Äî it smooths the signal by averaging values over a small time interval. You can think of it as performing a moving average across discrete sample points to approximate the original continuous signal.\n\nIt‚Äôs a simple, easy-to-implement method ‚Äî and that‚Äôs why we call it \"cheap.\" But here's the tradeoff: it‚Äôs far from ideal.\nIn the frequency domain, we‚Äôre no longer cleanly cutting off frequencies beyond a certain band. The sinc function doesn‚Äôt have sharp boundaries, so it doesn‚Äôt isolate frequencies as cleanly as the gate function would. That means the resulting reconstruction may lose some fidelity or allow unwanted frequency components to leak in.\n\nSo yes ‚Äî this filter is convenient and fast, but it comes at the cost of precision.\nNext, we‚Äôll explore how filtering in the spatial domain can help improve this.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 7 - Slide27.txt", "file_path": "Lecture 7\\Texts\\Slide27.txt", "content": "When we want a practical alternative to the ideal or cheap sampling filters, a Gaussian filter offers a great compromise.\n\nNow, what makes the Gaussian special is this: its Fourier transform is also a Gaussian. That‚Äôs quite rare ‚Äî most functions change significantly when transformed between the spatial and frequency domains. But the Gaussian stays in the same form, just scaled differently.\nSo, if you multiply by a Gaussian in the frequency domain, that corresponds to convolving with a Gaussian in the spatial or time domain ‚Äî and vice versa.\n\nWhy is this helpful? Because the Gaussian function decays very quickly. That means we avoid the problem of infinite ringing, like we saw with the sinc function. Instead of having a sharp cutoff, which can cause artifacts, the Gaussian gives a smooth transition and localized effect ‚Äî both in frequency and in space.\n\nIt's not mathematically perfect, but it‚Äôs practical. And in many real-world applications ‚Äî especially in imaging and signal processing ‚Äî that‚Äôs the trade-off we need.\nSo overall, the Gaussian sampling filter is a good, balanced choice that works well in both domains.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 7 - Slide28.txt", "file_path": "Lecture 7\\Texts\\Slide28.txt", "content": "Alright, let‚Äôs take a step further and make things more precise.\n\nEarlier, I showed you some cartoon-like pictures to build intuition. Now, let‚Äôs look at the actual math behind those ideas ‚Äî starting with the delta function.\n\nIn signal processing, the delta function plays a key role ‚Äî especially when we have a series of delta functions lined up at regular intervals. We often call this a comb, because visually, it looks just like the teeth of a hair comb.\nHere‚Äôs the key idea.\n\nImagine you have a bunch of delta functions spaced by a fixed time interval, T. We write that as:\u000bs of t equals the sum of delta of t minus n times T.\nNow, when you take the Fourier transform of this comb-shaped signal, something beautiful happens. You get another comb ‚Äî but this time in the frequency domain.\n\nThere‚Äôs a simple rule:\nIf the spacing in time is T,\nThen the spacing in frequency becomes 1 over T.\nThe impulses are still evenly spaced, but now they live in the frequency domain. And their height is scaled by a factor of 1 over T.\nSo this transformation turns one comb into another ‚Äî just flipped into frequency space, with inverted spacing.\nThis is a fundamental result you‚Äôll see in many signal processing books. Different authors may use different symbols ‚Äî like f or u for frequency, or j or i in the exponential ‚Äî but the meaning is the same.\n\nAlso, keep in mind that delta functions are really convenient in Fourier analysis. When you plug a delta function into the Fourier formula, it simplifies easily ‚Äî because a delta picks out a single point.\nAnd when you shift the delta in time ‚Äî say, to delta of t minus some value ‚Äî the result in frequency becomes a complex exponential, like e to the j 2 pi f zero t. That‚Äôs thanks to the shift theorem.\n\nSo, the takeaway here is:\u000ba comb of delta functions in time becomes another comb in frequency ‚Äî just with spacing flipped and amplitude scaled.\nThis sets the stage for understanding how sampling works in both domains. We‚Äôll build on this in the slides that follow.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 7 - Slide29.txt", "file_path": "Lecture 7\\Texts\\Slide29.txt", "content": "Let‚Äôs now take a closer look at the Fourier transform of this impulse train ‚Äî and I‚Äôll explain it in a way that‚Äôs easier to follow, even if not perfectly rigorous.\n\nSuppose we have a signal made of delta functions spaced by a fixed interval, capital T. This is a periodic function, right? It repeats every T. So we can use a Fourier series to represent it.\n\nWe write this function, s T of t, as a sum of delta functions at every multiple of T:\u000bs T of t equals the sum over n of delta of t minus n T.\nNow, since it's periodic, we can also express it as a sum of complex exponentials ‚Äî that‚Äôs what a Fourier series does. So we write:\u000bs T of t equals the sum over n of c n times e to the power j 2 pi n t over T,\u000bwhere c n is the Fourier coefficient.\nTo compute c n, we take the standard approach:\u000bc n equals 1 over T times the integral over one period of s T of t multiplied by e to the power negative j 2 pi n t over T.\nBut here‚Äôs the trick ‚Äî remember that s T of t is a train of delta functions. That means when you integrate, only the delta at t = 0 survives. \n\nThe result is simply:\u000bc n equals 1 over T.\nSo when you plug this back in, your entire Fourier series becomes a sum of complex exponentials, each with coefficient 1 over T.\nNow, there‚Äôs a well-known identity:\u000bA sum of complex exponentials with equal spacing and equal weights becomes a train of delta functions in the frequency domain.\n\nSo finally, we can write:\u000bs T of t transforms to (1 over T) times s 1 over T of u.\u000bThat is, the train of impulses in time maps to another train of impulses in frequency, with flipped spacing and scaled amplitude.\nThis is what we call a Fourier pair. A comb in one domain becomes another comb in the other domain ‚Äî with the spacing inverted and amplitude scaled.\nThis idea is fundamental in signal processing. You‚Äôll see it over and over ‚Äî whether you‚Äôre analyzing sampling, modulation, or reconstruction.\n\nNow, I‚Äôll admit ‚Äî this version of the derivation is not entirely rigorous. It's a bit hand-wavy. A more precise proof would involve deeper mathematical tools, especially when it comes to convergence. But for our purposes, this intuitive explanation gives you the right picture.\n\nSo just keep this in mind:\u000bA periodic sequence of impulses in time transforms into another periodic sequence in frequency, and the spacing and amplitude follow a simple reciprocal relationship.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 30, "slide_filename": "Slide30.txt", "slide_annotation": "Lecture 7 - Slide30.txt", "file_path": "Lecture 7\\Texts\\Slide30.txt", "content": "Let‚Äôs take a moment to revisit this important concept ‚Äî the impulse train, or what we often call a \"comb\", and its mirror relationship in the Fourier domain.\n\nOn the left side, we see a train of delta functions spaced by a constant interval, capital T. This lives in the time domain. And on the right, we see another train of delta functions ‚Äî this time in the frequency domain ‚Äî spaced by one over T.\n\nThis is a classic Fourier pair. When you take the Fourier transform of a periodic train of impulses in time, you get another train of impulses in frequency. And the spacing in the frequency domain is the reciprocal of the spacing in the time domain.\n\nNow, let‚Äôs look at the lower pair of plots.\nHere, the red comb has impulses spaced by a small interval, delta t. In that case, the spacing in the frequency domain becomes capital P, which equals one over delta t.\nAgain, we see the same idea ‚Äî spacing in one domain leads to reciprocal spacing in the other. This reciprocal relationship is a fundamental feature of the Fourier transform.\nThe height or amplitude also changes accordingly. Specifically, when the spacing decreases ‚Äî say, from T to delta t ‚Äî the impulses become more densely packed, and the total energy gets spread out more in frequency space. So the amplitudes must be adjusted accordingly to conserve energy.\n\nSo in summary:\u000bA periodic impulse train in time becomes a periodic impulse train in frequency.\u000bThe spacing flips ‚Äî T becomes 1 over T.\u000bThe amplitudes are scaled appropriately.\nThis relationship is so central to sampling, reconstruction, and many other applications in signal processing. You‚Äôll see it again and again ‚Äî both in theory and in practice.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 31, "slide_filename": "Slide31.txt", "slide_annotation": "Lecture 7 - Slide31.txt", "file_path": "Lecture 7\\Texts\\Slide31.txt", "content": "Now let‚Äôs dive into the process of sampling a signal ‚Äî how we go from the continuous world to the digital one.\n\nLook first at the top-left plot. That‚Äôs our original continuous signal, shown over a finite interval, from minus T over 2 to plus T over 2. Our goal is to digitize this ‚Äî to convert it into a form that we can process on a computer.\nAnd to do that, we take measurements at evenly spaced time intervals. This step is crucial, because without any prior knowledge about the signal, we don't know where we should sample more densely or more sparsely ‚Äî so we just use uniform sampling.\n\nWe model this sampling process mathematically by multiplying the signal with a train of impulses ‚Äî those red vertical lines spaced by delta t. This models a measurement being taken at regular intervals ‚Äî just like a thermometer recording temperature every hour.\n\nWhen we multiply the continuous function by this comb of impulses, we‚Äôre essentially pulling out values at those discrete points. The result is a sampled signal, where the values are available only at those locations. That‚Äôs what you see in the next plot below ‚Äî the discrete signal.\n\nNow shift your attention to the right-hand side ‚Äî to the frequency domain.\nThe original continuous signal, when transformed using the Fourier transform, gives us a spectrum ‚Äî and here it's shown to be concentrated between minus W and plus W. That‚Äôs the bandwidth of the signal ‚Äî most of its energy lies within that frequency range.\nAnd here‚Äôs something interesting: if a signal is limited in time ‚Äî meaning it's non-zero only within a certain interval ‚Äî then its Fourier transform will spread out infinitely. But if the time-domain signal is smooth, then the energy in its spectrum decays very fast. So in practice, we can often ignore the tiny tails and just focus on the dominant part within minus W to W.\n\nNow, here comes the key insight.\nWhen we sample the time-domain signal using a comb ‚Äî a series of impulses spaced by delta t ‚Äî then in the frequency domain, the Fourier transform becomes a convolution. That convolution replicates the original spectrum at regular intervals ‚Äî and those intervals are one over delta t, which we denote as capital P.\nSo in the frequency domain, we now have multiple copies of the original spectrum, all spaced by P. These are sometimes called spectral replicas.\n\nAnd here‚Äôs the big question: how small should delta t be? Or in other words ‚Äî how dense should our sampling be ‚Äî so that these spectral copies don‚Äôt overlap?\nBecause if they overlap, we get aliasing, and the original signal can‚Äôt be recovered. But if P ‚Äî that is, one over delta t ‚Äî is large enough, then these copies stay nicely separated, and we can isolate just one of them to recover the original signal perfectly.\n\nSo in short, if we sample densely enough, we don't lose any information. We can convert a continuous signal to a digital one and then reconstruct it ‚Äî as if nothing was lost.\nAnd that naturally brings us to the next topic ‚Äî what is the minimum sampling rate we need to guarantee perfect recovery?\nLet‚Äôs take a look.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 32, "slide_filename": "Slide32.txt", "slide_annotation": "Lecture 7 - Slide32.txt", "file_path": "Lecture 7\\Texts\\Slide32.txt", "content": "Let‚Äôs go back to the real form of the Fourier series.\n\nHere‚Äôs the idea: if you have a periodic function ‚Äî let‚Äôs call it f of t ‚Äî you can express it as a weighted sum of sines and cosines.\n\nThe formula goes like this:\nf of t equals\u000ba naught divided by 2,\u000bplus the sum, from n equals 1 to N,\u000bof a n times cosine of 2 pi n t,\u000bplus b n times sine of 2 pi n t.\n\nIn this expression, a naught is the average value of the function over one full period. That‚Äôs also called the DC component.\nThe rest of the terms ‚Äî the cosine and sine parts ‚Äî describe how the function varies over time. Each n corresponds to a different frequency: the higher the value of n, the higher the frequency.\n\nNow, how do we compute those coefficients ‚Äî a naught, a n, and b n?\nLet‚Äôs start with a naught divided by 2.\nThat‚Äôs equal to the integral of f of t from 0 to 1.\u000bIn other words, you‚Äôre just averaging the function over one period.\nNext, to compute a n,\u000byou take 2 times the integral from 0 to 1\u000bof f of t times cosine of 2 pi n t.\nAnd to compute b n,\u000bYou take 2 times the integral from 0 to 1\u000bof f of t times sine of 2 pi n t.\nEach coefficient tells you how much of that sine or cosine wave is present in the original function.\n\nNow here's something important:\u000bThe lowest frequency in this series appears when n equals 1. That corresponds to a frequency of 2 pi.\u000bThe highest frequency depends on N, which is the upper limit of the sum ‚Äî and that gives us a maximum frequency of 2 pi times N.\nSo by adjusting N, we can control how detailed our approximation is.\nThis is the real-valued Fourier series ‚Äî a sum of sine and cosine waves, each scaled by how much of that frequency exists in your signal.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 33, "slide_filename": "Slide33.txt", "slide_annotation": "Lecture 7 - Slide33.txt", "file_path": "Lecture 7\\Texts\\Slide33.txt", "content": "Now, let‚Äôs connect Fourier series with sampling.\n\nSuppose you have a function, like the one shown here, and you want to represent it over time using a Fourier series. The key idea is this:\nIf your function is defined over a unit interval ‚Äî let‚Äôs say from zero to one ‚Äî then the standard real-form Fourier series can represent it as a sum of cosine and sine terms, just like we saw earlier.\nThat‚Äôs perfectly fine when your function repeats every one second. But in general, a real-world signal may repeat over a different period ‚Äî maybe two seconds, or ten seconds, or any value we call capital P.\n\nSo, how do we handle that?\nThe answer is: you simply normalize your time axis. You scale the time so that the function fits into a unit interval. Then you apply the standard Fourier series there.\nOnce you‚Äôve built the series, you can scale it back to its original time frame ‚Äî from zero to capital P ‚Äî and you get a Fourier expansion that models the function over its true period.\n\nSo to summarize: the Fourier series has the flexibility to represent any periodic function, no matter what its period is. You just need to adjust for that period in the formulation ‚Äî and the structure stays exactly the same.\nThis makes Fourier analysis an incredibly powerful tool when dealing with signals of any repeating pattern.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 34, "slide_filename": "Slide34.txt", "slide_annotation": "Lecture 7 - Slide34.txt", "file_path": "Lecture 7\\Texts\\Slide34.txt", "content": "Let‚Äôs now talk about estimating the DC component in a signal.\n\nSuppose we have a continuous function, and we sample it over the interval from zero to one. The first question we should ask is: how can we estimate the DC term?\n\nRemember, in a Fourier series, the DC component is the constant part of the signal ‚Äî and mathematically, it‚Äôs given by the average value of the function over one full period. So, for a continuous function, that just means integrating from zero to one and dividing by the length of the interval.\n\nNow, in practice, we don't work with continuous signals ‚Äî we sample them. That means we take a finite number of measurements, evenly spaced across the interval. So how do we estimate the DC component from those samples?\nVery simply: we add up all the sampled values, and then we take the average. That average gives us a good estimate of the DC component ‚Äî the constant term in the Fourier series. \nSo once we‚Äôve taken care of the DC term, we still need to determine the coefficients for the sine and cosine terms ‚Äî the oscillating parts of the signal.\n\nLet‚Äôs say we want to recover N cosine coefficients and N sine coefficients. That gives us a total of 2 times N unknowns. And to solve for 2N unknowns, we‚Äôll need at least 2N independent equations ‚Äî meaning 2N sampled values.\nSo the takeaway is this: to fully recover a periodic signal using its Fourier series up to N harmonics, you need at least 2N equally spaced sample points. That gives you just enough information to solve for all the Fourier coefficients.\nAnd again, the DC component is simple ‚Äî just average your samples.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 35, "slide_filename": "Slide35.txt", "slide_annotation": "Lecture 7 - Slide35.txt", "file_path": "Lecture 7\\Texts\\Slide35.txt", "content": "Now let‚Äôs explore an interesting point in signal representation ‚Äî how sine and cosine terms can be combined.\n\nIn Fourier analysis, you‚Äôll often see expressions involving both sine and cosine terms. For example, something like A sine theta plus B cosine theta. This is a common way to write periodic signals, using a mix of sines and cosines.\nBut here‚Äôs the neat part: this combination can actually be rewritten as a single sine function ‚Äî with an amplitude and a phase shift. In other words, we can express it as R sine theta plus alpha, where R is the amplitude and alpha is the phase.\n\nThis identity is not just useful ‚Äî it‚Äôs powerful. It tells us that instead of dealing with two separate unknowns, A and B, we can represent the same information using two different unknowns: R, the amplitude, and alpha, the phase.\nSo nothing is lost ‚Äî we‚Äôre simply transforming the representation. Instead of working in the sine-cosine basis, we switch to amplitude and phase.\n\nIf you're curious about the proof or how this identity works, you can easily find tutorials or short videos online that walk through the steps in detail.\nBut the main takeaway here is: whenever you have a sine and cosine pair, you can always combine them into a single sine with a shifted angle. It‚Äôs a cleaner and often more intuitive way to represent oscillating signals ‚Äî especially when we want to talk about signal magnitude and delay.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 36, "slide_filename": "Slide36.txt", "slide_annotation": "Lecture 7 - Slide36.txt", "file_path": "Lecture 7\\Texts\\Slide36.txt", "content": "Let‚Äôs now take a heuristic view to better understand the relationship between sampling and signal reconstruction.\nSuppose we represent our signal using a sum of sinusoids. Each term looks like this: A n sine of 2 pi nu t plus phi n. Here, A n is the amplitude, phi n is the phase, and nu is the frequency.\n\nSo for each sinusoidal component, we need to determine two unknowns ‚Äî amplitude and phase. That means, for N such components, we end up with 2N unknowns in total.\n\nNow, here‚Äôs the key point. To determine those two unknowns for each sinusoid ‚Äî amplitude and phase ‚Äî we need at least two data points per cycle. Think of it like trying to draw a sine wave: if you only know one point, you can‚Äôt say much. But with two points, you start to get the shape ‚Äî its size and where it starts.\n\nSo, for the highest frequency in our signal ‚Äî which we‚Äôll call nu ‚Äî we need to sample at least twice per cycle. This leads to the idea of spacing between samples. To avoid missing any important information, the spacing between adjacent samples must be less than or equal to 1 over 2 nu.\n\nIn more familiar terms, that means the sampling rate ‚Äî measured in samples per second ‚Äî must be greater than 2 times nu. This is the famous Nyquist sampling rate.\n\nWhy does this matter? Because if we sample below this rate, we risk losing information. Our signal might be distorted or misrepresented ‚Äî a problem known as aliasing. But if we sample at twice the highest frequency or more, we guarantee that every sinusoidal component is accurately captured.\nSo the Nyquist rate gives us a fundamental guideline: sample fast enough to resolve the highest frequency present in the signal.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 37, "slide_filename": "Slide37.txt", "slide_annotation": "Lecture 7 - Slide37.txt", "file_path": "Lecture 7\\Texts\\Slide37.txt", "content": "Now we‚Äôre entering the most important part of this discussion ‚Äî the mathematical foundation behind the sampling theorem.\n\nLet‚Äôs start by recalling our goal: we want to take a continuous signal, sample it at regular intervals, and then perfectly reconstruct the original signal from just those samples. But this only works under certain conditions ‚Äî particularly, when the sampling rate is high enough to avoid aliasing. That‚Äôs what the Nyquist theorem guarantees.\n\nNow, let‚Äôs walk through this derivation step by step. It‚Äôs a bit more mathematical, but I‚Äôll explain the reasoning behind each part.\n\nWe begin by expressing the function f of t as the inverse Fourier transform of a product ‚Äî specifically, a low-pass filter in the frequency domain multiplied by a sampled version of the original function‚Äôs spectrum. Symbolically, we write:\nf of t equals the inverse Fourier transform of the product of capital Pi  P of u and the convolution of f hat of u with S P of u.\n\nNext, we apply properties of the Fourier transform. We pull out the convolution and apply inverse transforms separately. This leads to:\nf of t equals the inverse transform of Pi  P of u, convolved with the inverse transform of f hat of u multiplied by S  P of u.\nThen, we take this result back to the time domain.\n\nHere‚Äôs what we get:\u000bf of t equals P times sinc of pi P t, convolved with f of t times S 1 over P of t.\nThis expression means we‚Äôre convolving the sinc function ‚Äî which comes from the inverse transform of a box function in frequency ‚Äî with the sampled version of the original function.\n\nAnd from there, we arrive at the reconstruction formula you‚Äôve probably seen before:\nf of t equals the sum over k from minus infinity to infinity of f at k over P times sinc of pi times the quantity t minus k over P over pi times the quantity t minus k over P.\nThis is the classic sinc interpolation formula. It tells us that, under the right sampling conditions ‚Äî meaning, the sampling rate is at least twice the highest frequency in the signal ‚Äî the continuous signal can be reconstructed exactly by convolving the samples with the sinc function.\n\nSo in summary: sampling in the time domain corresponds to periodic replication in the frequency domain. When the signal is bandlimited and the sampling rate is high enough, we can perfectly reconstruct it using this formula.\nThis is the core of the sampling theorem. And this formula ‚Äî the final one on the slide ‚Äî is one of the most beautiful results in signal processing.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 38, "slide_filename": "Slide38.txt", "slide_annotation": "Lecture 7 - Slide38.txt", "file_path": "Lecture 7\\Texts\\Slide38.txt", "content": "Let‚Äôs go through a helpful example using a two-dimensional rectangular function. This is a classic case to build intuition around how the Fourier transform behaves in two dimensions.\n\nImagine a rectangle that‚Äôs centered at the origin. It has a total width of capital X along the x-axis, and a total height of capital Y along the y-axis. The function value is constant ‚Äî say, equal to 1 ‚Äî inside this rectangle, and zero outside.\n\nWe now take the 2D Fourier transform of this rectangle. The formula looks a bit dense, but the steps are straightforward.\n\nWe start with the double integral of the original function times the complex exponential ‚Äî e to the power minus j 2 pi times the quantity u x plus v y. That‚Äôs the standard definition of the 2D Fourier transform.\nSince the function is separable ‚Äî meaning it depends independently on x and y ‚Äî we can break the double integral into two parts:\none over x, and one over y.\nEvaluating each of these integrals separately gives us a product of two expressions.\nEach part turns into a sinc function. In the x-direction, we get sin of pi times X u over pi X u ‚Äî that‚Äôs sinc of pi X u. And in the y-direction, it‚Äôs sinc of pi Y v.\n\nSo the full 2D Fourier transform ends up as the product of these two sinc functions ‚Äî one in the u-direction, and one in the v-direction ‚Äî scaled by the area of the original rectangle, which is X times Y.\nNow if you look at the plot here, you‚Äôll see a 3D surface that shows the magnitude of the Fourier transform ‚Äî the peak is at the center, and the ripples decay outward. That shape comes directly from the sinc functions.\n\nWhy are we showing this?\nBecause it mirrors the result we saw earlier: when a rectangular function is defined in one domain ‚Äî here it‚Äôs the spatial domain ‚Äî its transform becomes a sinc function in the other domain ‚Äî in this case, the frequency domain.\nThis is closely related to the idea we discussed on the previous slide, where the rectangle ‚Äî or gate ‚Äî was in the frequency domain, and its transform was a sinc function in time. Whether you go forward or inverse, the core relationship stays similar.\nSo while this example is technically a bit different ‚Äî it‚Äôs 2D, and it‚Äôs a forward transform ‚Äî the intuition still applies. Rectangles and sinc functions are transform pairs. And we use that fact again and again in signal processing and imaging.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 39, "slide_filename": "Slide39.txt", "slide_annotation": "Lecture 7 - Slide39.txt", "file_path": "Lecture 7\\Texts\\Slide39.txt", "content": "Now, let‚Äôs continue line by line and see how each step connects.\nWe‚Äôre now looking at this part of the derivation ‚Äî specifically the fourth equation on the slide.\n\nHere, we have the product of two terms:\na sinc function, written as sine of pi P t over pi P t,\nand the convolution with another function in square brackets.\nLet‚Äôs first focus on the sinc part. This comes from taking the inverse Fourier transform of the rectangular function in the frequency domain ‚Äî the gate function we introduced earlier. When we take its inverse Fourier transform, we get a sinc function in the time domain. That‚Äôs because a rectangle in frequency always corresponds to a sinc in time. So that‚Äôs where this sine of pi P t over pi P t comes from ‚Äî it‚Äôs our sinc kernel.\n\nNow, let‚Äôs turn to the function inside the brackets.\nHere, we‚Äôre performing an inverse Fourier transform on the product of two terms:\nthe Fourier transform of our original function,\nand a periodic impulse train in frequency, with spacing capital P.\nBy the convolution theorem, multiplying two functions in the frequency domain is equivalent to convolving their inverse transforms in the time domain. So we convolve the original function f of t with the inverse transform of this impulse train in the frequency domain.\n\nAnd what is the inverse transform of a periodic impulse train in frequency?\nIt gives us another periodic impulse train ‚Äî this time in time ‚Äî with spacing equal to one over capital P. So this entire expression simplifies to f of t convolved with a sum of shifted delta functions, spaced by one over P.\nWe can express that more clearly on the next line. The equation now becomes:\nf of t equals sinc of pi P t multiplied by the sum over all k from minus infinity to infinity of f of t times delta of t minus k over P.\nAnd because convolution with a delta function simply samples the function, this expression simplifies further ‚Äî giving us the familiar formula for signal reconstruction.\n\nSo finally, we arrive at the last equation on this slide:\nf of t equals the sum over k, from negative to positive infinity,\u000bof f evaluated at k over P,\u000btimes sinc of pi times the quantity t minus k over P.\nThis is the reconstruction formula for band-limited signals. It tells us that if the signal was sampled at or above the Nyquist rate ‚Äî meaning no aliasing occurred ‚Äî then we can recover the original continuous function perfectly by summing shifted sinc functions, each scaled by the sampled value.\n\nThat‚Äôs the power of the sampling theorem. It doesn‚Äôt just allow us to digitize signals ‚Äî it tells us exactly how to recover them.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 40, "slide_filename": "Slide40.txt", "slide_annotation": "Lecture 7 - Slide40.txt", "file_path": "Lecture 7\\Texts\\Slide40.txt", "content": "Now let‚Äôs take a moment to talk about an important concept in signal processing ‚Äî the comb function and how it behaves under the Fourier transform.\n\nLet‚Äôs start with this equation at the top. It says:\ns of t equals the sum over all n of delta of t minus n times capital T.\nThis is what we call a delta train or comb function in the time domain. It‚Äôs a series of impulses, evenly spaced by T units.\nNow, when we take the Fourier transform of this comb, something very elegant happens.\nIt becomes another comb ‚Äî but in the frequency domain.\n\nSpecifically, the transform is:\ns of f equals one over capital T times the sum over all n of delta of f minus n over capital T.\nSo, in words, a comb in time with spacing T becomes a comb in frequency with spacing 1 over T. And we get a scaling factor of 1 over T in front.\nThis is a fundamental symmetry in Fourier analysis: when a signal is periodic in time, its spectrum is discrete in frequency ‚Äî and vice versa.\n\nNow look at the plots in the middle of the slide.\nOn the left side, we see a train of impulses spaced T apart along the time axis. On the right, we have the corresponding Fourier transform ‚Äî another train of impulses but now spaced 1 over T along the frequency axis.\nSo there‚Äôs this beautiful reciprocal relationship: if your delta train is spaced T in time, its Fourier counterpart is spaced 1 over T in frequency.\nAnd this brings us to the red box at the bottom of the slide. These equations generalize the idea.\n\nLet‚Äôs say you have a sum of delta functions shifted by n over P. That‚Äôs a comb with spacing 1 over P.\nThen, its Fourier transform becomes P times a sum of delta functions spaced P apart. The scaling factor flips ‚Äî one becomes P, and the spacing flips from 1 over P to P.\nAnd if we put a scaling factor of 1 over P in front of the comb, then the Fourier transform is simply a comb with spacing P, with no extra scaling factor.\nSo again, delta trains are their own Fourier transforms ‚Äî up to a scaling and spacing change.\n\nThis property is essential when we talk about sampling, because whenever we sample a signal, we‚Äôre essentially multiplying it by a comb function in time. And that multiplication creates repeated spectra in the frequency domain ‚Äî spaced according to the sampling rate.\nSo that‚Äôs the comb and its mirror ‚Äî one of the most powerful dualities in Fourier theory.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 41, "slide_filename": "Slide41.txt", "slide_annotation": "Lecture 7 - Slide41.txt", "file_path": "Lecture 7\\Texts\\Slide41.txt", "content": "Now let‚Äôs focus on the last few steps of the derivation and clarify what‚Äôs happening here, line by line.\nPreviously, we had the convolution of two time-domain signals: one was a sinc function, the other was a sum of scaled impulses. That brought us to this expression:\nf of t equals sinc of pi P t, divided by pi P t, convolved with the sum from k equals minus infinity to infinity of f of t times delta of t minus k over P.\nLet‚Äôs unpack that.\nFirst, remember that this sum is what we call a delta train ‚Äî an infinite series of delta functions spaced at intervals of 1 over P.\n\nNow you might ask ‚Äî wait a minute, where did the 1 over P factor go?\nGood observation.\nOriginally, the delta train had a scaling factor of 1 over P. But earlier in the derivation, we also had a P multiplying the sinc function. So those two terms ‚Äî P and 1 over P ‚Äî cancel out. That‚Äôs why we don‚Äôt see the 1 over P anymore in the final expression.\n\nSo what do we have now?\nWe have a continuous function ‚Äî that‚Äôs the sinc ‚Äî multiplying a series of impulses that are sampling the original function at regular intervals.\nThis is the essence of the sampling process.\nThe function f of t is being sampled at every t equals k over P, and the result is a series of weighted sinc functions. Each one is centered at t equals k over P and scaled by the corresponding sample value, f of k over P.\nAnd that brings us to the final expression:\nf of t equals the sum from k equals minus infinity to infinity of f of k over P, multiplied by sinc of pi P times t minus k over P, divided by pi P times t minus k over P.\nThat‚Äôs the sampling theorem.\n\nIt tells us that if a signal is band-limited ‚Äî that is, its frequency content is restricted ‚Äî then we can reconstruct the entire continuous-time signal exactly from its samples, as long as the sampling rate is high enough.\nEach sample contributes a scaled sinc function, and the sum of all those sinc functions reconstructs the original signal.\nThis formula is both elegant and powerful ‚Äî and it lies at the foundation of all modern digital signal processing.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 42, "slide_filename": "Slide42.txt", "slide_annotation": "Lecture 7 - Slide42.txt", "file_path": "Lecture 7\\Texts\\Slide42.txt", "content": "Let‚Äôs take a step back now and look at how analog signals are converted to digital.\nAt the top, we have a smooth, continuous signal ‚Äî this is your analog signal ‚Äî represented here as f of t, where t is time.\n\nNow, to digitize this signal, we begin with the process of sampling. In the middle plot, you see a train of delta functions ‚Äî these are evenly spaced impulses in time. Each one of them marks a point where we will sample the analog signal.\n\nNext, what do we do?\nWe multiply the continuous signal by this train of impulses. That‚Äôs shown in the bottom figure.\nThis multiplication essentially picks out the values of the original signal at those discrete time points ‚Äî and sets everything else to zero. In other words, it‚Äôs like punching holes in the continuous function, keeping only the values at regular intervals.\nThe result is a series of weighted impulses, where the height of each spike corresponds to the signal‚Äôs value at that instant.\nAnd this is the essence of analog-to-digital conversion.\n\nWe‚Äôve turned a smooth, continuous-time signal into a discrete-time representation ‚Äî something that can now be stored, processed, and transmitted digitally.\nThis is the first key step in digital signal processing ‚Äî and it's entirely grounded in the mathematics of the sampling theorem we just discussed.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 43, "slide_filename": "Slide43.txt", "slide_annotation": "Lecture 7 - Slide43.txt", "file_path": "Lecture 7\\Texts\\Slide43.txt", "content": "Now let‚Äôs focus on this final and very important step.\nWhat you see here is the reconstruction formula ‚Äî a powerful result that tells us how to rebuild the original continuous-time signal, f of t, from its discrete samples.\nHere‚Äôs how it works.\n\nWe start with a set of sampled values. These are the values of f of t taken at regular intervals ‚Äî specifically at times k over P. These are the points where our delta functions were placed during sampling.\n\nNow, we take each of those sampled values ‚Äî each f of k over P ‚Äî and multiply it by a special function called the sinc function. You can see it written here as sine of pi times P times t minus k over P divided by pi times P times t minus k over P.\nThis sinc function looks like a smooth oscillating wave that decays gradually away from its center. And what‚Äôs important is that it has the perfect shape to reconstruct smooth curves from discrete data points.\n\nNow here‚Äôs the key idea.\nEach sampled value acts like a weight, and each weight generates a shifted copy of the sinc function centered at that sample point. So at k over P, we get a sinc function that peaks there, scaled by the sample value at that point.\nThen we add up all these sinc functions ‚Äî one for each sample.\n\nThe result? A continuous, smooth signal ‚Äî which is exactly the original signal f of t that we started with.\nThis is how sampling and reconstruction come together. Each delta function \"copies\" the sinc function. Each copy is scaled by the corresponding sample value. And the sum of all these scaled sinc functions recreates the original analog signal.\nSo this final formula ‚Äî this infinite sum ‚Äî tells us something remarkable:\nAs long as we sample carefully, we can fully reconstruct a continuous signal from just its discrete samples.\nThat‚Äôs the beauty of the sampling theorem.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 44, "slide_filename": "Slide44.txt", "slide_annotation": "Lecture 7 - Slide44.txt", "file_path": "Lecture 7\\Texts\\Slide44.txt", "content": "To wrap up our discussion, let‚Äôs take a closer look at this final visual ‚Äî it ties everything together beautifully.\nAt the top, we see a simple rectangular function, labeled f of x. It‚Äôs a block function centered at zero. Next to it is h of x, which is a train of two delta functions ‚Äî one at minus a over two and the other at plus a over two. Their amplitudes are scaled by one-half.\n\nNow, when we convolve f of x with h of x, the result ‚Äî g of x ‚Äî is essentially just two shifted copies of the original function. Each delta acts like a copying machine, reproducing f of x at the location of the delta. This is a perfect illustration of how convolution with delta functions results in replicated versions of a signal.\n\nNow let‚Äôs look at the bottom half of the slide ‚Äî we‚Äôre moving into the frequency domain.\nThe Fourier transform of the original block function, f of x, becomes a smooth sinc-like curve, F of k. The delta train, h of x, transforms into a cosine wave pattern, H of k. And the product in the frequency domain, F of k times H of k, gives us G of k ‚Äî a modulated version of the original Fourier transform.\nThis shows us that convolution in the time domain becomes multiplication in the frequency domain ‚Äî one of the central results of Fourier theory.\n\nNow, relating this back to the sampling theorem...\nWhen we sample a continuous signal, we are essentially multiplying it by a train of delta functions. In the time domain, this multiplication picks out the values at those sampling points. But in the frequency domain, it leads to repetitions ‚Äî or aliases ‚Äî of the original spectrum.\nAnd when we reconstruct, we don't use the simple rectangle anymore ‚Äî instead, we use the sinc function, which is the Fourier transform of an ideal low-pass filter. But the principle is the same: the delta functions trigger copies, and these copies are scaled by the actual sample values ‚Äî f of k over P.\n\nAnd here‚Äôs the key takeaway:\nIf we sample fast enough ‚Äî meaning at more than twice the highest frequency in the original signal ‚Äî and if the signal is band-limited, then all those sinc functions will line up perfectly, and we can fully recover the original signal.\nThat‚Äôs the heart of the sampling theorem. It‚Äôs not just an abstract formula ‚Äî it‚Äôs a practical method that allows us to move between continuous and digital signals with full confidence.\n\nThis slide beautifully summarizes the theory. From delta copies to sinc reconstruction, it‚Äôs all about understanding how convolution builds signals piece by piece.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 45, "slide_filename": "Slide45.txt", "slide_annotation": "Lecture 7 - Slide45.txt", "file_path": "Lecture 7\\Texts\\Slide45.txt", "content": "Let‚Äôs now take a moment to revisit a topic that you‚Äôve likely seen before ‚Äî linear systems.\n\nHere, we‚Äôre looking at the classic equation: A x equals b.\u000bThat‚Äôs a system of linear equations, where A is your matrix, x is the unknown vector, and b is the observed result or measurement.\nNow, solving for x in this system is something you‚Äôve probably done many times before ‚Äî especially if you've worked through a textbook like Elementary Linear Algebra by Howard Anton, shown here.\n\nBut here's the key question for us:\u000bWhat if the unknown vector x is sparse?\nIn other words, what if most of the elements in x are actually zero ‚Äî and only a few are nonzero?\nThis situation comes up a lot in modern signal processing and imaging. We no longer assume that a signal is densely sampled or has energy spread all over. Instead, we often find that signals have a sparse structure, especially when we represent them in a suitable basis, like wavelets or gradients.\n\nNow, here‚Äôs where it gets really interesting ‚Äî and this is where modern theory breaks free from traditional Nyquist sampling.\nInstead of sampling a signal very densely ‚Äî at twice its maximum frequency ‚Äî we can sample much more sparsely, and still recover the full signal.\n\nHow?\nWe change our assumption.\u000bInstead of assuming the signal is band-limited, we assume it's sparse in some transform domain. Maybe it's sparse in the wavelet domain, or maybe the total variation is small ‚Äî meaning the signal changes smoothly, or only has a few sharp transitions.\nThen, among all possible solutions that match the measurements, we choose the sparsest one.\n\nThis is the idea behind compressed sensing, and it has revolutionized the way we think about sampling and recovery.\nSo that green button on the slide ‚Äî it‚Äôs there to say: pause and think. This is a major shift in the traditional mindset. It opens the door to powerful new methods for signal reconstruction from limited data ‚Äî especially in imaging and beyond.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 46, "slide_filename": "Slide46.txt", "slide_annotation": "Lecture 7 - Slide46.txt", "file_path": "Lecture 7\\Texts\\Slide46.txt", "content": "Let‚Äôs take a step further and talk about a powerful idea that shows up across science and engineering ‚Äî and that‚Äôs sparsity.\nYou can see the green button here, which means this concept is important, but it‚Äôs okay if the details feel abstract for now. Just follow the intuition.\n\nTraditionally, signal processing has focused on signals that are band-limited ‚Äî which means their frequencies don‚Äôt go beyond a certain point. Based on that assumption, we rely on Nyquist sampling: sample at least twice the maximum frequency, and you can perfectly reconstruct the signal.\n\nBut what if we go beyond that assumption?\nModern theory says: you don‚Äôt always need to sample that densely. Instead of requiring a signal to be band-limited, we assume that it‚Äôs sparse in some domain. That means ‚Äî in the right representation, most of the values are zero, or nearly zero.\nThink about images. They may look complex, but when you apply a wavelet transform ‚Äî like the ones shown in the graphics ‚Äî most of the wavelet coefficients are close to zero. Only a few carry meaningful information. This is sparsity.\nOn the left, we have an illustration from the book Wavelets in Physics. It shows how signals in physics often exhibit localized spikes ‚Äî sparse behavior. In the center and top-right, you see surfaces where most values are flat or close to zero, with just a few sharp peaks ‚Äî again, sparse structure.\n\nEven in biology ‚Äî like the cellular structure of a leaf ‚Äî patterns emerge that are spatially compact and repeated. This biological system, shown on the right, is highly structured and organized. And this kind of structure is often sparse when expressed in the right basis.\n\nNow, here‚Äôs why sparsity matters.\nIf you don‚Äôt have enough measurements to fully determine a signal ‚Äî that is, if your system is underdetermined ‚Äî then infinitely many solutions could explain the data. But if you know the signal is sparse, you can ask: among all possible solutions, which one is the sparsest?\nThat‚Äôs the central idea behind compressed sensing.\nBy assuming sparsity, and by choosing the solution with the fewest nonzero components, you can accurately recover the original signal ‚Äî even from very limited data.\n\nSo this concept ‚Äî sparsity ‚Äî is not just a mathematical trick. It‚Äôs something we see everywhere: in physics, in images, in biology, and in many natural and engineered systems. Recognizing it gives us a powerful way to reconstruct signals and solve problems more efficiently than ever before.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 7", "slide_number": 47, "slide_filename": "Slide47.txt", "slide_annotation": "Lecture 7 - Slide47.txt", "file_path": "Lecture 7\\Texts\\Slide47.txt", "content": "Alright, let‚Äôs wrap up with the big picture.\n\nThis slide summarizes the key relationships between time and frequency domains ‚Äî especially how signals and their spectra behave when we move from continuous to discrete representations.\nAt the very top left, we start with a continuous-time signal ‚Äî a smooth pulse that exists over a limited time interval. On the right, we see its frequency content, also confined to a certain bandwidth.\n\nNow, as we go down the slide, we perform sampling in time. That‚Äôs the red comb of vertical lines. We're picking values from the original signal at regular intervals ‚Äî let‚Äôs say every delta t.\nWhat does this do in the frequency domain?\nIt creates multiple copies of the spectrum, repeated again and again. This is called spectral replication. And if these replications don‚Äôt overlap, we can perfectly reconstruct the original signal.\nThat's the essence of the sampling theorem ‚Äî to avoid distortion, we must sample fast enough, at least twice the highest frequency present in the signal.\n\nNow, let‚Äôs look at the lower half of the slide.\nHere, we‚Äôre sampling in the frequency domain instead. That‚Äôs shown by the green comb ‚Äî regularly spaced frequency spikes.\nWhat happens to the signal in the time domain?\nIt becomes periodic ‚Äî the original signal now repeats over time, again and again.\n\nSo the big idea is this:\nSampling in time causes replication in frequency.\nSampling in frequency causes repetition in time.\nThese two processes mirror each other ‚Äî one in time, the other in frequency. This duality is at the heart of signal processing.\nNow, I know this diagram is dense, but if you walk through it carefully ‚Äî step by step ‚Äî you‚Äôll see how everything we‚Äôve covered comes together.\n\nWe‚Äôll cover this in more detail on Friday. In the meantime, I suggest reviewing this flow using the lecture slides and working through the derivations.\nEven if there are a few typos in the book chapter, don‚Äôt worry. The key ideas are all here. And if you‚Äôre curious, feel free to preview the next topic ‚Äî the Discrete Fourier Transform ‚Äî in the book as well.", "total_slides_in_lecture": 47}
{"lecture": "Lecture 8", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 8 - Slide1.txt", "file_path": "Lecture 8\\Texts\\Slide1.txt", "content": "Hello everyone, in this lecture, we‚Äôll continue our foundational story and focus on understanding the discrete Fourier transform, or DFT, and its inverse, the IDFT. These slides and my book draft will be your main reference, and they provide a clear, step-by-step path for following the key steps and get the main idea.\n\nThe content you see here is part of a larger foundation in medical imaging. Once you have a solid grasp of these concepts, it will be much easier to understand advanced topics like X-ray tomography, nuclear imaging, and MRI.\n\nFourier analysis is central to all of these areas. We‚Äôll use it to analyze and process signals in both continuous and discrete forms, and to move back and forth between the time or spatial domain and the frequency domain, in either a continuous or discrete form.\n\nIn practice, we digitize signals and then perform operations such as convolution, Fourier transforms, and inverse Fourier transforms in one dimension, two dimensions, and beyond. This lecture will begin building the tools you need for that process.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 8 - Slide2.txt", "file_path": "Lecture 8\\Texts\\Slide2.txt", "content": "We‚Äôre moving along right on track in our journey through this course.\u000b\nIf you‚Äôve had the chance to preview the reading materials for today, that‚Äôs great ‚Äî it will help you connect the concepts more quickly. If not, that‚Äôs fine too.\n\nJust follow along closely, and make time afterward to review the main ideas. Building this habit of reinforcing concepts as we go will make your understanding stronger and more intuitive, especially as the topics become more mathematical.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 8 - Slide3.txt", "file_path": "Lecture 8\\Texts\\Slide3.txt", "content": "In our last lecture, we examined the sampling theorem in detail, focusing on key concepts such as the sampling rate and the Nyquist rate. By now, you should have a clear idea of why, under certain conditions, we can perfectly recover a continuous signal from its sampled version.\n\nHere‚Äôs the key idea:\u000bWe start with a continuous signal, f of t, shown here. When we sample it at regular time intervals ‚Äî let's call that interval delta-t ‚Äî we obtain a discrete sequence of values. The sampling theorem tells us that, if certain conditions are met, we can reconstruct the original continuous signal from these samples without losing any information.\n\nThat‚Äôs remarkable ‚Äî because even though we only record values at specific time points, and there‚Äôs ‚Äúmissing‚Äù information in between, the theorem guarantees perfect recovery. But this only works if the original signal‚Äôs Fourier transform is band-limited. That means its frequency spectrum is essentially zero outside a certain range, from minus w to plus w.\n\nIn the frequency domain, sampling corresponds to a convolution between the original continuous Fourier spectrum and a periodic train of delta functions. This delta train in frequency comes from the Fourier transform of the impulse train in time. The spacing between the deltas in frequency ‚Äî which we‚Äôll call P ‚Äî is the reciprocal of the sampling interval delta-t.\n\nTo avoid any distortion, P must be at least 2 W. This ensures there is no overlap between the repeated copies of the spectrum in the frequency domain. You can think of each delta function as a ‚Äúcopying machine‚Äù ‚Äî each one shifts and reproduces the original spectrum at regular intervals. If the copies don‚Äôt overlap, the original information is preserved perfectly.\n\nIn practice, we can apply a low-pass filter to keep only the central copy of the spectrum, then perform an inverse Fourier transform to reconstruct the exact original signal.\n\nNow, a small note about the special case where P equals exactly 2 W: here, the copies just touch at a single point. This doesn‚Äôt cause problems unless that single point is a delta function with a finite value ‚Äî in which case it could contribute to the result. But in most practical signals, that single point has no significant effect.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 8 - Slide4.txt", "file_path": "Lecture 8\\Texts\\Slide4.txt", "content": "Now let‚Äôs look at the special case where P equals exactly 2 w.\n\nSuppose our continuous signal is a pure sine wave with a frequency of 1 hertz. That means it completes exactly one cycle in one second. In the frequency domain, this is a band-limited signal ‚Äî its maximum frequency is 1 hertz.\nIf we sample at exactly twice that frequency ‚Äî so at 2 hertz ‚Äî we meet the Nyquist rate exactly. In this case, we take two samples within each cycle. For a pure sinusoid, two unknowns fully describe it: the amplitude and the phase. In theory, two samples per cycle should be enough to determine those two unknowns.\n\nHowever, here‚Äôs the problem. Depending on where the samples fall, we might end up with the exact same sample value each time ‚Äî for example, always sampling at the zero crossings of the sine wave. In that case, all our samples would be zero, and we would have no reliable way to determine the original signal.\n\nIn the Fourier domain, a pure sine wave corresponds to a pair of delta functions located at its positive and negative frequency components. When we sample at exactly 2 w, the duplicated spectra in the frequency domain just touch each other. If the touching points happen to be delta functions, those deltas overlap, and we can no longer separate the contributions from each one. Mathematically, it‚Äôs like having the equation X plus Y equals C ‚Äî you know the sum, but not the individual values. This is an aliasing problem.\n\nThat‚Äôs why, in practice, sampling exactly at 2 w is risky. To be safe, we require PPP ‚Äî the spacing between repeated spectra in frequency ‚Äî to be slightly greater than 2 w. In other words, the sampling rate should be a little higher than twice the maximum frequency in the signal. This extra margin ensures that no overlap occurs, and reconstruction remains reliable.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 8 - Slide5.txt", "file_path": "Lecture 8\\Texts\\Slide5.txt", "content": "Here, we have the whole derivation of the sampling theorem summarized on one page.\n\nYou see a few steps underlined in red. These are the important turning points in the math ‚Äî the places where something key happens. In a detailed lecture, we would walk through each of them slowly, but here, I want to focus on the big picture.\nWe start in the frequency domain, where the spectrum of our signal is multiplied by a periodic train of impulses. When we bring this back to the time domain, that multiplication turns into a convolution. The result of that convolution involves a special function called ‚Äúsinc,‚Äù which naturally appears when you take the Fourier transform of a rectangular shape in frequency.\n\nNext, when we write out the convolution, we see it as an infinite sum ‚Äî a series of shifted sinc functions. Each one is scaled by the value of our signal at a sample point. The distance between these samples is delta-t, and that‚Äôs simply one over P, the spacing in the frequency domain.\n\nThe final line is the famous Shannon interpolation formula. In words, it says: take each sample of your signal, put a sinc function centered at that sample, scale it by the sample‚Äôs value, and then add them all together. Do this for every sample ‚Äî stretching infinitely in both directions ‚Äî and you get your original continuous signal back exactly, as long as the sampling conditions are satisfied.\n\nThis is a beautiful result. It tells us that perfect reconstruction is not just possible ‚Äî it‚Äôs a direct consequence of the sampling theorem. And this formula is one of the core foundations of all digital signal processing.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 8 - Slide6.txt", "file_path": "Lecture 8\\Texts\\Slide6.txt", "content": "Let‚Äôs look at a good case ‚Äî when our sampling rate is high enough to meet the sampling theorem‚Äôs requirement.\n\nThe black curve here is our original, continuous signal. Think of it as the ‚Äútrue‚Äù signal. The blue points show where we took our samples ‚Äî a limited number of values, spaced evenly in time.\n\nHere‚Äôs the magic: if the sampling rate is greater than twice the maximum frequency in the signal ‚Äî the Nyquist rate ‚Äî then we can take these discrete samples and use the Shannon interpolation formula to reconstruct the original signal perfectly.\nWhen we do that here, the reconstructed signal (shown in red) lies exactly on top of the original black curve. They match so perfectly that you can‚Äôt even see the difference.\n\nThis is the sampling theorem in action. For band-limited signals, as long as we sample fast enough, we can go back and forth between continuous and discrete forms without losing information. You can repeat this process over and over, with different sample sets, and it will work every time.\nIn short ‚Äî with the right sampling rate, theory and practice agree perfectly.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 8 - Slide7.txt", "file_path": "Lecture 8\\Texts\\Slide7.txt", "content": "Now, here‚Äôs a bad case ‚Äî when the sampling rate is too low.\n\nThe black curve is again our original, continuous signal. The blue points are where we took our samples. And the red curve is what we get when we try to reconstruct the signal from those samples.\nAt the sample locations, the reconstructed red curve passes exactly through the blue points ‚Äî so it matches the original at those specific spots. But between the sample points, the red curve drifts far away from the true black signal.\n\nThis mismatch is caused by aliasing. Because our sampling rate is less than twice the maximum frequency in the signal, the repeated spectra in the frequency domain overlap. That overlap distorts the information, and once it happens, no amount of interpolation can recover the original shape.\n\nSo, while the reconstruction looks fine exactly at the sampled points, the continuous waveform in between is completely wrong. This is why respecting the sampling theorem is essential ‚Äî if you don‚Äôt sample fast enough, the damage is permanent.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 8 - Slide8.txt", "file_path": "Lecture 8\\Texts\\Slide8.txt", "content": "Let‚Äôs step back and see the big picture.\n\nThere are really two parts here. On the top half, we have discretized the signal in the time domain. This means we start with a continuous signal, sample it at regular intervals, and get a discrete set of values ‚Äî something we can store and process in a digital computer. If the sampling theorem is satisfied, this step causes no information loss.\nBut the story doesn‚Äôt end there. Even though our signal is now discrete in time, its Fourier spectrum is still continuous. And a continuous spectrum cannot be stored directly in a computer either.\n\nThat‚Äôs where the second part comes in ‚Äî discretizing the spectrum. This is the central topic of this lecture. The idea is to take the continuous Fourier spectrum and multiply it by a periodic train of delta functions in the frequency domain. This multiplication in frequency corresponds to convolution in the time domain, which effectively copies our time-domain signal over and over again, spaced apart by a fixed period t.\n\nIn other words, sampling in one domain causes repetition ‚Äî or duplication ‚Äî in the other domain. When we discretize both the time-domain signal and the frequency-domain spectrum, we end up with a periodic structure in both domains.\nThe key point here is that now, both our signal in time and its spectrum in frequency are discrete and periodic. This makes them fully compatible with digital processing. And this is the foundation for moving toward the discrete Fourier transform.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 8 - Slide9.txt", "file_path": "Lecture 8\\Texts\\Slide9.txt", "content": "Signal sampling is a fundamental concept, but it can be confusing if you only look at it quickly. So let‚Äôs slow down and really understand it.\nAt the top, we have our continuous signal, f x. This is defined for every value of x and could represent anything ‚Äî a sound wave, an image profile, or a measured signal.\n\nNext, we have what‚Äôs called a Shah function, or an impulse train. This is simply a series of delta functions, each spaced t units apart. Mathematically, it‚Äôs written as the sum of delta functions, where each one is located at x equals n T, with n being an integer. You can think of this impulse train as a ruler that marks the exact points where we‚Äôll take our samples.\n\nWhen we multiply the continuous signal f of x by this impulse train, we get the sampled function. This multiplication is done point-by-point: at each delta function‚Äôs location, we keep the value of f of x; everywhere else, the result is zero.\nSo the sampled signal is really the original signal‚Äôs values, ‚Äúattached‚Äù to the impulses in the train. The impulses mark the positions, and the heights of the impulses carry the signal‚Äôs values at those positions.\n\nIn other words, the sampled function is just the original function, but only at discrete, regularly spaced points. This is the first step in moving from the continuous world to the digital world.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 8 - Slide10.txt", "file_path": "Lecture 8\\Texts\\Slide10.txt", "content": "When we sample a signal in the time domain ‚Äî or the spatial domain ‚Äî it creates a very specific effect in the frequency domain. Sampling in one domain is equivalent to performing a convolution in the other domain.\n\nLet‚Äôs walk through it.\u000bWe start with our original spectrum, which we‚Äôll call F of u. Here, u represents frequency, and u-max is the highest frequency present in the signal ‚Äî its bandwidth.\nWhen we multiply our signal in time by an impulse train ‚Äî a set of regularly spaced spikes ‚Äî that multiplication turns into a convolution in frequency. The result is that the original spectrum gets repeated again and again, spaced apart by the sampling frequency.\n\nIn this diagram, the middle shape is the original spectrum. To the left and right, you can see identical copies. The spacing between them is one over capital T, where capital T is the sampling interval in time.\nIf our sampling frequency ‚Äî which is one over T ‚Äî is at least twice the maximum frequency in the signal, then these repeated copies don‚Äôt touch each other. In that case, we can simply keep the central copy and reconstruct the original signal perfectly.\n\nBut if we sample too slowly, the copies overlap. This overlap distorts the spectrum, and once that happens, there‚Äôs no way to separate the original from the distortion. That‚Äôs the aliasing problem we saw in the earlier ‚Äúbad case‚Äù example.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 8 - Slide11.txt", "file_path": "Lecture 8\\Texts\\Slide11.txt", "content": "Here‚Äôs the Nyquist theorem in action.\n\nIn the diagram, you can see the original spectrum and its repeated copies created by sampling. If our sampling rate is too low, these copies start to overlap. When that happens, the frequency components from one copy mix with those from another. This is aliasing ‚Äî and once it occurs, we can‚Äôt tell which part of the spectrum came from the original signal and which part came from the overlap.\n\nTo avoid aliasing, we must make sure that the highest frequency in the signal ‚Äî we call it u-max ‚Äî is less than one divided by twice the sampling interval. In spoken terms, the sampling frequency must be greater than twice u-max. This threshold is called the Nyquist frequency.\n\nWhen the condition is satisfied, we can use a rectangular filter ‚Äî often called a gate function ‚Äî to select just the central copy of the spectrum. That single copy contains all the information needed to perfectly reconstruct the original signal.\nIt‚Äôs like DNA testing ‚Äî you don‚Äôt need the whole organism, just a small sample, because it contains the complete blueprint. In the same way, one clean copy of the spectrum contains everything we need.\n\nBut if the sampling rate is too low and overlap happens, no filter can separate the mixed components. The information is lost forever.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 8 - Slide12.txt", "file_path": "Lecture 8\\Texts\\Slide12.txt", "content": "Now let‚Äôs see why aliasing makes recovery non-unique.\n\nIn this diagram, the red and blue curves represent two different sets of frequency components. When aliasing happens, these components overlap in the frequency domain. At the points where they overlap, what we measure is just the sum of their values.\nFor example, imagine that at a certain frequency, the blue component has a value of A, and the red component also has some value. When they overlap, all we see is their sum. That means many different combinations of blue and red could produce the exact same total.\n\nIn other words, you can‚Äôt tell exactly how much came from the blue curve and how much came from the red one. The information about their contributions is lost.\nThis is why aliasing is such a problem ‚Äî once two spectra overlap, there is no unique way to separate them. No matter how you process the data, you can‚Äôt reconstruct the original spectrum with certainty.\n\nThat‚Äôs why the Nyquist theorem‚Äôs condition ‚Äî sampling faster than twice the highest frequency ‚Äî is so important. It prevents this kind of overlap, keeping the spectrum unique and recoverable.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 8 - Slide13.txt", "file_path": "Lecture 8\\Texts\\Slide13.txt", "content": "Up to this point, we‚Äôve been talking about aliasing and how to avoid it. Let‚Äôs now assume we‚Äôve done that ‚Äî our sampling rate is always high enough, and the spectra are well separated. Aliasing is no longer an issue.\n\nOn the top row, you see our original continuous-time signal on the left. On the right, it‚Äôs been sampled ‚Äî we now have a series of discrete values at regular time intervals.\n\nOn the bottom row, you see what this means in the frequency domain. On the left is the original spectrum, limited to a maximum frequency, u-max. On the right is the sampled spectrum ‚Äî multiple, non-overlapping copies, spaced apart by the sampling frequency, which is one over capital T.\n\nSo yes, the signal is now discrete in time, and its spectrum is nicely replicated without overlap. But our job isn‚Äôt finished yet. The spectrum is still continuous in frequency. And if we want to fully digitize the signal ‚Äî so that both the time domain and the frequency domain are discrete ‚Äî we still have another step to go.\nThat step is discretizing the spectrum, which is exactly where we‚Äôre heading next.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 8 - Slide14.txt", "file_path": "Lecture 8\\Texts\\Slide14.txt", "content": "Now that our signal is discrete in time and aliasing is avoided, the next step is to discretize the Fourier spectrum.\nOn the right, we start with the continuous, periodic Fourier spectrum of our sampled signal. The peaks are separated because our time-domain sampling rate was high enough. That‚Äôs good ‚Äî no overlap here.\n\nOur goal is to make the frequency domain discrete as well. To do that, we multiply the spectrum by a periodic train of impulses ‚Äî shown here in green. This is just like what we did before in the time domain, but now we‚Äôre applying it in the frequency domain.\nMultiplication in the frequency domain corresponds to convolution in the time domain. So, when we apply this green impulse train to the spectrum, the effect in the time domain is that our discrete signal gets duplicated at regular intervals, spaced by capital T.\nHere, delta-u ‚Äî the spacing between impulses in the frequency domain ‚Äî equals one over capital T, where capital T is the period of each repeated signal in the time domain.\n\nAs before, we must choose our frequency-domain sampling rate carefully. The impulses in frequency must be close enough together ‚Äî in other words, our spacing delta-u must be small enough ‚Äî to avoid overlaps in the time domain. This is the same logic as before, just in the other domain.\n\nBy doing this, we end up with a signal that is discrete in both the time domain and the frequency domain. This is the essential step that brings us to the discrete Fourier transform, or DFT.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 8 - Slide15.txt", "file_path": "Lecture 8\\Texts\\Slide15.txt", "content": "Let‚Äôs restate the big idea in another way.\n\nWe start with a continuous function in the time domain, which we‚Äôll call f of t. It has a continuous Fourier transform, capital F of u, in the frequency domain. These two are perfectly related ‚Äî one can be transformed into the other with no loss of information.\n\nNow, if we sample f of t at the Nyquist rate or higher, we get a discrete version of the signal ‚Äî here shown as g of t. And because our sampling rate satisfies the theorem, there‚Äôs no loss of information in moving from the continuous signal to its discrete version.\nThat discrete-time signal, g of t, also has its own Fourier transform ‚Äî here represented as capital G of u. And again, as long as we have sampled properly, this discrete spectrum contains the same information as the continuous spectrum, capital F of u.\n\nThe key point is that the Fourier transform is an invertible process. That means moving from f of t to capital F of u, or from g of t to capital G of u, can be done in either direction without losing information ‚Äî as long as the sampling theorem is respected.\n\nSo whether we choose to work in the continuous domain or in the discrete domain, we can perform equivalent signal analysis. The difference is that in the real world, our computers are digital. That means we do everything in the discrete domain ‚Äî but thanks to the theory, we can be confident that our results match what we would get in the continuous world.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 8 - Slide16.txt", "file_path": "Lecture 8\\Texts\\Slide16.txt", "content": "Let‚Äôs now bring everything together in one big picture, but with a bit more detail.\n\nWe start with a continuous function, f of t, and its continuous Fourier spectrum, capital F of u. Our goal is to sample both the time domain and the frequency domain in a way that makes sense for digital processing.\n\nFirst, let‚Äôs ask: how many samples will we have in the time domain?\nThis depends on two things:\nThe sampling interval in time, delta-t. This is equal to one over capital P, where capital P is the spacing in the frequency domain. The smaller the delta-t, the more data points we collect.\nThe total duration of the signal, capital T. The longer we record, the more samples we have.\nThe total number of samples in the time domain ‚Äî call it N ‚Äî is simply the total duration T divided by delta-t. Since delta-t is one over P.\n\nWe can write:\u000bN equals T multiplied by P.\nNow let‚Äôs switch to the frequency domain.\nHere, the total span of one period in frequency is capital P. The sampling interval in frequency, delta-u, is equal to one over capital T. So the total number of samples in frequency over one period ‚Äî call it M ‚Äî is the total span P divided by delta-u. That‚Äôs again P multiplied by T.\nThis symmetry is important:\u000bThe number of data points in the time domain and the number of data points in the frequency domain are the same ‚Äî both equal to P multiplied by T.\n\nSo whether we‚Äôre sampling in time or in frequency, the total number of samples is determined by the product of the total span in one domain and the sampling density in the other.\n\nThis is the balanced relationship that underlies the discrete Fourier transform, and it‚Äôs why we can move between time and frequency representations so efficiently in digital signal processing.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 8 - Slide17.txt", "file_path": "Lecture 8\\Texts\\Slide17.txt", "content": "Now let‚Äôs highlight the key variables and the important relationship between them.\n\nIn the time domain, the total number of samples is T times P. Here, capital T is the total duration of the signal, and capital P is the spacing in the frequency domain. So we have N equal to T multiplied by P.\nIn the frequency domain, we have the exact same number of samples, also T multiplied by P. We call this number M, and here P is the total span of one period in frequency, while T controls the sampling density in frequency.\n\nIf we take the reciprocal of N, we get a very useful relationship:\u000b1 over N equals delta-t times delta-u,\u000bwhere delta-t is the sampling step size in time, and delta-u is the sampling step size in frequency.\nThis is an expression of the duality between time and frequency. If we fix the total number of samples N, making delta-t smaller ‚Äî meaning we sample more finely in time ‚Äî will make delta-u larger, meaning the frequency samples are spaced farther apart. And the reverse is also true.\n\nIn simple terms: if you zoom in more on one domain, you automatically zoom out in the other. This balance is built into the mathematics of the Fourier transform and is something we‚Äôll use later when we look at applications.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 8 - Slide18.txt", "file_path": "Lecture 8\\Texts\\Slide18.txt", "content": "Now we‚Äôre ready to move from the continuous Fourier transform to the discrete Fourier transform.\n\nWhen we sample a continuous signal, it stops being a smooth curve that exists at every moment in time. Instead, it becomes the product of the original signal and a series of equally spaced spikes ‚Äî what we call delta functions. These spikes occur at time intervals of delta-t, which means each spike is separated by one divided by capital P in time. In other words, we only keep the signal‚Äôs value at these regular time points, and everywhere else is zero.\n\nWe can describe this mathematically as a sum of delta spikes. Each spike occurs at a time equal to n divided by P, where n goes from zero up to N minus one. The height of each spike is simply the value of the original signal at that time. You can imagine it as a row of vertical lines, each carrying the amplitude of the signal at its sampling point.\nThis gives us the discrete version of the signal in the time domain. Even though it‚Äôs just a set of separate values, it still represents a function of time, so we can take its Fourier transform.\n\nWhen we do that, the Fourier transform turns into a sum over all the sampled points. The oscillating factor ‚Äî what we call the kernel ‚Äî still has the form ‚Äúe to the power of minus j times two-pi times the frequency u times n divided by P,‚Äù but now we only evaluate it at those sampled time points.\n\nThe outcome is what we call the direct Fourier transform of the sampled signal. This is how we connect the continuous theory to the practical, digital form of the discrete Fourier transform that we use in computation.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 8 - Slide19.txt", "file_path": "Lecture 8\\Texts\\Slide19.txt", "content": "The discrete Fourier transform we just discussed is actually a periodic function. Let‚Äôs understand why.\n\nThink back to the Fourier series. In the time domain, a Fourier series is built from sinusoidal components ‚Äî each one a sine or cosine wave. And by definition, each of these components is periodic. Whether we have just one frequency or many, the result will still be periodic.\n\nNow, in this expression at the top, the left-hand side shows the Fourier series in the time domain, with terms involving t over capital T. On the right-hand side, we have an almost identical formula ‚Äî but here the variable t has been replaced by u, and T has been replaced by P. This tells us that in the frequency domain, we also get a periodic function.\n\nThe periodicity comes directly from the fact that the Fourier transform of a sampled signal is made up of these repeating sinusoidal components. Each one repeats over and over, giving us a continuous function in the frequency domain that is also periodic.\nThis is exactly what we saw in the big picture earlier: when we sample in one domain, the Fourier transform becomes periodic in the other domain.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 8 - Slide20.txt", "file_path": "Lecture 8\\Texts\\Slide20.txt", "content": "Up to now, we‚Äôve mostly talked about sampling in the time domain. But we can also sample directly in the Fourier domain.\n\nLet‚Äôs start with the Fourier transform of our signal, f-hat of u. If we multiply this spectrum by a train of delta functions in the frequency domain ‚Äî just like we did earlier in the time domain ‚Äî we get a sampled version of the spectrum. That means we only keep its values at certain frequency points, which we can label as u-zero, u-one, all the way up to u (N minus 1).\nHere, the spacing between these frequency samples is 1 over capital T, where capital T is the period in the time domain. The index m runs from zero to N minus one, just like it did in the time-domain case. And the total number of samples in the frequency domain is the same as in the time domain ‚Äî both equal to capital N.\n\nMathematically, we can write the sampled spectrum at position u m as a sum over all the time-domain samples f of n over P, multiplied by a complex exponential. The exponent has a nice symmetry: it‚Äôs e to the power of minus j, 2 pi, n times m, divided by capital N.\n\nThis symmetry is important ‚Äî it shows that sampling in the Fourier domain has exactly the same mathematical structure as sampling in the time domain. The formulas look almost identical, just with the roles of time and frequency swapped.\n\nThis parallel is one of the reasons the discrete Fourier transform is so elegant ‚Äî it‚Äôs the same process, whether you‚Äôre thinking in terms of time samples or frequency samples.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 8 - Slide21.txt", "file_path": "Lecture 8\\Texts\\Slide21.txt", "content": "Now we can state the discrete Fourier transform \n\nWe‚Äôve sampled the signal at times\u000bt n equals to n divided by capital P, for n equals to 0, 1, ‚Ä¶, N minus 1.\u000bWe‚Äôll read the spectrum at frequencies\u000bu m equals to m divided by capital T, for m equals to 0, 1, ‚Ä¶, N minus 1,\u000bwith N equals to T times P.\n\nThe DFT value at u m is\n‚Äúsum from n equals zero to N minus one, f of n over P, into e to the power of minus j, two pi, m n over N.‚Äù\nThis is just an inner product with a complex sinusoid of frequency index m.\u000bEach output is complex: its magnitude tells us how much of that frequency is present, and its angle (the phase) tells us the shift.\n\nA few quick anchors:\nt n equals to n over P are the time samples.\nu m equals to m over T are the frequency samples.\nThe exponential ‚Äúe to the minus j two pi m n over N‚Äù is the rotating basis wave.\n\nNext, we‚Äôll write the inverse DFT and see why a factor of one over N naturally appears so we can reconstruct the time samples exactly.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 8 - Slide22.txt", "file_path": "Lecture 8\\Texts\\Slide22.txt", "content": "Up to now, we‚Äôve been writing our time samples as t-zero, t-one, t-two, and so on, and our frequency samples as u-zero, u-one, u-two, and so on. But when we work with sampled data, what really matters are the integer positions of these samples.\n\nSo, to make things simpler, we‚Äôll label them using integers in square brackets. For example:\nf of zero means the value of the function at time sample t-zero.\nf of one means the value at time sample t-one.\nf of N minus one means the value at the last time sample, t N minus one.\nWe do the same in the frequency domain:\nf-hat of zero means the Fourier transform value at frequency sample u-zero.\nf-hat of one is at u-one.\nAnd so on, up to f-hat of N minus one.\n\nWith this notation, the discrete Fourier transform reads like this:\nf-hat of m  equals the sum from n equals zero to N minus one of f of n, multiplied by e to the power of minus j, two pi, m times n divided by N.\nHere, n is the index for time-domain samples, and m is the index for frequency components.\nEach term is just the sample value f of n  multiplied by a sinusoidal basis wave at frequency index m. Adding all those terms together gives the strength of that frequency in the signal ‚Äî that‚Äôs our Fourier coefficient.\n\nWe can also see this as a matrix multiplication:\nThe time samples f of zero, f of one, ‚Ä¶, f of N minus one form a column vector.\nThey are multiplied by an N-by-N matrix whose entries are these complex exponentials e to the power of minus j, two pi, m times n divided by N.\nThe result is another vector containing all the frequency samples f-hat of zero, f-hat of one, ‚Ä¶, f-hat of N minus one.\nFor the first frequency index, m equals zero, every exponential equals one, so the first Fourier coefficient is simply the sum of all the time samples.\n\nLater, we‚Äôll talk about scaling factors to account for the sampling step size, but for now this integer-index form keeps the DFT definition clean and easy to use.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 8 - Slide23.txt", "file_path": "Lecture 8\\Texts\\Slide23.txt", "content": "Once we have all the discrete Fourier coefficients ‚Äî that is, f-hat of m  for m equals zero up to N minus one ‚Äî we can recover the original sampled signal values f of n  by using the inverse discrete Fourier transform.\n\nIn words, the formula says:\nf of n  equals one over capital N, times the sum from m equals zero to N minus one of f-hat of m, multiplied by e to the power of plus j, two pi, m times n divided by N.\nHere, n is the time-sample index, and m is the frequency-sample index.\nThe plus sign in the exponent is important ‚Äî it‚Äôs the opposite of the minus sign we used in the forward discrete Fourier transform. This sign change is what allows us to reverse the process.\n\nThe factor of one over capital N also plays a key role. When we computed f-hat of zero in the forward transform, we were summing all the time samples, but we didn‚Äôt average them. This factor in the inverse transform takes care of that averaging, so we get the correct values back.\nSome people prefer a symmetric version: instead of putting all the scaling in the inverse transform, they split it evenly ‚Äî using one over the square root of N in both the forward and inverse formulas. This is purely a matter of definition; the math works either way.\n\nYou can also see this in matrix form:\nIn the forward transform, we multiply the time-sample vector by the Fourier matrix of complex exponentials.\nIn the inverse transform, we multiply the frequency-sample vector by the inverse of that matrix, which has the plus sign in the exponent and the one over N factor out front.\n\nWith these two formulas ‚Äî forward and inverse ‚Äî we can move back and forth between a discretized time-domain signal and its discretized frequency-domain spectrum, with no information loss, as long as the sampling theorem was satisfied in the first place.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 8 - Slide24.txt", "file_path": "Lecture 8\\Texts\\Slide24.txt", "content": "Let‚Äôs talk about why we have this factor of one over capital N in the inverse discrete Fourier transform.\nEarlier, I mentioned that 1 over N is equal to delta-t times delta-u. Here, delta-t is the sampling interval in the time domain, and delta-u is the sampling interval in the frequency domain.\n\nThink of it this way: when we move from the continuous Fourier transform to the discrete version, we‚Äôre replacing continuous integrals with sums. In the time domain, we can picture our original continuous function as being approximated by a collection of rectangles ‚Äî each rectangle has a height equal to the sample value f of n , and a width of delta-t.\nLikewise, in the frequency domain, the Fourier spectrum is also represented as a collection of rectangles ‚Äî each one has a height equal to f-hat of m  and a width of delta-u.\n\nWhen we go from the spectrum back to the signal, we‚Äôre essentially adding up all of these little rectangular pieces. The factor of delta-t times delta-u captures the combined effect of this discretization in both domains. And since 1 over N equals delta-t times delta-u, that‚Äôs why the factor appears in the inverse DFT.\nThis is the discrete equivalent of what happens in the continuous Fourier transform, where we also have a scaling factor in the inverse formula. The difference is that here, the scaling is tied directly to our sampling steps in both domains.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 8 - Slide25.txt", "file_path": "Lecture 8\\Texts\\Slide25.txt", "content": "The factor of 1 over capital N comes directly from how the continuous Fourier transform becomes a discrete one when we sample.\n\nIn the continuous world, both the forward and inverse Fourier transforms are integrals. But when we digitize the process, we replace those integrals with sums. That‚Äôs what we see in this figure ‚Äî the smooth area under the curve is replaced by a set of thin rectangular strips. Each strip has a height equal to the function value at a sample point, and a width equal to the sampling interval.\nIn the time domain, that width is delta-t, the spacing between time samples. In the frequency domain, it‚Äôs delta-u, the spacing between frequency samples.\n\nWhen we perform a forward Fourier transform, the discretization introduces a factor of delta-t. When we perform the inverse transform, the discretization introduces a factor of delta-u. Multiply those together, and you get delta-t times delta-u ‚Äî which we know is equal to one over capital N.\nThis is why, in the inverse discrete Fourier transform, we include the 1 over N factor ‚Äî it accounts for both steps of discretization: first in time, then in frequency.\n\nAnd remember, there‚Äôs one more difference between the forward and inverse formulas: the forward transform uses a minus sign in the exponent, and the inverse transform uses a plus sign. This sign change is what lets the two operations perfectly undo each other.\nSo, the 1 over N is there because of how sampling replaces integrals with sums in both domains. It‚Äôs the scaling factor that ensures the forward and inverse transforms work together exactly.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 8 - Slide26.txt", "file_path": "Lecture 8\\Texts\\Slide26.txt", "content": "The second way to look at the discrete Fourier transform is from the perspective of harmonics.\nWhen we do a forward DFT, we can think of it as multiplying our vector of time samples by a square matrix of complex exponentials. This matrix acts like a rotation in a high-dimensional space ‚Äî it changes our coordinates from the time-domain representation to the frequency-domain representation.\n\nThe set of Fourier basis functions ‚Äî the sinusoids at different discrete frequencies ‚Äî form an orthonormal basis. That means they are all perpendicular to each other in this mathematical space. The forward transform rotates our coordinates from the ‚Äútime axis‚Äù basis to the ‚Äúfrequency axis‚Äù basis. The inverse transform simply rotates them back.\n\nThis idea generalizes. In one dimension, the Fourier basis is just sines and cosines. But in two or three dimensions, or on a sphere, we can define more complex sinusoidal patterns, called harmonics. On a sphere, these are the spherical harmonics ‚Äî each one corresponding to a certain ‚Äúfrequency‚Äù pattern over the surface.\n\nThe key property is the same: if you take two different harmonics and compute their inner product, the result is zero ‚Äî they are orthogonal. If you take one harmonic and do an inner product with itself, you get one ‚Äî it‚Äôs normalized.\nSo whether we are talking about simple sines and cosines, or more complex spherical harmonics, the Fourier transform is still just expressing a signal in terms of an orthogonal set of basis functions, then rotating between these bases.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 8 - Slide27.txt", "file_path": "Lecture 8\\Texts\\Slide27.txt", "content": "Here we‚Äôre looking at a particular orthonormal basis used in Fourier analysis.\n\nOur basis functions have the form:\ne to the power of j, two pi, m times t divided by capital T\nwhere m is the frequency index.\nIf we take two of these basis functions, one with index m and the other with index n, and compute their inner product ‚Äî meaning we integrate the product of one and the complex conjugate of the other over the full interval from zero to capital T ‚Äî two things can happen:\n\nIf m and n are different, the integral is zero. That means the functions are orthogonal ‚Äî they share no common component.\nIf m and n are the same, the integral is one. That means each basis function has a unit norm, or length equal to one.\nThis is exactly what ‚Äúorthonormal‚Äù means: the functions are orthogonal to each other, and each one has length one.\n\nBecause the Fourier basis has this orthonormal property, we can represent any signal as a sum of these basis functions without losing information. And we can go back and forth between the time-domain representation and the frequency-domain representation using the discrete Fourier transform and its inverse, knowing that the relationship is exact when the sampling theorem is satisfied.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 8 - Slide28.txt", "file_path": "Lecture 8\\Texts\\Slide28.txt", "content": "What we see here is just another way to write the discrete Fourier transform and its inverse ‚Äî same math, just different symbols. You‚Äôll often see different papers or books use different notations, so it‚Äôs important to recognize that they all mean the same thing.\n\nLet‚Äôs say we have N data points in the time domain. We‚Äôll call them h k, where k runs from zero to N minus one. These are our measured samples ‚Äî they could represent anything, such as temperature values taken at different times.\n\nTo get the frequency-domain representation, we compute H n, the Fourier coefficients, using the formula:\nH n equals the sum from k equals zero to N minus one of h k times e to the power of minus j, two pi, k n divided by N.\nThis is the forward discrete Fourier transform. The minus sign in the exponent tells us we‚Äôre rotating our coordinates in one direction in this N-dimensional space.\n\nThe inverse transform takes us back from the frequency coefficients H n to the time samples h k:\nh k equals one over N, times the sum from n equals zero to N minus one of H n times e to the power of plus j, two pi, k n divided by N.\nHere, the plus sign in the exponent means we‚Äôre rotating back, and the factor of one over N is the scaling we discussed earlier ‚Äî it accounts for the sampling steps in both time and frequency. Some definitions split the scaling evenly between the forward and inverse transforms to make them look perfectly symmetric, but that‚Äôs just a matter of convention.\n\nThe key points:\nWe have N samples, so we only need N orthogonal basis functions.\nThose basis functions are harmonics whose frequencies differ by a constant increment.\nThe forward and inverse transforms are nearly symmetric ‚Äî the main difference is the sign in the exponent and where we put the scaling factor.\n\nFrom a computational point of view, each row of the Fourier transform matrix has N elements. To compute one Fourier coefficient, we do N multiplications and N additions. Since we have N coefficients to compute, the total work is proportional to N-squared.\nThis N-squared growth in computations was a big deal in the early days of signal processing, when N could be very large. That‚Äôs why the development of the Fast Fourier Transform, or FFT, was such a breakthrough ‚Äî it reduced this cost dramatically.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 8 - Slide29.txt", "file_path": "Lecture 8\\Texts\\Slide29.txt", "content": "The Fast Fourier Transform, or FFT, is an efficient algorithm for computing the discrete Fourier transform. It was published by Cooley and Tukey in 1965, and it completely changed the way we do signal processing.\n\nTo give you an idea of its impact, in 1969, analyzing 2,048 data points from a seismic trace using the standard DFT took more than 13 hours. Using the FFT on the exact same machine, the same analysis took just 2.4 seconds. That‚Äôs an enormous improvement.\nI remember when I was in primary school, a 2,000-point seismic data analysis could take more than a full day to process. But with FFT, the same task could be finished in under three seconds.\n\nThis kind of speedup is critical in many applications, especially those that need real-time results ‚Äî whether it‚Äôs analyzing signals from an airplane‚Äôs sensors, monitoring seismic activity, or performing real-time image reconstruction.\nThe reason FFT is so powerful is that it reduces the computational complexity.\n\nA naive DFT requires about N-squared operations ‚Äî that‚Äôs N multiplications and additions for each of the N outputs.\nThe FFT reduces this to N times log-base-2 of N operations.\nThat‚Äôs a huge difference. For example, if N is 1,000, the log-base-2 of N is about 10. So instead of doing a million operations, we only need about ten thousand. The larger N gets, the more dramatic the savings become.\n\nBecause of this efficiency, FFT still plays an essential role in modern signal processing, and it‚Äôs also widely used in areas like medical imaging, machine learning, and convolution-based computations.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 30, "slide_filename": "Slide30.txt", "slide_annotation": "Lecture 8 - Slide30.txt", "file_path": "Lecture 8\\Texts\\Slide30.txt", "content": "Because the inverse discrete Fourier transform is so similar to the forward transform, we can design a fast algorithm for it as well. This is called the inverse FFT, or IFFT.\n\nIn MATLAB, the fft function computes the forward FFT, and the ifft function computes the inverse FFT. For example, fft of X, N computes an N-point FFT. If the vector X has fewer than N points, it pads with zeros; if it has more, it truncates. The IFFT works the same way, just in reverse.\n\nIf we were to implement the Fourier transform exactly as in the mathematical definition, we would do a summation for each frequency index k. Each summation is essentially an inner product ‚Äî it requires N multiplications and N additions. And we need to do this for each of the N output points. That‚Äôs N times N operations ‚Äî N squared in total.\nThe FFT and IFFT are like built-in shortcuts. You don‚Äôt need to know all the details of how they work to use them ‚Äî just like you don‚Äôt need to know how your phone connects to your friend when you press the call button. You can treat FFT and IFFT as black boxes: you give them the data, and they give you the result quickly and efficiently.\n\nWith FFT and IFFT, Fourier analysis becomes practical for real-time work. Once we can efficiently move between the time domain and the frequency domain, we can use this power for many applications ‚Äî performing convolution, estimating spectra, removing noise, detecting patterns or contours in images, and much more.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 31, "slide_filename": "Slide31.txt", "slide_annotation": "Lecture 8 - Slide31.txt", "file_path": "Lecture 8\\Texts\\Slide31.txt", "content": "Let‚Äôs look at our first example: how to compute convolution using the Fast Fourier Transform, or FFT.\n\nYou already know the direct method ‚Äî the one we practiced in class. You take one sequence, flip it, shift it, multiply corresponding terms, and sum them up. Then you repeat for each shift. This gives the convolution directly in the time domain.\nIf the first sequence has length n and the second has length m, the convolution result has n plus m minus 1 points. That‚Äôs how many shifts we need to perform. And in terms of computation, this direct method takes time proportional to n squares, where n is roughly the size of the data.\nBut there‚Äôs a much faster, indirect method. Instead of doing the convolution directly in the time domain, we switch to the frequency domain using the FFT.\n\nHere‚Äôs the MATLAB approach:\nFirst, compute the FFT of x ‚Äî that gives us its frequency spectrum.\nThen, compute the FFT of y ‚Äî the spectrum of the filter or second sequence.\nIn the frequency domain, convolution turns into multiplication. So we simply multiply the two spectra, element by element.\nFinally, apply the inverse FFT to that product to return to the time domain.\nThis works because of the convolution theorem ‚Äî it says convolution in the time domain is equivalent to multiplication in the frequency domain.\nIf you compare the results from the direct method and the FFT-based method in MATLAB, you‚Äôll see they match exactly. The key difference is efficiency:\n\nDirect convolution: about N square operations.\nFFT-based convolution: about n log n operations ‚Äî much faster for large n.\nOne small detail: in discrete Fourier analysis, both the time domain and the frequency domain are treated as periodic. That means this FFT-based convolution actually performs circular convolution unless you handle padding carefully.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 32, "slide_filename": "Slide32.txt", "slide_annotation": "Lecture 8 - Slide32.txt", "file_path": "Lecture 8\\Texts\\Slide32.txt", "content": "Let‚Äôs look at a second example, and here‚Äôs where things get interesting.\n\nWe again compute convolution in two ways:\nFirst, the direct method in the time domain.\nSecond, the FFT-based method, multiplying in the frequency domain and taking the inverse FFT.\n\nThis time, the results are different. Why?\nIt comes down to the fact that in discrete Fourier analysis, our signals are treated as periodic, not single, isolated sequences, but infinitely repeated copies.\nImagine x as one segment in time, and y as another. In the continuous world, when we shift one relative to the other, we only overlap the part that exists in the original copy. But in the discrete Fourier model, when you shift, parts of one copy can wrap around and overlap with the start of the other copy.\nIt‚Äôs as if your ‚Äúneighbor‚Äù signal is spilling into your yard ‚Äî and at the same time, you‚Äôre spilling into your other neighbor‚Äôs yard on the left.\n\nThat wrap-around interaction is exactly what circular convolution means:\nThe signals are treated as if arranged around a circle.\nWhen you slide one past the end, it reappears at the other side.\nThe FFT-based convolution performs this circular convolution by default, which is why the result here doesn‚Äôt match the direct, linear convolution.\nLater, we‚Äôll see how to avoid this wrap-around effect so the FFT method matches the true linear convolution.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 33, "slide_filename": "Slide33.txt", "slide_annotation": "Lecture 8 - Slide33.txt", "file_path": "Lecture 8\\Texts\\Slide33.txt", "content": "We just saw that FFT-based convolution gives us circular convolution by default, where the ‚Äúneighboring‚Äù copies wrap around and interfere.\n\nThe simplest way to avoid that wrap-around is to make sure those neighbors are far enough away so they can‚Äôt overlap during the shift. That‚Äôs what zero padding does.\nWe take our original signals and append enough zeros to both so that the total length is at least N plus M minus 1, where N and M are the original lengths of the two sequences.\nIn this example, each signal is only 5 samples long, but we pad them out to length 16. Now, when we perform the FFT, multiply in the frequency domain, and take the inverse FFT, the ‚Äúwrap-around‚Äù region is all zeros ‚Äî so the result matches the direct, linear convolution exactly.\n\nIn other words, zero padding is like making your yard so wide that your neighbors are too far away to cause any trouble.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 34, "slide_filename": "Slide34.txt", "slide_annotation": "Lecture 8 - Slide34.txt", "file_path": "Lecture 8\\Texts\\Slide34.txt", "content": "This picture shows the idea of zero padding in a visual way.\n\nOn top, the red triangles represent one signal, and the blue triangles represent the other. Without padding, these shapes are repeated periodically, so when you slide them for convolution, parts from the ‚Äúneighboring copies‚Äù wrap around and interfere ‚Äî that‚Äôs the circular convolution effect we saw earlier.\n\nNow, by adding enough zeros, we artificially extend the period. This pushes the neighboring copies farther apart, so when we perform the convolution, only the single red copy and the single blue copy overlap. The neighbors never get close enough to contribute.\nThe result is that our circular convolution now matches the linear convolution exactly ‚Äî because there‚Äôs no wrap-around. The earlier mismatch happened simply because we didn‚Äôt have enough zeros, so the neighbors still overlapped.\n\nThis is the essence of zero padding: make the ‚Äúyard‚Äù wide enough so your neighbors can‚Äôt mess up your calculation.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 35, "slide_filename": "Slide35.txt", "slide_annotation": "Lecture 8 - Slide35.txt", "file_path": "Lecture 8\\Texts\\Slide35.txt", "content": "If you‚Äôd like to explore these ideas further, MathWorks has some excellent tutorials and examples in its documentation.\n\nThis particular page walks through the relationship between linear and circular convolution in MATLAB. It explains the key conditions under which the two are equivalent, and shows how zero padding can be used to make circular convolution behave like linear convolution.\n\nYou‚Äôll also find clear MATLAB examples here ‚Äî creating vectors, padding them to the right length, applying the FFT, multiplying in the frequency domain, and using the inverse FFT to get back to the time domain.\nSo, if you want to reinforce what we‚Äôve just covered or see it implemented step-by-step, this is a great place to start.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 36, "slide_filename": "Slide36.txt", "slide_annotation": "Lecture 8 - Slide36.txt", "file_path": "Lecture 8\\Texts\\Slide36.txt", "content": "Our second example is spectral analysis.\nHere, we use an important relationship: one divided by capital N is equal to delta t multiplied by delta nu.\u000bIn words, delta t is the sampling interval in the time domain, and delta nu ‚Äî sometimes called delta u ‚Äî is the spacing between frequency samples in the Fourier domain.\n\nWhat this tells us is that if we keep delta t fixed and the total number of samples N fixed, then our frequency spacing delta nu is also fixed. That means, if a signal contains two frequencies that are very close together, the spectrum may not have enough resolution to separate them.\n\nOne way to improve the resolution is called zero padding. This means we add zeros to the end of the time-domain signal, which increases the total number of samples N. Increasing N makes the spacing in the frequency domain, delta nu, smaller ‚Äî giving us a finer frequency grid and better spectral resolution.\n\nIn this MATLAB example, we create a signal by adding two sinusoids.\u000bThe first term is the cosine of two times pi times one hundred times t.\u000bThe second term is the sine of two times pi times two-hundred two point five times t.\nWe sample t from zero to one second in steps of zero point zero zero one seconds, which corresponds to a sampling frequency of one kilohertz.\u000bWithout zero padding, the frequency step size may be too large to clearly separate the peaks at one hundred hertz and one-hundred two point five hertz.\u000bWith zero padding, the peaks become distinct.\n\nIf you want the full MATLAB walkthrough, you can follow the link at the bottom of the slide.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 37, "slide_filename": "Slide37.txt", "slide_annotation": "Lecture 8 - Slide37.txt", "file_path": "Lecture 8\\Texts\\Slide37.txt", "content": "Now, let‚Äôs see what happens when we perform the Fourier transform without using zero padding.\n\nWe take our signal x and compute its discrete Fourier transform using the FFT function.\u000bWe keep only the first half of the spectrum, because for a real-valued signal, the second half is just the mirror image.\u000bThen we normalize by dividing by the length of x, and scale the amplitudes appropriately.\u000bThe frequency axis is calculated from zero to half the sampling rate, in steps of the frequency resolution.\n\nOn the plot, you see two peaks in blue. The first peak is at exactly one hundred hertz ‚Äî that one is well captured because it matches the frequency grid.\u000bThe second component, however, is at one hundred two point five hertz, which does not align neatly with the frequency samples.\u000b\nAs a result, the peak is lower than it should be ‚Äî you can see that it only reaches about half of its true amplitude ‚Äî and it spreads into nearby frequency bins.\n\nThis illustrates the limitation of the frequency resolution when we don‚Äôt use zero padding.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 38, "slide_filename": "Slide38.txt", "slide_annotation": "Lecture 8 - Slide38.txt", "file_path": "Lecture 8\\Texts\\Slide38.txt", "content": "Now, let‚Äôs see what happens when we use zero padding.\u000b\nWe take our original signal and extend it in the time domain by adding a large number of zeros at the end.\u000bThis does not change the actual information in the signal, but it does increase the total number of samples.\nWhen we compute the Fourier transform of this longer sequence, the frequency spacing between the points in the spectrum becomes smaller.\u000b\nThis means we get a finer frequency grid ‚Äî more points between zero and the Nyquist frequency.\nAs a result, when we plot the spectrum, both peaks ‚Äî the one at one hundred hertz and the one at one hundred two point five hertz ‚Äî are now represented much more accurately.\u000b\nYou can see that their amplitudes are correctly estimated, and the peaks are sharper.\nSo, zero padding is a simple but powerful trick for improving the visual resolution of your spectrum.\u000bIt doesn‚Äôt actually add new information, but it lets you see the frequency components more clearly.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 39, "slide_filename": "Slide39.txt", "slide_annotation": "Lecture 8 - Slide39.txt", "file_path": "Lecture 8\\Texts\\Slide39.txt", "content": "Before we wrap up, I want to highlight something that often feels like magic in signal processing.\u000b\nA signal cannot be both time-limited and band-limited at the same time.\nIf a signal is strictly limited in time ‚Äî meaning it exists only within a certain interval ‚Äî its frequency spectrum will stretch infinitely.\u000bAnd if a signal is strictly limited in frequency ‚Äî meaning it contains only a finite range of frequencies ‚Äî then it must extend infinitely in time.\n\nIn practice, we often work with signals that are approximately time-limited and approximately band-limited.\u000bThis approximation, along with clever techniques like sampling, zero padding, and windowing, allows us to do what might seem impossible ‚Äî turning the ‚Äúimpossible‚Äù into something that works perfectly well for our needs.\u000bThat‚Äôs the kind of ‚Äúmagic‚Äù you see every day in signal processing", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 40, "slide_filename": "Slide40.txt", "slide_annotation": "Lecture 8 - Slide40.txt", "file_path": "Lecture 8\\Texts\\Slide40.txt", "content": "Now let‚Äôs step back and look at the big picture.\u000bEverything we‚Äôve discussed so far seems to work perfectly ‚Äî but there‚Äôs an important approximation hidden here.\nWe often say: if a signal is finite in time ‚Äî meaning it has a limited duration ‚Äî then its Fourier spectrum is also limited to a finite interval.\u000b\nIn reality, that‚Äôs not strictly true.\u000bIf a signal is perfectly finite in the time domain, its spectrum will actually extend infinitely. It will get smaller and smaller as you move away from the center, but it never truly stops.\nYou might argue that if we take the frequency range from minus W to plus W, and make W large enough, the parts outside this range become so small that they can be ignored.\u000bAnd yes, in practical engineering, we often do that. But from a mathematical point of view, we have to be careful.\n\nWhy?\u000bBecause when we discretize, the spectrum is periodically repeated ‚Äî we get infinitely many copies of it.\u000bEven if a single copy has very small values far away from the center, those small values from every copy can add up.\u000bThe further away a copy is, the less it contributes ‚Äî but since there are infinitely many of them, the sum of all those tiny contributions might still be significant.\nThis is why convergence is not always guaranteed and why mathematical rigor is important when making these approximations.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 41, "slide_filename": "Slide41.txt", "slide_annotation": "Lecture 8 - Slide41.txt", "file_path": "Lecture 8\\Texts\\Slide41.txt", "content": "Let me give you a simple example to show why ‚Äúgetting smaller‚Äù isn‚Äôt always enough.\n\nImagine a curve where the height at any point is equal to one divided by the position along the horizontal axis. As you move farther to the right, the values get smaller and smaller ‚Äî for example, at position one the value is one, at position two it‚Äôs one-half, at position three it‚Äôs one-third, at position four it‚Äôs one-fourth, and so on.\n\nYou might think: ‚ÄúThese numbers are tiny ‚Äî if we keep adding them, the total should stay small.‚Äù\nBut here‚Äôs the surprising fact: even if you start far out, like at one over one million, and then keep adding one over two million, one over three million, and so on, the total will still grow without limit.\n\nIn other words, an infinite number of tiny contributions can still add up to something infinitely large.\nAnd this is exactly the kind of situation we face in Fourier analysis: very small contributions from infinitely many frequency copies can still combine into something significant.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 42, "slide_filename": "Slide42.txt", "slide_annotation": "Lecture 8 - Slide42.txt", "file_path": "Lecture 8\\Texts\\Slide42.txt", "content": "This idea connects to the famous wheat and chessboard problem, which is a classic example of exponential growth.\n\nImagine a chessboard with sixty-four squares. On the first square, you place one grain of wheat. On the next square, you double it ‚Äî two grains. On the third square, you double it again ‚Äî four grains. Then eight, sixteen, thirty-two, and so on. By the time you fill all the squares, the total number of grains becomes unimaginably large ‚Äî enough to cover the Earth with a thick layer of wheat.\nThis illustrates how quickly exponential growth can get out of control.\n\nNow, in our Fourier problem, for our approximation trick to work, the leftover terms ‚Äî those tiny contributions from far-away frequencies ‚Äî must shrink faster than one divided by the position number. That means the function must be smooth enough so that its spectrum decays quickly.\n\nIn fact, for most practical functions, after some pre-processing and smoothing, the decay is even faster ‚Äî often like one divided by the position number squared. And in that case, adding up all the small contributions still gives you a small total. This is what allows our approach to work.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 8", "slide_number": 43, "slide_filename": "Slide43.txt", "slide_annotation": "Lecture 8 - Slide43.txt", "file_path": "Lecture 8\\Texts\\Slide43.txt", "content": "So, at this point, we have completed the main part of our discussion on Fourier analysis. In preparing these lectures, I‚Äôve drawn from multiple resources ‚Äî including this excellent textbook from Stanford University‚Äôs course ‚ÄúThe Fourier Transform and Its Applications‚Äù ‚Äî as well as my own insights and examples.\n\nThis textbook is designed for a full semester, and it‚Äôs quite comprehensive. My goal here has been to distill the most essential concepts for our biomedical engineering context. I‚Äôve also added examples and explanations from other sources, as well as my own understanding, to create what I like to think of as a ‚ÄúBME version‚Äù of linear systems and Fourier analysis.\n\nSo far, you have all the core material. In the next lecture, our teaching assistant will go over some homework problems to help reinforce your understanding and sharpen your skills. After that, we‚Äôll move on to our next major topic ‚Äî network image quality.\nThat means the most mathematically challenging part of the course ‚Äî Fourier analysis ‚Äî is now behind us.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 9", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 9 - Slide1.txt", "file_path": "Lecture 9\\Texts\\Slide1.txt", "content": "Hello, everyone.\u000bLet‚Äôs take a few minutes before we dive into today‚Äôs MATLAB tutorial to connect what we‚Äôve learned so far.\nOver the past lectures, we‚Äôve covered some very important foundations ‚Äî linear systems, convolution, the delta function, Fourier series, Fourier transforms, and the sampling theorem. These ideas are deeply connected, and if you understand them well, everything that follows will be much easier. If you are still unsure about any of these topics, now is a great time to go back and review.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 9 - Slide2.txt", "file_path": "Lecture 9\\Texts\\Slide2.txt", "content": "Anyway, so this is our schedule. We are on schedule, so no problem.\n\nLet me briefly walk you through the big picture again.\nA linear system is one where we can predict the output for any input by combining the system‚Äôs response to simple building blocks. Convolution is the tool that allows us to do exactly that ‚Äî it takes the input signal and the system‚Äôs impulse response, and combines them to give the output.\nThe Fourier series is another powerful idea. It tells us that any periodic signal can be written as a sum of sine and cosine waves. If we make the period infinitely long, this naturally becomes the Fourier transform, which works for non-periodic signals.\n\nBut computers don‚Äôt deal with continuous signals. We must first discretize them ‚Äî that is, represent them as sequences of numbers. This involves sampling in time, and sometimes quantizing amplitudes. The sampling theorem assures us that, if we sample fast enough ‚Äî at least twice the highest frequency in the signal ‚Äî we can reconstruct the original waveform perfectly under ideal conditions.\nOnce we sample in time, something interesting happens in the frequency domain: the spectrum becomes periodic. This leads us to the discrete Fourier transform. And if we compute it efficiently, we use the fast Fourier transform, or FFT.\nThese tools are not just theory. They are powerful methods for denoising signals, extracting features, and understanding system behavior. But please remember ‚Äî typing ‚Äúfft‚Äù in MATLAB is easy; the real value comes from understanding what that result means and why it looks the way it does.\n\nToday, I will guide you through some hands-on MATLAB examples so you can see how Fourier analysis works on discrete data, and how these ideas connect back to the theory we‚Äôve been building. This is a great chance to make sure the concepts are clear in your mind.\nAlright, let‚Äôs get started.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 9 - Slide3.txt", "file_path": "Lecture 9\\Texts\\Slide3.txt", "content": "Let me give you a quick roadmap for today‚Äôs session.\n\nWe‚Äôll begin with discrete convolution. This is simply the convolution operation applied to signals that are already in discrete form ‚Äî sequences of numbers rather than continuous waveforms. We‚Äôll explore how to compute it, and why sometimes we add extra zeros at the ends of our signals ‚Äî a technique called zero-padding. We‚Äôll also see the idea of circular convolution, and how it relates to regular, or linear, convolution.\nNext, we‚Äôll move into spectral analysis. This is one of the most common applications of Fourier analysis. Here again, zero-padding plays an important role ‚Äî but now it‚Äôs in the frequency domain. We‚Äôll also talk about refined spectral bins, which allow us to see frequency details more clearly.\n\nFinally, we‚Äôll take what we‚Äôve learned and apply it to two-dimensional filtering for images. Using the Fourier transform, we‚Äôll perform noise removal to clean up an image, and edge enhancement to bring out important structures.\nSo, in short ‚Äî discrete convolution, spectral analysis, and 2D image filtering. We‚Äôll work through these step by step, connecting the math to hands-on MATLAB demonstrations.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 9 - Slide4.txt", "file_path": "Lecture 9\\Texts\\Slide4.txt", "content": "Now, let‚Äôs take a moment to visualize what the discrete Fourier transform ‚Äî or DFT ‚Äî really means.\nOn this slide, you see two columns. On the left, we have signals in the time domain. On the right, we have their corresponding representations in the frequency domain. The arrows between them remind us that the Fourier transform takes us from one domain to the other, and the inverse Fourier transform brings us back.\n\nEach row here is a different example. In some cases, we‚Äôre multiplying two signals in the time domain. In others, we‚Äôre convolving them. And each time, there‚Äôs a matching operation in the frequency domain ‚Äî because multiplication in one domain corresponds to convolution in the other, and vice versa. This is one of the most important dual relationships in Fourier theory.\n\nYou can also see how different shapes in the time domain create specific patterns in the frequency domain. For example:\nA narrow pulse in time spreads out in frequency.\nA broad, smooth shape in time becomes more concentrated in frequency.\nSampling a continuous signal in time produces repeated copies ‚Äî or ‚Äúaliases‚Äù ‚Äî in the frequency domain.\nBy moving row by row, you can start to see the bigger picture: every change we make to a signal in one domain has a predictable effect in the other. This is exactly why Fourier analysis is so powerful ‚Äî it gives us two different but connected ways of looking at the same signal.\nAs we go through the rest of today‚Äôs examples, keep these relationships in mind. They‚Äôll help you understand not just the MATLAB results, but also the deeper reason why those results look the way they do.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 9 - Slide5.txt", "file_path": "Lecture 9\\Texts\\Slide5.txt", "content": "Here‚Äôs a simple, visual way to think about convolution.\u000bImagine you have two hand shapes. One is fixed, and the other is flipped and shifted.\nIn convolution, we take one function ‚Äî or one signal ‚Äî and flip it around, just like turning a right hand into a left hand. Then, we slide it along the other signal. At each position, we calculate how much the two shapes overlap. The amount of overlap becomes the value of the convolution at that point.\n\nThe picture here shows this idea using two hands. The vertical axis represents flipping, and the horizontal axis represents shifting. You can imagine that at certain positions, the hands align perfectly, giving a large overlap, while at other positions they hardly touch at all, giving a small or zero overlap.\n\nThis is exactly what happens in math: convolution measures the similarity between a signal and a shifted, flipped version of another signal. This idea works whether we are dealing with shapes like these hands, continuous waveforms, or sequences of numbers.\nBy keeping this visual in mind, convolution will feel much less abstract. It‚Äôs just sliding one pattern over another and measuring how well they match at each shift.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 9 - Slide6.txt", "file_path": "Lecture 9\\Texts\\Slide6.txt", "content": "Here‚Äôs our first step in the convolution process ‚Äî calculating the value at n equals zero.\n\nOn the top left, the red arrows show f of k, our first discrete signal. On the bottom left, the green arrows show g of negative k, which means we have flipped the second signal in time.\nThe formula in the middle tells us what convolution is in mathematical terms: we take the sum over all k of x of k multiplied by h of n minus k. \n\nHere, x is our input, and h is our system‚Äôs impulse response.\nAt n equals zero, we line up the flipped signal so that its zero index matches the zero index of the original signal. You can think of this like the hand illustration on the right: one hand flipped, the other fixed, and their starting points aligned at zero.\n\nNow, at each matching position of the red and green arrows, we multiply the values together and then sum all those products. That sum is the value of the convolution at n equals zero.\nThis is the key step ‚Äî convolution is nothing more than multiplying corresponding values from two aligned sequences, and then summing the results. We repeat this process for each shift of n to get the full convolution.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 9 - Slide7.txt", "file_path": "Lecture 9\\Texts\\Slide7.txt", "content": "Now let‚Äôs see what happens when we compute the convolution at n equals eight.\n\nJust like before, the red arrows on the top represent f of k, and the green arrows on the bottom show g of eight minus k ‚Äî meaning the second signal has been flipped and shifted so that its index aligns with n equals eight.\nIn our hand analogy, you can imagine sliding the flipped hand far enough to the right so that only the outer fingers ‚Äî the ‚Äúend fingers‚Äù ‚Äî are touching. This is the opposite alignment from what we saw at n equals zero.\n\nAt this position, only a few of the red and green arrows overlap. We multiply those overlapping values and then sum them up. The result gives us the convolution value at n equals eight.\nBy repeating this shifting process for every value of n, we build up the entire convolution sequence, one position at a time.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 9 - Slide8.txt", "file_path": "Lecture 9\\Texts\\Slide8.txt", "content": "Let‚Äôs walk through this functional example, which will connect directly to what we‚Äôll do in MATLAB later.\n\nWe start with h of n, shown in the first plot ‚Äî a simple rectangular pulse. Then we have x of n in the second plot ‚Äî think of it as three rectangular pulses in a row, one shown in red and the other two in blue.\nIf we take these two signals and perform a linear convolution ‚Äî shown in the third plot ‚Äî we get a series of triangular shapes. This makes sense: convolving two rectangular pulses produces a triangle. Here we have three such triangles because x of n contains three rectangles.\n\nNow, in the fourth plot, something changes. We create x  N of n ‚Äî this means x of n is now periodically extended with N samples per period. In other words, we‚Äôve made it repeat over and over, which is why we call it circularly extended.\nIn the fifth plot, we see the linear convolution of this periodic x  N of n with h of n. Notice what happens: instead of three clean, separate triangles, the edges wrap around, and the red and blue parts start overlapping. These are edge effects caused by the circular extension.\nFinally, the last plot shows the composite output. The green section in the middle is the portion unaffected by edge effects. This part matches the original linear convolution result and is what we actually want.\n\nThe lesson here is important: to avoid these unwanted overlaps in circular convolution, we often use zero-padding ‚Äî adding extra zeros to the signals before performing the convolution. This ensures that the result matches the true linear convolution without distortion at the edges.\nWe‚Äôll explore this zero-padding step in detail in a few slides, and you‚Äôll see how MATLAB handles it.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 9 - Slide9.txt", "file_path": "Lecture 9\\Texts\\Slide9.txt", "content": "Here‚Äôs a simple MATLAB example you can try for yourself.\n\nWe start by defining two discrete signals ‚Äî or vectors in MATLAB ‚Äî x and h. In this case:\u000bx equals 5,4,3,2,1\u000bh equals 1,2,3,4,5\nNext, we perform a convolution using MATLAB‚Äôs built-in conv function:\u000by equals conv x, h;\nFinally, we plot the result:\u000bplot y; ylim 0 to 100;\n\nWhat do we see? A clean triangular shape. This is exactly what we expect when we linearly convolve two finite sequences that look like ramps in opposite directions. The convolution produces a sequence that starts small, rises to a peak ‚Äî here at the center ‚Äî and then falls symmetrically.\nThis simple example is a good reminder that convolution is not mysterious. It‚Äôs just a systematic way of multiplying and summing overlapping parts of two signals. And MATLAB makes it easy to visualize.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 9 - Slide10.txt", "file_path": "Lecture 9\\Texts\\Slide10.txt", "content": "Now let‚Äôs think about circular convolution.\n\nEarlier, with linear convolution, we had just one pair of signals sliding past each other. In this hand analogy, that meant one top hand and one bottom hand.\n\nHere, with circular convolution, the situation is different. The signal is treated as if it repeats periodically ‚Äî just like in our earlier example from Wikipedia. So instead of one top hand, we now have several copies in a row, as shown here with three hands.\nWhen we slide the lower signal, we can‚Äôt ‚Äúrun out‚Äù of data on either end, because the pattern simply wraps around. That means parts of the signal from the end overlap with parts from the beginning.\n\nIn the picture, you can see that the fingers from the lower hands are overlapping with the fingers from the upper hands ‚Äî not just in the middle, but also across the boundaries. This wrapping-around effect is exactly what creates the edge overlaps we saw earlier.\nIf we don‚Äôt want those overlaps to distort our result, we need to use zero-padding before performing the convolution. That‚Äôs the key difference between the clean linear convolution result and the wrapped-around circular convolution result.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 9 - Slide11.txt", "file_path": "Lecture 9\\Texts\\Slide11.txt", "content": "Now, let‚Äôs see how zero-padding helps us fix the edge effect problem we saw with circular convolution.\n\nIn the earlier example, the red and green lines ‚Äî representing the start and end of the main region we care about ‚Äî were very close together. This caused the signal to wrap around and overlap in places we didn‚Äôt want.\nZero-padding means we insert extra zeros at the ends of our signals before performing the convolution. In this hand analogy, adding zero-padding is like putting some extra space between the hands. The top and bottom hands are now farther apart before they start overlapping.\n\nAs a result, the main section we care about ‚Äî here marked in red ‚Äî remains untouched by unwanted overlaps. The green section, which previously had interference from wrapping, now stays clean.\n\nThis is exactly how we prevent the distortion seen in the Wikipedia example: by padding with enough zeros, the circular convolution result becomes identical to the true linear convolution result.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 9 - Slide12.txt", "file_path": "Lecture 9\\Texts\\Slide12.txt", "content": "Here‚Äôs a straightforward MATLAB example to show linear convolution in action.\n\nWe start with two sequences:\u000bx equals 5,4,3,2,1,0\u000bh equals 1,2,3,4,5,6\nThe first figure shows both sequences plotted together. The blue line is x ‚Äî starting high and decreasing. The red line is h ‚Äî starting low and increasing.\nNow we use MATLAB‚Äôs built-in conv function to compute their linear convolution:\u000blconv equals conv x, h;\n\nWhen we plot the result, we see the familiar triangular shape. It rises as the two signals increasingly overlap, reaches a peak when their centers align, and then falls as the overlap decreases.\n\nIf this feels familiar, it‚Äôs because we used the same function ‚Äî conv ‚Äî earlier when convolving the RC circuit‚Äôs impulse response with the unit step function in your homework. The process is exactly the same here; only the input sequences are different.\nThis example is a nice reminder that MATLAB makes it very easy to visualize convolution, and seeing the result in a plot helps reinforce what‚Äôs happening mathematically.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 9 - Slide13.txt", "file_path": "Lecture 9\\Texts\\Slide13.txt", "content": "Here‚Äôs where we can clearly see the difference between linear convolution and circular convolution without zero padding.\n\nIn MATLAB, we can perform circular convolution using the Fourier transform approach. First, we take the FFT of our two sequences, x and h. Then, we multiply their spectra point by point. Finally, we take the inverse FFT to bring the result back into the time domain.\nMathematically, multiplication in the frequency domain is equivalent to convolution in the time domain ‚Äî but here‚Äôs the key point: if we don‚Äôt use zero-padding, the convolution we get is circular by nature.\nIn the plot below, the blue curve is the circular convolution result without zero-padding. The red dashed curve is the linear convolution result from our earlier example.\n\nNotice how the blue curve is distorted ‚Äî it doesn‚Äôt match the clean triangular shape of the red curve. That distortion comes from the wrapping-around effect we talked about earlier: parts of the signal from the end are interfering with parts from the beginning.\nThis is why zero-padding is essential when we want the frequency-domain method to give the same result as linear convolution. Without it, circular convolution will give you something different.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 9 - Slide14.txt", "file_path": "Lecture 9\\Texts\\Slide14.txt", "content": "Here we see why zero-padding is the key to making circular convolution match linear convolution.\n\nWe begin by setting N to be the length of x plus the length of h, minus 1. This value of N is exactly the length of the linear convolution result ‚Äî something you‚Äôve already seen in your RC circuit homework.\nNext, we create padded versions of our two signals:\nx pad starts with the original values of x, followed by enough zeros so that its length is N.\nh pad is created in exactly the same way, starting with h and then adding zeros to reach length N.\n\nNow, we take the FFT of these zero-padded signals, multiply them point by point in the frequency domain, and take the inverse FFT to return to the time domain.\n\nWhen we plot the result, we see that the zero-padded circular convolution ‚Äî shown in blue ‚Äî lies exactly on top of the linear convolution ‚Äî shown in red. This perfect match confirms that zero-padding has removed the unwanted wrap-around effects and given us the correct, undistorted result.\n\nSo the takeaway is clear: if you‚Äôre using the FFT method to compute convolution, always add enough zero-padding to get the true linear convolution.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 9 - Slide15.txt", "file_path": "Lecture 9\\Texts\\Slide15.txt", "content": "Now that we‚Äôve explored convolution in detail ‚Äî both in the time domain and through the FFT method ‚Äî let‚Äôs move into one of the most common and powerful applications of the Fourier transform: spectral analysis.\n\nSpectral analysis is about looking at a signal in the frequency domain to see what frequencies are present and how strong they are. This is incredibly useful in engineering, science, and even everyday technology ‚Äî from analyzing audio signals, to detecting features in images, to identifying patterns in biomedical data.\n\nIn the upcoming slides, we‚Äôll take an example signal, examine its spectrum, and then see how techniques like zero-padding and refining spectral bins help us get a clearer, more detailed view of its frequency content.\nLet‚Äôs start by looking at our example signal and breaking it down step by step.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 9 - Slide16.txt", "file_path": "Lecture 9\\Texts\\Slide16.txt", "content": "Let‚Äôs start our spectral analysis with a simple but quite subtle example.\n\nWe‚Äôll create a signal that contains two different pure tones ‚Äî one at one hundred hertz and the other at two hundred two point five hertz.\nIn MATLAB, we first set the sampling frequency, F s, to one thousand hertz. That means we take one thousand samples every second.\nNext, we create the time vector, t. It starts at zero seconds and increases in steps of zero, zero point zero zero one seconds ‚Äî that‚Äôs one millisecond ‚Äî until just before one second.\n\nNow, our signal x is built by adding two parts:\nFirst, a cosine wave with frequency one hundred hertz. In MATLAB, that‚Äôs written as: cosine of open parenthesis two times pi times one hundred times t close parenthesis.\nSecond, a sine wave with frequency two hundred two point five hertz. In MATLAB, that‚Äôs: sine of open parenthesis two times pi times two hundred two point five times t close parenthesis.\nWhen we add these two waveforms together, we get x, which contains both frequencies.\n\nIf we plot only the first one hundred samples, we can see the combined oscillations. Sometimes the waves reinforce each other, producing larger peaks; other times they partially cancel out, making smaller peaks.\nThis is the time-domain view. Next, we‚Äôll use the Fourier transform to find these frequencies in the frequency domain ‚Äî and then we‚Äôll explore how zero-padding can help us make those frequencies stand out more clearly.  It takes some effort to appreciate the benefit of zero-padding!", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 9 - Slide17.txt", "file_path": "Lecture 9\\Texts\\Slide17.txt", "content": "Now, let‚Äôs look at the spectrum of our synthetic signal without applying any zero-padding.\n\nWe perform the Fourier transform of our signal using the FFT function. This gives us the discrete frequency components of the signal. Then, we plot the magnitude of the FFT against frequency, with frequency in hertz along the horizontal axis.\nFrom the plot, we can clearly see a sharp spike at one hundred hertz. That spike corresponds exactly to the first or cosine component we built into the signal.\n\nBut notice something important ‚Äî the second or sine component, at two hundred two point five hertz, does not appear as a clear spike. Instead, it‚Äôs blurred and not well-resolved. This happens because two hundred two point five hertz is not exactly aligned with one of the FFT‚Äôs default spectral bins, which are spaced one hertz apart.\n\nSo, without zero-padding, the FFT resolution is limited by the number of samples we have. The first frequency happens to align perfectly with a bin, so it looks sharp. The second one falls between bins, so its energy is spread out over multiple points in the plot, making it appear a bit blurry and less distinct.\nWe‚Äôll see in the next slide how zero-padding helps us refine the spectral bins and make that second frequency stand out much more clearly.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 9 - Slide18.txt", "file_path": "Lecture 9\\Texts\\Slide18.txt", "content": "Now let‚Äôs see what happens when we add zero-padding before taking the Fourier transform.\n\nThis time, when we call the FFT function, we specify a length of two thousand. That means we take our original signal and append enough zeros to make it two thousand points long before computing the FFT.\nBy doing this, we change the spacing between frequency bins. Previously, without padding, the bins were one hertz apart. Now, the bin spacing is F s divided by two thousand, which equals zero point five hertz. This finer spacing allows us to more accurately pinpoint frequencies that fall between the old one-hertz bins.\n\nWhen we plot the result, we still see the sharp spike at one hundred hertz. Also, we now see a very clear spike at two hundred two point five hertz ‚Äî the second frequency in our original signal.\nThe reason is simple: with zero-padding, we‚Äôve increased the frequency resolution of the spectrum, making it possible to distinguish both components of our signal cleanly.\n\nSo, zero-padding doesn‚Äôt create new information ‚Äî it simply lets us view the spectrum at a finer scale, revealing details that would otherwise be hidden between coarse frequency bins.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 9 - Slide19.txt", "file_path": "Lecture 9\\Texts\\Slide19.txt", "content": "Let‚Äôs look at a more complex example ‚Äî a continuous signal that contains several frequency components.\n\nWe start with the same two frequencies from our earlier example: a cosine wave at one hundred hertz and a sine wave at two hundred two point five hertz.\nThen, we add three more components:\nA cosine wave at forty-five hertz\nA cosine wave at four hundred seven hertz\nAnd a sine wave at four hundred forty-five point eight hertz\n\nWhen we add all these together, we get a signal that looks quite messy in the time domain ‚Äî as you can see in the plot. The rapid oscillations and varying amplitudes come from the interaction of all these frequencies.\nLooking at this time-domain plot alone, it‚Äôs not easy to tell exactly what frequencies are present or how strong they are. That‚Äôs where the Fourier transform becomes incredibly valuable ‚Äî it lets us switch to the frequency domain, where each frequency component appears as a distinct peak, making the signal‚Äôs composition much clearer.\n\nIn the next step, we‚Äôll apply the Fourier transform to this signal and see how it reveals the underlying structure that‚Äôs hidden in the time-domain view.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 9 - Slide20.txt", "file_path": "Lecture 9\\Texts\\Slide20.txt", "content": "Here‚Äôs the spectrum of our multi-frequency signal without zero-padding and without refining the frequency bins.\n\nWhen we take the FFT and plot the magnitude against frequency in hertz, some frequencies stand out clearly, while others do not.\nLook at forty-five hertz ‚Äî this is a clean, discrete value, and it aligns perfectly with an FFT bin. As a result, its spike reaches the maximum amplitude of one, just as we would expect.\nBut for the frequencies at two hundred two point five hertz and four hundred forty-five point eight hertz, things are different. Because these frequencies do not align exactly with the FFT‚Äôs default bin spacing, their energy is spread across multiple bins. This makes their peaks appear lower than one and less sharply defined.\n\nSo, without zero-padding ‚Äî and therefore without finer bin spacing ‚Äî our frequency resolution is limited. Frequencies that happen to fall exactly on a bin are well-resolved, but others that fall in between bins are smeared out and not as clear.\n\nNext, we‚Äôll see how refining the bins through zero-padding improves our ability to clearly detect those non-integer frequencies.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 9 - Slide21.txt", "file_path": "Lecture 9\\Texts\\Slide21.txt", "content": "Now, let‚Äôs refine our frequency bins by a factor of two ‚Äî the same idea we used earlier when going from one-hertz bins to zero-point-five-hertz bins.\n\nWe do this by setting a bin-refine factor equal to two, and then increasing the FFT length to be two times the number of samples in our original time vector. In practice, this means appending enough zeros to double the length of the signal before computing the FFT.\nWhen we plot the result, the improvement is clear: the frequency at two hundred two point five hertz is now well resolved, showing a sharp and distinct peak.\n\nHowever, notice that the frequency at four hundred forty-five point eight hertz is still not well resolved. Even with a bin spacing of zero-point-five hertz, this frequency does not fall exactly on one of the discrete FFT bins, so its energy is still spread across multiple bins. The peak is visible but not as sharp or as tall as the perfectly aligned frequencies.\n\nThe takeaway is that zero-padding and bin refinement improve resolution, but they cannot create a perfectly sharp spike unless the frequency exactly matches a discrete bin location.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 9 - Slide22.txt", "file_path": "Lecture 9\\Texts\\Slide22.txt", "content": "Now, let‚Äôs refine our frequency bins even further ‚Äî this time by a factor of four.\n\nIn MATLAB, that means setting the bin-refine factor to four and increasing the FFT length to four times the length of our original time vector. In other words, we append enough zeros so that the signal length is quadrupled before computing the FFT.\nBy doing this, our frequency bin spacing changes from one hertz to zero-point-two hertz. This very fine resolution means that even frequencies like four hundred forty-five point eight hertz ‚Äî which were previously hard to resolve ‚Äî now align closely with a bin and show a sharp, well-defined spike.\n\nWhen we plot the result, every frequency in our multi-component signal ‚Äî forty-five hertz, one hundred hertz, two hundred two point five hertz, four hundred seven hertz, and four hundred forty-five point eight hertz ‚Äî is clearly visible, each with a maximum amplitude of one.\nThis example confirms that refining the bin spacing through zero-padding greatly improves our ability to detect non-integer frequencies. With enough refinement, even challenging frequencies can be resolved as sharply as those that naturally align with the original bin spacing.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 9 - Slide23.txt", "file_path": "Lecture 9\\Texts\\Slide23.txt", "content": "So far, we‚Äôve looked at the Fourier transform in one dimension ‚Äî signals that vary with time. But Fourier analysis is just as powerful in two dimensions, and that‚Äôs exactly what we need for image processing.\n\nIn the context of images, the two dimensions are the horizontal and vertical spatial coordinates, rather than time. Just as a one-dimensional Fourier transform tells us what frequencies are present in a signal, a two-dimensional Fourier transform tells us what spatial frequencies are present in an image.\n\nThis is a critical tool in image analysis, because many image processing tasks ‚Äî such as noise removal or edge enhancement ‚Äî are easier to understand and perform in the frequency domain.\nIn the next slides, we‚Äôll see how 2D Fourier transforms can be applied to filtering images, starting with noise removal and then moving on to edge enhancement.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 9 - Slide24.txt", "file_path": "Lecture 9\\Texts\\Slide24.txt", "content": "Here we see the concept of the two-dimensional Fourier transform.\n\nThe equations at the top describe two processes:\nThe forward 2D Fourier transform, which takes a spatial-domain function ‚Äî something that varies in the x and y directions ‚Äî and expresses it in terms of its spatial frequency components.\nThe inverse 2D Fourier transform takes that frequency-domain representation and reconstructs the original spatial-domain function.\n\nIn our example, the starting function is shown as a sideways rectangle in the spatial domain. When we perform the 2D Fourier transform, we obtain a frequency-domain representation ‚Äî shown here in the rainbow-colored plot. The colors represent the amplitude of different spatial frequency components, and the values can be complex, with both real and imaginary parts.\nJust as in the one-dimensional case, the Fourier transform lets us switch back and forth between two perspectives:\nIn the spatial domain, we see shapes and patterns directly.\n\nIn the frequency domain, we see how much of each spatial frequency is present in the image.\nUnderstanding this relationship is essential for image processing, because many operations ‚Äî such as filtering, noise removal, and edge detection ‚Äî can be performed more effectively in the frequency domain.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 9 - Slide25.txt", "file_path": "Lecture 9\\Texts\\Slide25.txt", "content": "In image processing, noise can often hide important details. This is a common problem in real-world applications, especially when images are captured in low light, transmitted over noisy channels, or acquired using sensors in a challenging environment.\nHere, we start with an image ‚Äî the well-known ‚ÄúLena‚Äù test image ‚Äî that has been corrupted by noise. In the top left, you can see the noisy image in the spatial domain.\n\nWhen we take the two-dimensional Fourier transform of this image, shown in the bottom left, we see its frequency-domain representation. The bright spot in the center corresponds to low-frequency components, which carry the main shapes and structures of the image. The scattered speckles throughout the spectrum represent higher-frequency noise.\nIf we want to reduce noise, we can selectively remove certain frequency components. One simple way is to set the outer regions of the spectrum to zero, while keeping only the frequencies near the center ‚Äî this is essentially a low-pass filter in the frequency domain. In the bottom right, you can see this filtered spectrum.\n\nFinally, applying the inverse Fourier transform gives us the restored image in the top right. The noise is greatly reduced, and the underlying features ‚Äî such as the woman‚Äôs face and hat ‚Äî become much clearer.\nThis kind of frequency-domain noise suppression is a powerful technique in imaging, allowing us to recover important details that would otherwise be lost.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 9 - Slide26.txt", "file_path": "Lecture 9\\Texts\\Slide26.txt", "content": "We‚Äôve just seen how Fourier transforms can be used to remove noise. Now, let‚Äôs look at two related techniques ‚Äî low-pass filtering and high-pass filtering ‚Äî and how they affect an image.\n\nStarting with the original image of the building, shown in the top left, we first take its two-dimensional Fourier transform. This gives us the frequency-domain representation, shown in the bottom left.\nIf we want to blur the image or remove fine details, we apply a low-pass filter. In the frequency domain, this means keeping only the low-frequency components near the center of the spectrum ‚Äî as shown in the middle bottom image ‚Äî and setting the high frequencies to zero. After applying the inverse Fourier transform, we get the middle top image: the overall structure is preserved, but the sharp edges are softened.\n\nOn the other hand, if we want to highlight edges and fine details, we use a high-pass filter. This is done by removing the low-frequency center and keeping only the higher frequencies in the spectrum, as shown in the bottom right. The resulting image, in the top right, emphasizes edges and textures, but loses smooth gradients.\nLow-pass and high-pass filters are fundamental tools in image processing ‚Äî whether we want to smooth an image, detect edges, or prepare data for further analysis.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 9 - Slide27.txt", "file_path": "Lecture 9\\Texts\\Slide27.txt", "content": "In image processing, we often use specialized filters to enhance certain features or to suppress unwanted information. Here are three common examples, shown both in the spatial domain and in the frequency domain.\n\nFirst, the Unsharp Filter. Despite the name, this filter is used to sharpen images. It works by subtracting a blurred version of the image from the original, which enhances the edges. In the spatial domain, you can see the filter kernel ‚Äî dark around the edges, light in the center ‚Äî designed to emphasize changes in intensity.\n\nNext, the Gaussian Filter. This is a smoothing filter, which reduces noise and small details. In the spatial domain, it appears as a soft, bell-shaped pattern ‚Äî bright in the middle, fading smoothly outward. Its frequency-domain representation shows that it preserves low frequencies while gradually suppressing high frequencies.\n\nFinally, the Sobel Filter. This is an edge detection filter, which highlights regions of rapid intensity change. In the spatial domain, you can see its distinctive pattern ‚Äî one side light, the other dark ‚Äî which responds strongly to vertical or horizontal edges depending on its orientation. In the frequency domain, its pattern reflects the fact that it suppresses low frequencies and emphasizes specific directional components.\n\nMATLAB provides built-in functions for these filters, including fspecial for 2D and fspecial3 for 3D filtering, as well as options to design your own custom kernels. By choosing the right filter and domain of application ‚Äî spatial or frequency ‚Äî we can control exactly what features of an image are enhanced or suppressed.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 9 - Slide28.txt", "file_path": "Lecture 9\\Texts\\Slide28.txt", "content": "Here we have an example showing the effects of different image filters, both in the spatial domain and in the frequency domain.\n\nIn the top left is our original image ‚Äî a close-up of an eye. Next to it, in the top right, we see its two-dimensional Fourier transform, which shows the frequency content of the image.\n\nOn the second row, we have three filtered versions of the image in the spatial domain:\nUnsharp Image ‚Äî produced by applying an unsharp filter, which enhances edges and fine details by subtracting a blurred version of the image from the original.\nGaussian Blurred Image ‚Äî generated using a Gaussian filter, which smooths the image by reducing high-frequency details, resulting in a softer appearance.\nSobel Filtered Image ‚Äî created using the Sobel operator, which emphasizes edges, particularly strong intensity changes. In the eye image, you can clearly see the contours of the iris, eyelid, and surrounding features.\nThe third row shows the corresponding frequency-domain representations for each filter.\n\nFor the Gaussian blur, you can see that high-frequency components are greatly reduced, as indicated by the darker edges in the spectrum.\nFor the unsharp filter, higher frequencies are more pronounced, showing stronger edge components.\nFor the Sobel filter, the spectrum highlights frequencies associated with sharp transitions in the image, which is why it‚Äôs effective for edge detection.\n\nBy choosing different filters, we can highlight or suppress specific image features ‚Äî whether we want to sharpen details, blur textures, or extract edges for further analysis.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 9", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 9 - Slide29.txt", "file_path": "Lecture 9\\Texts\\Slide29.txt", "content": "That brings us to the end of today‚Äôs lecture.\n\nWe‚Äôve covered quite a lot ‚Äî from discrete convolution and zero-padding, to spectral analysis, bin refinement, and practical applications in two-dimensional image filtering. I hope you now have a clearer picture of how these concepts connect and how they are applied in real-world signal and image processing.\n\nTake some time to review the examples and try the MATLAB code on your own. Seeing the results firsthand will help solidify your understanding.\nThank you for your attention, and I look forward to seeing you in the next lecture.", "total_slides_in_lecture": 29}
{"lecture": "Lecture 10", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 10 - Slide1.txt", "file_path": "Lecture 10\\Texts\\Slide1.txt", "content": "Good afternoon, everyone. Today, we begin Lecture 10, which focuses on networks.\u000b\nBefore we dive into the details, let me remind you that this topic builds directly on what we‚Äôve covered in linear systems and Fourier analysis. If you haven‚Äôt reviewed those concepts recently, now would be a great time to revisit them, because they‚Äôll help you see the deeper connections between the theory and the applications we‚Äôll explore today.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 10 - Slide2.txt", "file_path": "Lecture 10\\Texts\\Slide2.txt", "content": "We are right on schedule in our journey through this material. So let's start.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 10 - Slide3.txt", "file_path": "Lecture 10\\Texts\\Slide3.txt", "content": "Before we dive into networks, let‚Äôs recall the two related ideas we‚Äôve already discussed ‚Äî functions and systems.\u000b\nA function is a mathematical concept. It takes an input, processes it according to some rule, and produces an output. You can think of it in terms of a domain and a range.\u000bA system is essentially the same idea, but we often use this term in engineering. We talk about inputs and outputs, and the system could be simple or complex.\n\nNow, a network is really just a more complex kind of system ‚Äî one made up of many interconnected subsystems. The internet is a perfect example: millions of computers linked together, exchanging information. From a mathematical perspective, you can think of a network as a very complicated function, often involving many variables and composite functions linked together.\n\nIn electrical engineering, we often study electrical networks, where components like resistors, capacitors, and inductors are connected to form a circuit. In modern artificial intelligence, we have neural networks, which connect many artificial neurons to process information ‚Äî in some ways, similar to how biological neurons communicate through electrical signals in our nervous system.\n\nToday, we‚Äôll start with the more classic engineering example: the electrical network.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 10 - Slide4.txt", "file_path": "Lecture 10\\Texts\\Slide4.txt", "content": "Let‚Äôs start with the most basic electrical component ‚Äî the resistor.\u000b\nYou already know Ohm‚Äôs Law, which tells us that the voltage across a resistor equals the current flowing through it multiplied by its resistance. In words,\u000bv equals i times R.\nWe can also rearrange this to say that the current i equals the voltage v divided by the resistance R.\u000bThis means the current is directly proportional to the applied voltage and inversely proportional to the resistance.\nThe name resistor comes from its function ‚Äî it resists the flow of electric current. In other words, it limits how many electrons can pass through it for a given voltage.\n\nNext, we‚Äôll move on to another fundamental component ‚Äî the capacitor.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 10 - Slide5.txt", "file_path": "Lecture 10\\Texts\\Slide5.txt", "content": "Here we have the capacitor ‚Äî another fundamental circuit component.\u000b\nWhen we connect a voltage source, such as a battery, across the capacitor, electrons are pushed toward one plate, creating a buildup of negative charge. On the opposite plate, a positive charge accumulates. Importantly, the capacitor itself doesn‚Äôt let electrons pass directly through; instead, the charges stay separated by an insulating material called the dielectric.\nAs more charge accumulates, an opposing electric field develops across the plates. Eventually, this field balances the applied voltage, and the current flow stops.\n\nMathematically, the relationship between voltage and current for a capacitor is given by:\u000bI equals C times the derivative of v with respect to time.\u000bIn words, the current is proportional to how quickly the voltage changes over time.\u000bWe can also write it in integral form:\u000bv equals one over C times the integral of i with respect to time, plus the initial voltage.\n\nThis is different from a resistor, where voltage and current are simply proportional. For a capacitor, the key is the rate of voltage change ‚Äî no change in voltage means no current flows.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 10 - Slide6.txt", "file_path": "Lecture 10\\Texts\\Slide6.txt", "content": "Now, let‚Äôs look at the inductor. Its voltage‚Äìcurrent relationship is similar in form to the capacitor‚Äôs, but with the roles of voltage and current reversed.\n\nFor an inductor, the voltage v is equal to the inductance L multiplied by the rate of change of current ‚Äî in other words,\u000bv equals L times the derivative of i with respect to time.\n\nWe can also express this in integral form:\u000bI equals one over L times the integral of v with respect to time, plus the initial current.\nSo, for a capacitor, current depends on how quickly voltage changes. For an inductor, voltage depends on how quickly current changes. Capacitors store energy in an electric field, while inductors store energy in a magnetic field.\n\nThese three components ‚Äî resistor, capacitor, and inductor ‚Äî are the building blocks of classic electrical circuits. Understanding their voltage‚Äìcurrent relationships is essential before we move on to phasors, which will make analyzing AC circuits much easier.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 10 - Slide7.txt", "file_path": "Lecture 10\\Texts\\Slide7.txt", "content": "Let‚Äôs take a moment to revisit complex numbers, because they form the foundation for understanding phasors.\n\nA complex number has two parts ‚Äî a real part, x, and an imaginary part, y. We can represent this on a complex plane, with the horizontal axis for the real part and the vertical axis for the imaginary part. This is like a Cartesian coordinate system.\nBut there‚Äôs another way to represent the same point ‚Äî using polar coordinates. Instead of x y, we describe it by its distance from the origin, called the magnitude r, and its angle from the real axis, called the phase angle phi.\n\nMathematically, we can write this as r times e to the power i phi. Here, e to the power i phi comes from Euler‚Äôs formula, which says e to the i phi equals cosine phi plus i times sine phi. Multiplying r by those components gives us back x y.\n\nWhen we analyze sinusoidal signals, like in AC circuits, we often know the frequency in advance. In that case, the important quantities are the magnitude r, which tells us the signal‚Äôs strength, and the phase angle phi, which tells us where the waveform starts in time.\nInstead of writing out the full exponential, we can use the simpler phasor notation: r angle phi. For example, if we know an AC current has a magnitude of 5 amperes and a phase angle of 30 degrees, we can write it as ‚Äú5 at angle 30 degrees.‚Äù\n\nThis phasor approach is a very convenient way to describe sinusoidal signals, and as we‚Äôll see, it makes circuit analysis much simpler.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 10 - Slide8.txt", "file_path": "Lecture 10\\Texts\\Slide8.txt", "content": "Let‚Äôs take a closer look at the imaginary unit i and understand it from a geometric perspective using Euler‚Äôs formula:\u000be to the i theta equals cosine theta plus i times sine theta.\n\nIf we set theta to 90 degrees, e to the i 90¬∞ gives us i. You can think of multiplying by i as rotating a vector by 90 degrees in the complex plane. For example, if we start with the unit vector at 1 on the real axis and multiply by i, it rotates to point straight up along the imaginary axis.\n\nIf we multiply by i again ‚Äî another 90-degree rotation ‚Äî we end up at negative 1 on the real axis. In other words, i times i equals ‚Äì1. That‚Äôs the origin of the idea that i is the square root of negative one.\n\nMore generally, multiplying by e to the i theta rotates a vector by an angle theta, while multiplying by a real number r simply scales its length. This gives a very natural, visual way to think about complex multiplication ‚Äî scaling changes the magnitude, and the exponential term changes the direction.\n\nAddition is also straightforward: if you have two complex numbers, u and v, you add their real parts and their imaginary parts separately. Graphically, that‚Äôs just adding two vectors tip-to-tail to get u plus v.\nWith these rules for addition and multiplication, we have a complete algebraic system for complex numbers. And this system is extremely powerful for representing sinusoidal waves ‚Äî the building blocks of Fourier analysis ‚Äî as well as for solving wave equations in fields like ultrasound and electromagnetics.\n\nThis is why the phase representation, using complex numbers, is such a fundamental tool in engineering and physics.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 10 - Slide9.txt", "file_path": "Lecture 10\\Texts\\Slide9.txt", "content": "Now let‚Äôs see how phase, or phasor, representation applies directly to a capacitor‚Äôs voltage‚Äìcurrent relationship.\nIn basic DC circuits, Ohm‚Äôs Law tells us that voltage equals current times resistance. But for a capacitor, we normally describe the relationship in terms of a derivative or an integral ‚Äî for example, i equals C times dv over dt.\n\nHowever, when we work in the phasor domain, something elegant happens. The derivative operation in the time domain becomes a simple multiplication by j omega in the frequency domain. This means that, for sinusoidal signals, the voltage‚Äìcurrent relationship for a capacitor can look just like Ohm‚Äôs Law ‚Äî except that instead of a real resistance, we have a complex quantity called impedance.\nHere‚Äôs the key point: in any linear circuit made up of resistors, capacitors, and inductors, if we drive it with a pure sinusoidal signal, the output will also be a sinusoid at the exact same frequency. The amplitude may change, and the waveform may shift in phase, but the frequency remains unchanged.\n\nFor example, suppose the voltage across a capacitor is A cosine omega t. Differentiating this to find the current shifts the waveform by 90 degrees and multiplies its amplitude by C omega. In phasor notation, this phase shift is written as ‚Äúat an angle of + 90 degrees.‚Äù\nFinally, in the phasor domain, the capacitor‚Äôs impedance is 1 over j omega C. That‚Äôs directly analogous to resistance in Ohm‚Äôs Law ‚Äî except here it‚Äôs a complex value, reflecting the fact that voltage and current are out of phase.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 10 - Slide10.txt", "file_path": "Lecture 10\\Texts\\Slide10.txt", "content": "For an inductor, the process is very similar to what we saw with a capacitor, but here it‚Äôs the current that we start with.\n\nSuppose the current is sinusoidal: B cosine omega t. The voltage across the inductor is given by v equals L times di/dt. Differentiating the current shifts the waveform by 90 degrees and changes the amplitude by a factor of L omega.\nIn phasor notation, if the current is B at angle 0 degrees, then the voltage becomes L omega B at angle 90 degrees. That phase shift means the voltage leads the current by 90 degrees in an inductor ‚Äî the exact opposite of a capacitor, where the current leads the voltage by 90 degrees.\n\nIn the phasor domain, this relationship is written simply as V equals j omega L times I. Here, j omega L plays the same role for an inductor that resistance R plays for a resistor ‚Äî it‚Äôs the impedance. The only difference is that it‚Äôs a complex quantity, reflecting the phase relationship between voltage and current.\n\nSo once again, phasor notation allows us to generalize Ohm‚Äôs Law to all three basic components ‚Äî resistor, capacitor, and inductor ‚Äî using the single idea of impedance.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 10 - Slide11.txt", "file_path": "Lecture 10\\Texts\\Slide11.txt", "content": "Up to now, we‚Äôve talked about resistors, capacitors, and inductors separately. Each has its own voltage‚Äìcurrent relationship, and each can be expressed in the phasor domain.\n\nInstead of calling all these effects ‚Äúresistance,‚Äù we use a broader term ‚Äî impedance. Impedance describes how any circuit element opposes the flow of alternating current, and it applies to all three components.\nFor a resistor, impedance is simply R.\u000bFor a capacitor, impedance is 1 over j omega C.\u000bFor an inductor, impedance is j omega L.\nIn each case, impedance is defined as voltage divided by current ‚Äî Z equals V over I. This is the same structure as Ohm‚Äôs Law, but with Z taking the place of R.\n\nWhen we combine components in a circuit, the total impedance depends on whether they are connected in series or in parallel. Later, we‚Äôll learn how to calculate equivalent impedance for these different configurations. But the important takeaway here is that impedance unifies the behavior of resistors, capacitors, and inductors into one general concept, making AC circuit analysis much simpler.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 10 - Slide12.txt", "file_path": "Lecture 10\\Texts\\Slide12.txt", "content": "Let‚Äôs now step back and look at a general electrical network. I‚Äôll use this simple example to introduce three important terms: node, branch, and loop.\n\nA node is a point where two or more circuit components meet. These components could be resistors, capacitors, inductors, or sources like batteries and current generators. In our diagram, each place where elements connect is a node.\nA branch is a single circuit element between two nodes. You can think of it as one ‚Äúarm‚Äù of the circuit, carrying current from one node to another.\nA loop is a closed path that goes through a sequence of branches and nodes without passing through any branch or node more than once. In other words, it‚Äôs a single, non-repeating path that starts and ends at the same point.\n\nUsing these definitions, you might ask: in this diagram, how many nodes are there? And how many loops? This kind of reasoning will become important when we apply Kirchhoff‚Äôs laws for circuit analysis.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 10 - Slide13.txt", "file_path": "Lecture 10\\Texts\\Slide13.txt", "content": "So, how many nodes are in this circuit? Let‚Äôs apply the definition.\n\nA node is any point where two or more branches connect. Here, all the points along this conducting path are electrically the same ‚Äî the wires are ideal conductors with zero resistance ‚Äî so we treat them as one single node. This first node connects resistor R 1, resistor R 2, resistor R 3, and the voltage source.\n\nThe other connection point, down here, is the second node. It ties together the lower ends of R 2 and R 3, the negative terminal of the voltage source, and the current source.\nSo even though there are multiple connection spots physically drawn, electrically, we have just two distinct nodes in this circuit.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 10 - Slide14.txt", "file_path": "Lecture 10\\Texts\\Slide14.txt", "content": "Now let‚Äôs talk about loops ‚Äî and more specifically, independent loops.\n\nA loop is a closed path in the circuit that passes through a sequence of branches and returns to the starting point without crossing any branch more than once. But not all loops are equally useful in analysis. We focus on independent loops, which each contain at least one branch that is not part of any other loop in the set.\n\nIn this example, we have three independent loops:\nLoop 1 includes the voltage source, resistor R 1, and resistor R 2.\nLoop 2 includes resistor R 2 and resistor R 3.\nLoop 3 includes resistor R 3 and the current source.\nYou might notice that other closed paths exist ‚Äî for example, one that goes all the way around the outside ‚Äî but these can be derived from combinations of the independent loops. They don‚Äôt give us any new information.\n\nThe reason this concept is important is that each independent loop gives us one equation when we apply Kirchhoff‚Äôs Voltage Law. These equations are the building blocks for solving circuit problems with multiple unknowns, such as the current through each branch or the voltage across each resistor.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 10 - Slide15.txt", "file_path": "Lecture 10\\Texts\\Slide15.txt", "content": "The first rule we use in circuit analysis is Kirchhoff‚Äôs Current Law, or KCL. It‚Äôs very straightforward: for any node in a circuit, the total current flowing into the node must equal the total current flowing out.\n\nThink of the node as a junction ‚Äî electrons flow in from some branches and out through others. They don‚Äôt pile up or vanish at the node. If two amperes flow in from one branch and three amperes flow in from another, then a total of five amperes must flow out through the remaining branch.\nYou can picture this like people moving through a doorway: the number of people entering must match the number leaving, assuming no one stays inside the doorway. Or like cars passing through an intersection: cars can‚Äôt just appear or disappear; what comes in must go out.\n\nKCL is essentially a statement of conservation of charge ‚Äî charge is neither created nor destroyed at a node. This simple but powerful rule will be one of the main tools we use to solve circuit equations.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 10 - Slide16.txt", "file_path": "Lecture 10\\Texts\\Slide16.txt", "content": "The second key rule is Kirchhoff‚Äôs Voltage Law, or KVL. It states that if you travel around any closed loop in a circuit, the sum of all the voltage rises and drops must equal zero.\n\nYou can think of it like taking a hike around a loop trail that starts and ends at the same place. Along the way, you might climb uphill ‚Äî that‚Äôs like a voltage rise from a battery ‚Äî and you might walk downhill ‚Äî that‚Äôs like a voltage drop across a resistor. By the time you return to where you started, your total elevation change is zero.\nIn a circuit, it‚Äôs the same idea but with electrical potential instead of gravitational potential. As electrons move through a voltage source, they gain energy; as they pass through resistive elements, they lose energy. Around a complete loop, the total gains and losses cancel out exactly ‚Äî a direct consequence of energy conservation.\n\nIn this example, the voltage source V 1 is 10 volts. As we move around the loop, we see drops of 2 volts, 3 volts, and 1 volt across different resistors, and zero volts across the ideal conducting wire. That leaves one resistor with an unknown drop. To satisfy KVL, it must drop 4 volts so that the algebraic sum is zero.\nWhen we combine KVL with Kirchhoff‚Äôs Current Law, we can write enough equations to solve for all the unknown currents and voltages in even a complex electrical network.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 10 - Slide17.txt", "file_path": "Lecture 10\\Texts\\Slide17.txt", "content": "If you start with the simplest case ‚Äî a single loop ‚Äî the problem is straightforward. You can solve it directly, just like using Ohm‚Äôs Law: voltage equals impedance times current.\n\nAs you make the network more complex, you can add loops, nodes, or both. Each time you add a loop, you introduce a new unknown, but KVL gives you one more independent equation to match it. Each time you add a node, you also introduce a new unknown; however, KCL provides the additional equation needed.\n\nIf you add both a node and a loop at the same time, you introduce two unknowns ‚Äî for example, splitting a branch into two paths so currents can differ ‚Äî but you also gain two equations: one from KCL at the new node and one from KVL around the new loop.\nThe key point is that no matter how you expand the network, KCL and KVL together give you exactly the number of independent equations you need to solve for all the unknown currents and voltages. This balance is what ensures that a well-defined electrical network always has a unique solution.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 10 - Slide18.txt", "file_path": "Lecture 10\\Texts\\Slide18.txt", "content": "Let‚Äôs solve this example step by step.\n\nWe have two voltage sources and two loops. Our goal is to find three unknown currents:\nI 1, which flows through the left loop,\nI 2, which flows through the right loop,\nand I‚ÇÉ, which flows along the bottom branch.\nFirst, we apply Kirchhoff‚Äôs Current Law at node c. In words, the current I 1 flowing into the node plus the current I 2 flowing into the node must equal the current I‚ÇÉ flowing out. That‚Äôs our first equation:\u000b‚ÄúI one plus I two equals I three.‚Äù\n\nNext, we apply Kirchhoff‚Äôs Voltage Law to the green loop, moving clockwise through points b, e, f, c, and back to b. As we go around the loop:\npassing through the fourteen-volt battery in the opposite direction gives a minus fourteen volts,\npassing through the ten-volt battery in the opposite direction gives minus ten volts,\npassing through the six-ohm resistor with current I 1 gives plus six times I 1,\nand passing through the four-ohm resistor with current I 2 gives minus four times I 2.\n\nAdding them all up and setting the total to zero, we have:\u000b‚Äúnegative fourteen, minus ten, plus six times I one, minus four times I two, equals zero.‚Äù\nFinally, we apply Kirchhoff‚Äôs Voltage Law to the blue loop, moving through points a, b, c, d, and back to a. This gives:\u000b‚Äúten, minus six times I one, minus two times I three, equals zero.‚Äù\n\nNow we have three equations:\nI one plus I two equals I three.\nNegative fourteen minus ten plus six I one minus four I two equals zero.\nTen minus six I one minus two I three equals zero.\n\nSolving these, we find:\u000bI one equals two amperes,\u000bI two equals negative three amperes ‚Äî meaning it actually flows in the opposite direction we assumed ‚Äî\u000band I three equals negative one ampere.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 10 - Slide19.txt", "file_path": "Lecture 10\\Texts\\Slide19.txt", "content": "This slide is a summary that generalizes Ohm‚Äôs Law for alternating current circuits.\nIn direct current circuits, we know the familiar form: ‚Äúvoltage equals resistance times current.‚Äù In alternating current circuits, thanks to phasor notation, we can keep the same simple structure ‚Äî voltage equals impedance times current ‚Äî but now impedance can be a complex number, depending on the type of component.\n\nFor a resistor, impedance is simply R, and voltage and current are in phase ‚Äî their peaks occur simultaneously.\nFor a capacitor, the impedance is ‚Äúone divided by j omega C,‚Äù which in polar form is ‚Äúone over omega C at an angle of negative ninety degrees.‚Äù This means the voltage lags the current by ninety degrees. In other words, the current reaches its peak before the voltage does.\n\nFor an inductor, the impedance is ‚Äúj omega L,‚Äù or in polar form ‚Äúomega L at an angle of positive ninety degrees.‚Äù This means the voltage leads the current by ninety degrees ‚Äî the voltage peaks first, and the current builds up afterward.\nIf you think about it intuitively:\nAn inductor resists sudden changes in current, so when you first apply voltage, the current starts small and gradually increases.\nA capacitor resists changes in voltage, so when you first apply current, the voltage takes time to build.\n\nMathematically, we derive these results from the voltage‚Äìcurrent relationships we saw earlier, but phasor notation makes them look just like Ohm‚Äôs Law ‚Äî only with complex impedance replacing simple resistance.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 10 - Slide20.txt", "file_path": "Lecture 10\\Texts\\Slide20.txt", "content": "Let‚Äôs learn how to replace two impedances with a single equivalent impedance.\n\nSeries connection ‚Äî one after the other on the same branch:\u000bThe same current flows through both, and the voltage drops add.\u000b\nSo the equivalent is:\u000b‚ÄúZ sub e equals Z one plus Z two.‚Äù\u000bThis works even when the impedances are complex.\nA one-line proof, spoken: assume a current I flows.\u000bThe total voltage is I times Z sub e.\u000bIt is also I times Z one plus I times Z two.\u000bSet them equal and cancel I: ‚ÄúZ sub e equals Z one plus Z two.‚Äù\nParallel connection ‚Äî the two elements share the same two nodes:\u000bThe voltage across each is the same, and the currents add.\u000b\nSo the equivalent is:\u000b‚Äúone over Z sub e equals one over Z one plus one over Z two,‚Äù\u000bor, equivalently,\u000b‚ÄúZ sub e equals Z one times Z two, divided by Z one plus Z two.‚Äù\nQuick proof, spoken: let the common voltage be V.\u000bCurrents are I 1 = V over Z 1 and I 2 = V over Z 2.\u000bTotal current I = I 1 plus I 2 = V times (1 over Z 1 plus 1 over Z 2).\u000bBut I = V over Z sub e.\u000bCancel V to get the reciprocal form above.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 10 - Slide21.txt", "file_path": "Lecture 10\\Texts\\Slide21.txt", "content": "Once we know how to combine impedances into a single equivalent value, we can simplify any branch of a network. Each branch can then be treated as just two things: an impedance and a source ‚Äî either a voltage source, a current source, or both. If there are more components in the branch, we simply combine them into one equivalent impedance before solving.\n\nAfter that, solving the network becomes a two-step process:\u000bStep one: find the overall currents and voltages using the simplified equivalent impedances.\u000bStep two, if needed, break the equivalent impedance back into its original components and distribute the results to each part.\n\nNow let‚Äôs apply this to the idea of a voltage divider. In the circuit on the left, we have an input voltage split between a resistor and a capacitor. The fraction of the input voltage that appears across the capacitor is given by the transfer function:\n‚ÄúCapital T of omega equals one divided by j omega C, all over R plus one divided by j omega C.‚Äù\nIf we simplify, that becomes:\u000b‚ÄúT of omega equals one over one plus j times omega over omega sub c,‚Äù\u000bwhere omega sub c equals one over R times C.\n\nThis formula tells us how the circuit responds to different frequencies. If the input is a pure sinusoidal wave, we just plug its frequency into omega and get the scaling factor for the output.\nAnd if the input is an arbitrary waveform, Fourier analysis lets us break it into many sinusoidal components, each with its own frequency. \n\nWe apply the transfer function to each frequency separately, then sum them back up to get the total output.\nThis particular circuit is called a low-pass filter because it passes low-frequency signals with little attenuation, but strongly reduces high-frequency components. At very low frequencies, omega is close to zero, so the transfer function is close to one ‚Äî the output nearly matches the input. At very high frequencies, omega is large, the transfer function becomes very small, and the output is greatly reduced.\n\nThe inductor‚Äìresistor version on the right works the same way, except its cutoff frequency omega sub c equals R divided by L.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 10 - Slide22.txt", "file_path": "Lecture 10\\Texts\\Slide22.txt", "content": "When we move into the Fourier domain, any signal can be represented as a collection of frequency components. Filters are circuits or systems that selectively pass or block these components based on their frequency.\n\nA low-pass filter allows low-frequency components to pass through while attenuating or reducing high-frequency components. In the idealized diagram here, the passband is shown as a flat rectangle up to the cutoff frequency, omega sub c, and then it drops to zero in the stopband. In real circuits, the transition is not perfectly sharp ‚Äî it‚Äôs more gradual ‚Äî but the concept is the same. At the cutoff frequency, the signal power has dropped to half its low-frequency value.\nA high-pass filter does the opposite: it blocks low-frequency signals and passes the high-frequency ones.\nA band-pass filter only allows frequencies within a certain range to pass, blocking both lower and higher frequencies.\nA band-stop filter, sometimes called a notch filter, removes frequencies within a specific range but passes those outside that range.\n\nAll of these can be analyzed using the same approach:\nDecompose the input signal into sinusoidal components using Fourier analysis.\nFor each frequency, calculate the system‚Äôs response using impedance and the phasor form of Ohm‚Äôs Law: voltage equals impedance times current.\n\nCombine the individual frequency responses to reconstruct the total output.\nFor DC circuits with constant voltages, we only need algebraic equations ‚Äî just resistance times current equals voltage. But with AC signals or sinusoidal sources, the relationships involve derivatives and integrals. Phasor notation transforms those into simple algebraic equations, letting us solve AC problems with the same ease as DC problems.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 10 - Slide23.txt", "file_path": "Lecture 10\\Texts\\Slide23.txt", "content": "In this sense, an alternating current circuit can be treated just like a direct current circuit ‚Äî once we use phasor notation and impedance. That‚Äôs why we say AC is DC in disguise.\n\nBy representing AC voltages and currents as phasors, we turn differential equations into simple algebraic ones, just like in DC analysis. This dramatically simplifies the process of solving AC circuit problems.\nThis clever idea was first developed decades ago by an engineer at the GE Global Research Center, not far from here. It‚Äôs now a standard computational tool in electrical engineering, and it‚Äôs one of the reasons AC circuit analysis can be so elegant.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 10 - Slide24.txt", "file_path": "Lecture 10\\Texts\\Slide24.txt", "content": "Here‚Äôs a very typical and practical circuit ‚Äî the voltage divider.\nWe start with an input voltage, which is split between two resistors or impedances, R 1 and R 2. The voltage drop across the second resistor becomes our output signal. This is a common way to pass a signal from one stage of a system to the next.\n\nA real-world example might be a sensor connected to a measurement device. The human body, for instance, has its own electrical impedance. If we were measuring a cardiac signal, we could model the body‚Äôs impedance as R 1. The second resistor, R 2, could represent the input impedance of our measuring instrument.\n\nFrom an engineering perspective, we usually have three goals when designing such a circuit:\nLarge input resistance ‚Äî We want R1 to be large so that most of the input voltage appears across it, ensuring we capture as much of the signal as possible.\nSmall output resistance ‚Äî We want R2 to be small so the next stage in the circuit can take the signal without losing much voltage.\nHigh gain ‚Äî If possible, we want to amplify the signal to make it easier to detect and process.\n\nThese goals can sometimes conflict ‚Äî for example, making R 1 large helps with capturing the signal, but we also want R 2 small for driving the next stage. Circuit design is about balancing these trade-offs.\nIn the example at the bottom right, R S is our sensor, whose resistance changes slightly by a factor of x in response to something like temperature or pressure. R_L is the load resistor. If x equals zero, the sensor is at its nominal value. When x changes, even by a small amount, the output voltage V out changes.\n\nMathematically, V out equals V in multiplied by the ratio R 2 over R 1 plus R 2. This is just the voltage divider rule we‚Äôve seen before. In practice, the change in x might be very small, so the change in V out is tiny ‚Äî often just a few millivolts. If we want to amplify this tiny change, we ideally remove the constant background voltage so that only the small variation gets amplified. This allows for much higher gain without overloading the system.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 10 - Slide25.txt", "file_path": "Lecture 10\\Texts\\Slide25.txt", "content": "One way to remove the background signal and make small changes easier to detect is by using a Wheatstone bridge.\n\nThe Wheatstone bridge is essentially two voltage dividers connected side by side. On the left divider, we have resistors R 1 and R 2; on the right divider, we have resistors R 3 and R4. The output voltage, V_out, is measured between the midpoints of these two dividers.\nIf the ratios of the resistors are chosen so that R 1 over R 2 equals R 4 over R 3, the two dividers produce exactly the same voltage at their midpoints, and V_out is zero. This is called the null mode ‚Äî no signal at the output when the bridge is balanced.\n\nNow, if one of the resistors ‚Äî typically R 3 ‚Äî is actually a sensor whose resistance changes slightly, say from R knot to R knot times (1 plus x), the bridge becomes unbalanced. This small change in R 3 produces a nonzero V out, even though the original background voltage was canceled out.\n\nIn deflection mode, we can write V out as the supply voltage V c c multiplied by the difference between the left divider ratio and the right divider ratio. Because the bridge starts balanced, even a very small change in R 3 creates a measurable output.\nThe advantage is that we‚Äôve removed the large constant voltage and isolated only the change we care about. This makes it possible to apply high gain to that small signal without saturating the system.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 10 - Slide26.txt", "file_path": "Lecture 10\\Texts\\Slide26.txt", "content": "Up to now, we‚Äôve looked at components that behave in a linear way ‚Äî resistors, capacitors, and inductors ‚Äî all of which follow the generalized Ohm‚Äôs Law, voltage equals impedance times current.\n\nBut many real-world circuit elements are nonlinear components. Their voltage‚Äìcurrent relationship does not form a straight line.\nFor example, a diode only allows significant current to flow in one direction once the forward voltage reaches a certain threshold ‚Äî about 0.7 volts for silicon and 0.3 volts for germanium. In reverse bias, very little current flows until breakdown occurs.\nTransistors, such as the NPN bipolar junction transistor shown here, can amplify signals. By carefully biasing the base‚Äìemitter junction, a small change in base current controls a much larger change in collector current. This allows weak input signals to be amplified into stronger output signals.\n\nWe also have digital logic gates ‚Äî OR, AND, XOR, NOR, NAND, and NOT ‚Äî which are the building blocks of digital electronics. They process binary signals, producing specific outputs based on logical rules. By combining these gates, we can create flip-flops and sequential circuits, which store and manipulate digital information.\n\nNonlinear components make it possible to build amplifiers, oscillators, logic circuits, and microprocessors. They open the door to modern electronics ‚Äî where linear analysis still plays a role, but must be combined with device-specific nonlinear behavior.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 10 - Slide27.txt", "file_path": "Lecture 10\\Texts\\Slide27.txt", "content": "Just as we can have linear systems and nonlinear systems, we also have nonlinear electronic components.\n\nOne of the most fundamental is the diode. A diode has a strong directional preference for current flow. In its forward direction, the current rises rapidly after a certain threshold voltage ‚Äî about 0.7 volts for silicon. In the reverse direction, the current is extremely small until breakdown occurs. This is a clearly nonlinear voltage‚Äìcurrent relationship.\n\nIf we combine two diodes in a specific configuration, we create a transistor. A transistor takes a small signal at its input and produces a much larger version of the same signal shape at its output ‚Äî it acts as an amplifier. This ability to magnify weak signals makes the transistor an essential building block for electronics.\n\nMultiple transistors can be combined to perform logic operations. For example, an AND gate outputs a high voltage only if both inputs are high. An OR gate outputs a high voltage if either input is high. More complex gates, like NAND, NOR, and XOR, can be built from these basic ideas.\nBy connecting logic gates, we can create circuits that add, multiply, store data, and make decisions ‚Äî the very basis of digital computers.\n\nThis is why in electronics we need to understand both linear and nonlinear systems. Linear components handle predictable analog relationships, while nonlinear components give us amplification, switching, and the foundation for all digital processing.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 10 - Slide28.txt", "file_path": "Lecture 10\\Texts\\Slide28.txt", "content": "As we just discussed, we often want three things in a circuit:\nvery high input resistance,\nvery low output resistance,\nand a large gain or magnification factor.\nThe operational amplifier ‚Äî or op amp ‚Äî delivers exactly that.\n\nInside an op amp are many transistors and other components, all arranged to meet these requirements. The internal circuitry can look complicated, but for analysis, we can use a simple model:\nWe apply a small input voltage, V in, between the non-inverting and inverting inputs. The op amp multiplies this by a large gain factor, G, to produce the output voltage, V out equals G times V in.\nIn the ideal model:\nThe gain is infinitely large.\n\nThe input impedance is infinite, so the op amp doesn‚Äôt draw current from the source ‚Äî we capture the full signal.\nThe output impedance is zero, so the op amp can drive the next stage without loss.\nReal op amps don‚Äôt reach these ideal values, but they can come close ‚Äî for example, input impedance in the megaohm to teraohm range, and output impedance as low as a few hundred ohms.\n\nAn op amp is an active component ‚Äî it needs its own power supply, typically labeled V S plus and V S minus. This internal power lets it amplify signals, something passive components like resistors, capacitors, and inductors cannot do on their own.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 10 - Slide29.txt", "file_path": "Lecture 10\\Texts\\Slide29.txt", "content": "Even though an operational amplifier is an active component, the basic circuit laws still apply ‚Äî Kirchhoff‚Äôs Current Law and Kirchhoff‚Äôs Voltage Law always hold.\n\nBut for op amps, we have two special rules that make analysis much easier:\nRule one: The two input terminals are at the same voltage.\u000bThis happens because the op amp‚Äôs gain is extremely large. Even the tiniest difference between the inputs is amplified so much that the output adjusts to make the two inputs essentially equal in voltage.\nRule two: No current flows into either input terminal.\u000bThe input impedance of an op amp is extremely high ‚Äî ideally infinite ‚Äî so no measurable current enters the inputs.\n\nThese two rules are the foundation for almost all op amp circuit analysis. They let us treat the inputs as if they are at the same voltage and as if they draw no current, which simplifies solving even complex amplifier circuits.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 30, "slide_filename": "Slide30.txt", "slide_annotation": "Lecture 10 - Slide30.txt", "file_path": "Lecture 10\\Texts\\Slide30.txt", "content": "Let‚Äôs look at this inverting amplifier and walk through the analysis step-by-step using our two op amp rules.\n\nFirst, the non-inverting input is connected to ground.\u000bBy rule one, the inverting input is also at zero volts ‚Äî that‚Äôs our virtual ground.\nBy rule two, no current flows into the op amp input.\u000bThat means all the current coming through the input resistor R-i from the input voltage must flow through the feedback resistor R-f to the output.\n\nNow, the current through R-i equals the input voltage minus zero, divided by R-i.\u000bThe current through R-f equals zero minus the output voltage, divided by R-f.\n\nSince these currents are the same, we can say:\u000bInput voltage divided by R-i equals negative output voltage divided by R-f.\nIf we multiply both sides by R-f, we get:\u000bOutput voltage equals negative R-f over R-i, times the input voltage.\nThe negative sign means the output is inverted compared to the input.\u000bThe size of the gain is the value of R-f divided by the value of R-i.\n\nSo, with just those two op amp rules, we can find the input‚Äìoutput relationship without worrying about the op amp‚Äôs internal complexity.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 31, "slide_filename": "Slide31.txt", "slide_annotation": "Lecture 10 - Slide31.txt", "file_path": "Lecture 10\\Texts\\Slide31.txt", "content": "Operational amplifiers can be connected in many different configurations, each producing a specific function.\n\nHere you see several common modules:\nA non-inverting amplifier, where the gain equals one plus the ratio of R2 over R1.\nAn inverting amplifier, where the gain equals negative R2 over R1.\nA differentiator, which produces an output proportional to the rate of change of the input signal.\nAn integrator, which produces an output proportional to the time integral of the input signal.\nA differential amplifier, which amplifies the difference between two input signals.\nAll of these can be analyzed using the same two op amp rules we discussed earlier: the inputs are at the same voltage, and no current flows into the input terminals.\n\nWhile the details of each configuration can get more involved, for our purposes, it‚Äôs enough to understand the fundamental idea: by choosing how we connect resistors, capacitors, and feedback paths, we can make the op amp perform a wide range of analog signal processing tasks.\n\nIf you‚Äôre interested in the full derivations and formulas, the linked reference has complete explanations ‚Äî but for this class, understanding the basic concepts is sufficient.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 32, "slide_filename": "Slide32.txt", "slide_annotation": "Lecture 10 - Slide32.txt", "file_path": "Lecture 10\\Texts\\Slide32.txt", "content": "We can also use operational amplifiers in measurement setups, particularly for impedance matching.\n\nLooking into the input port of an op amp, the impedance is very high. This means it can capture a signal from a source without significantly loading it. On the other hand, when we take the signal from the output port, the impedance is very low. This allows the signal to drive the next stage efficiently. In this way, the op amp satisfies the two opposing requirements we discussed earlier: high input impedance and low output impedance.\n\nThis principle applies to both DC and AC circuit analysis. Remember the voltage‚Äìcurrent relationships for resistors, capacitors, and inductors. When you connect these components into a network, Kirchhoff‚Äôs Voltage Law and Kirchhoff‚Äôs Current Law give you enough equations to solve for all unknowns.\n\nThe beauty of phasor notation is that for AC steady-state circuits, you don‚Äôt need to solve differential equations. You simply work with algebraic equations, just as in DC analysis. This is why, in a sense, we can say ‚ÄúAC is DC in disguise‚Äù ‚Äî at least for steady-state analysis.\nOf course, if you‚Äôre dealing with transients ‚Äî for example, when a signal is suddenly applied ‚Äî you would still need to solve the time-domain differential equations. But once the system reaches steady state, algebraic methods are enough.\n\nThese circuit concepts are applied in many areas ‚Äî from sensing signals in medical imaging to filtering unwanted noise. In fact, we‚Äôve already seen examples where Fourier analysis helps us understand how circuits pass or block certain frequencies. This is crucial for designing data acquisition systems that can detect and process signals effectively.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 33, "slide_filename": "Slide33.txt", "slide_annotation": "Lecture 10 - Slide33.txt", "file_path": "Lecture 10\\Texts\\Slide33.txt", "content": "Let me take you back to an event from several years ago. We organized a symposium at the AAAS Annual Meeting ‚Äî that‚Äôs the American Association for the Advancement of Science. If you‚Äôre familiar with Science magazine, that‚Äôs actually the flagship publication of AAAS.\n\nThis meeting is one of the most important gatherings in the scientific community. It brings together experts from every field ‚Äî medicine, engineering, mathematics, physics, education ‚Äî you name it. The aim is not just to share research, but also to promote broad scientific understanding and collaboration.\nOur symposium was titled X-ray Imaging Innovations for Biomedicine. We discussed advances in CT technology, covering both hardware improvements and algorithmic innovations. We also looked at how imaging can play a more predictive and personalized role in medicine.\n\nThe meeting is structured into many symposiums running in parallel. We had our session, shared our work, exchanged ideas, and learned from other fields. It was a valuable experience ‚Äî not only for presenting our research, but also for gaining perspectives from outside our immediate domain.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 34, "slide_filename": "Slide34.txt", "slide_annotation": "Lecture 10 - Slide34.txt", "file_path": "Lecture 10\\Texts\\Slide34.txt", "content": "The day after our own symposium, I found myself free for the afternoon, so I decided to attend another session titled The Technology of Artificial Intelligence. It was led by Demis Hassabis, the co-founder and CEO of DeepMind ‚Äî one of the world‚Äôs leading AI companies, later acquired by Google.\n\nI listened to several fascinating presentations. The speakers shared breakthroughs in AI research, from neuroscience-inspired architectures to game-playing systems that were pushing the limits of machine learning. The atmosphere was electric ‚Äî you could feel that this was more than just incremental progress.\n\nThat afternoon convinced me of something important: AI had truly entered a revolutionary stage. The technology was no longer just an academic curiosity; it was becoming a powerful tool with transformative potential, ready to impact science, healthcare, and our understanding of the human mind. It was clear that an exciting new era was unfolding.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 35, "slide_filename": "Slide35.txt", "slide_annotation": "Lecture 10 - Slide35.txt", "file_path": "Lecture 10\\Texts\\Slide35.txt", "content": "That experience sparked an idea in my mind. Over the past few centuries, humanity has gone through several major transformations. We began with the Industrial Revolution ‚Äî when machines took over physical labor and reshaped manufacturing, transportation, and the way we lived. Then came the Information Revolution, where computers and the internet allowed us to store, process, and share knowledge at unprecedented speed.\n\nNow, with the rise of machine learning and artificial intelligence, we are entering what I like to call the Intelligence Revolution. This is not just about automating tasks ‚Äî it‚Äôs about creating machines that can learn, adapt, and make decisions, performing tasks that were once thought to be exclusively human.\n\nThis shift will influence every aspect of life ‚Äî from science and medicine to education, communication, and beyond. It‚Äôs a change as profound as the revolutions before it, but one that reaches directly into the realm of thought and reasoning.\n\nWhen we look at the history of science, we can see that it has evolved through distinct paradigms over time. Thousands of years ago, science was primarily empirical. People described natural phenomena based on what they could directly observe ‚Äî carefully recording patterns in nature without necessarily understanding the underlying mechanisms.\n\nA few hundred years ago, the theoretical branch emerged. This was a major leap forward: scientists began building mathematical models and generalizations, allowing them not just to describe the world, but to predict and explain it.\nIn the past few decades, we‚Äôve added a computational branch. With powerful computers, we can now simulate complex phenomena that are impossible to test directly, exploring theories in silico before ever stepping into a laboratory.\n\nAnd today, we have entered a new era ‚Äî the era of data exploration, often called eScience. Here, we unify theory, experiment, and simulation, but our driving force is data itself. Instruments and sensors capture enormous amounts of information, which is then processed by software and stored in vast databases. From there, we can apply data analytics, statistics, and increasingly, machine learning to extract patterns, features, and relationships directly from the data.\n\nThis approach ‚Äî using big data and AI to uncover new knowledge ‚Äî is so different that some call it the ‚Äúfourth paradigm‚Äù of science. It represents a shift in thinking: instead of starting with a theory, we can sometimes let the data speak for itself.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 36, "slide_filename": "Slide36.txt", "slide_annotation": "Lecture 10 - Slide36.txt", "file_path": "Lecture 10\\Texts\\Slide36.txt", "content": "Here, we see the structure of a biological neuron ‚Äî the basic building block of the nervous system.\n\nA neuron has several main parts. The dendrites act like antennas, receiving incoming signals from other neurons. These signals travel toward the cell body, which contains the nucleus and all the essential cellular machinery to keep the neuron alive and functioning.\nFrom the cell body, information is sent down a long projection called the axon. This is essentially the neuron‚Äôs transmission cable, carrying electrical signals ‚Äî known as action potentials ‚Äî toward the axon terminals. Along the way, the signal may be sped up by segments of myelin sheath, which act as insulation.\n\nAt the axon terminals, the neuron communicates with its target cells ‚Äî which could be other neurons, muscle fibers, or glands ‚Äî across small gaps called synapses. Communication here happens chemically, through the release of neurotransmitters, or in some cases, electrically.\n\nThe important thing to remember is that neurons are not just passive wires. They integrate inputs, make decisions about whether to fire, and adapt over time. This biological model is what inspired the design of artificial neural networks in machine learning.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 37, "slide_filename": "Slide37.txt", "slide_annotation": "Lecture 10 - Slide37.txt", "file_path": "Lecture 10\\Texts\\Slide37.txt", "content": "Here is a simplified view of how neurons work. Think of this as the basic flow of information in our nervous system.\n\nIncoming signals ‚Äî whether from the outside world or from other neurons ‚Äî arrive at the dendrites. These dendrites act as the input terminals of the neuron. They collect all incoming information, both excitatory and inhibitory.\nIf the combined strength of these signals is strong enough to cross a certain threshold, the neuron generates an electrical pulse called an action potential. This signal travels down the axon ‚Äî the long fiber you see here ‚Äî and moves toward the axon terminals.\n\nAt the axon terminals, the neuron communicates with the next cell, often through a synapse. There, chemical messengers or direct electrical connections pass the information forward, continuing the chain of processing in the nervous system.\nThis is the fundamental process behind everything from reflexes to complex thought ‚Äî and it‚Äôs also the biological inspiration for how artificial neural networks process information in AI systems.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 38, "slide_filename": "Slide38.txt", "slide_annotation": "Lecture 10 - Slide38.txt", "file_path": "Lecture 10\\Texts\\Slide38.txt", "content": "Now, here is the simplest way to think about it.\n\nAt the top, you see a biological neuron. It receives input through many branches, called dendrites. All of these signals are gathered in the main cell body, the soma. If the combined signal is strong enough, the neuron generates an electrical pulse that travels down the axon. Finally, it reaches the output interface ‚Äî the synapse ‚Äî where it passes the signal on to other neurons.\nBelow that is the mathematical model we use for an artificial neuron. Each input is represented as a variable, and not all inputs carry the same importance, so we assign a weight to each one. Larger weights mean that input has a bigger influence; smaller weights mean less influence.\n\nThe neuron first performs a linear operation ‚Äî a weighted sum of all the inputs, which is essentially an inner product. But that‚Äôs not enough. Just like in biology, small random fluctuations ‚Äî small ‚Äúnudges‚Äù ‚Äî are ignored. Only when the total input crosses a certain threshold does the neuron respond strongly. This is where the nonlinear transformation comes in, producing the actual output.\nSo, in short: first we sum the inputs in a weighted way, then we pass the result through a nonlinear function that decides whether to respond ‚Äî and how strongly.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 39, "slide_filename": "Slide39.txt", "slide_annotation": "Lecture 10 - Slide39.txt", "file_path": "Lecture 10\\Texts\\Slide39.txt", "content": "Now let‚Äôs look at how we represent a neuron in a computer ‚Äî what we call a perceptron.\n\nA perceptron takes multiple inputs, each labeled here as a 1, a 2, a 3,‚Ä¶, a n. Each input has a weight, shown as w 1,w 2,w 3,‚Ä¶,w n‚Äã, which controls how important that input is.\n\nThe first thing we do is multiply each input by its weight. Then we sum them all together ‚Äî this is an inner product operation. Often, we also add an extra term called a bias, b, which shifts the output. You can think of the bias as an offset ‚Äî a way to control the baseline response of the neuron, even if all the inputs are zero.\n\nOnce we have this weighted sum, we pass it through a nonlinear function, which we usually call the activation function. Here it‚Äôs labeled \nsigma. This step is essential ‚Äî without it, the perceptron would just be a simple linear device.\nThere are many choices for the activation function. A common one is the sigmoid, which produces an S-shaped curve. Another is the hyperbolic tangent, or tanh, which gives outputs between minus one and plus one. There are also piecewise-linear functions such as the ReLU ‚Äî the rectified linear unit ‚Äî where the output is zero for negative inputs and increases proportionally for positive inputs.\n\nSo in short: a perceptron takes weighted inputs, adds them up, applies a bias, and then uses a nonlinear transformation to produce its output. This artificial neuron is the basic building block for artificial neural networks ‚Äî just as resistors, capacitors, and inductors are building blocks in electrical circuits.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 40, "slide_filename": "Slide40.txt", "slide_annotation": "Lecture 10 - Slide40.txt", "file_path": "Lecture 10\\Texts\\Slide40.txt", "content": "Now that we understand a single perceptron, let‚Äôs see what happens when we connect many of them. This gives us an artificial neural network.\n\nAt the far left, we have the input layer ‚Äî each circle here receives one piece of information from our data. These could be pixel values from an image, measurements from a sensor, or any other kind of features we want the network to process.\nNext, we have one or more hidden layers. Each neuron in a hidden layer takes the outputs from the previous layer, applies its weights, sums them up, passes the result through its activation function, and then sends the output to the next layer. These hidden layers are where the network learns increasingly complex patterns.\n\nFinally, at the far right, we have the output layer. In a classification task, each neuron here might represent a possible label ‚Äî for example, ‚Äúdog,‚Äù ‚Äúcat,‚Äù or ‚Äúcar.‚Äù The neuron with the highest output value would be the network‚Äôs prediction.\n\nHere‚Äôs the key idea:\nA single neuron can only separate very simple patterns.\nBy stacking many layers, the network can build up a hierarchy of features.\nThe first layer might detect basic edges or colors.\nThe next layer might detect shapes like wheels or ears.\nHigher layers combine these into entire objects, like a car or a cat.\n\nTraining the network works like this: we start with random weights, so the predictions are essentially guesses. We compare the network‚Äôs output to the correct answer and compute the error. Then, using a process called backpropagation, we send this error backward through the network, adjusting the weights slightly to reduce the error next time.\nWith enough data, enough layers, and many training cycles, the network learns to recognize very complex patterns ‚Äî the kind of capability that powers things like self-driving cars, medical image analysis, and voice recognition.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 41, "slide_filename": "Slide41.txt", "slide_annotation": "Lecture 10 - Slide41.txt", "file_path": "Lecture 10\\Texts\\Slide41.txt", "content": "Once you have the basic idea of how a neural network works, you can imagine that there‚Äôs no single way to connect the neurons. In fact, researchers have developed many different network topologies, each suited for specific tasks.\n\nHere in this chart, you can see a variety of examples.\nAt the simplest level, we have the perceptron and feed-forward networks, where information flows in one direction from input to output.\nThen there are radial basis networks and more specialized architectures like recurrent neural networks or RNNs, which loop information back so the network can remember past inputs.\nLong Short-Term Memory networks ‚Äî LSTMs ‚Äî and Gated Recurrent Units, or GRUs, are powerful variants for processing sequences, such as speech or time-series data.\n\nOther designs, like autoencoders and variational autoencoders, are used for compressing data and then reconstructing it, often to find hidden patterns or generate new examples.\nYou also see Boltzmann machines, Hopfield networks, and deep belief networks, which are useful for certain types of learning and pattern recognition.\n\nThe key point is that these different architectures are like tools in a toolbox. Some are better for images, some for language, some for prediction over time. Once you understand the fundamentals ‚Äî inputs, weights, activation functions, and training ‚Äî all these variations are just different ways of wiring those same building blocks together.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 42, "slide_filename": "Slide42.txt", "slide_annotation": "Lecture 10 - Slide42.txt", "file_path": "Lecture 10\\Texts\\Slide42.txt", "content": "It‚Äôs important to realize that not every neural network configuration will be equally useful. Designing an effective network requires engineering insight ‚Äî the architecture has to match the nature of the problem.\n\nFor example, a simple feed-forward structure can be thought of as a kind of multi-scale analysis, similar to how wavelet transforms work in signal processing. If your data has features at different scales, you need a structure that can capture them.\nConvolutional operations, widely used in image and signal processing, are rooted in the mathematics of linear systems. They‚Äôre excellent for recognizing local patterns that repeat across space or time.\nShortcut connections and feedback loops, inspired by control theory, can help stabilize training and improve learning in very deep networks.\n\nIn some cases, we even borrow ideas from game theory ‚Äî for example, adversarial mechanisms like GANs, where two networks compete to improve each other.\nAnd when we‚Äôre working with oscillations, waveforms, or electromagnetic signals, concepts from complex analysis ‚Äî such as amplitude and phase ‚Äî can be built directly into the network using complex-valued weights.\n\nThe takeaway here is that neural network design is not guesswork. It‚Äôs about choosing the right principles from mathematics and engineering, and embedding them into the architecture so that it‚Äôs well-matched to the problem at hand.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 43, "slide_filename": "Slide43.txt", "slide_annotation": "Lecture 10 - Slide43.txt", "file_path": "Lecture 10\\Texts\\Slide43.txt", "content": "Now, how do we actually optimize a neural network so that it performs well?\u000bAs I mentioned earlier, when you start with a fresh network, its performance is usually terrible ‚Äî the outputs are essentially random. The idea is to adjust the parameters ‚Äî the weights, which we represent as a vector w ‚Äî in such a way that the network‚Äôs predictions get closer and closer to the desired results.\n\nWe measure the difference between the network‚Äôs current output and the target output using what we call a loss function, or an error function. The goal is to make this error as small as possible.\nOne common approach is gradient descent. We start with an initial set of weights ‚Äî often chosen randomly. Then, in each training step, we look at the slope of the error function with respect to each weight. This slope, or gradient, tells us which direction increases the error ‚Äî so we move in the opposite direction, reducing the error.\n\nMathematically, the new weight vector equals the old weight vector, minus a small fraction of the gradient. That fraction is controlled by the learning rate, a simple scaling factor between zero and one. If the learning rate is too big, the updates may overshoot, and the training becomes unstable. If it‚Äôs too small, the network learns very slowly.\n\nBy repeatedly updating the weights in this way, the error decreases step by step. Eventually, if everything is set up well, the process converges and the network produces accurate outputs.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 44, "slide_filename": "Slide44.txt", "slide_annotation": "Lecture 10 - Slide44.txt", "file_path": "Lecture 10\\Texts\\Slide44.txt", "content": "Once you understand the concepts, you might want to try them out in practice. MATLAB now offers a Machine Learning Toolbox ‚Äî a powerful package that helps you perform parameter optimization, data clustering, and pattern recognition.\nWith this toolbox, you can access a variety of ready-made functions for tasks like feature extraction, classification, and regression. It also comes with tutorials, application notes, and built-in demos, so you can learn by experimenting.\n\nFor example, you can try leaf recognition, fingerprint detection, or even music genre classification ‚Äî all with the same framework. The advantage is that you don‚Äôt have to start coding everything from scratch. Instead, you can focus on understanding the algorithms, adjusting parameters, and seeing how those changes affect performance.\n\nThis makes it an excellent platform for quickly testing ideas, prototyping solutions, and getting a feel for how machine learning techniques behave with real data.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 45, "slide_filename": "Slide45.txt", "slide_annotation": "Lecture 10 - Slide45.txt", "file_path": "Lecture 10\\Texts\\Slide45.txt", "content": "Once we define a loss function ‚Äî the measure of difference between the network‚Äôs output and the ground truth ‚Äî our goal is to reduce that difference step by step. We do this by adjusting the network‚Äôs parameters in a way that moves us downhill on the loss surface.\n\nYou can imagine this process like standing on a hilly landscape and trying to reach the lowest valley. At each step, we look around, find the steepest downward direction, and take a small move in that direction. This is the essence of gradient descent.\nOver time, with each update, the loss gets smaller and smaller until the network converges to a good solution ‚Äî ideally, a point where further changes no longer improve the performance.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 46, "slide_filename": "Slide46.txt", "slide_annotation": "Lecture 10 - Slide46.txt", "file_path": "Lecture 10\\Texts\\Slide46.txt", "content": "Here, you see a visual demonstration from an online animation that shows how a neural network learns during training. Backpropagation is the key idea behind this process.\n\nThink of it like this ‚Äî the network makes a guess, compares that guess to the correct answer, and measures the error. Then, instead of adjusting everything randomly, it calculates how much each parameter contributed to that error. Using this information, it sends a correction signal backward through the layers, adjusting the parameters step by step.\nWith enough training cycles, the network gradually reshapes its decision boundaries, as you see in the animation, until it can classify almost all points correctly.\n\nIf you‚Äôre curious, I encourage you to watch the video linked here to see backpropagation in action.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 47, "slide_filename": "Slide47.txt", "slide_annotation": "Lecture 10 - Slide47.txt", "file_path": "Lecture 10\\Texts\\Slide47.txt", "content": "This is an article I wrote for IEEE Access titled A Perspective on Deep Imaging.\nAs imaging scientists, we‚Äôve traditionally thought of images as something you start with to process or analyze. But in medical imaging, it‚Äôs the opposite ‚Äî the image is the result of a long reconstruction process.\n\nWhat‚Äôs exciting is that with machine learning, we can rethink that process entirely. Instead of following the conventional pipeline step by step, we can train neural networks to directly generate high-quality images from the raw data itself. This opens the door to faster reconstructions, better image quality, and even reduced radiation dose in certain applications.\nIn short, deep learning isn‚Äôt just another tool in the box ‚Äî it has the potential to transform the way we produce and interpret medical images.\n\nNow let‚Äôs look at one practical example ‚Äî improving the signal-to-noise ratio in images.\nIn medical imaging, noise is inevitable. It can come from the physics of the scanner, from the patient‚Äôs movement, or from trying to keep the radiation dose low. Traditionally, we use filtering techniques to clean up an image, but these often blur fine details along with the noise.\n\nWith modern machine learning, we can do better. By training neural networks on large datasets ‚Äî for example, the thousands of scans acquired every day in hospitals ‚Äî the system can learn what a ‚Äúclean‚Äù image should look like, without erasing important diagnostic details. Instead of throwing those routine scans away, we can use them to teach the network.\nAs a result, we can transform a noisy scan into one that is much clearer, improving both diagnostic confidence and patient safety.\nSo that‚Äôs the big picture ‚Äî and we‚Äôll wrap up here for today.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 48, "slide_filename": "Slide48.txt", "slide_annotation": "Lecture 10 - Slide48.txt", "file_path": "Lecture 10\\Texts\\Slide48.txt", "content": "Before we close, let me show you a roadmap that pulls together the foundational ideas we‚Äôve been discussing.\n\nThis diagram is a kind of ‚Äúbig picture‚Äù for the mathematical and engineering concepts underlying modern imaging and machine learning. It includes complex numbers, Fourier transforms, convolution, and sampling theory ‚Äî all of which form the language we use to describe signals and images.\n\nYou can see how these elements connect: from basic mathematics like Euler‚Äôs formula, to signal operations such as convolution and correlation, to the Fourier series and transforms that let us work in the frequency domain. We also have system models from electrical engineering, like impedance and circuit laws, and finally, neural network building blocks ‚Äî showing how the same principles extend into modern AI.\n\nThink of this as your conceptual map. Each box here represents a tool in your toolbox, and together, they form the foundation for advanced methods in imaging, machine learning, and beyond.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 49, "slide_filename": "Slide49.txt", "slide_annotation": "Lecture 10 - Slide49.txt", "file_path": "Lecture 10\\Texts\\Slide49.txt", "content": "So here, you can see that all of these relationships are brought together in one poster.\n\nSome of the diagrams‚Äîlike the one we have here‚Äîactually exist in several different published versions. In this case, I‚Äôve redrawn the figure myself to avoid any copyright issues.\n\nFor a classroom lecture, using the original version is generally fine. But if you eventually want to publish your work, you either need to get formal permission to use those copyrighted figures or you should redraw them in your own style so they‚Äôre fully original.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 10", "slide_number": 50, "slide_filename": "Slide50.txt", "slide_annotation": "Lecture 10 - Slide50.txt", "file_path": "Lecture 10\\Texts\\Slide50.txt", "content": "For your homework, I‚Äôd like you to work through this sensitivity analysis problem.\n\nThe idea is to determine how to choose the resistor values so that the circuit‚Äôs sensitivity is as high as possible. Sensitivity here is defined as the derivative of V out‚Äã with respect to x, where x represents a small fractional change in the sensor resistance.\nIn other words, if x changes just a little, we want V out‚Äã to change as much as possible. That‚Äôs what we mean by ‚Äúhigh sensitivity.‚Äù\n\nYou‚Äôll be able to adjust both R L‚Äã and R S‚Äã. If you work through the math, you‚Äôll find that the sensitivity reaches its maximum when the two resistances are equal.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 11", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 11 - Slide1.txt", "file_path": "Lecture 11\\Texts\\Slide1.txt", "content": "Welcome, everyone. Today we begin Lecture Eleven, which is all about image quality assessment. This is really a landmark lecture in our course. After today, we‚Äôll move on to imaging modalities themselves. By then, the green textbook you have will serve as your main reference, and I‚Äôll continue using my slides to bring in a consistent story and sometimes involving state-of-the-art developments.\n\nSo why is quality assessment so important? Think about it this way: once an imaging system produces an image, the very first question we must ask is, is this image good enough? Is it clear, reliable, and suitable for diagnosis? That is what quality assessment is all about ‚Äî it‚Äôs the final piece of the foundation we need before diving into the modalities.\n\nSo, with that background, let‚Äôs begin our journey into image quality assessment.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 11 - Slide2.txt", "file_path": "Lecture 11\\Texts\\Slide2.txt", "content": "We are right on schedule in our journey through this material.\n\nIf you‚Äôve had a chance to look over the reading materials for today‚Äôs lecture, that‚Äôs great ‚Äî it will make it easier to connect the ideas we cover. If not, that‚Äôs fine too. I encourage you to follow along closely and take time afterward to review the main points. Consistently reinforcing concepts as you go will help you develop a stronger and more intuitive grasp, especially as the material becomes more mathematically detailed", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 11 - Slide3.txt", "file_path": "Lecture 11\\Texts\\Slide3.txt", "content": "Here you can see the textbook‚Äôs outline of Chapter Five, which I have actually moved earlier in our course. The reason is simple: once you understand the basics of image quality assessment, you can apply these ideas to almost any imaging modality.\nNo matter whether we‚Äôre talking about X-ray, CT, MRI, ultrasound, or optical imaging, the general principles remain the same. Concepts such as resolution, signal-to-noise ratio, contrast-to-noise ratio, and artifacts are universal. They may appear in different forms, but the underlying logic applies everywhere.\n\nThe good news is that this chapter is relatively easy reading compared to some of the heavier mathematical topics we‚Äôve already tackled. The ideas are straightforward. If you follow carefully and think through the examples, you‚Äôll see how each concept connects to real images.\n\nFor today‚Äôs lecture, I will guide you mainly through three key aspects of image quality assessment. We‚Äôll start with general mathematical measures, then move to system-specific characteristics, and finally consider task-specific and human-observer models. This will give you a well-rounded view of what it means to evaluate image quality in practice.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 11 - Slide4.txt", "file_path": "Lecture 11\\Texts\\Slide4.txt", "content": "Here‚Äôs the outline for today‚Äôs lecture. We‚Äôll approach image quality assessment from three perspectives: general measures, system-specific measures, and task-specific measures.\n\nFirst, we‚Äôll talk about general measures. Imagine you have an image and you also know the ground truth ‚Äî the ideal image you want to reconstruct. The goal is to measure how close your reconstructed image is to the ground truth. The simplest way is to calculate the difference between the two. In mathematics, we often describe this using the idea of a distance in a vector space. The most familiar example is the Euclidean distance, which leads directly to the mean squared error, or MSE.\n\nThere are other distances too. One example I‚Äôll briefly mention is the Kullback‚ÄìLeibler divergence, or KL distance. This is a bit trickier, so I won‚Äôt test you on it, but it‚Äôs good to be aware of. And then there is a very practical and powerful concept called structural similarity, or SSIM. This has become extremely influential ‚Äî the original paper has been cited tens of thousands of times ‚Äî because it captures how humans actually perceive similarity between images. We‚Äôll look at why this idea is so effective.\n\nNext, we move to system-specific measures. Think of the imaging system as a kind of camera ‚Äî it could be an X-ray detector, an ultrasound probe, or an MRI scanner. Every camera has its own specifications. Here we‚Äôll talk about how noisy the images are, how well the system can distinguish a signal from background noise, and what kind of resolution it provides. Resolution itself has several dimensions: spatial resolution, contrast resolution, temporal resolution, and spectral resolution. We‚Äôll also talk about artifacts ‚Äî those misleading structures that appear in images even though they don‚Äôt exist in reality. Understanding noise, resolution, and artifacts is key to judging the performance of any imaging system.\n\nFinally, we‚Äôll cover task-specific measures. This perspective is slightly different. Instead of asking ‚ÄúHow good is my camera?‚Äù, we ask ‚ÄúHow well does this imaging system perform a specific clinical task?‚Äù For example, can it reliably detect whether a bone is fractured? Can it identify a tumor in the lung? The most important thing in medicine is whether the imaging system helps doctors and patients make correct decisions. Even if a system isn‚Äôt perfect in a technical sense, if it consistently allows us to succeed in the clinical task, then it‚Äôs doing its job.\n\nSo these three aspects ‚Äî general, system-specific, and task-specific ‚Äî are connected, but they are not identical. Together they give us a complete picture of how to assess image quality. We‚Äôll begin with the first: general measures.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 11 - Slide5.txt", "file_path": "Lecture 11\\Texts\\Slide5.txt", "content": "Let‚Äôs begin with one of the most basic and widely used quality measures: the Mean Squared Error, or MSE.\nSuppose we have two images: the true image, which we‚Äôll call y, and the reconstructed image, which we‚Äôll call y-hat. Each image is made up of many pixels, indexed by i. If the image is 512 by 512, then the total number of pixels, n, is over 260,000.\n\nThe formula for MSE is simple:\u000bMSE equals one over n, times the sum from i equals one to n, of the difference between y-sub-i and y-hat-sub-i, squared.\nIn words, this means we compare the two images pixel by pixel. At each pixel, we take the difference, square it so that positive and negative errors don‚Äôt cancel out, and then average over all pixels. That gives us the mean squared error.\nNow, let‚Äôs go a bit deeper. When we estimate a parameter ‚Äî say theta ‚Äî we often write the estimate as theta-hat. The error between theta-hat and the true value theta can be analyzed in expectation, meaning averaged over many trials. When you expand the algebra, you find that the mean squared error naturally splits into two parts.\n\nThe first part is the variance. This tells us how much our estimates fluctuate around their average value. You can think of variance as a measure of random scatter.\n\nThe second part is the bias squared. This measures the difference between the average of our estimates and the true value. If our method consistently overshoots or undershoots, that‚Äôs bias.\n\nSo, in summary: MSE equals variance plus bias squared. Variance captures random error, bias captures systematic error, and together they define the total error.\n\nThis decomposition is very useful. It reminds us that an algorithm might have low variance but high bias, or vice versa. Understanding both helps us judge the quality of an estimator or an image reconstruction method.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 11 - Slide6.txt", "file_path": "Lecture 11\\Texts\\Slide6.txt", "content": "Now, the mean squared error is not the only way to measure differences. There are several variants, each with slightly different properties.\n\nThe first, which we‚Äôve already discussed, is the Mean Squared Error, or MSE. This is the average of the squared differences between prediction and truth.\nA closely related measure is the Root Mean Squared Error, or RMSE. Here we simply take the square root of the mean squared error. Why? Because this brings the units back to the same scale as the original measurement. For example, if we are measuring pixel intensities, RMSE will be expressed in the same units as those intensities, which makes it easier to interpret.\n\nOne important property of squaring is that it emphasizes larger errors much more strongly. If a difference is 100, squaring turns it into 10,000. That means MSE and RMSE heavily penalize large deviations.\nSometimes we want a measure that treats all errors more equally. That‚Äôs where the Mean Absolute Error, or MAE, comes in. Instead of squaring, we take the absolute value of the difference at each pixel, then average. This is also called the L1 norm, while MSE is associated with the L2 norm. The L1 norm is less sensitive to outliers compared to the L2 norm.\n\nFinally, we have the Mean Absolute Percentage Error, or MAPE. This is the mean absolute error expressed as a percentage of the true value. In other words, it‚Äôs MAE divided by the ground truth at each point, multiplied by 100 percent. This can be useful when we want to understand an error in relative terms ‚Äî for example, saying ‚Äúthe error is 5 percent‚Äù rather than giving a raw number.\n\nSo, these different distance measures ‚Äî MSE, RMSE, MAE, and MAPE ‚Äî give us different perspectives on error. The choice depends on the problem: do we want to penalize large errors more, or do we care more about relative error percentages?", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 11 - Slide7.txt", "file_path": "Lecture 11\\Texts\\Slide7.txt", "content": "So far, these measures seem very reasonable. Think about it this way: you have one signal or one image, and you also have a standard ‚Äî the ground truth. \n\nBy comparing them pixel by pixel, we‚Äôre essentially measuring the difference between two curves, or between two surfaces, or even between two volumes in three dimensions.\nThe formula reduces to something very intuitive: it‚Äôs about the area between the two curves. The yellow region you see here represents those differences. The larger the area, the greater the error.\n\nThis is why MSE and related measures are so widely used ‚Äî they give us a direct and interpretable way to say how close or how far two images are. It‚Äôs simple, mathematically neat, and visually intuitive.\n\nBut here‚Äôs an important point: while this is a good first step, it is not the whole story. Measuring differences point by point tells us something, but not everything. In medical imaging, we also care about structural similarity, system behavior, and clinical tasks. So, as we continue, you‚Äôll see that this is only the beginning.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 11 - Slide8.txt", "file_path": "Lecture 11\\Texts\\Slide8.txt", "content": "Now, let me briefly mention another type of distance, called information divergence. This is where probability theory comes in.\n\nSuppose you don‚Äôt just have two images, but instead you have two probability distributions ‚Äî for example, two different histograms of pixel values. The question becomes: how do we measure the difference between these two distributions?\nOne option is to use Euclidean distance, just as before. But there is a more meaningful way in the context of information theory. This is called the Kullback‚ÄìLeibler divergence, or KL distance for short.\n\nThe formula looks a bit unusual:\u000bKL divergence equals the sum over x of p of x, multiplied by the logarithm of p of x divided by q of x.\nYou don‚Äôt need to worry too much about the details ‚Äî this is beyond the scope of our lecture ‚Äî but the idea is important. The KL divergence is always greater than or equal to zero, and it becomes exactly zero if and only if the two distributions are identical.\n\nOne interesting property is that the KL divergence is not symmetric. In other words, the distance from P to Q is not the same as the distance from Q to P. That may sound strange, but it has a good analogy. Think of climbing a mountain: going uphill is much harder than going back downhill, even though it‚Äôs the same physical path. In the same way, KL divergence measures directionality in information.\n\nSo, while we won‚Äôt use this directly in our course, it‚Äôs good to be aware that such information-based distances exist. They play a big role in areas like machine learning and statistical signal processing.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 11 - Slide9.txt", "file_path": "Lecture 11\\Texts\\Slide9.txt", "content": "Now, just for your broader knowledge, let me connect this to another important concept in information theory: mutual information.\n\nMutual information is a way to measure how much knowing one random variable tells us about another. For example, suppose we have two variables, X and Y. If they are completely independent, then measuring X tells us nothing about Y. In that case, their mutual information is zero. On the other hand, if X and Y are perfectly dependent ‚Äî meaning that once you know X, you completely know Y ‚Äî then their mutual information is very high. Most real-world situations fall somewhere in between.\n\nMathematically, mutual information can actually be expressed in terms of the KL divergence. Specifically, it‚Äôs the KL divergence between the joint distribution of X and Y, and the product of their marginal distributions. Don‚Äôt worry about the details of the formula ‚Äî the key idea is that it quantifies how much information one variable provides about the other.\nYou can also think of it this way: when you measure one variable, how much does your uncertainty about the other variable decrease? That decrease in uncertainty is exactly what mutual information captures.\n\nIn practice, this idea is very useful in areas like image registration, where we align two images. Instead of just matching pixel intensities, we can maximize the mutual information between the two images. That way, even if the images look very different in terms of brightness or contrast, we can still measure how well they correspond.\n\nSo, mutual information is essentially a generalization of correlation, but in the language of information theory. It goes beyond simple linear relationships and captures any kind of statistical dependence.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 11 - Slide10.txt", "file_path": "Lecture 11\\Texts\\Slide10.txt", "content": "Mutual information, which we just discussed, is actually defined in terms of another very fundamental concept: entropy.\n\nSo, what is entropy in this context? Think of it as a measure of uncertainty. If you have a probability distribution that is very spread out and uniform, then you have a lot of uncertainty ‚Äî you don‚Äôt really know what the outcome will be. That means the entropy is high.\nOn the other hand, if the distribution is sharply peaked ‚Äî like a delta function, where one outcome is guaranteed ‚Äî then there is no uncertainty. In that case, the entropy is very low, even zero.\n\nSo entropy tells us how much information, or how much unpredictability, is contained in a random variable. In information theory, this is a central concept because information itself is really about reducing uncertainty.\nNow, mutual information can be written as the difference between two entropies: the entropy of Y by itself, minus the entropy of Y given X. In other words, it measures how much uncertainty about Y is reduced when you know X. That‚Äôs exactly what we mean by ‚Äúhow much does X tell us about Y.‚Äù\nAgain, you don‚Äôt need to get bogged down in the formulas here. The key point is: entropy captures uncertainty, and mutual information measures how two variables share or reduce that uncertainty.\n\nFor our purposes, I just want you to know that these information-theoretic measures ‚Äî KL divergence, mutual information, and entropy ‚Äî are very powerful, but they are more advanced than what we need right now. Think of them as tools in the background, which complement the simpler measures like mean squared error.\n\nAnd with that, we will soon return to the more practical measures that are directly used in image quality assessment.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 11 - Slide11.txt", "file_path": "Lecture 11\\Texts\\Slide11.txt", "content": "Now let‚Äôs see why mean squared error, or MSE, is not always good enough.\nAt the top left, we have the original image ‚Äî the best version, taken under ideal conditions. Below it, you see five different degraded versions of the same image. Some look noisy, some are blurry, and some have other distortions. Clearly, to the human eye, these images do not look equally good.\n\nBut here‚Äôs the problem: when we compute the mean squared error between the original image and each of these five degraded ones, the result is the same ‚Äî two hundred and twenty-five. Mathematically, MSE tells us they are equally different from the original.\nVisually, though, that‚Äôs obviously not true. Some versions look much closer to the original, while others look far worse. Our eyes immediately pick up those differences, but MSE cannot.\n\nAnd this is the key point: MSE does not reflect human perception very well. It measures pixel-wise differences, but it cannot capture whether the overall structure of the image is preserved.\n\nThis leads us to the next important idea ‚Äî we need a measure that better matches what humans actually see. That‚Äôs where structural similarity, or SSIM, comes in.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 11 - Slide12.txt", "file_path": "Lecture 11\\Texts\\Slide12.txt", "content": "Now let‚Äôs think about this from the perspective of the human visual system, or HVS.\n\nThe human eye does not look at images pixel by pixel, the way mean squared error does. MSE simply compares each pixel individually, treats every error the same, and then adds them up. That‚Äôs a bottom-up approach ‚Äî starting from the smallest units and working upward.\nBut our visual system works very differently. We are much more sensitive to structural information ‚Äî the patterns, edges, and relationships that give an image its overall form. For example, even a small change in the background, or a shift in texture, is something we can notice right away. Our brains are highly adapted to detect these kinds of contextual changes.\n\nIn the classical view, the focus was on error visibility: if you see a discrepancy, count it as an error. In the newer view, the focus shifts to structural distortion: what matters is whether the structure of the image has been preserved.\nThis also connects to the way our vision system interprets images. Rather than starting with tiny details, many researchers argue that we first process the global structure ‚Äî sometimes described in terms of topological features. These are high-level properties, such as whether objects are connected, how many distinct regions there are, or whether certain shapes remain intact. Importantly, these properties don‚Äôt change if the image is stretched, rotated, or rescaled.\n\nSo here‚Äôs the philosophical shift: instead of measuring differences pixel by pixel, we want to measure how much of the structural information has been preserved. That is what structural similarity is all about, and it‚Äôs why it is far more aligned with how humans actually see images.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 11 - Slide13.txt", "file_path": "Lecture 11\\Texts\\Slide13.txt", "content": "Here we arrive at what I like to call an instant classic. This is the landmark 2004 paper by Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli, titled Image Quality Assessment: From Error Visibility to Structural Similarity.\n\nBefore this work, most image quality measures focused on error visibility ‚Äî essentially counting differences pixel by pixel, the way mean squared error does. What this paper introduced was a completely different perspective: instead of measuring errors, we should measure structural similarity.\nThe idea is built on the assumption that the human visual system is highly tuned to extract structural information from images. So rather than asking, ‚ÄúHow many errors can we see?‚Äù, the SSIM framework asks, ‚ÄúHow well is the structure of the image preserved?‚Äù\n\nThis shift in thinking turned out to be incredibly powerful. The paper has been cited tens of thousands of times, and SSIM quickly became a standard tool not only in image processing research but also in practical applications like video compression, image restoration, and medical imaging.\nSo, from this point forward, when we talk about perceptual image quality, we are really building on the foundation laid by this work.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 11 - Slide14.txt", "file_path": "Lecture 11\\Texts\\Slide14.txt", "content": "Now let‚Äôs look at an example to see how the Structural Similarity Index, or SSIM, actually works in practice.\nOn the top left, we have the reference image ‚Äî the original, undistorted version. If we compare this image with itself, the SSIM score is exactly 1. That makes sense, because they are identical.\n\nNow, compare the original with the two images next to it. On the top row, both look quite similar to the original. One produces an SSIM value of 0.949, the other 0.989. These numbers are very close to one, reflecting the fact that the images are almost identical to our eyes.\nOn the bottom row, however, we see very different results. One image has been heavily pixelated, another blurred, and another corrupted with noise. When compared to the original, their SSIM values drop significantly ‚Äî around 0.67, 0.69, and about 0.72. That means they retain only about two-thirds of the structural similarity.\n\nThe important thing here is that SSIM values line up with what we visually perceive. Images that look good to us score close to one. Images that look distorted or degraded score much lower. That is the power of SSIM ‚Äî it bridges the gap between mathematical measurement and human vision.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 11 - Slide15.txt", "file_path": "Lecture 11\\Texts\\Slide15.txt", "content": "Now let‚Äôs look at how the Structural Similarity Index, or SSIM, is actually computed.\nSuppose we have two images, which we‚Äôll call signal X and signal Y. The first step is to look at their luminance, which simply means the average brightness level. We measure the mean intensity of each image and make sure we are comparing them on the same scale.\n\nNext, we look at the contrast. This is captured by the standard deviation ‚Äî how much the pixel values vary around the mean. A high standard deviation means strong contrast, while a low standard deviation means the image looks flat or washed out. So we compute the standard deviation for each image to quantify its contrast.\n\nAfter normalizing for luminance and contrast, we focus on the most important part: the structure. This step captures how patterns of pixels in one image relate to patterns in the other ‚Äî whether the edges, textures, and fine details line up.\n\nFinally, SSIM combines all three comparisons ‚Äî luminance, contrast, and structure ‚Äî into a single value between zero and one. A score of one means the images are structurally identical. A lower score means important structural information has been lost.\n\nSo, in summary: SSIM works by checking three things ‚Äî do the images have the same brightness, the same contrast, and the same structural patterns? When all three align, we say the images are very similar.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 11 - Slide16.txt", "file_path": "Lecture 11\\Texts\\Slide16.txt", "content": "As I mentioned, the Structural Similarity Index is built on three components: luminance, contrast, and structure ‚Äî or L, C, and S for short.\n\nLet‚Äôs start with luminance. Suppose we have an image X with N pixels. To find its average brightness, or mean, we simply add up all the pixel values and divide by N. This gives us the mean value, which we call mu of X.\nNext, we remove the mean by subtracting the mean of X from every pixel. This is the first step of normalization ‚Äî it centers the image so that the average brightness is zero.\n\nOnce we have the mean, we can also compute the standard deviation. This measures how much the pixel values vary around the mean. Notice that the formula here uses 1 divided by N minus 1, not just N. That detail comes from statistics ‚Äî it gives us an unbiased estimate of the standard deviation.\nFinally, we can normalize the image even further by dividing each pixel by the standard deviation. After this second step of normalization, the transformed image will have a mean of zero and a standard deviation of one. In other words, it has been rescaled so that brightness and contrast are standardized.\nWe repeat this process for both images, X and Y. Only then do we compare them ‚Äî first in terms of luminance, then contrast, and finally their structural relationship.\n\nSo, these are the basic operations behind SSIM. They are simple statistical tools, but together they allow us to capture how similar two images are in terms of brightness, contrast, and structure.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 11 - Slide17.txt", "file_path": "Lecture 11\\Texts\\Slide17.txt", "content": "When we construct a similarity measure, the goal is to capture not just how different two images are, but how similar they are. To do this properly, we want our measure to satisfy three basic conditions, or postulates.\n\nFirst, symmetry. The similarity between X and Y should be the same as the similarity between Y and X. In other words, order doesn‚Äôt matter. If you compare image A with image B, you should get the same result as comparing image B with image A.\nSecond, boundedness. The similarity value should always be between zero and one. A score of one means the images are perfectly identical. A score closer to zero means they are very different.\nThird, a unique maximum. The similarity should only reach the maximum value of one if the two images are exactly the same, pixel for pixel. That way, the measure has a clear and meaningful interpretation.\n\nThese three conditions ‚Äî symmetry, boundedness, and unique maximum ‚Äî make sure that a similarity measure behaves logically and reliably. SSIM is designed to satisfy all of these postulates.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 11 - Slide18.txt", "file_path": "Lecture 11\\Texts\\Slide18.txt", "content": "So let‚Äôs begin with the first component of SSIM: the luminance comparison.\n\nRemember, luminance simply means the average brightness of an image. If two images have the same mean intensity, they should score highly on luminance similarity. If one image is much brighter or darker than the other, the similarity should be lower.\n\nMathematically, the luminance comparison is written like this:\nL of X and Y equals two times mu-X times mu-Y, plus a constant C1, all divided by mu-X squared plus mu-Y squared, plus that same constant C1.\nHere, mu-X and mu-Y represent the average brightness of images X and Y. The constant C1 is added to make the formula stable ‚Äî otherwise, if both means are very close to zero, the denominator becomes tiny and the ratio unstable.\nTo choose C1, we take the square of K1 times L, where L is the dynamic range of pixel values. For an 8-bit grayscale image, L is 255. K1 is just a very small number, much less than one.\n\nNow, notice a few important properties:\nIf the two means are equal, then the numerator and denominator are the same, so the luminance comparison equals 1, meaning perfect similarity.\nIf the two means are very different, the numerator becomes small relative to the denominator, so the luminance comparison approaches 0, meaning poor similarity.\n\nThis simple formula ensures that luminance similarity satisfies the three postulates we discussed earlier: it is symmetric, bounded between 0 and 1, and it reaches the maximum value of 1 only when the two images have identical mean brightness.\nSo luminance is the first step in measuring structural similarity. Next, we‚Äôll look at contrast comparison.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 11 - Slide19.txt", "file_path": "Lecture 11\\Texts\\Slide19.txt", "content": "Let‚Äôs take a closer look at the luminance term and see why it behaves the way we want.\n\nHere, we let mu-X, the mean of image X, be represented by the letter A. And we let mu-Y, the mean of image Y, be represented by the variable X. Now, the question is: as X changes, what value of X will maximize the luminance comparison?\nTo answer this, we take the formula for the luminance term and compute its first derivative with respect to X. By setting that derivative equal to zero, we can find the point where the function reaches its maximum.\n\nWhen we go through the algebra, the conclusion is very clear: the maximum occurs when X equals A, in other words, when the mean brightness of image Y is equal to the mean brightness of image X. At that point, the luminance comparison reaches its maximum possible value, which is one.\nThis confirms the intuition: two images will only be perfectly similar in luminance if their average brightness levels are the same. If one image is brighter or darker than the other, the luminance comparison will drop accordingly.\n\nSo this mathematical exercise is really just a proof of what we already expect: the luminance term is maximized when the two images have the same mean brightness.\nThat‚Äôs the first aspect of structural similarity. Next, we‚Äôll move on to the second aspect: contrast comparison.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 11 - Slide20.txt", "file_path": "Lecture 11\\Texts\\Slide20.txt", "content": "The second component of SSIM is the contrast comparison.\nEven if two images have the same average brightness, they may still look very different if their contrast is not the same. One might look sharp and vivid, while the other looks flat or washed out. To capture this, we use the standard deviation of pixel values ‚Äî sigma-X for image X and sigma-Y for image Y.\n\nThe formula for contrast comparison looks very similar to the one we used for luminance. It is written as:\nC of X and Y equals two times sigma-X times sigma-Y, plus a constant C2, divided by sigma-X squared plus sigma-Y squared, plus that same constant C2.\n\nHere again, the constant is added to prevent instability when values are close to zero. C2 is defined as K2 times L squared, where L is the dynamic range of the image.\nNow, notice how this works:\nIf the two images have the same standard deviation, meaning their contrasts are the same, then the numerator and denominator match, and the contrast comparison equals 1.\nIf one image has much higher or lower contrast than the other, then the ratio becomes smaller, and the similarity drops.\nThis design is also consistent with how the human visual system works. For example, in a very dark room, small changes in brightness can be quite noticeable. But in a very bright, high-contrast scene, the same small changes are much harder to see. The formula reflects this effect, sometimes called contrast masking.\n\nSo contrast is the second pillar of structural similarity ‚Äî it ensures that two images not only have the same average brightness but also the same level of variation around that average.\nNext, we‚Äôll look at the third pillar: the structural comparison.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 11 - Slide21.txt", "file_path": "Lecture 11\\Texts\\Slide21.txt", "content": "Now let‚Äôs analyze the contrast term a bit more carefully.\n\nThe key idea here is that the human visual system does not respond to absolute changes in brightness, but rather to relative changes. This is exactly what Weber‚Äôs law describes. It states that the smallest noticeable change in a stimulus is proportional to the background level of that stimulus.\n\nFor example, if you are in a very dark room, even a tiny spot of light is noticeable right away. But if you are in a brightly lit room, the very same tiny light would be invisible. The background intensity is so high that the small change is masked.\nYou‚Äôve probably experienced this outdoors: at night, under a dark sky, you can see countless stars. But during the day, those same stars are still there ‚Äî yet you can‚Äôt see them, because the bright background light overwhelms them.\nThe same principle applies to hearing. In a quiet room, even a whisper can be heard. But in a noisy party, you need to shout for anyone to notice.\n\nMathematically, this means that the sensitivity of the contrast measure depends on the ratio of the change, delta-X, to the baseline, X. If X is small, even a small delta makes a big difference. If X is large, the same delta hardly matters.\nSo the contrast term in SSIM naturally captures this idea ‚Äî it reflects the fact that our visual system is tuned to relative, not absolute, differences. This is why the measure agrees so well with human perception.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 11 - Slide22.txt", "file_path": "Lecture 11\\Texts\\Slide22.txt", "content": "Here‚Äôs a simple example of a changeover background.\n\nLook at the first column on the left. The top image has about 10 dots, while the bottom has about 20 dots. The difference is 10. When you compare them, it‚Äôs very obvious that the bottom one has more dots than the top one.\nNow look at the second column. The top image has about 110 dots, and the bottom has about 120 dots. Again, the difference is 10. But this time, it‚Äôs much harder to notice the difference.\n\nWhy? Because the background level ‚Äî the total number of dots ‚Äî is so much higher. A change of 10 is a large fraction when the background is only 10, but it‚Äôs a tiny fraction when the background is already 110.\nThis illustrates the principle we just discussed: what our visual system detects is not the absolute change, but the relative change ‚Äî delta divided by the background.\n\nSo, when we design a similarity measure, we want it to reflect this property of human vision. And that‚Äôs exactly what the contrast comparison in SSIM does. It behaves in a way that is consistent with how we see.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 11 - Slide23.txt", "file_path": "Lecture 11\\Texts\\Slide23.txt", "content": "The third and final component of SSIM is the structural comparison.\n\nHere‚Äôs the idea. Once we normalize each image ‚Äî removing its mean brightness and adjusting its contrast ‚Äî what‚Äôs left is essentially its structure. This is where we ask: do the patterns, textures, and shapes in the two images align?\nMathematically, this is captured using the cross-correlation between the two images. We call it sigma-X-Y, which measures how the variations in image X line up with the variations in image Y. To make the measure stable, we again introduce a constant, just like we did with luminance and contrast.\n\nAnother way to think about this is in terms of vectors in high-dimensional space. Imagine each image as a vector, with each pixel value being one coordinate. The structural similarity is then like taking the inner product of these two vectors, normalized by their lengths. Geometrically, this is just the cosine of the angle between the two vectors.\n\nIf the two vectors point in exactly the same direction ‚Äî meaning the images are identical up to scaling ‚Äî the angle is zero, and the similarity is maximized.\nIf they are completely uncorrelated, the inner product is small, and the similarity is low.\nThis is the same intuition we used when studying Fourier analysis: a Fourier coefficient is computed by projecting a signal onto a basis function. Here, we are projecting one image onto another and measuring how well they align.\n\nSo, structural comparison boils down to asking: once brightness and contrast are accounted for, do the fine details and patterns of the two images still match?\nThat completes the three pillars of SSIM: luminance, contrast, and structure. Next, we‚Äôll see how they combine into the full similarity measure.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 11 - Slide24.txt", "file_path": "Lecture 11\\Texts\\Slide24.txt", "content": "Now let‚Äôs connect this to a very important mathematical principle: the Cauchy‚ÄìSchwarz inequality.\n\nYou may remember this from linear algebra. It tells us that the inner product of two vectors is always less than or equal to the product of their lengths. Equality holds only when the two vectors are linearly related ‚Äî in other words, when one is just a scaled version of the other.\n\nWhy is this important here? Well, recall that in structural comparison, we represent each image as a vector in a high-dimensional space. The structural similarity is essentially the normalized inner product between those two vectors.\nThe Cauchy‚ÄìSchwarz inequality guarantees that this similarity value will always lie between zero and one, and it will reach one only when the two images are structurally identical, differing at most by a scaling factor.\n\nSo, this inequality is the mathematical reason why the structural comparison term works. It‚Äôs not just a rule chosen arbitrarily ‚Äî it‚Äôs grounded in solid geometry.\nAnd as engineers and scientists, it‚Äôs always valuable to know not just how something works, but why. Understanding the principle behind the rule allows you to be creative and adapt these ideas in new situations.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 11 - Slide25.txt", "file_path": "Lecture 11\\Texts\\Slide25.txt", "content": "And now, we can finally put everything together.\n\nWe‚Äôve defined three components: luminance comparison, contrast comparison, and structural comparison. Each one captures a different aspect of how two images may look alike or differ. The Structural Similarity Index, or SSIM, is built by combining all three.\nMathematically, the SSIM of X and Y is written as:\nluminance term raised to the power alpha, multiplied by the contrast term raised to the power beta, multiplied by the structure term raised to the power gamma.\n\nThe parameters alpha, beta, and gamma simply allow us to adjust the relative importance of the three components. In practice, we usually set all three equal to one, giving equal weight to luminance, contrast, and structure.\nTo keep the formula stable, we also include constants ‚Äî C1, C2, and C3. These prevent divisions by very small numbers, which could make the result unstable. For convenience, researchers often set C3 equal to half of C2. These are empirical design choices, not strict theory, but they work well in practice.\nWith these simplifications, we arrive at the familiar form of SSIM that is widely used today. This measure is often referred to as the Universal Quality Index with stabilizing constants added.\n\nThe remarkable thing is how well SSIM works. Despite its simplicity, it has become one of the most widely used metrics for image quality, both in research and in industry. Even today, when we use deep learning and advanced algorithms for image processing, SSIM and mean squared error remain the standard reference metrics.\n\nSo, this is the moment where SSIM is born ‚Äî a simple but powerful measure that aligns closely with human perception and has stood the test of time.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 11 - Slide26.txt", "file_path": "Lecture 11\\Texts\\Slide26.txt", "content": "Here‚Äôs another example that demonstrates the power of SSIM.\n\nThe image in the top left, marked (a), is the reference image ‚Äî the ground truth. The other five images are distorted versions, each with a different type of degradation: contrast stretching, mean shift, JPEG compression, blurring, and salt-and-pepper noise.\nIf we were to use mean squared error, all of these distorted images would end up with roughly the same error value. But visually, they do not look equally bad. Some are close to the original, while others are clearly worse.\n\nNow look at the SSIM scores shown here. The contrast-stretched image has an SSIM close to 0.92, which makes sense because it looks fairly good. The mean-shifted version scores about 0.90, also quite close. But the JPEG-compressed version drops down to around 0.69, and the blurred version to 0.70. The salt-and-pepper noisy image scores about 0.77.\nIf you compare these numbers with what your eyes tell you, the agreement is clear. Images that look closer to the original score higher; images that look worse score lower.\n\nThis is why SSIM is so widely used. It captures the essential aspects of the human visual system ‚Äî brightness, contrast, and structure ‚Äî in a single number. And in practice, this means we can use SSIM to guide optimization. For example, when designing a communication channel or an imaging system, we can adjust the system parameters to maximize SSIM, ensuring that the images produced look good to the human eye.\n\nSo, SSIM is not just a theoretical measure ‚Äî it is a practical tool that connects mathematics directly to human perception.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 11 - Slide27.txt", "file_path": "Lecture 11\\Texts\\Slide27.txt", "content": "Because SSIM works so well, researchers have developed many extensions of the method.\n\nThe version we‚Äôve been discussing applies to grayscale images, where each pixel has only one intensity value. But in reality, most images are in color. So naturally, the first extension was to adapt SSIM for color image quality assessment. This involves measuring similarity not only in brightness and contrast, but also in the relationships between color channels.\n\nAnother extension is to video quality assessment. Here, SSIM is applied not just frame by frame, but also across time, because our eyes are sensitive to temporal consistency. This has become very important in areas like video compression and streaming.\nThere is also multi-scale SSIM, which evaluates images at different levels of resolution. This is especially useful because human vision itself operates on a multi-scale level ‚Äî we notice both fine details and large structures, depending on how we view an image.\nAnd finally, there is complex wavelet SSIM, which can handle images that have complex values, such as those produced in MRI. This is a powerful extension that broadens SSIM to applications in medical imaging and beyond.\n\nThe core idea of SSIM ‚Äî comparing luminance, contrast, and structure ‚Äî has proven to be so flexible that it has been adapted to various domains.\n\nNow, let me pause here and use an analogy that connects to your own experience. When we design an exam, we want the scores to spread out enough to show meaningful differences among students. If everyone scores 90 or higher, we can‚Äôt distinguish performance very well. Ideally, the mean should be somewhere in the middle, say around 50, with variation above and below. That way, the test reveals the true distribution of understanding.\nIt‚Äôs the same idea with SSIM: we normalize by the mean and by the variation, so we can focus on the meaningful structural differences between signals.\n\nSo, whether we‚Äôre evaluating images or student performance, the principle is the same: measure differences relative to expectations and variability, not in absolute terms.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 11 - Slide28.txt", "file_path": "Lecture 11\\Texts\\Slide28.txt", "content": "Now we move into the second part of our lecture: system-specific measures.\n\nSo far, we‚Äôve discussed general mathematical ways of comparing images ‚Äî measures like mean squared error, KL divergence, and SSIM. These are important, but they don‚Äôt tell the whole story, because image quality also depends heavily on the imaging system itself.\nEvery imaging system ‚Äî whether it‚Äôs an X-ray detector, an MRI scanner, or an ultrasound probe ‚Äî has its own characteristics and limitations. To assess the quality of images produced by a system, we need to look at metrics that reflect the system‚Äôs performance.\n\nThis includes measures such as:\nNoise, and the related quantities signal-to-noise ratio (SNR) and contrast-to-noise ratio (CNR).\nResolution, which can be spatial, contrast, temporal, or spectral, depending on what aspect of the system we are evaluating.\nAnd finally, artifacts, which are false or misleading structures that appear in the image because of imperfections in the imaging process.\nTogether, these system-specific measures tell us how good a given imaging device is at capturing and representing reality.\n\nSo let‚Äôs begin this section with one of the most important system-specific measures: the signal-to-noise ratio, or SNR.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 11 - Slide29.txt", "file_path": "Lecture 11\\Texts\\Slide29.txt", "content": "In engineering, one of the most familiar terms you‚Äôll encounter is the Signal-to-Noise Ratio, or SNR.\nWhenever we take a measurement ‚Äî whether it‚Äôs a photograph, an MRI scan, or an ultrasound image ‚Äî we always have some amount of noise. Noise is unavoidable. It arises from the fundamental randomness of physical processes, and ultimately from quantum mechanics itself. Nothing is ever perfectly certain.\n\nOn top of this background noise, we have the signal ‚Äî the part of the measurement that actually carries meaningful information.\nSNR is a simple but powerful way of expressing how strong the signal is relative to the noise. Mathematically, it is defined as the ratio of signal power to noise power. Since power is proportional to the square of amplitude, you can also write SNR as the square of the signal amplitude divided by the noise amplitude.\n\nHere‚Äôs the intuition:\nIf the SNR is around 1, the signal varies in the same range as the noise. That means the signal is barely visible.\nIf the SNR is 5 or 10 or higher, the signal stands out clearly above the noise, and it can be easily detected.\n\nA related measure is the Contrast-to-Noise Ratio, or CNR. Instead of comparing one signal to background noise, we compare the difference between two signals ‚Äî for example, a feature versus its background ‚Äî divided by the noise level. This measure is very common in medical imaging because it directly reflects how well we can distinguish one structure from another.\n\nSo, SNR tells us whether a signal can rise above the noise at all, while CNR tells us whether two signals can be distinguished against that noisy background.\nAnd next, we‚Äôll move on to another fundamental aspect of image quality: resolution.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 30, "slide_filename": "Slide30.txt", "slide_annotation": "Lecture 11 - Slide30.txt", "file_path": "Lecture 11\\Texts\\Slide30.txt", "content": "Next, let‚Äôs talk about spatial resolution, which is one of the most important measures of image quality.\n\nThe simplest way to think about spatial resolution is in terms of how well an imaging system can distinguish two objects that are close together. For example, imagine two very small, bright points ‚Äî like two tiny tumors. Mathematically, we describe each of those points as a delta function, which represents a perfect single dot.\n\nBut in reality, no imaging system can reproduce a perfect dot. Instead, each point becomes blurred into a small disk, usually shaped like a Gaussian curve. This blur is called the point spread function, or PSF. It tells us how the system responds to a single point source.\nNow, suppose we have two points. If they are far apart, even though each one is blurry, you can still clearly see two separate spots. But as they move closer together, the two blurred shapes begin to overlap. Eventually, when the separation between them is too small, the two spots merge and appear as one.\n\nThe critical threshold is defined by the full width at half maximum, or FWHM, of the point spread function. In other words, when the distance between the two points is equal to the width of the blur at half its height, that‚Äôs about the limit of what the imaging system can resolve. Any closer, and the system can no longer distinguish the two points.\n\nThis is why we say spatial resolution is the minimum separation at which two objects can still be seen as distinct. Often, when people talk about ‚Äúresolution‚Äù in imaging, this is what they mean: the ability to resolve fine detail.\nAnother way to analyze resolution ‚Äî which we‚Äôll get to shortly ‚Äî is through the modulation transfer function, or MTF, which uses Fourier analysis to quantify how well different spatial frequencies are preserved in the image.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 31, "slide_filename": "Slide31.txt", "slide_annotation": "Lecture 11 - Slide31.txt", "file_path": "Lecture 11\\Texts\\Slide31.txt", "content": "Another way to describe spatial resolution is through the Modulation Transfer Function, or MTF.\n\nHere‚Äôs the idea: every image can be broken down into sinusoidal components using Fourier analysis. These components can be low frequency ‚Äî representing smooth, gradual changes ‚Äî or high frequency ‚Äî representing fine details like sharp edges or thin lines.\nIf we feed a sinusoidal pattern into an imaging system, the system does not pass all frequencies equally well. Low-frequency components pass through almost perfectly. But as the frequency increases, the system begins to attenuate, or weaken, those components. At very high frequencies, the system may blur them so much that they become invisible.\n\nThe MTF curve shows this behavior. On the left side, at low frequencies, the modulation is close to 100 percent. As frequency increases, the response gradually drops. Eventually, at very high frequencies, the response falls to zero ‚Äî meaning the system can no longer reproduce those details.\nA practical way to think about this is with a line-pair phantom, where black and white bars alternate like a test pattern. At low frequencies, the bars are wide, and the system can reproduce them clearly. At high frequencies, the bars get narrower and closer together. At some point, the system can no longer distinguish the bars ‚Äî they blur into a uniform gray. That point defines the system‚Äôs resolution limit.\n\nSo MTF gives us a frequency-domain way of measuring resolution. It tells us not just whether two points can be separated, but how well the system preserves detail across different scales.\nThis measure is widely used in both medical imaging and general optics. It provides a more complete picture of resolution compared to the single-number definition from the point spread function.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 32, "slide_filename": "Slide32.txt", "slide_annotation": "Lecture 11 - Slide32.txt", "file_path": "Lecture 11\\Texts\\Slide32.txt", "content": "Next, let‚Äôs look at contrast resolution, sometimes called low-contrast resolution.\n\nThis is different from the high-contrast resolution we just discussed. High-contrast resolution deals with distinguishing small, bright details ‚Äî for example, two tiny dots very close together.\n\nContrast resolution, on the other hand, is about the ability of an imaging system to detect subtle differences in intensity. The structures may not be small ‚Äî they may even be large ‚Äî but if their contrast relative to the background is very low, they may be difficult to see.\nIn the example shown here, notice how in one image the faint circular structures stand out more clearly, while in the other they are barely visible against the noisy background. This difference reflects the system‚Äôs contrast resolution.\nClinically, this is very important. For example, in CT imaging, a tumor may look very similar in intensity to surrounding soft tissue, because both are made of similar biological material. A system with good contrast resolution allows the tumor to be detected clearly, while a noisy or lower-quality system might completely obscure it.\n\nBeyond contrast resolution, there are two more types of resolution we should briefly mention.\nTemporal resolution refers to how quickly an imaging system can capture a snapshot. A high-speed system can freeze motion ‚Äî like capturing a sharp image of a beating heart. But if the acquisition is too slow, moving objects blur together. This is why, when you take a photo of a moving car with your phone, you sometimes see motion blur. In medical imaging, temporal resolution is critical for dynamic studies, such as cardiac CT or MRI.\nSpectral resolution refers to the ability of a system to distinguish between different frequencies or energies. In vision, this corresponds to color perception ‚Äî someone with color blindness has poor spectral resolution. In imaging systems, spectral resolution allows us to distinguish X-rays of different energies or ultrasound waves of different frequencies. Better spectral resolution means finer discrimination of subtle differences in material properties.\n\nFinally, let‚Äôs talk about artifacts. Artifacts are false structures that appear in images but are not really there. They are ‚Äúghosts‚Äù created by limitations or mismatches in the imaging process. For example, in CT, the motion of the heart can create blurry or duplicated structures. In ultrasound, echoes may bounce back and forth and appear as multiple spots even though only one structure exists. Recognizing and understanding artifacts is critical, because they can mislead interpretation.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 33, "slide_filename": "Slide33.txt", "slide_annotation": "Lecture 11 - Slide33.txt", "file_path": "Lecture 11\\Texts\\Slide33.txt", "content": "Now let‚Äôs look at a specific example of artifacts ‚Äî in this case, metal artifacts in CT imaging.\n\nSuppose a patient has a hip fracture and receives metal implants. When we take X-rays or CT scans, the metal is so dense that it blocks or severely distorts the X-ray beams. The reconstruction algorithm, however, assumes it has complete and accurate data along every path. It doesn‚Äôt ‚Äúknow‚Äù that some of the information is missing or corrupted.\n\nAs a result, the system produces streaks and bands radiating from the metal. These bright and dark lines are not real anatomical structures ‚Äî they are purely computational artifacts caused by missing or distorted data.\nIn the figure here, the first column shows uncorrected CT images with severe streak artifacts. The last column shows the ground truth, what the anatomy should look like.\n\nResearchers have developed various ways to reduce these artifacts. One common method is NMAR, or normalized metal artifact reduction, which improves the image somewhat. More recently, deep learning methods, such as convolutional neural networks, have been applied to further clean up the image and recover structures hidden by artifacts.\nYou can see that the CNN results are closer to the ground truth, with clearer anatomy and fewer streaks.\n\nThe key point here is that artifacts are not real structures, but they can easily confuse interpretation. They depend on the imaging modality, the reconstruction algorithm, and the clinical situation. That‚Äôs why recognizing artifacts ‚Äî and knowing how to reduce them ‚Äî is so important in medical imaging.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 34, "slide_filename": "Slide34.txt", "slide_annotation": "Lecture 11 - Slide34.txt", "file_path": "Lecture 11\\Texts\\Slide34.txt", "content": "So far, we‚Äôve covered general measures like MSE, KL distance, and SSIM, and then system-specific measures such as noise, resolution, and artifacts.\n\nNow we move to the third and most important part: task-specific measures.\nWhen manufacturers design an imaging system, they often advertise its specifications ‚Äî resolution, noise levels, dose reduction, acquisition speed, and so on. These system specs are useful, but in practice, they don‚Äôt directly answer the most important question: Can the system help us make the right clinical decision?\n\nThink about it. As patients or physicians, we don‚Äôt actually care whether an image has 0.5-millimeter resolution or 1-millimeter resolution, or whether the noise level is 2 percent or 5 percent. What we really care about is whether the image allows us to detect a tumor, diagnose a fracture, or rule out disease with confidence.\n\nThat‚Äôs why task-based assessment is so important. It focuses on the end goal: whether the imaging system can successfully support the clinical task.\n\nSo in this last part of the lecture, we‚Äôll discuss task-specific measures such as sensitivity and specificity, ROC and AUC analysis, human and mathematical observers, and even modern approaches like neural networks and radiomics.\n\nThese measures shift our attention from the imaging device itself to the diagnostic performance of the entire workflow, which is ultimately what matters most in medical imaging.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 35, "slide_filename": "Slide35.txt", "slide_annotation": "Lecture 11 - Slide35.txt", "file_path": "Lecture 11\\Texts\\Slide35.txt", "content": "Here we see a typical scene from a radiology department. A radiologist, with years of training and experience, carefully reviews CT scans and X-rays. Their task is not to admire image sharpness or pixel resolution ‚Äî it is to answer very specific clinical questions:\n\nDoes this smoker have a lung tumor?\nAre the coronary arteries narrowed?\nIs there evidence of disease that requires treatment?\n\nThis highlights why task-specific measures are so important.\nSystem-specific measures ‚Äî like spatial resolution, SNR, or temporal resolution ‚Äî are useful, but they are not the end goal. They only tell us about the technical performance of the imaging system. What really matters is whether the system can support the clinical task: detecting, diagnosing, and guiding treatment decisions.\n\nSo in this part of the lecture, we‚Äôll look at task-specific measures such as sensitivity, specificity, ROC curves, and observer studies, which are all designed to assess performance in clinical terms rather than engineering terms.\n\nThis shift in perspective is critical: we move from asking ‚ÄúHow sharp is this image?‚Äù to asking ‚ÄúCan this image help us detect disease reliably?‚Äù", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 36, "slide_filename": "Slide36.txt", "slide_annotation": "Lecture 11 - Slide36.txt", "file_path": "Lecture 11\\Texts\\Slide36.txt", "content": "Let‚Äôs make this idea of task-specific measures more concrete with a simple example.\n\nImagine you are doing edge detection on an image ‚Äî for instance, outlining the Eiffel Tower. Each pixel can belong to one of two classes: edge or non-edge. Similarly, in medical imaging, each case can be classified as disease present or disease absent.\n\nThis gives us four possible outcomes:\nTrue Positive: The patient has a tumor, and the system or doctor correctly identifies it.\nTrue Negative: The patient is healthy, and the report correctly says no tumor.\nFalse Positive: The patient is healthy, but the system says there is a tumor. This can cause unnecessary anxiety, follow-up scans, or even invasive procedures.\nFalse Negative: The patient does have a tumor, but the system or doctor misses it. This is often the most serious error, because it can delay diagnosis and treatment.\n\nIn clinical practice, both types of errors matter. False positives burden patients emotionally and financially, while false negatives can cost lives by missing opportunities for early treatment.\nThat‚Äôs why doctors and imaging researchers often talk about sensitivity and specificity. Sensitivity reflects how good a system is at detecting true positives ‚Äî catching disease when it is really there. Specificity reflects how good a system is at avoiding false alarms ‚Äî correctly identifying when disease is not present.\n\nThese terms can be confusing at first, but they are central to evaluating diagnostic imaging systems. They shift the focus away from just image sharpness or resolution and toward what really matters: whether the imaging system can support accurate medical decisions.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 37, "slide_filename": "Slide37.txt", "slide_annotation": "Lecture 11 - Slide37.txt", "file_path": "Lecture 11\\Texts\\Slide37.txt", "content": "Now that we‚Äôve defined the four possible outcomes ‚Äî true positives, true negatives, false positives, and false negatives ‚Äî we can formally define sensitivity and specificity.\n\nSensitivity is the proportion of true positives among all actual positive cases. In formula form, it is:\nSensitivity equals TP over TP plus FN.\n\nIn other words, sensitivity answers the question: ‚ÄúIf the patient really has the disease, how likely are we to detect it?‚Äù\nFor example, suppose 100 patients truly have lung cancer. If our system correctly detects 50 of them, but misses the other 50, then the sensitivity is 50%. Sensitivity is essentially a measure of how confident we are when we say ‚ÄúYes, disease is present.‚Äù\nNow let‚Äôs move to specificity.\nSpecificity is the proportion of true negatives among all actual negative cases. The formula is:\nSpecificity equals TN over TN plus FP.\n\nThis answers the question: ‚ÄúIf the patient really does not have the disease, how likely are we to correctly say no?‚Äù\nFor example, if 100 healthy people are scanned, and 95 are correctly reported as negative while 5 are mistakenly labeled as positive, the specificity is 95%. Specificity is essentially a measure of how confident we are when we say ‚ÄúNo, there is no disease.‚Äù\n\nSo, in summary:\nSensitivity tells us how good we are at catching disease cases.\nSpecificity tells us how good we are at avoiding false alarms.\n\nBoth are critical in clinical imaging. High sensitivity ensures we don‚Äôt miss disease, while high specificity ensures we don‚Äôt overwhelm patients with unnecessary worry or treatment.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 38, "slide_filename": "Slide38.txt", "slide_annotation": "Lecture 11 - Slide38.txt", "file_path": "Lecture 11\\Texts\\Slide38.txt", "content": "Now, to make things even more interesting, we introduce two more important concepts: positive predictive value (PPV) and negative predictive value (NPV).\nPositive predictive value, or PPV, tells us: ‚ÄúIf the imaging study says a patient is abnormal, how likely is it that they really have the disease?‚Äù\n\nThe formula is:\nPPV equals True Positives over (True Positives plus False Positives).\nSo, among all the patients labeled as abnormal, PPV is the fraction who actually do have the disease.\nOn the other hand, negative predictive value, or NPV, tells us: ‚ÄúIf the imaging study says a patient is normal, how likely is it that they really are disease-free?‚Äù\n\nThe formula is:\nNPV equals True Negatives over (True Negatives plus False Negatives).\nSo, among all the patients labeled as normal, NPV is the fraction who truly have no disease.\nLet‚Äôs put this in clinical terms. Suppose we are using CT to detect a tumor. If the scan flags a suspicious spot, PPV tells us how likely it is that this really is a tumor and not just a false alarm. If the scan comes back clear, NPV tells us how confident we can be that the patient truly has no tumor.\n\nSo, sensitivity and specificity tell us how the test performs in an abstract sense, relative to ground truth. PPV and NPV tell us what the test result actually means for the patient sitting in front of us.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 39, "slide_filename": "Slide39.txt", "slide_annotation": "Lecture 11 - Slide39.txt", "file_path": "Lecture 11\\Texts\\Slide39.txt", "content": "Here‚Äôs a concrete example using tuberculosis screening with chest X-ray. In this study, nearly 2,000 people were tested, but only 30 actually had tuberculosis.\n\nLet‚Äôs break this down.\nAmong the 30 true TB patients, 22 were correctly reported as positive, while 8 were missed.\nAmong the 1,790 healthy individuals, 51 were incorrectly reported as positive, but the majority ‚Äî 1,739 ‚Äî were correctly classified as negative.\n\nNow, from this table, we can calculate the standard measures:\nSensitivity: 22 out of 30 true TB cases were detected ‚Üí about 73%. This means the system caught almost three-quarters of real cases.\nSpecificity: 1,739 out of 1,790 healthy cases were correctly identified ‚Üí about 97%. This shows the system rarely gave false alarms.\nPositive Predictive Value (PPV): 22 out of 73 reported positives were real TB cases ‚Üí only 30%. This is much lower, meaning that when the system flagged a case as positive, it was wrong more often than it was right.\nNegative Predictive Value (NPV): 1,739 out of 1,747 reported negatives were truly healthy ‚Üí nearly 99.5%. This means a negative result was highly reliable.\nDiagnostic Accuracy: The proportion of all correct results, positive and negative, was about 97%.\nPrevalence: Only 30 out of 1,820 people had TB ‚Üí around 1.6%.\n\nNotice something important here: although accuracy and NPV are very high, the PPV is quite low. That‚Äôs because the disease is rare in this population ‚Äî when prevalence is low, even a small number of false positives can outweigh the true positives.\n\nThis example shows why we need to interpret these metrics carefully, especially in screening programs. Sensitivity, specificity, PPV, NPV, and prevalence all interact, and each tells us something different about how the test performs in real-world conditions.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 40, "slide_filename": "Slide40.txt", "slide_annotation": "Lecture 11 - Slide40.txt", "file_path": "Lecture 11\\Texts\\Slide40.txt", "content": "Now let me introduce one of the most powerful tools in diagnostic performance evaluation: the receiver operating characteristic curve, or ROC curve.\n\nHere‚Äôs the key idea: sensitivity and specificity are not fixed properties of a test. They depend on the threshold we use to decide between ‚Äúdisease‚Äù and ‚Äúno disease.‚Äù\n\nFor example, if I set the threshold very low, I‚Äôll call almost everything abnormal. That means I‚Äôll catch nearly every true case, so sensitivity will be very high. But I‚Äôll also generate many false alarms, so specificity will drop.\nOn the other hand, if I set the threshold very high, I‚Äôll call almost everything normal. That means I‚Äôll have very few false alarms ‚Äî so specificity will be excellent. But I‚Äôll also miss many real cases, so sensitivity will be low.\n\nThe ROC curve captures this trade-off. On the x-axis, we plot 1 ‚Äì specificity, which is the false positive rate. On the y-axis, we plot sensitivity, the true positive rate. By sweeping through all possible thresholds, we trace out the curve.\nThe ROC curve shows the overall performance of the test across all possible decision boundaries. And importantly, the area under the ROC curve, or AUC, provides a single number to summarize diagnostic accuracy. An AUC of 1.0 means perfect classification. An AUC of 0.5 ‚Äî a diagonal line ‚Äî means the test is no better than random guessing.\n\nSo the ROC curve is widely used in medical imaging research, because it provides a clear, quantitative way to compare diagnostic systems and observer performance.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 41, "slide_filename": "Slide41.txt", "slide_annotation": "Lecture 11 - Slide41.txt", "file_path": "Lecture 11\\Texts\\Slide41.txt", "content": "Here‚Äôs another way to look at the ROC curve ‚Äî in terms of true positive fraction and false positive fraction.\n\nThe true positive fraction is simply the sensitivity: among all real disease cases, what fraction we detect correctly.\u000bThe false positive fraction is equal to one minus specificity: among all healthy cases, what fraction we mistakenly call positive.\nWhen we plot sensitivity on the vertical axis against 1 ‚Äì specificity on the horizontal axis, we get the ROC curve.\n\nIf the curve lies along the diagonal dashed line, it means the test is performing no better than random guessing ‚Äî the true positive fraction increases at the same rate as the false positive fraction.\nBut if the curve bends upward toward the top-left corner, that indicates good diagnostic performance: we are capturing many true positives while keeping false positives relatively low.\n\nSo in practice, the more the ROC curve bulges toward the upper left, the better the test or imaging system is at separating disease from non-disease.\nThis is exactly why the ROC framework is so valuable. It captures the trade-off between sensitivity and specificity in a single curve, and it gives us a clear visual and quantitative way to compare different diagnostic systems.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 42, "slide_filename": "Slide42.txt", "slide_annotation": "Lecture 11 - Slide42.txt", "file_path": "Lecture 11\\Texts\\Slide42.txt", "content": "Let‚Äôs imagine the ideal case of diagnosis.\n\nHere we have two groups: on the left, the non-diseased population, shown in red; on the right, the diseased population, shown in green.\nSuppose we measure a certain feature ‚Äî for example, the diameter of a vessel or the size of a tumor. In an ideal world, the distributions of healthy and diseased cases do not overlap at all. Healthy cases always fall into the red range, and diseased cases always fall into the green range.\nWith this perfect separation, we can place a threshold right between the two distributions. Anything to the right is declared diseased, and anything to the left is declared healthy.\n\nIn this situation, the diagnostic test is flawless. There are no false positives and no false negatives. Sensitivity and specificity are both 100%. The ROC curve would go straight up to the top-left corner and across the top ‚Äî a perfect AUC of 1.0.\nOf course, this is rarely the case in real clinical imaging. Features usually overlap between healthy and diseased groups, which makes things more complicated. But this ideal case provides a useful reference point for understanding what we‚Äôre aiming for.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 43, "slide_filename": "Slide43.txt", "slide_annotation": "Lecture 11 - Slide43.txt", "file_path": "Lecture 11\\Texts\\Slide43.txt", "content": "In reality, things are rarely so clear-cut.\n\nMost of the time, the distributions of healthy and diseased patients overlap. That means the same measured value could belong either to a healthy individual or to someone with a disease.\nFor example, suppose we run a blood test or measure a tumor-related biomarker. Healthy patients tend to cluster around one range, while diseased patients cluster around another. But because of biological variation, measurement noise, and overlapping physiology, there is no perfect separation.\n\nSo when the two distributions overlap, a threshold placed in the middle will inevitably create two types of errors. Some diseased patients will fall below the threshold and be misclassified as healthy ‚Äî these are false negatives. Some healthy patients will fall above the threshold and be misclassified as diseased ‚Äî these are false positives.\n\nThis overlap is exactly what forces us to think carefully about where to place the decision threshold. And it‚Äôs what makes tools like the ROC curve so valuable, because they let us analyze the trade-off between sensitivity and specificity across all possible thresholds.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 44, "slide_filename": "Slide44.txt", "slide_annotation": "Lecture 11 - Slide44.txt", "file_path": "Lecture 11\\Texts\\Slide44.txt", "content": "Now, let‚Äôs take our overlapping case and place a conservative threshold, shown by the blue line here.\n\nWith this choice, notice what happens. For diseased patients, only those who fall far to the right of the threshold are called positive. That means many truly diseased patients on the left side are missed. The sensitivity ‚Äî or true positive rate ‚Äî drops to around 50%.\nOn the other hand, for healthy patients, almost all are correctly classified. Only a very small slice on the right is misclassified as diseased. That means the false positive fraction is low, and specificity is high.\n\nSo in ROC space, where we plot sensitivity against 1 minus specificity, this operating point appears toward the lower-left corner. It reflects a cautious decision-making style: you‚Äôre very reluctant to call a disease, so you minimize false alarms. But the trade-off is that you miss a lot of true cases.\nThis illustrates the key idea: where you set the threshold directly determines the balance between sensitivity and specificity.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 45, "slide_filename": "Slide45.txt", "slide_annotation": "Lecture 11 - Slide45.txt", "file_path": "Lecture 11\\Texts\\Slide45.txt", "content": "Now let‚Äôs look at a moderate threshold setting.\n\nThis time, we shift the decision boundary slightly to the left. What happens? More of the diseased distribution now falls to the right of the threshold. That means a greater fraction of patients with the disease are correctly identified. In other words, the sensitivity increases.\nBut the trade-off is clear. By moving the line left, we also capture more of the non-diseased distribution in the red region. That means more healthy patients are incorrectly flagged as diseased. The false positive fraction increases.\n\nOn the ROC curve, you can see this move as a step upward ‚Äî higher sensitivity ‚Äî but also a step to the right ‚Äî higher false positive fraction. In other words, we‚Äôve shifted from the black cross to the yellow cross on this plot.\n\nThis ‚Äúmoderate‚Äù threshold represents a more balanced decision-making strategy: better at catching true cases, but at the cost of more false alarms.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 46, "slide_filename": "Slide46.txt", "slide_annotation": "Lecture 11 - Slide46.txt", "file_path": "Lecture 11\\Texts\\Slide46.txt", "content": "Now let‚Äôs look at an even more aggressive threshold.\n\nBy moving the cutoff further to the left, we dramatically reduce the chance of missing diseased patients. Almost everyone with the disease is now flagged as positive. That means our sensitivity is very high ‚Äî close to 100%.\nBut the price is steep. With the threshold this low, about half of the non-diseased population also falls into the positive range. In other words, the false positive fraction climbs to nearly 50%. Many healthy patients would be told they might have a problem, leading to unnecessary worry and follow-up tests.\n\nOn the ROC curve, this decision strategy places us near the top-right portion ‚Äî very sensitive, but much less specific.\nSo, if you imagine sliding this threshold back and forth, you‚Äôre really tracing out the ROC curve. Different thresholds give you different balances between catching true positives and avoiding false alarms. And as I‚Äôll show you next, the quality of the diagnostic test itself is reflected in how far this ROC curve bends toward the upper-left corner.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 47, "slide_filename": "Slide47.txt", "slide_annotation": "Lecture 11 - Slide47.txt", "file_path": "Lecture 11\\Texts\\Slide47.txt", "content": "Now you see how the entire ROC curve is formed.\n\nAt one extreme, if I simply call every patient ‚Äúpositive,‚Äù I would achieve 100% sensitivity ‚Äî I would never miss a diseased case. But at the same time, I would also reach 100% false positive fraction, meaning every healthy patient is incorrectly flagged.\nAt the other extreme, if I call everyone ‚Äúnegative,‚Äù I would achieve 100% specificity, but sensitivity would drop to zero.\nBy moving the decision threshold step by step, we trace the ROC curve from the lower-left corner, where both sensitivity and false positives are low, toward the upper-right corner.\n\nAn ideal diagnostic system bends sharply upward and hugs the left and top axes, giving an area under the curve, or AUC, close to 1. A poor test, on the other hand, falls close to the diagonal line, which is equivalent to flipping a coin ‚Äî nothing more than random guessing.\n\nSo, the ROC curve captures the complete balance between sensitivity and specificity across all thresholds. And the area under this curve provides a single, powerful number that summarizes diagnostic performance. That‚Äôs why ROC analysis has become such a central tool in medical imaging, in machine learning, and in clinical decision-making.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 48, "slide_filename": "Slide48.txt", "slide_annotation": "Lecture 11 - Slide48.txt", "file_path": "Lecture 11\\Texts\\Slide48.txt", "content": "Now, let‚Äôs step back and think about what diagnostic performance really means.\n\nNot every doctor reads images with the same skill. Some physicians, with years of training and experience, can clearly separate diseased from non-diseased cases. For them, the two distributions hardly overlap, and their ROC curve bends sharply toward the upper-left corner ‚Äî that‚Äôs excellent diagnostic power.\n\nOthers, perhaps junior residents still in training, may not yet recognize subtle features. Their separation is weaker, the overlap is greater, and the ROC curve lies closer to the diagonal. In the extreme case, if you cannot tell disease from normal at all, your decision is no better than flipping a coin ‚Äî that‚Äôs the chance line.\n\nSo diagnostic performance is shaped by two things: the reader‚Äôs skill and the technology‚Äôs power. Better imaging systems reduce overlap, but equally important is the ability of the radiologist to interpret the image correctly.\n\nThat‚Äôs why, in practice, the same scan might lead to different conclusions depending on who reads it. A highly trained radiologist can extract subtle discriminating features that others may miss. This is exactly what the ROC curve captures ‚Äî the combined effect of technology and human expertise.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 49, "slide_filename": "Slide49.txt", "slide_annotation": "Lecture 11 - Slide49.txt", "file_path": "Lecture 11\\Texts\\Slide49.txt", "content": "Now let‚Äôs quantify what we‚Äôve been discussing. The receiver operating characteristic, or ROC curve, gives us a full picture of diagnostic performance across different decision thresholds. But how do we summarize the curve with a single number?\nThat‚Äôs where the area under the ROC curve, or AUC, comes in.\n\nIf the curve falls along the diagonal, the area is 0.5. That means the test has no predictive value ‚Äî you‚Äôre essentially flipping a coin. A perfect test, one that never misses disease and never gives false alarms, would trace along the top and left borders, with an AUC equal to 1. In practice, no system achieves that ideal, because there‚Äôs always some chance of error.\nSo most real diagnostic tests fall somewhere in between. The higher the area under the curve, the better the test is at distinguishing diseased from non-diseased cases.\n\nThis is why AUC has become such a standard benchmark. It condenses all those trade-offs between sensitivity and specificity into a single number. And just like in teaching or training, performance varies ‚Äî some students, or some doctors, do exceptionally well; others struggle. The ROC and its AUC make that difference visible in a very clear, quantitative way.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 50, "slide_filename": "Slide50.txt", "slide_annotation": "Lecture 11 - Slide50.txt", "file_path": "Lecture 11\\Texts\\Slide50.txt", "content": "Here is a real-world example that makes these abstract concepts much more concrete.\nThis plot shows the diagnostic performance of 108 radiologists in the United States, taken from a study by Beam and colleagues. On the vertical axis, you see the true positive fraction ‚Äî in other words, sensitivity. On the horizontal axis, you see the false positive fraction ‚Äî that is, one minus specificity.\n\nEach point here represents a single radiologist‚Äôs performance. And what stands out immediately is how widely they are spread. Some radiologists operate near the upper-left corner, meaning they consistently detect disease with very few false alarms. Others are closer to the diagonal, where their decisions are only slightly better than random guessing.\n\nSo even though all of these doctors are trained professionals looking at the same kinds of images, their diagnostic accuracy varies enormously. That variation has practical consequences. Finding a skilled radiologist can make the difference between catching an early tumor and missing it altogether.\nIt‚Äôs very similar to what we see in education and research. A strong student working with the right mentor can achieve exceptional results, while another, equally intelligent student might struggle under weaker guidance. In both cases, performance isn‚Äôt just about the system ‚Äî it‚Äôs also about the human observer.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 51, "slide_filename": "Slide51.txt", "slide_annotation": "Lecture 11 - Slide51.txt", "file_path": "Lecture 11\\Texts\\Slide51.txt", "content": "Here‚Äôs another striking example, this time from a classic chest film study by Dr. E. James Potchen in 1999.\nWhat you‚Äôre looking at are ROC curves comparing three groups: the top 20 radiologists, the bottom 20 radiologists, and a group of 71 radiology residents.\n\nNotice the clear separation. The top 20 radiologists perform extremely well ‚Äî their ROC curve stays high, close to the upper-left corner. That means they consistently detect abnormalities with very few false alarms.\nIn contrast, the bottom 20 radiologists struggle. Their ROC curve lies much closer to the diagonal, which means their decisions are only a little better than chance.\nAnd then there are the residents. They fall somewhere in between ‚Äî they‚Äôre still training, still developing the skills to interpret subtle features in chest films.\n\nWhat this tells us is that diagnostic performance is not determined by the technology alone. The human factor ‚Äî training, experience, and even natural ability ‚Äî plays a huge role. The same X-ray image can be interpreted very differently depending on who is reading it.\nThis is why task-specific measures are so important. At the end of the day, what really matters is not just whether the imaging system produces a sharp picture, but whether that picture supports accurate clinical decisions.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 52, "slide_filename": "Slide52.txt", "slide_annotation": "Lecture 11 - Slide52.txt", "file_path": "Lecture 11\\Texts\\Slide52.txt", "content": "Now that we‚Äôve seen how radiologists vary in their diagnostic performance, let‚Äôs ask an important question: can we systematically model this decision-making process?\n\nThis brings us to the concept of model observers. A model observer is essentially a mathematical framework or a computational algorithm that simulates the way a human might read an image and make a decision.\nThe idea is motivated by a practical challenge. Human reader studies are expensive, time-consuming, and often inconsistent ‚Äî one doctor may interpret an image differently than another. If we want to optimize imaging systems or evaluate new reconstruction methods, relying on large-scale human studies is not always feasible.\n\nSo instead, we can build a numerical observer that ‚Äúlooks‚Äù at the images and performs the task ‚Äî whether that‚Äôs detecting a small tumor, classifying an abnormality, or distinguishing between signal and noise.\nIn this review paper by Xin He and Subok Park from the FDA, the authors summarize the foundations of model observers in medical imaging research. They describe how these observers can be based on rigorous statistical decision theory, how they can be tailored to mimic human visual performance, and how they‚Äôre applied in practice ‚Äî from system optimization to regulatory science.\n\nIn the next few slides, I‚Äôll introduce you to some of the key types of model observers, starting from the ideal observer, and then moving toward more practical approximations like the Hotelling observer and modern approaches involving machine learning and neural networks.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 53, "slide_filename": "Slide53.txt", "slide_annotation": "Lecture 11 - Slide53.txt", "file_path": "Lecture 11\\Texts\\Slide53.txt", "content": "Before we define model observers in detail, let‚Äôs step back and look at how we can mathematically describe the imaging process itself.\n\nIn most medical imaging systems, the process of acquiring an image can be represented by a very simple but powerful equation:\ng = Hf plus n.\nHere, f represents the object we are trying to image ‚Äî for example, the radioactivity distribution in nuclear medicine or the internal structure of a patient in CT.\n\nThe imaging system is described by H, which acts like a mapping or an operator. You can think of it as a large matrix that tells us how each point in the object contributes to the measured image. For instance, in CT, H would represent all the X-ray projections and how they are combined.\nThen comes n, the noise. Every real imaging system is contaminated by noise ‚Äî whether it‚Äôs photon noise, electronic noise, or other random fluctuations. That uncertainty is inherent to the physics of measurement.\n\nFinally, g is what we actually record ‚Äî the image data. So what you see on the screen is not the object itself, but rather the result of the object being transformed by the system response and corrupted by noise.\nThis linear system model is the foundation for both image reconstruction and image analysis. It‚Äôs also where model observers come in: they take this same mathematical description and use it to predict how well a system can perform a diagnostic task.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 54, "slide_filename": "Slide54.txt", "slide_annotation": "Lecture 11 - Slide54.txt", "file_path": "Lecture 11\\Texts\\Slide54.txt", "content": "Now let‚Äôs take the imaging model and apply it to a very common clinical task ‚Äî binary classification.\nIn medicine, this usually means asking a simple yes-or-no question. For example, does this patient have a tumor, or not?\n\nMathematically, we describe this with two competing hypotheses.\nThe first hypothesis says the image only contains the background ‚Äî in other words, normal anatomy ‚Äî plus noise.\nThe second hypothesis says the image contains both the background and an additional signal, such as a tumor, again with noise added.\n\nHere, we think of the background as the normal structures in the body, and the signal as the diagnostic feature we‚Äôre trying to detect. Depending on the situation, we might know exactly what that signal looks like, or only know its general statistical properties. For example, in phantom studies, we know exactly what pattern is inserted, but in real patients, tumors can vary in shape, size, and contrast.\nTo simplify the discussion, we often imagine a ‚Äúclean‚Äù background image without noise, and a ‚Äúclean‚Äù signal image without noise. The actual measurement we record is just those two components, combined with noise from the imaging process.\n\nSo in essence, the classification task is: given the measured image, do we believe it came from the background-only case, or from the background-plus-signal case?\nThis very simple framework ‚Äî background versus background plus signal ‚Äî forms the basis of many observer models in medical imaging.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 55, "slide_filename": "Slide55.txt", "slide_annotation": "Lecture 11 - Slide55.txt", "file_path": "Lecture 11\\Texts\\Slide55.txt", "content": "Now, let‚Äôs introduce the idea of the ideal observer.\nIn the context of binary classification, the ideal observer is defined as the one who makes use of all the statistical information available in order to maximize task performance. In other words, it‚Äôs the absolute gold standard ‚Äî the best decision-maker you could have, whether human or machine.\n\nHow does it work? Imagine you have an image, which we‚Äôll call g. For this image, there are two possible explanations:\nHypothesis zero says it comes from a normal case, with no abnormality.\nHypothesis one says it comes from an abnormal case, where a signal, such as a tumor, is present.\n\nThe ideal observer looks at both possibilities and asks: given the data I see, how likely is it that it came from the normal case, and how likely is it that it came from the abnormal case?\nThe decision is then based on whichever likelihood is greater. If the abnormal case is more likely, the observer calls it abnormal; if the normal case is more likely, the observer calls it normal.\n\nThis simple but powerful rule ‚Äî often called the likelihood ratio test ‚Äî provides an upper bound on performance. No other observer, whether it‚Äôs another model or even an expert human reader, can systematically do better.\nThat‚Äôs why the ideal observer always produces the highest possible ROC curve for the task. It gives us a theoretical benchmark ‚Äî a way to measure how close or how far real systems and real doctors are from perfection.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 56, "slide_filename": "Slide56.txt", "slide_annotation": "Lecture 11 - Slide56.txt", "file_path": "Lecture 11\\Texts\\Slide56.txt", "content": "The ideal observer, as we just discussed, requires complete statistical knowledge of the problem. But in practice, we almost never have that luxury. So, instead, we turn to simplified models.\n\nOne of the most important of these is the Hotelling Observer.\nThe idea is straightforward: rather than using all the complex statistics of the image, we take the image data and apply a linear template to it. In other words, we reduce the entire image into a single number ‚Äî a test statistic. That number is then used to decide whether the case is more likely normal or abnormal.\n\nThe Hotelling Observer is designed to be the best possible linear observer. What does ‚Äúbest‚Äù mean here? It means the observer chooses its template in a way that maximizes the signal-to-noise ratio ‚Äî the separation between the diseased and non-diseased cases. The larger the separation, the easier the decision becomes.\nThis is why the Hotelling Observer is so widely studied. It strikes a balance: not as powerful as the ideal observer, but still mathematically optimal within the class of linear decision rules.\n\nAnd if we don‚Äôt even want to use all the image data, we can go a step further. We might only use selected features or specific channels of the image, instead of the full dataset. That gives us the Channelized Hotelling Observer ‚Äî a practical simplification that still works well in many real-world medical imaging problems.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 57, "slide_filename": "Slide57.txt", "slide_annotation": "Lecture 11 - Slide57.txt", "file_path": "Lecture 11\\Texts\\Slide57.txt", "content": "Now, let‚Äôs talk about what is called the channelized observer. The idea here is to simplify the problem of analyzing very high-dimensional image data.\n\nInstead of working with every single pixel value, which can be overwhelming, we break the image down into a set of channels. Each channel acts like a filter or a template that extracts a particular aspect of the image. Think of them as different ‚Äòviews‚Äô or ‚Äòfeatures‚Äô of the same data.\nWhen we apply these channels to the image, we get a series of scalar responses ‚Äî simple numbers that summarize how the image looks under each channel. We then collect these responses into a much smaller vector, which is far easier to work with than the original full image data.\nThis is what‚Äôs known as a channelized observer. Reducing the dimensionality allows us to perform statistical analysis more efficiently, while still retaining the essential information needed to judge image quality or detect a signal.\n\nSo in short, channelized observers strike a balance: they simplify the computational task but still capture the critical diagnostic features", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 58, "slide_filename": "Slide58.txt", "slide_annotation": "Lecture 11 - Slide58.txt", "file_path": "Lecture 11\\Texts\\Slide58.txt", "content": "To make the idea of channelized observers more concrete, let me give you an example.\n\nHere you see four channels illustrated both in the frequency domain, across the top row, and in the spatial domain, across the bottom row. Each channel acts like a filter. In the frequency domain, these filters look like concentric rings that select specific frequency ranges. When we transform them back into the spatial domain, they appear as blurred or oscillatory patterns.\nWhy do we do this? Because instead of trying to analyze the entire frequency spectrum all at once, we divide it into pieces. Each channel captures information from one part of the spectrum. When we apply these filters to an image, we can see how strongly the image responds to each band of frequencies.\n\nThis is very powerful because medical images contain structures at many different scales ‚Äî some large and smooth, others fine and detailed. Using channels allows us to separate these scales and study them systematically.\nSo, these four channels are just one example, but in practice, you could design more channels depending on how much detail you want to capture.‚Äù", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 59, "slide_filename": "Slide59.txt", "slide_annotation": "Lecture 11 - Slide59.txt", "file_path": "Lecture 11\\Texts\\Slide59.txt", "content": "Now let‚Äôs move into the topic of radiomics, which is an exciting area in modern medical imaging.\nRadiomics is all about turning images into data. Instead of relying only on what a radiologist can see with the naked eye, we extract hundreds or even thousands of quantitative features from medical images.\n\nFor example, once we outline a tumor on an image, we can study different categories of features:\nShape features tell us about the geometry ‚Äî is the tumor round, irregular, or elongated?\nIntensity features describe how bright or dark the pixels are, and how their distribution looks in a histogram.\nTexture features capture the patterns inside the tumor, such as whether it looks smooth, coarse, or heterogeneous. These can be computed using statistical methods like gray-level co-occurrence matrices or run-length matrices.\n\nAfter we collect these features, we don‚Äôt stop there. The next step is to feed them into a prediction model. This model may perform tasks like selecting the most important features, classifying patients into groups, or predicting outcomes such as how well a patient will respond to treatment.\n\nSo radiomics really bridges imaging with data science. It takes us beyond the visual impression and allows us to mine the hidden information in images, often leading to insights that are not visible to the human eye.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 60, "slide_filename": "Slide60.txt", "slide_annotation": "Lecture 11 - Slide60.txt", "file_path": "Lecture 11\\Texts\\Slide60.txt", "content": "Up to this point, we have mostly talked about linear observers, such as the Hotelling observer and its channelized version. But in reality, many problems in medical imaging are nonlinear.\n\nTake the XOR problem as a classic example. If you try to separate the inputs using a simple straight line, you quickly find it cannot be done. Linear observers fail in such situations because the data are not linearly separable.\n\nTo overcome this, we use nonlinear observers, often built using neural networks. Here, you see a simple network with two input variables, two hidden units, and one output. By combining the information in a nonlinear way, the network can correctly separate the classes.\nThis idea is powerful for medical imaging tasks. Images are complex ‚Äî disease and non-disease patterns may overlap, and linear models are not enough to make accurate decisions. Neural networks, however, can learn complicated nonlinear boundaries, allowing them to detect subtle patterns and interactions that traditional methods miss.\n\nSo nonlinear observers represent the next step: moving from simple linear discrimination toward flexible, data-driven models that can capture the true complexity of biological signals.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 61, "slide_filename": "Slide61.txt", "slide_annotation": "Lecture 11 - Slide61.txt", "file_path": "Lecture 11\\Texts\\Slide61.txt", "content": "Supervised learning builds on this idea by training models directly on labeled data. For example, in the XOR problem, linear classifiers cannot draw a single straight line to separate the classes. But by introducing hidden layers and nonlinear activation functions, a neural network can learn to separate the classes correctly.\n\nThe figure shows how the inputs are transformed step by step through hidden units and weights, leading to correct outputs for all training examples. This principle extends far beyond toy problems‚Äîit is the basis of modern deep learning, where very large networks can learn hierarchical features from medical images and achieve state-of-the-art performance.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 62, "slide_filename": "Slide62.txt", "slide_annotation": "Lecture 11 - Slide62.txt", "file_path": "Lecture 11\\Texts\\Slide62.txt", "content": "Here we see a fuzzier, more realistic version of the XOR problem. Instead of perfectly separable points, the data is noisy and overlaps. This is much closer to what we encounter in medical imaging, where disease and non-disease cases are not cleanly separated but instead distributed with overlaps.\n\nThe nonlinear classifier can still learn a curved boundary that separates the classes reasonably well. This illustrates why nonlinear observers and machine learning are crucial: they can adapt to complex, messy data, rather than relying on oversimplified linear rules.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 63, "slide_filename": "Slide63.txt", "slide_annotation": "Lecture 11 - Slide63.txt", "file_path": "Lecture 11\\Texts\\Slide63.txt", "content": "Deep radiomics takes things one step further. Instead of manually extracting features, we use convolutional neural networks (CNNs) to learn them directly from the image data.\n\nIn this workflow, random patches from images are used to train a CNN to recognize tumor regions. Multiple layers of convolution and pooling extract increasingly abstract features, from simple edges to complex patterns. The responses from the deep layers can then be aggregated across the whole image, forming high-dimensional feature maps.\n\nThese learned features are then encoded and passed to classifiers for prediction tasks, such as distinguishing between tumor subtypes or predicting patient outcomes. Deep radiomics represents the cutting edge‚Äîcombining the statistical rigor of radiomics with the power of deep learning to capture complex image biomarkers.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 11", "slide_number": 64, "slide_filename": "Slide64.txt", "slide_annotation": "Lecture 11 - Slide64.txt", "file_path": "Lecture 11\\Texts\\Slide64.txt", "content": "Finally, here is your homework. You will use the MATLAB code available online to compute the Structural Similarity Index, or SSIM, between two images. SSIM is a perceptual metric that compares luminance, contrast, and structure.\n\nYour task is two-fold. First, compute SSIM for a pair of images‚Äîeither the ones shown here or another pair of your choice. Second, design an example where sensitivity is 90% and specificity is 80%, and show your calculations.\n\nThis exercise is meant to reinforce your understanding of both image quality metrics and diagnostic performance measures. The due date is one week later, so please manage your time accordingly.", "total_slides_in_lecture": 64}
{"lecture": "Lecture 12", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 12 - Slide1.txt", "file_path": "Lecture 12\\Texts\\Slide1.txt", "content": "Last time, we talked about image quality assessment.\u000bToday, we begin a new part of the course ‚Äî focusing on imaging modalities. And the very first modality we will discuss is X-ray imaging.\nBefore we dive in, let me share a quick study tip. When you are learning a subject as complex as medical imaging, it is really important to build good habits: preview the material before class, review it after class, and pay special attention to the key ideas. I will repeat important concepts as needed to reinforce your understanding.\n\nEarlier in the course, I showed you a poster that summarized the foundation of medical imaging. I also created a similar poster for my graduate-level class, and students really liked it. These visual maps help connect the big picture with the details.\nTools like this can make complicated subjects easier to follow.\n\nSo with that in mind, let‚Äôs move forward and begin our journey into X-ray radiography.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 12 - Slide2.txt", "file_path": "Lecture 12\\Texts\\Slide2.txt", "content": "we are right on schedule, and today marks the beginning of our detailed study of imaging modalities.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 12 - Slide3.txt", "file_path": "Lecture 12\\Texts\\Slide3.txt", "content": "At the end of the course, you can create a summary poster for X-ray. It would be a nice exercise to build your own poster that highlights the main points of each modality. Of course, the poster I made for my graduate class goes much deeper, with governing equations. But here, for the undergraduate level, our goal is to provide a clear and introductory exposure.\n\nSo today, let me give you an outline. I think it‚Äôs always good to start with a roadmap, so you know where we are heading. For X-ray imaging, I‚Äôll use this outline format to guide our discussion.\nWe will cover three fundamental aspects: the source, the attenuation, and the detector. This sequence is very logical. First, you need an X-ray source. Without a source, nothing begins ‚Äî it‚Äôs like trying to cook without rice. Next, those X-rays must interact with the human body. If there is no interaction, the photons just pass through like air, and we gain no information. If the body absorbs all the X-rays completely, again, no information escapes. The useful case is partial absorption: different tissues absorb X-rays to different degrees. That difference creates contrast, and contrast is what carries the diagnostic information.\n\nFinally, we need a detector to capture that information. The X-ray photons that escape the body must be recorded so that we can reconstruct an image. So in summary, the imaging chain has three main parts: the source, the interaction with the body, and the detector. Together, they form the foundation of X-ray radiography.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 12 - Slide4.txt", "file_path": "Lecture 12\\Texts\\Slide4.txt", "content": "The story of X-ray discovery is actually a wonderful example of serendipity in science. In 1895, Wilhelm Conrad R√∂ntgen was experimenting with a cathode ray tube ‚Äî essentially an early electron gun ‚Äî when he noticed something unexpected. A nearby fluorescent screen began to glow, even though the tube was enclosed in a box. By accident, he had discovered a new kind of invisible ray that could pass through many opaque materials. He called it the ‚ÄúX-ray,‚Äù where the letter X simply meant ‚Äúunknown.‚Äù\n\nOne of the first images he captured was of his wife‚Äôs hand, showing her bones and even her wedding ring. That single picture immediately revealed the enormous potential of this discovery for medicine.\nR√∂ntgen‚Äôs work was recognized worldwide, and in 1901, he was awarded the very first Nobel Prize in Physics.\nSo from a chance observation in the laboratory came one of the most important tools in medical diagnosis ‚Äî the X-ray.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 12 - Slide5.txt", "file_path": "Lecture 12\\Texts\\Slide5.txt", "content": "R√∂ntgen made his discovery while experimenting with a cathode tube. At that time, he didn‚Äôt know what was happening ‚Äî nobody did. He placed the tube inside a box, and unexpectedly, a nearby fluorescent screen lit up. When he looked closer, he noticed that objects could block or shape this invisible radiation. For example, a metallic cross placed in front of the tube cast a clear shadow on the screen. This was something completely new ‚Äî it couldn‚Äôt be explained without assuming that some kind of invisible physical rays were being produced.\n\nBecause the nature of these rays was unknown, he called them ‚ÄúX-rays.‚Äù Later, they became known as R√∂ntgen rays, in honor of his discovery.\nThis breakthrough was so important that R√∂ntgen was awarded the very first Nobel Prize in Physics. Today, of course, we understand X-rays in much greater detail, but the original observation was truly remarkable for its time.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 12 - Slide6.txt", "file_path": "Lecture 12\\Texts\\Slide6.txt", "content": "X-rays are a special type of electromagnetic radiation. They occupy a specific portion of the electromagnetic spectrum.\n\nThe part of the spectrum we are most familiar with is visible light, because we can see it with our eyes. But just because we cannot see other forms of electromagnetic radiation does not mean they don‚Äôt exist. Think about your smartphone. You don‚Äôt see the electromagnetic waves, but you constantly receive emails, messages, and photos from anywhere in the world. These are all carried by electromagnetic radiation.\nElectromagnetic waves are beautifully described by Maxwell‚Äôs equations. They are sinusoidal waves traveling through space, with an electric field and a magnetic field that are perpendicular to each other. Together, they form the foundation for how light and all electromagnetic radiation behave.\n\nNow, across this wide spectrum, different portions can be used for different purposes. For example, X-rays are used for radiography and CT imaging. Gamma rays are used in nuclear imaging, like PET and SPECT. Infrared and visible light are used for optical imaging and pathology. Radio waves and microwaves are used in magnetic resonance imaging. So, depending on the wavelength and frequency, each portion of the spectrum serves a different role in medicine.\n\nTo describe the energy of electromagnetic radiation, we use a fundamental relationship:\u000bE equals h times f, which is also equal to h c over lambda.\nHere, h is Planck‚Äôs constant, f is frequency, c is the speed of light, and lambda is the wavelength. From this equation, you can see that energy is directly proportional to frequency and inversely proportional to wavelength.\n\nIn medical imaging, we work with very small wavelengths, usually measured in nanometers and angstroms. One nanometer is 10 to the power of minus 9 meters. One angstrom, written as a small circle above the letter A, is even smaller ‚Äî 10 to the power of minus 10 meters.\nSo this slide shows you clearly where X-rays fit in the electromagnetic spectrum, and why their energy levels make them so useful for medical imaging.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 12 - Slide7.txt", "file_path": "Lecture 12\\Texts\\Slide7.txt", "content": "If we narrow down within the electromagnetic spectrum, we can clearly see where X-rays sit. Their wavelengths are extremely small, ranging roughly from one-tenth of a nanometer up to ten nanometers. Within this narrow window, we classify radiation as X-rays.\n\nBut not all X-rays are the same. When the frequency is higher ‚Äî and remember, higher frequency means higher energy ‚Äî we call them hard X-rays. When the frequency is lower, corresponding to longer wavelengths, the energy is weaker, and we call them soft X-rays.\n\nDifferent kinds of X-rays are useful for different applications. For example, in mammography, we use relatively soft X-rays because we want sensitivity to soft tissue structures. But if we want to penetrate thicker parts of the body or denser tissues like bone, we need hard X-rays, with energies that can go above 100 kilo-electron-volts.\n\nIn non-medical settings, even higher-energy X-rays are used. For example, in airport security screening, luggage may be very thick or contain dense objects. In those cases, stronger X-rays are required; otherwise, all the radiation would be absorbed, and no useful information would come out. So, depending on the wavelength and energy, X-rays can be tuned for very different purposes ‚Äî from delicate imaging in medicine to powerful scanning in security.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 12 - Slide8.txt", "file_path": "Lecture 12\\Texts\\Slide8.txt", "content": "We saw this briefly back in the first lecture, but let‚Äôs look again. This is a standard X-ray radiograph ‚Äî in this case, a chest X-ray. It‚Äôs a single two-dimensional projection of the body, captured on film or in digital form.\n\nWhat you see here is everything superimposed: the heart, the lungs, the ribs ‚Äî all layered on top of each other in the same image. Because of that overlap, radiography is not always the clearest view. Still, it provides valuable insight into the human body.\nFor example, if a patient has a broken bone or if there is a foreign object like a bullet, it shows up immediately. That‚Äôs why radiography is so important in emergency rooms ‚Äî it‚Äôs fast, widely available, and gives doctors a quick look inside.\n\nBut keep in mind, this is only a projected view. Moving from projection imaging, like radiography, to cross-sectional imaging, like CT, was a huge leap forward in the field of medical imaging.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 12 - Slide9.txt", "file_path": "Lecture 12\\Texts\\Slide9.txt", "content": "Here we have X-ray computed tomography, or CT. Unlike a simple radiograph, CT allows us to see a clear cross-sectional view of the body. With CT, the resolution can be extremely fine, down to about 300 microns, which is three-tenths of a millimeter. That means we can visualize very small structures and details inside the body.\n\nHow do we actually make a cross-sectional image from X-rays? That is the central question of CT, and it will be the focus of our next lecture. For now, I just want to give you a sense of how powerful this technology is.\nWhen you see images like this, you realize how useful X-ray imaging can be. Hopefully, this motivates you to dig deeper ‚Äî to understand how the X-ray source, the detectors, and the reconstruction process all work together to extract as much information as possible from the human body.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 12 - Slide10.txt", "file_path": "Lecture 12\\Texts\\Slide10.txt", "content": "Looking into the future, X-ray imaging may become far more portable and accessible. Imagine carrying a powerful but compact X-ray device ‚Äî perhaps even something integrated into a smartphone. It may sound futuristic, but nothing is stopping us from thinking outside the box.\n\nOne open question is how to make X-ray imaging as cheap and convenient as possible. Believe it or not, there are papers suggesting that even something as simple as peeling Scotch tape can generate X-rays under certain conditions. If you‚Äôre curious, you can look up this fascinating idea.\nSo while today we use large, expensive machines, the future may bring surprising and creative ways to generate and use X-rays. New ideas like these could turn out to be very important for both medicine and technology.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 12 - Slide11.txt", "file_path": "Lecture 12\\Texts\\Slide11.txt", "content": "You may remember this TED-Ed video, ‚ÄúHow X-rays See Through Your Skin,‚Äù which I showed you earlier. When I first shared it, it already had a large number of views, and since then, the count has only grown. That tells you how many people around the world are curious about the history and the science of X-rays.\n\nIt‚Äôs a short, engaging explanation of how X-rays work, and I encourage you to watch it again after today‚Äôs lecture. With the background you now have, you may notice details you didn‚Äôt fully appreciate before. Revisiting resources like this, once you have more context, is a great way to deepen your understanding.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 12 - Slide12.txt", "file_path": "Lecture 12\\Texts\\Slide12.txt", "content": "Medical X-rays are essential. Techniques such as plain radiographs and CT scans are the backbone of modern radiology. In fact, some of my colleagues and mentors often say: if you had to pick just one imaging modality as the cornerstone of modern medicine, it would be X-rays. If you take away X-ray imaging, hospitals simply cannot function effectively. If you take away any other modality, they can still manage, as long as X-rays remain available.\n\nThere are many reasons for this. X-ray imaging is relatively cheap and widely available. It provides high resolution and fast speed, which are crucial in clinical practice. Another advantage, and one I want to emphasize, is its high geometric accuracy.\n\nWhat does that mean? Think about MRI: it produces beautiful images, but sometimes those images are geometrically distorted. A tumor might appear shifted, much like how a pencil looks bent when you place it in a glass of water. If a surgeon were to rely on that distorted MRI image without correction, they might target the wrong spot. CT and X-ray imaging, by contrast, give you precise geometric accuracy. This accuracy allows us to deliver radiation beams for therapy or guide surgical procedures with confidence.\n\nX-ray imaging is also both sensitive and specific. Sensitivity means that if the disease is present, the imaging will detect it. Specificity means that if the disease is not present, the imaging will not falsely indicate it. So in many cases, X-rays provide a very reliable diagnostic tool. And even when they cannot fully answer the question, they guide us toward which additional imaging modality to use next.\n\nOf course, there are downsides. The most important is that X-rays are a form of ionizing radiation, which means there is some risk. But if the radiation dose is controlled, the risk is manageable. To put it in perspective, a typical CT scan delivers a dose roughly comparable to what you would receive on an international flight, for example, from the United States to China. If you are comfortable taking that trip, then you should not worry too much about a single CT scan. Still, we always work hard to minimize the dose, and over the past decade, remarkable progress has been made in reducing it ‚Äî while maintaining high diagnostic quality.\n\nAnother limitation is that X-rays have poor contrast for soft tissue. Structures like the brain, liver, or tumors are not as clearly distinguished as they are with MRI. In many cases, we need to inject contrast agents to improve visibility.\nFinally, from a global perspective, the medical X-ray market is huge and continues to grow ‚Äî billions of dollars worldwide, with steady expansion each year. This reflects not only how important X-rays are today, but also how much innovation continues in this field.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 12 - Slide13.txt", "file_path": "Lecture 12\\Texts\\Slide13.txt", "content": "Medical X-ray imaging generally comes in two main forms.\n\nOn the left, you see the simpler and more cost-effective version: a single projection image, like a chest X-ray. It captures one view at a time, and everything inside the body is superimposed together in that picture.\nOn the right, the principle is extended. At any single instant, you still record just one projection. But instead of stopping there, the X-ray source and detector rotate around the patient. At the same time, the patient table can also move forward or backward. By acquiring projections from many different angles, we collect enough information to reconstruct detailed cross-sectional slices. When these slices are stacked together, they form a volumetric image of the entire body.\n\nSo, while projection radiography gives us a fast overview, computed tomography transforms those multiple views into a three-dimensional representation ‚Äî a much richer source of information.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 12 - Slide14.txt", "file_path": "Lecture 12\\Texts\\Slide14.txt", "content": "Here is another common application of X-ray imaging. In addition to medicine, you encounter X-rays almost every time you go through an airport. Large machines like this are used routinely to screen luggage.\n\nX-ray imaging in this context is designed to detect dangerous items such as explosives or weapons. It has become a standard part of modern security systems, and most of us are so accustomed to it that we hardly think about the technology at work.\nSo, this concludes our general introduction. At this point, you should have a clear idea of what X-rays are, where they fall in the electromagnetic spectrum, and some of their major practical uses ‚Äî in both healthcare and security.\n\nNow we move on to the more detailed and important part of today‚Äôs lecture: the X-ray imaging chain ‚Äî the source, the interactions, and the detector.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 12 - Slide15.txt", "file_path": "Lecture 12\\Texts\\Slide15.txt", "content": "Now, once you have both the X-ray source and the detector, you can build an imaging system. Simply put, if you hold them together in the right configuration, you can start performing medical X-ray imaging.\n\nAt the heart of the system is the X-ray tube. This is the device that actually generates the X-rays. We will look at its working principle in more detail, but for now, just remember: the X-ray tube is the source that makes the entire imaging chain possible.\n\nSo, the first step in understanding X-ray imaging is to understand the X-ray source.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 12 - Slide16.txt", "file_path": "Lecture 12\\Texts\\Slide16.txt", "content": "Now let‚Äôs take a closer look at how an X-ray tube works, because this is the heart of the X-ray source. The figure you see here illustrates the essential components.\n\nAt one end, we have a filament, which is usually made of tungsten wire. The filament is coiled so that it has more surface area, and when we pass a strong current through it, the wire becomes extremely hot. This heating process frees up electrons ‚Äî they become ‚Äúboiled off‚Äù from the surface of the filament.\nNext, we apply a very high voltage potential between the filament, which serves as the cathode, and the target, which is called the anode. The anode is also made of tungsten. The electric field between the cathode and the anode accelerates the electrons at very high speeds toward the tungsten target.\n\nWhen these high-energy electrons strike the anode, their interactions with the tungsten atoms produce X-rays. This process also generates an enormous amount of heat. In fact, more than 99 percent of the energy is converted into heat, and less than one percent actually becomes X-rays.\nManaging this heat is a major engineering challenge. The inside of the tube is kept under a high vacuum so that electrons can move freely without colliding with air molecules. But even in a vacuum, the tungsten target would quickly overheat if it were not carefully designed. The solution is to use a rotating anode. By spinning the target rapidly, the electron beam does not strike the same spot continuously. Instead, the heat is spread over a larger area, preventing the tungsten from melting.\n\nModern X-ray tubes are sealed inside durable glass or ceramic housings and are manufactured with extremely high precision. My colleagues at GE Global Research, for example, have worked on these tubes for many years, refining the design step by step. While the basic principle has remained the same since R√∂ntgen‚Äôs time, continuous improvements have made the tubes more efficient, more powerful, and capable of delivering the high flux of X-rays needed in today‚Äôs imaging systems.\n\nSo in summary: heat the filament, release electrons, accelerate them with high voltage toward a tungsten anode, and X-rays are generated at the point of impact.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 12 - Slide17.txt", "file_path": "Lecture 12\\Texts\\Slide17.txt", "content": "Now, let‚Äôs ask the question: why is it that an X-ray tube can actually generate X-rays? The answer lies in two main physical mechanisms. This is what I call our first ‚Äúred diamond,‚Äù or key concept.\n\nThe first mechanism is called Bremsstrahlung radiation, a German word meaning ‚Äúbraking radiation.‚Äù Here‚Äôs what happens: a primary electron beam, accelerated toward the target, passes close to the nucleus of a tungsten atom. Because the nucleus is positively charged and the electron is negatively charged, there is an attractive force. This force bends the trajectory of the electron. Whenever a charged particle is accelerated or decelerated, it gives off radiation. In this case, that radiation is X-rays. Since the amount of bending can vary, the emitted X-rays span a continuous range of energies. That‚Äôs why Bremsstrahlung is also called continuous radiation.\n\nThe second mechanism is called characteristic radiation, sometimes explained in terms of the photoelectric effect. In this case, a high-energy primary electron collides with an inner-shell electron of the tungsten atom and kicks it out of its orbit. That leaves a vacancy in the inner shell. An outer-shell electron then drops down to fill the gap. The energy difference between the two shells is released in the form of an X-ray photon. Because the energy levels of atomic shells are discrete, the emitted X-ray has a very specific energy ‚Äî a ‚Äúcharacteristic‚Äù value that depends on the target material.\n\nSo, to summarize: Bremsstrahlung gives us a broad, continuous X-ray spectrum, while characteristic radiation adds sharp peaks at specific energies. Together, these two processes explain how an X-ray tube produces the mixture of radiation that we use for imaging.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 12 - Slide18.txt", "file_path": "Lecture 12\\Texts\\Slide18.txt", "content": "Now that we‚Äôve discussed how X-rays are generated, let‚Äôs look at the spectrum they produce.\nFirst, notice the broad, continuous curve ‚Äî this comes from Bremsstrahlung radiation. You can see that at lower photon energies, the intensity is relatively high, and as the energy increases, the intensity decreases. However, the very lowest-energy X-rays don‚Äôt actually appear in the output, because they are absorbed inside the tungsten target itself. Since the radiation is produced beneath the surface, the tungsten material acts as a natural filter, removing the very soft X-rays before they can escape.\n\nNext, at higher energies ‚Äî around 70 kilo-electron-volts and above, for tungsten ‚Äî you start to see sharp peaks. These are the characteristic X-rays, labeled here as K-alpha and K-beta. They occur only when the incoming electron energy is high enough to knock out inner-shell electrons, allowing outer electrons to fall into those vacancies and release very specific amounts of energy.\nThe maximum X-ray energy you can obtain is determined by the tube voltage. For example, if the tube is operated at 120 kilovolts, then the maximum possible photon energy is 120 kilo-electron-volts. However, those very high-energy photons are less common compared to the middle range.\n\nFinally, the intensity of the spectrum ‚Äî meaning the number of X-ray photons produced ‚Äî depends strongly on the tube current. A higher current means more electrons striking the target, which produces more X-ray photons across the spectrum.\nSo the shape of the X-ray spectrum is governed by three key factors: the filtering effect of the target, the discrete characteristic peaks, and the limits set by the tube voltage and current.\nAnd keep in mind ‚Äî this will also connect directly to one of your homework problems, so be sure to review it carefully.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 12 - Slide19.txt", "file_path": "Lecture 12\\Texts\\Slide19.txt", "content": "This slide, taken from your textbook, shows some important parameters of the X-ray tube.\n\nLet‚Äôs start with the cathode. In practice, the cathode is often shaped in a particular way and charged so that it helps to focus the electron beam. Sometimes, additional focusing coils are also placed in the middle region. The purpose is always the same ‚Äî to narrow the electron beam and concentrate it onto a small area of the anode. That small region becomes the X-ray focal spot.\n\nWhy is the focal spot size important? If the spot is too large, X-rays are emitted from a broad area, and the resulting image becomes blurry, similar to motion blur in photography. A smaller focal spot, on the other hand, produces much sharper images.\n\nThere‚Äôs also a clever design trick here called the line-focus principle. The anode surface is angled at some angle, theta. The electrons strike the sloped surface, and X-rays are emitted. Because of the geometry, the apparent or effective focal spot size is equal to the actual size times the sine of theta. This way, you can achieve a smaller effective focal spot while still spreading the heat over a larger physical area. It‚Äôs a neat compromise between sharp imaging and thermal management.\n\nFinally, the field of view, or coverage, depends on the distance from the focal spot to the patient. As the patient moves farther away, the coverage widens. The maximum coverage is limited by the anode angle.\nSo, to summarize: focusing gives us a sharp spot, the angled anode helps reduce blur while managing heat, and the geometry of the tube determines how wide a field of view we can capture.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 12 - Slide20.txt", "file_path": "Lecture 12\\Texts\\Slide20.txt", "content": "We‚Äôve now reviewed the conventional X-ray tube, its parameters, and how it works. Let me briefly mention a new type of X-ray tube, which represents a more advanced and expensive technology.\nInstead of using a solid tungsten anode, this design uses a liquid metal anode. A thin jet of liquid metal flows continuously downward. An electron beam, generated in a way similar to what we discussed before, strikes the liquid metal stream. The interaction between the electrons and the liquid target produces X-rays.\n\nThe advantage of this design is that the liquid metal can absorb and dissipate heat much more effectively than a solid target. As a result, the system can generate a much brighter X-ray beam ‚Äî higher intensity and higher flux ‚Äî without destroying the anode.\nThese tubes are very costly, often priced in the range of hundreds of thousands of dollars. We won‚Äôt test you on the details of this system, but it‚Äôs good to be aware of such innovations. They illustrate how researchers continue to push the limits of X-ray technology, refining designs to achieve better performance.\n\nI should also add a deeper point for you to think about. When we describe Bremsstrahlung radiation, it may sound puzzling: electrons are attracted to the nucleus, so at one stage they seem to gain energy, while at another stage they lose energy. How can net radiation be produced if these processes are symmetric? The key is that the trajectory of the electron is bent. That bending means the electron is accelerated in a new direction, and acceleration of charge always produces radiation. This is the fundamental physical basis ‚Äî conservation laws and electromagnetic theory together explain why X-rays are emitted.\n\nSo, while the equations provide a clear description, it‚Äôs important to connect them with the underlying physics. That‚Äôs what allows us to understand not just how X-rays are generated, but also how new designs, like this liquid metal tube, can open the door to brighter and more efficient X-ray sources.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 12 - Slide21.txt", "file_path": "Lecture 12\\Texts\\Slide21.txt", "content": "By now, you understand how X-rays are generated. That knowledge positions you well, not only for deeper research in this field but also for practical discussions, even in job interviews.\n\nNow we move to the next important part of the imaging chain ‚Äî X-ray attenuation. Once X-rays are produced by the tube, or in some cases by a synchrotron radiation source, we use them to probe an object or a patient. As the X-rays travel through matter, some of them are absorbed, some are scattered, and some pass through with little interaction.\n\nIt‚Äôs in this process ‚Äî the way X-rays are attenuated by different tissues or materials ‚Äî that valuable diagnostic information is encoded. Understanding attenuation is crucial because it directly determines the contrast and quality of the images we can produce.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 12 - Slide22.txt", "file_path": "Lecture 12\\Texts\\Slide22.txt", "content": "So, how exactly do X-rays get attenuated?\n\nThink about it this way: when an X-ray beam passes through the body, several outcomes are possible. If there is no interaction at all, the X-rays travel straight through as if the body were completely transparent. That doesn‚Äôt help us, because no useful information is recorded.\nAt the other extreme, if there is total absorption, none of the X-rays make it out. That also gives us no signal.\nThe most useful case is partial absorption, and that occurs through two main processes: scattering and the photoelectric effect. We‚Äôll talk about these in more detail on the next slides.\n\nNow, at the atomic level, here‚Äôs what is happening: X-rays are photons, tiny packets of energy. When they strike the body, they may interact with molecules, atoms, or even inner electrons. Some photons pass through unchanged, forming the primary beam. Others are absorbed completely, transferring all their energy to the material. And still others are scattered ‚Äî deflected in new directions, sometimes losing energy in the process.\nIt‚Äôs the balance of these interactions that produces image contrast. Different tissues in the body ‚Äî bone, muscle, fat, lung ‚Äî attenuate X-rays to different degrees, which is why we can see structures in an X-ray image.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 12 - Slide23.txt", "file_path": "Lecture 12\\Texts\\Slide23.txt", "content": "Here we see the overall picture of how X-ray photons interact with matter. At the atomic level, these interactions can be grouped into four main categories.\n\nThe first case is no interaction. The photon simply passes through the material unchanged. You could say the photon gets ‚Äúlucky‚Äù ‚Äî it doesn‚Äôt collide with anything. That photon contributes to the primary beam that reaches the detector.\nThe second case is total absorption, also known as the photoelectric effect. Here, the photon is completely absorbed by an atom. It transfers all its energy to an inner electron, which is ejected as a photoelectron. The atom is left with a vacancy, and when electrons from higher shells fall into that vacancy, characteristic X-rays can be emitted.\n\nThe other two cases involve scattering. In coherent, or Rayleigh scattering, the photon is deflected at a very small angle, but its energy remains the same. Because the frequency and phase of the photon do not change, we call this coherent scattering.\nIn contrast, incoherent scattering, also called Compton scattering, happens when the photon collides with an outer-shell electron and transfers part of its energy to it. The electron is ejected, and the photon is scattered at a larger angle with reduced energy. The energy lost by the photon is taken up by the electron and eventually dissipates as heat.\n\nFor medical imaging, the two most important interactions are the photoelectric effect and Compton scattering. The photoelectric effect gives rise to absorption contrast, while Compton scattering dominates at higher energies and contributes to image noise and radiation dose.\nSo, to summarize: no interaction, total absorption, coherent scattering, and Compton scattering ‚Äî these four categories describe the essential ways X-rays interact with tissue.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 12 - Slide24.txt", "file_path": "Lecture 12\\Texts\\Slide24.txt", "content": "Here is a summary of the different types of X-ray interactions with matter. For medical X-rays, the two dominant processes are the photoelectric effect and Compton scattering.\nScattering can be divided into Compton and coherent (or Rayleigh) scattering. But in practice, coherent scattering contributes only a very small portion and is usually not considered important for medical imaging.\n\nNow, in clinical X-ray systems, we typically operate with tube voltages up to about 140 kilovolts peak, or kVp. When the energy is below about 80 kVp, the photoelectric effect is the dominant interaction. Above 80 kVp, Compton scattering becomes more significant. This is why, in diagnostic imaging, we see a transition in the relative contributions of these two effects.\n\nTraditionally, scattering was treated mainly as a nuisance because it reduces image contrast. But modern research shows that small-angle Compton scattering actually carries valuable information. In fact, subtle differences in scattering patterns may help distinguish benign from malignant tumors. However, detecting and analyzing this information requires specialized techniques, which go beyond the scope of our class.\n\nIn addition to these, there are two higher-energy interactions: pair production and photodisintegration. These processes become relevant only when the photon energy is very high, well above the diagnostic range. For example, in radiation therapy, where high-energy X-rays or proton beams are directed at tumors, pair production and photodisintegration play important roles. In that context, the goal is not diagnosis, but treatment ‚Äî essentially using radiation to destroy tumor tissue.\n\nSo to summarize: in diagnostic radiology, photoelectric absorption and Compton scattering are the key effects. For therapeutic applications at higher energies, pair production and photodisintegration also come into play.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 12 - Slide25.txt", "file_path": "Lecture 12\\Texts\\Slide25.txt", "content": "Compton scattering has major implications for radiation protection.\n\nWhen we use X-rays on a patient, most of the useful photons travel through the body and reach the detector to form the image. But many photons do not follow this direct path ‚Äî instead, they are scattered in different directions. These scattered photons, especially those from Compton scattering, can expose people nearby.\n\nThat is why radiation protection is so critical. For patients, the exposure is usually occasional ‚Äî maybe once a year, or only when necessary. But for radiologists and technicians working in X-ray rooms every day, the exposure could accumulate over time if they are not protected.\nTo minimize this risk, we use lead shielding, such as lead glass windows and lead aprons. These barriers absorb scattered radiation and protect staff from unnecessary dose. Without them, repeated daily exposure could become dangerous.\n\nSo, one of the key lessons is that while X-rays are extremely valuable for diagnosis, we must always use proper shielding and safety measures to protect healthcare workers.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 12 - Slide26.txt", "file_path": "Lecture 12\\Texts\\Slide26.txt", "content": "As X-rays pass through biological tissue, they are progressively attenuated. This is a cumulative process: the number of photons decreases rapidly as the beam penetrates deeper.\n\nYou can think of the tissue as being made up of thin layers. Each layer absorbs a certain fraction of the incoming photons. A useful way to describe this is the half-value layer concept. Suppose you start with 1,000 X-ray photons. After passing through the first half-value layer, about half remain ‚Äî 500 photons. After a second half-value layer of the same thickness, the number drops to about 250. After a few layers, only a small fraction of the original photons survive.\n\nSo attenuation is really a matter of probability: each layer has the same chance of absorbing a photon, and the result is an exponential reduction in the beam.\nIt‚Äôs also worth noting that X-ray imaging is not very efficient overall. In the tube, only about one percent of the electron energy is converted into usable X-ray photons ‚Äî the rest is lost as heat. Then, after filtration and attenuation inside the patient, only a small percentage of the original photons actually reach the detector.\nThat‚Äôs why we need two things: smart algorithms to make the most of the limited data, and highly sensitive detectors to record even the relatively small number of photons that emerge.\n\nSo now you have the physical intuition. Next, we will introduce the mathematical description of attenuation ‚Äî Beer‚Äôs Law ‚Äî which provides a precise model for this exponential process.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 12 - Slide27.txt", "file_path": "Lecture 12\\Texts\\Slide27.txt", "content": "Now let‚Äôs describe X-ray attenuation in a more quantitative way.\n\nImagine a beam of X-ray photons entering a small region of tissue. You can think of this region as a pixel, a tiny picture element made of uniform material. The number of photons that enter is what we call the input intensity. The number that comes out on the other side is the output intensity.\n\nThe relationship between the input and output depends on two key factors. First, the property of the material, which we describe using the linear attenuation coefficient, written as the Greek letter mu. Second, the thickness of the material, which we call delta x.\nPutting this together, the output intensity equals the input intensity multiplied by an exponential decay term. In plain language, this means that as the X-ray beam passes through the tissue, the number of photons drops off exponentially with thickness.\nThis exponential law comes directly from solving a first-order differential equation. The idea is simple: if you look at a very thin layer, only a small fraction of the photons are absorbed, proportional to the thickness. Stack up many thin layers, and the result naturally becomes exponential.\nIn fact, this same mathematical behavior appears in many other fields ‚Äî chemical reactions, radioactive decay, and even population growth or decline.\n\nNow, in the human body, the situation is more complex because the X-ray beam usually passes through several tissues with different properties. In that case, the output is determined by the combined attenuation from all of them along the path.\nThis exponential attenuation law is fundamental to X-ray imaging. It is the mathematical backbone of both radiography and CT, because it precisely tells us how X-ray photons are reduced as they traverse the body.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 12 - Slide28.txt", "file_path": "Lecture 12\\Texts\\Slide28.txt", "content": "Now let‚Äôs extend the exponential attenuation model to the case where the X-ray beam passes through multiple elements along its path.\n\nSuppose the beam is divided into many small steps. For each step, there is a local attenuation coefficient and a thickness. When we add up all of these contributions, we arrive at a cumulative relationship.\nThis summation has a special name: we call it a ray-sum. The term simply means we are summing up contributions along a single ray of X-rays as it travels through the object.\n\nNow, notice what is known and what is unknown. The thickness of each element is something we set in the model. What we don‚Äôt know are the attenuation coefficients themselves ‚Äî those are the values we are trying to determine. So, what we really have is a linear combination of unknowns. That‚Äôs why this is described as a linear system.\n\nOn the other side of the equation, you see a logarithm. Taking the natural logarithm is just a mathematical way of undoing the exponential law we started with. This step allows us to link the unknown attenuation values inside the body with quantities we can actually measure.\nThink about it: the input intensity is known because we generate and calibrate the X-ray beam. The output intensity is measured directly by the detector. The ratio of the two gives us a measurable value, and taking the logarithm turns it into the sum of attenuation along the path.\nIf we make the elements smaller and smaller, the discrete sum becomes a continuous integral. In other words, in the limit, what we measure is a line integral of the attenuation coefficient along the path of the X-ray.\n\nThis is the key principle: with X-rays, we do not directly measure the attenuation at each point inside the body. Instead, we measure ray-sums ‚Äî overlapping signals that represent line integrals. By collecting many such measurements from different angles, we can mathematically reconstruct the image.\nSo this linear equation forms the bridge between the unknown interior of the body and the measurable data at the detector.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 12 - Slide29.txt", "file_path": "Lecture 12\\Texts\\Slide29.txt", "content": "So far, we‚Äôve discussed how X-rays are attenuated as they pass through tissue. \n\nNow let‚Äôs see how this attenuation creates contrast in an image. Imagine a piece of tissue that is mostly uniform, but with a small bump or nodule inside it. The X-ray beam passing through the normal tissue travels a distance x. \nIts signal is attenuated according to the exponential law:\nA = N naught e to the power of minus Œº x.\nNow consider the ray that passes through the nodule. This ray travels an additional thickness, z. Its output is therefore:\nB = N naught e to the power of minus Œº (x plus z).\nThe contrast is defined as the difference between the normal signal and the signal through the nodule, divided by the normal signal. Doing the math, we find:\nContrast = 1 minus e to the power of minus Œº z.\nNotice that the contrast depends only on Œº, the attenuation coefficient, and z, the extra thickness of the nodule.\n\nFor example, if the nodule is 1 centimeter thick, and Œº equals 1 per centimeter, then the contrast comes out to about 63 percent. But in real soft tissue, Œº is much smaller, closer to 0.2 per centimeter. In that case, the contrast becomes significantly lower, making small nodules much harder to detect.\n\nThis simple example shows the mechanism of attenuation contrast: the difference in signal arises because one path is slightly longer or passes through tissue with different attenuation properties. But in soft tissues, where Œº values are close together, contrast can be very limited ‚Äî and that is why techniques like contrast agents or advanced imaging methods are often necessary.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 30, "slide_filename": "Slide30.txt", "slide_annotation": "Lecture 12 - Slide30.txt", "file_path": "Lecture 12\\Texts\\Slide30.txt", "content": "Now we can summarize the X-ray measurement model, which is one of the most important concepts. That‚Äôs why I‚Äôve marked this slide with the largest red diamond.\n\nLet‚Äôs begin with the monochromatic model. Here, we assume the X-ray beam consists of photons all having the same energy. That‚Äôs an idealization, but it helps us understand the basics. In this case, the incoming intensity, I naught, is attenuated according to Beer's law. The output, which we call I-g, is simply I naught times e to the power of minus the line integral of Œº along the path. This means the measured signal is directly linked to the cumulative attenuation along the ray.\nIf you know the input I naught and measure the output I-g, then by taking the logarithm, you recover the line integral of the attenuation coefficient. That‚Äôs the foundation of reconstruction in X-ray imaging and CT.\n\nBut in reality, things are more complicated. An actual X-ray tube does not produce a single energy ‚Äî it generates a spectrum of energies. The detector, in most systems today, does not separate them. Instead, it integrates the total energy deposited, whether from soft X-rays or hard X-rays.\nThat leads us to the polychromatic model. In this case, we must integrate over the entire energy spectrum. For each energy, attenuation follows the exponential law, but the detector sums over all energies together. The result is still a single number ‚Äî a grayscale intensity ‚Äî but it is a weighted sum across the energy distribution.\n\nSo, the monochromatic model is simple and idealized, while the polychromatic model describes the true measurement process in clinical CT scanners. And this distinction is very important ‚Äî it explains why certain artifacts, like beam hardening, occur in practice.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 31, "slide_filename": "Slide31.txt", "slide_annotation": "Lecture 12 - Slide31.txt", "file_path": "Lecture 12\\Texts\\Slide31.txt", "content": "A very important concept in X-ray imaging is the K-edge of the linear attenuation coefficient.\n\nNow remember, Œº ‚Äî the attenuation coefficient ‚Äî is not a single fixed number. It depends strongly on the energy of the X-rays. In general, as energy increases, X-rays penetrate more easily, so the attenuation coefficient gradually decreases.\n\nBut at certain energies, something special happens. When the X-ray energy becomes high enough to knock out inner-shell electrons ‚Äî specifically, K-shell electrons ‚Äî the attenuation suddenly increases. This produces a sharp jump in the attenuation curve, known as the K-edge.\nThis feature is element-specific. Each element has its own characteristic K-edge energy. For example, in gold, the K-edge occurs at about 80.7 keV. That means if the X-ray photon energy just crosses this threshold, gold absorbs much more strongly due to the photoelectric effect.\nThis is very powerful because it allows us to do chemically specific imaging. If we use detectors that can resolve X-ray energy, we can identify where particular elements are present based on their K-edge signatures. This opens the door to material decomposition and even molecular-level imaging.\n\nIn current hospital CT systems, detectors typically integrate over all energies. That‚Äôs like measuring the total area under the attenuation curve ‚Äî you get a grayscale value, but not the details of the energy dependence. However, with modern energy-sensitive detectors, we can directly measure the shape of the curve. That means we can distinguish materials that would otherwise look similar in a conventional CT scan.\nThis concept is especially important for contrast agents. Elements like iodine, gadolinium, and gold nanoparticles all have distinct K-edges. With energy-resolved imaging, we can separate their signals, perform quantitative mapping, and even design targeted nanoparticle-based contrast agents for advanced molecular imaging.\n\nSo the K-edge is not just a physics detail ‚Äî it‚Äôs the foundation for the future of spectral CT and molecular X-ray imaging.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 32, "slide_filename": "Slide32.txt", "slide_annotation": "Lecture 12 - Slide32.txt", "file_path": "Lecture 12\\Texts\\Slide32.txt", "content": "Now let‚Äôs look at how contrast agents take advantage of the K-edge effect we just discussed.\nOn this plot, we see the mass attenuation curves for different substances: lipid, water, bone, iodine, gadolinium, and gold. Notice that lipid, water, and bone have smooth curves that gradually decrease as energy increases. Without extra information, their signals can overlap, and it‚Äôs very difficult to tell them apart.\n\nBut look closely at iodine, gadolinium, and gold. Each has a sharp jump in attenuation at its characteristic K-edge energy. Iodine has its edge at about 33 keV, gadolinium around 50 keV, and gold at about 80 keV. These discontinuities are like chemical ‚Äúfingerprints.‚Äù\nIf your detector simply integrates all energies, you cannot distinguish whether the bright signal you see comes from iodine, or from gold, or even a mixture of bone and water. But if you can measure the attenuation as a function of energy ‚Äî in other words, if you can see the shape of this curve ‚Äî then you can identify which element is responsible.\n\nThis is the basis of K-edge imaging. By designing energy-sensitive detectors, we can separate the signals of different contrast agents. Clinically, iodine is already the most common CT contrast agent. Gadolinium, which is widely used in MRI, can also be detected in spectral CT. And gold nanoparticles are being investigated for advanced applications, including molecular imaging and drug delivery, because of their strong K-edge at higher energies.\n\nSo the key message here is: traditional CT shows only grayscale intensity, but spectral CT can unlock these curves and make imaging chemically specific. That allows us not only to see anatomy, but also to distinguish materials and track contrast agents at the molecular level.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 33, "slide_filename": "Slide33.txt", "slide_annotation": "Lecture 12 - Slide33.txt", "file_path": "Lecture 12\\Texts\\Slide33.txt", "content": "Now we come to the final part of today‚Äôs lecture ‚Äî the X-ray detector.\n\nIn your textbook, you will see quite a bit of discussion about traditional film and early detection methods. Those are historically important, and I encourage you to read them so you have general knowledge. But in practice, those methods are now outdated. What really matters for us is how X-rays are detected in modern CT scanners and what technologies are shaping the future of X-ray imaging.\nModern detectors are digital. They convert incoming X-ray photons into electrical signals that can be processed by the computer. And today, there are two main classes of detector technologies that you should understand:\n\nEnergy-integrating detectors ‚Äî This is the technology used in nearly all current CT scanners. They simply collect all the incoming photons and integrate their total energy. Whether a photon has high energy or low energy, the detector just sums them up. That gives us grayscale images ‚Äî the kind you are familiar with in clinical CT. A variation of this is dual-energy imaging, where two different energy spectra are used to give some limited energy discrimination. Dual-energy CT is already widely applied, for example, to distinguish iodine contrast from calcium.\nPhoton-counting detectors ‚Äî This is the emerging, next-generation technology. Instead of summing all energies together, photon-counting detectors measure each incoming photon individually and record its energy. That means they can separate photons into different energy bins, giving us spectral information. With this approach, we can do material decomposition, K-edge imaging, and molecular-level X-ray imaging, which we just discussed with gold and gadolinium.\n\nSo, to summarize: energy-integrating detectors are the current standard, while photon-counting detectors represent the future. They promise higher resolution, lower dose, and much richer information content.\nThis transition from energy integration to photon counting is one of the most exciting directions in X-ray imaging research and clinical translation.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 34, "slide_filename": "Slide34.txt", "slide_annotation": "Lecture 12 - Slide34.txt", "file_path": "Lecture 12\\Texts\\Slide34.txt", "content": "Energy-integrating detectors are the most common, practical, and mature technology used in modern CT scanners. \n\nTheir principle is very simple: they collect the total energy deposited by all X-ray photons that arrive at the detector element. It doesn‚Äôt matter whether the incoming photons are high-energy or low-energy ‚Äî in the end, they are all added together.\n\nNow, this technology works in two modes:\nIndirect mode\u000bIn indirect mode, the X-ray photons first pass through a layer of material called a phosphor, such as cesium iodide. The phosphor absorbs the X-ray energy and converts it into visible light. Each X-ray photon produces many visible light photons. These visible photons then strike a photodiode layer, which converts the light into an electrical current. The current is then read out by the circuit. In this process, higher-energy X-ray photons produce more visible light, but once converted to charge, all signals are summed together. That‚Äôs why this method is called energy integrating or sometimes current integrating.\n\nDirect mode\u000bIn direct mode, there is no intermediate light conversion. Instead, the X-rays interact directly with a semiconductor material, such as amorphous selenium. The X-ray photons generate electron-hole pairs in the material. Under a high-voltage bias, these charges are separated and collected as an electrical current. Again, the detector integrates all charges into one signal, which represents the total X-ray energy deposited.\nSo in both indirect and direct modes, the detector does not distinguish between high- and low-energy photons. It simply reports a single number, proportional to the total energy of all photons that passed through the patient. This gives us the grayscale images we are familiar with in clinical CT.\n\nThis approach is robust, efficient, and reliable ‚Äî and it has been the backbone of medical CT imaging for decades.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 35, "slide_filename": "Slide35.txt", "slide_annotation": "Lecture 12 - Slide35.txt", "file_path": "Lecture 12\\Texts\\Slide35.txt", "content": "Now let‚Äôs talk about an important concept in X-ray imaging: the anti-scatter grid.\n\nAs we discussed earlier, when X-rays interact with the body, two main processes occur ‚Äî photoelectric absorption and scattering. In medical X-rays, scattering is very common. That means many photons change direction inside the body and then try to reach the detector from different angles. \n\nBut here‚Äôs the problem: what we really want to measure is the primary beam ‚Äî the photons that travel straight along the original X-ray path. These are the photons that contain the true line integral information we need for accurate imaging. If scattered photons are also recorded, they blur the image and reduce contrast.\n\nTo solve this, we use an anti-scatter grid. You can think of it as a sheet with many tiny channels, almost like thousands of pinholes. Only photons traveling in the correct direction, aligned with the primary X-ray beam, can pass through these channels and reach the detector. Scattered photons, coming in at an angle, are blocked by the grid walls, which are made of dense materials such as lead.\nThis dramatically improves image quality by reducing the effect of scatter. Of course, there are trade-offs. If the grid is very tall or tightly spaced, it rejects more scatter, but it can also block some of the primary beam, which reduces efficiency. Engineers must carefully design the grid‚Äôs height, spacing, and material to strike a balance between scatter rejection and preserving the useful signal.\n\nSo, in summary, the anti-scatter grid is a simple but powerful device that ensures we measure mostly the primary beam, giving us much clearer and more reliable images.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 36, "slide_filename": "Slide36.txt", "slide_annotation": "Lecture 12 - Slide36.txt", "file_path": "Lecture 12\\Texts\\Slide36.txt", "content": "Here you can clearly see the difference between using an anti-scatter grid and not using one.\n\nOn the left, with the grid, the anatomical structures are much sharper. The boundaries of the ribs, the lung fields, and even the soft tissues are more clearly defined.\nOn the right, without the grid, the image looks more foggy. That haziness comes from scattered photons that are superimposed on top of the fine structures. Essentially, the scatter washes out important details and reduces contrast.\n\nThink of it like looking out of an airplane. On a clear day, you can see the ground with sharp detail. But on a cloudy day, the clouds blur your view. In the same way, scattered X-ray photons act like a layer of clouds, blurring the image and hiding subtle differences.\nThis becomes especially important when detecting low-contrast tumors. Without a grid, these tumors might be completely hidden in the fog of scatter. With a grid, however, you remove much of that scatter, revealing the underlying details more accurately.\n\nSo, using an anti-scatter grid can make the difference between detecting a disease early and missing it entirely.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 37, "slide_filename": "Slide37.txt", "slide_annotation": "Lecture 12 - Slide37.txt", "file_path": "Lecture 12\\Texts\\Slide37.txt", "content": "Now let‚Äôs dive into dual-energy imaging, which is one of the most important developments in modern CT.\nAll the major CT manufacturers ‚Äî Siemens, GE, Philips, and Toshiba ‚Äî have worked on energy-sensitive imaging. While true multi-energy imaging is still a challenge, they have all developed practical approaches to perform dual-energy CT, which means collecting data using two different X-ray spectra.\n\nLet me explain the main strategies:\nSiemens uses dual-source CT. Here, the scanner has two X-ray tubes and two detectors mounted at an angle, each operating at a different voltage. For example, one tube might run at 80 kilovolts, while the other runs at 140 kilovolts. This gives you two spectra simultaneously, which means excellent temporal resolution and minimal motion artifact.\n\nGE uses kVp switching. In this method, there is only one X-ray tube, but the tube rapidly alternates its voltage between high and low values ‚Äî say, 140 and 80 kilovolts ‚Äî from one projection to the next. As a result, you still get two distinct spectra, but collected sequentially.\nPhilips uses dual-layer detectors. Instead of modifying the tube, they modify the detector. The top layer absorbs most of the low-energy X-rays, while the second layer captures the higher-energy photons that pass through. This way, low- and high-energy data are naturally separated at the detector level.\n\nToshiba uses dual-scan mode. In this case, the scanner acquires two full scans of the same patient: one at high voltage and one at low voltage. While this is simpler in concept, it has drawbacks. It takes longer, and the patient may move between scans. Also, if contrast material is injected, its distribution may change between the two acquisitions, leading to misregistration.\nSo in practice, the first three technologies ‚Äî dual-source, kVp switching, and dual-layer detectors ‚Äî are more effective. They provide two sets of spectral information within the same exam, which can then be used to separate materials, improve contrast, and reduce artifacts.\n\nThis is why dual-energy imaging is now widely used in clinical practice. It gives radiologists much more information than standard CT and opens the door to advanced applications like material decomposition and virtual contrast imaging.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 38, "slide_filename": "Slide38.txt", "slide_annotation": "Lecture 12 - Slide38.txt", "file_path": "Lecture 12\\Texts\\Slide38.txt", "content": "Before we go further into dual-energy imaging, let‚Äôs pause for a quick review of some basic chemistry concepts‚Äîspecifically, the atomic number and the mass number, because these concepts form the physical foundation of dual-energy CT.\n\nThe atomic number, usually denoted by Z, is simply the number of protons in the nucleus of an atom. It defines the identity of the element. For example, hydrogen has Z = 1, helium has Z = 2, carbon has Z = 6, and so on. If you change Z, you change the element itself.\nThe mass number, often written as A, is approximately the total number of protons plus neutrons in the nucleus. Since neutrons contribute to the mass but not the charge, A is usually larger than Z.\nTake helium as an example. Helium has Z = 2, meaning two protons. Most stable helium atoms also have two neutrons, giving a mass number of A = 4. That‚Äôs why you see the helium symbol here written with a 2 at the bottom and a 4 at the top.\n\nSo why is this important for us? Because the way X-rays interact with matter‚Äîwhether through photoelectric absorption or Compton scattering‚Äîdepends strongly on both the atomic number and the mass of the element. High atomic number elements like iodine, gadolinium, or gold interact with X-rays very differently from soft tissue, which is mostly carbon, oxygen, and hydrogen.\nAnd this difference is exactly what dual-energy imaging exploits: by probing tissues at two different energy levels, we can begin to separate and even identify different materials inside the body.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 39, "slide_filename": "Slide39.txt", "slide_annotation": "Lecture 12 - Slide39.txt", "file_path": "Lecture 12\\Texts\\Slide39.txt", "content": "So here is a classic expression for the X-ray attenuation coefficient.\n\nThe main idea is that attenuation comes from two parts: one part is the photoelectric effect, and the other part is Compton scattering.\nThe photoelectric effect is very strongly dependent on energy. In fact, as the X-ray energy increases, the contribution from the photoelectric effect drops very quickly ‚Äî that‚Äôs why low-energy X-rays are absorbed much more.\n\nThe Compton scattering term, on the other hand, has a more gradual dependence on energy. It comes from what is called the Klein‚ÄìNishina function, which describes how photons scatter off electrons. So if you put these two terms together, you can describe the full attenuation as a combination of photoelectric absorption and Compton scattering. Now here‚Äôs the powerful part: if we measure attenuation at two different X-ray energy spectra, we can separate these two contributions. Once that‚Äôs done, we can predict how the material would behave at any other energy.\n\nYou can think about this in two ways:\nPhysically, we are separating attenuation into its two main mechanisms: photoelectric and Compton.\nMaterial-wise, we are saying that the body can be thought of as a mixture of two basic materials ‚Äî for example, water and bone. Any other tissue can be represented as a combination of those two.\n\nThat‚Äôs the foundation of dual-energy CT: by using two X-ray spectra, we can get material-specific information, not just grayscale attenuation.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 40, "slide_filename": "Slide40.txt", "slide_annotation": "Lecture 12 - Slide40.txt", "file_path": "Lecture 12\\Texts\\Slide40.txt", "content": "So how do we actually perform a dual-energy measurement?\nThe key is that we collect data using two different X-ray spectra. Let‚Äôs call them spectrum one and spectrum two.\n\nNow, recall that the attenuation coefficient has two main components ‚Äî the photoelectric part and the Compton scattering part. When we take the line integral along an X-ray path, we can separate these contributions into two quantities. We usually call them A1 and A2.\nSo what happens is this: if we measure the transmitted X-ray signal with spectrum one, we get one equation that depends on A1 and A2. If we measure again with spectrum two, we get a second equation.\n\nNow we have two equations and two unknowns. That means we can solve for A1 and A2. And once we know A1 and A2, we can reconstruct the attenuation coefficient at any X-ray energy.\nThat‚Äôs the whole idea of dual-energy CT. By using two spectra, we get enough independent information to separate the photoelectric and Compton contributions. And from there, we can start to do material decomposition ‚Äî distinguishing water from bone, or even identifying contrast agents like iodine or gold.\n\nSo dual-energy imaging is not just a trick with hardware. It has a very solid physical and mathematical foundation.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 41, "slide_filename": "Slide41.txt", "slide_annotation": "Lecture 12 - Slide41.txt", "file_path": "Lecture 12\\Texts\\Slide41.txt", "content": "Now let‚Äôs move to the last topic in this section: photon-counting CT.\n\nUp to this point, we‚Äôve mostly been talking about conventional detectors. Those are what we call current-integrating detectors. What they really do is add up all the incoming X-ray photons across the entire energy spectrum, and then report one number. In other words, they just give us the area under the curve. That means we only get a grayscale image ‚Äî essentially black and white ‚Äî where each pixel represents total intensity.\n\nBut imagine if, instead of throwing all the energy information into one basket, we could measure the energy of each photon. That‚Äôs the principle of photon-counting detectors. Instead of giving you one number, they break the spectrum into many energy bins.\nSo now, instead of saying ‚Äúthis voxel attenuates X-rays by this much in total,‚Äù we can say, ‚Äúhere is how it attenuates soft X-rays, medium-energy X-rays, and high-energy X-rays.‚Äù That gives us a rich spectrum of information.\nAnd here‚Äôs the exciting part: with that energy-resolved data, we can make not just one grayscale image, but multiple images across energy ranges. \n\nWe can even assign colors to those ranges. The result is what we sometimes call spectral CT or molecular CT imaging, where each material can be distinguished by its unique spectral fingerprint.\nSo while conventional CT gives us one picture in shades of gray, photon-counting CT has the potential to give us multiple pictures at once, even in color. That‚Äôs the future direction of CT technology.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 42, "slide_filename": "Slide42.txt", "slide_annotation": "Lecture 12 - Slide42.txt", "file_path": "Lecture 12\\Texts\\Slide42.txt", "content": "Now let‚Äôs look a little closer at how a photon-counting detector actually works.\n\nIn many ways, the design is similar to direct detection. An incoming X-ray photon interacts with a semiconductor material ‚Äî typically cadmium telluride, or cadmium zinc telluride. This interaction generates an electron-hole pair. Under an applied bias voltage, those charges are quickly separated, creating a tiny current impulse.\n\nNow here is the critical part: in a photon-counting detector, the electronics are extremely fast and sensitive. Each X-ray photon produces its own electrical pulse. That pulse is then compared against a series of preset thresholds.\nIf the pulse amplitude falls between two thresholds ‚Äî say, between the fifth and the sixth ‚Äî we know immediately that the energy of that X-ray photon lies within that range. In other words, the detector doesn‚Äôt just say ‚ÄúI saw a photon.‚Äù It also says, ‚ÄúI saw a photon in this specific energy band.‚Äù\n\nWhat you see in this diagram is the circuit pathway: the signal is amplified, shaped, compared against thresholds, and then counted in real time. This entire process happens at the pixel level. And today‚Äôs state-of-the-art photon-counting detectors can have pixels as small as 55 microns. Each pixel has its own miniature readout circuit, often called an ASIC ‚Äî an application-specific integrated circuit. Inside each ASIC are hundreds or thousands of transistors, all designed for high-speed, low-noise performance.\n\nThis is truly high-tech engineering. And the result is not just a count of how many photons arrive, but a spectrum of how those photons are distributed across energy ranges. That is what enables spectral CT ‚Äî the next generation beyond conventional CT.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 43, "slide_filename": "Slide43.txt", "slide_annotation": "Lecture 12 - Slide43.txt", "file_path": "Lecture 12\\Texts\\Slide43.txt", "content": "Now we come to one of the most exciting applications of photon-counting CT, which is K-edge imaging.\n\nIf we are dealing only with the human body ‚Äî tissues like bone, soft tissue, and water ‚Äî then dual-energy CT is usually sufficient. With two basis materials, two measurements are enough to solve the equations, and you can predict the X-ray response at any energy level. That is why dual-energy imaging has been so successful in the clinic.\n\nBut the moment we introduce contrast agents ‚Äî iodine, gadolinium, or gold nanoparticles ‚Äî the situation changes. These materials have sharp K-edges at very specific energies. To detect and separate them reliably, we need true energy-resolved imaging.\nThat is exactly what a photon-counting detector provides. Instead of just two broad spectra, it measures photons across multiple energy bins. This allows us to do material decomposition and create chemically specific maps. In other words, we can actually tell apart iodine from calcium, or gold nanoparticles from bone, based on their spectral signatures.\n\nAnd this opens the door to something much more powerful: molecular and cellular imaging with CT. If you attach nanoparticles to specific molecular targets, photon-counting CT can visualize not just anatomy, but biology ‚Äî at the molecular level.\nOf course, there are additional benefits. Photon-counting improves dose efficiency, reduces beam hardening, and increases spatial resolution. But in my mind, the major breakthrough is K-edge imaging: the ability to map contrast agents and even nanoparticles with chemical specificity.\nThis is why the field is so excited about photon-counting CT ‚Äî it takes X-ray imaging beyond anatomy and into biology.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 44, "slide_filename": "Slide44.txt", "slide_annotation": "Lecture 12 - Slide44.txt", "file_path": "Lecture 12\\Texts\\Slide44.txt", "content": "And let me close with this very exciting development.\nJust last year, a major paper was published in the journal Radiology. This study, conducted by a team of physicians in collaboration with Siemens, reported the first human experience with contrast-enhanced photon-counting CT.\n\nWhat does this mean? For the very first time, we are seeing photon-counting CT applied in real clinical imaging of patients ‚Äî not just phantoms or animal models. The results demonstrated the feasibility and advantages of this new detector technology, including improved contrast, better noise performance, and the ability to separate materials with high precision.\n\nSo this is a milestone. What we have been discussing ‚Äî from X-ray physics, to attenuation, to detector evolution, to dual-energy imaging, and finally photon-counting ‚Äî is not just a research vision anymore. It is becoming a clinical reality.\nThis is where the field is going. And as engineers and scientists, your understanding of these fundamental principles will position you to contribute to the next generation of medical imaging.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 45, "slide_filename": "Slide45.txt", "slide_annotation": "Lecture 12 - Slide45.txt", "file_path": "Lecture 12\\Texts\\Slide45.txt", "content": "Here we see the landmark result.\n\nThese are the very first human CT images acquired with a photon-counting detector. In the top row, you can compare conventional energy-integrating CT with photon-counting CT. At this stage, the overall image quality and diagnostic value are already comparable to the best systems currently in use.\nBut what‚Äôs truly remarkable lies in the bottom row. With photon-counting CT, we can separate energy information and create maps ‚Äî for example, iodine concentration maps ‚Äî directly from the same scan. This means we are no longer limited to grayscale attenuation images. Instead, we can extract quantitative spectral information, detect contrast agents with high specificity, and even generate functional or molecular-level insights.\n\nSo this is not just an incremental improvement. This represents a paradigm shift. We are moving from conventional black-and-white CT into a new era of spectral and molecular CT imaging, powered by photon-counting technology.\nThe field is advancing rapidly, and these first results are just the beginning. With further improvements, photon-counting CT promises to fundamentally transform the way we diagnose and study human disease.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 46, "slide_filename": "Slide46.txt", "slide_annotation": "Lecture 12 - Slide46.txt", "file_path": "Lecture 12\\Texts\\Slide46.txt", "content": "Here is an inspiring analogy from biology. This is a microscopic view of the vertebrate retina ‚Äî truly an amazing design. You can see the two main types of photoreceptors: the rods and the cones.\n\nRods are highly sensitive to light intensity. They help us see in dim light, but they do not provide color information. In a way, they are like the energy-integrating detectors we discussed earlier ‚Äî they sum up all incoming light, or in our case, x-ray energy.\nCones, on the other hand, are responsible for color vision. They are less sensitive to intensity but can distinguish between red, green, and blue. In other words, they are like photon-counting detectors ‚Äî slower, but capable of providing spectral, or energy-specific, information.\n\nSo why does the human retina use both? Because together, they give us a complete picture. Rods allow us to detect faint light with high sensitivity, while cones let us see rich color details.\n\nThis hybrid approach is exactly the idea being explored in modern CT technology ‚Äî combining energy-integrating detectors for strong, high-flux signals with photon-counting detectors for detailed spectral information. Nature shows us that using both types of detectors in parallel can be the most efficient design.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 47, "slide_filename": "Slide47.txt", "slide_annotation": "Lecture 12 - Slide47.txt", "file_path": "Lecture 12\\Texts\\Slide47.txt", "content": "So here is the idea we are pursuing ‚Äî a hybrid detector array.\n\nInstead of using only one type of detector, why not combine them? In this design, most pixels are traditional energy-integrating detectors, which provide strong grayscale signals with high efficiency. But we strategically insert photon-counting detectors at certain positions. These pixels give us the spectral information ‚Äî the energy-resolved measurements that allow for material decomposition, K-edge imaging, and molecular contrast. By mixing the two, we get the best of both worlds: the robustness and speed of energy-integrating detectors, and the precision and specificity of photon-counting detectors.\n\nThis concept is similar to the retina analogy I mentioned earlier ‚Äî rods and cones working together. The rods give high sensitivity, the cones give color. Likewise, here the integrating detectors give us the structural information, while the photon-counting detectors add rich spectral detail.\nWe have already published several papers on this hybrid design, and it represents an exciting direction for the next generation of CT technology.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 48, "slide_filename": "Slide48.txt", "slide_annotation": "Lecture 12 - Slide48.txt", "file_path": "Lecture 12\\Texts\\Slide48.txt", "content": "Here I want to highlight the work of one of my former PhD students, James Bennett.\n\nJames focused on the problem of cost and practicality. As you know, photon-counting detectors are still very expensive. If we want to move this technology from the lab to widespread clinical use, we need solutions that balance performance with cost-effectiveness.\nHis dissertation, later published in IEEE Transactions on Biomedical Engineering, presented a clever system design for hybrid spectral micro-CT. He demonstrated how combining energy-integrating detectors with photon-counting detectors can achieve many of the benefits of full spectral imaging ‚Äî but at a fraction of the cost.\n\nThe results were promising: improved contrast resolution, better spatial resolution, and lower radiation dose. This work was an important step toward making spectral CT more practical and accessible for real biomedical applications.\n\nSo this is a good example of how doctoral research can contribute to both advancing the science and solving real-world challenges in medical imaging.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 49, "slide_filename": "Slide49.txt", "slide_annotation": "Lecture 12 - Slide49.txt", "file_path": "Lecture 12\\Texts\\Slide49.txt", "content": "Here is another example of our research direction. This work, also involving James and several collaborators, focused on image reconstruction for hybrid true-color micro-CT.\n\nThe goal was to demonstrate that when you combine hybrid detectors with proper reconstruction algorithms, you can actually generate color CT images. Instead of the usual grayscale representation, here you see different contrast agents represented in distinct colors.\nThe figure on the left shows a phantom study ‚Äî different inserts with iodine, gold, and other materials, clearly separated into unique color channels. On the right, you can see reconstructions from biological samples, where regions of interest are highlighted, again providing more specific information than traditional CT.\n\nSo this is really a proof of concept: with hybrid spectral CT, we can move beyond just structure and begin to recover molecular or functional information. This was an important step toward what we now call ‚Äútrue-color CT.‚Äù", "total_slides_in_lecture": 50}
{"lecture": "Lecture 12", "slide_number": 50, "slide_filename": "Slide50.txt", "slide_annotation": "Lecture 12 - Slide50.txt", "file_path": "Lecture 12\\Texts\\Slide50.txt", "content": "Alright, before we wrap up today‚Äôs lecture, let me leave you with the homework assignment.\nFrom now on, we‚Äôll be following the green textbook as our main reference. Each week I‚Äôll select a few problems that are especially useful for reinforcing the key concepts we‚Äôve covered.\n\nFor this week, please work on the following three problems:\nPage 50, Problem 1.1\nPage 51, Problem 1.3\nPage 51, Problem 1.6\nThese are not particularly difficult, but they will give you a chance to apply what we‚Äôve discussed in class and make sure the ideas really sink in.\n\nSo much for today‚Äôs lecture ‚Äî thank you, and I‚Äôll see you in the next class.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 13 - Slide1.txt", "file_path": "Lecture 13\\Texts\\Slide1.txt", "content": "Welcome back. We can see today‚Äôs topic is CT Reconstruction. \n\nIn this session, we‚Äôll build on that foundation and map raw X-ray measurements into cross-sectional images. By the end, you‚Äôll have a clear roadmap of the reconstruction pipeline and where classic methods, such as filtered back-projection and modern iterative ideas, fit into an online, real-world CT workflow.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 13 - Slide2.txt", "file_path": "Lecture 13\\Texts\\Slide2.txt", "content": "Let‚Äôs take a look at the roadmap: we‚Äôre at the CT Reconstruction milestone. Keep one core idea in mind from the physics background‚Äîthe roles of the X-ray source, the detector, and how a measurement is formed. Each reading can be interpreted as a line integral of the attenuation coefficient along a straight ray. \n\u000bWith that model in place, we now focus on algorithms that invert those line integrals to recover cross-sectional images. If you‚Äôd like more depth on the geometry or notation, the companion reading for this module provides it", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 13 - Slide3.txt", "file_path": "Lecture 13\\Texts\\Slide3.txt", "content": "Let‚Äôs take a look at a few habits that make this module easier.\u000b\nFirst, recommended reading: Ge‚Äôs book and the widely used ‚ÄúGreen Book.‚Äù Together, they give both intuition and practical formulas. Next, focus on the logic flow in the video lessons and slides‚Äîhow each idea leads to the next. Aim to capture the key concepts rather than memorize isolated facts.\u000b\nBuild skills by working on problems from homework and textbook questions; problem-solving is where concepts become durable.\u000bAdopt a simple routine: preview ‚Üí learn ‚Üí review ‚Üí repeat. Preview the slide headlines, watch the lesson, then review by summarizing in your own words and redoing one or two problems.\u000b\nFor foundations, give extra attention to Fourier analysis and the sampling theorem‚Äîthese are the grammar of modern imaging. They will pay off when we study CT filtered back-projection and, later, MRI pulse sequences and other modalities. If questions come up, post them in the course Q&A so others can benefit too.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 13 - Slide4.txt", "file_path": "Lecture 13\\Texts\\Slide4.txt", "content": "Here we observe the plan for CT reconstruction. We will connect data space‚Äîthe set of X-ray line integrals‚Äîto image space‚Äîthe distribution of attenuation, usually written as mu of x, y. Think of this as a duality of information: each view in the sinogram encodes a different slice of information about the same object.\n\nWe‚Äôll compare two families of methods:\nAlgebraic approach.\u000bWe model the scanner with linear equations: A times x equals b.\u000bThis viewpoint helps us reason about solution uniqueness‚Äîfor example, when A has full column rank‚Äîand about data independence and sufficiency, meaning we need enough non-redundant views and detector samples to determine x stably. Regularization can be added to handle noise or limited angles.\n\nAnalytic approach.\nThis relies on Fourier analysis. By the Fourier Slice Theorem, the one-dimensional Fourier transform of a projection at angle theta equals a radial slice through the two-dimensional Fourier transform of the object at the same angle. \u000b\nFrom this principle, we derive filtered back-projection (FBP): first apply a frequency-domain ramp filter‚Äîmagnitude of omega‚Äîto each projection, then smear, or back-project, the filtered data across the image grid. FBP is fast and remains the standard for many clinical workflows.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 13 - Slide5.txt", "file_path": "Lecture 13\\Texts\\Slide5.txt", "content": "Let‚Äôs take a moment to underline the information. Like in physics, we have a particle property and a wave property. For image reconstruction, we can make an analogy. We can view the underlying image in two ways.\u000b\nFirst, we view the image as a collection of pixels if the image is two-dimensional, or as pixels for three-dimensional images‚Äîa collection of particles‚Äîhere meaning picture elements; the pixels, the pixels (for 3-D, these are often called voxels).\u000b\nThe other way is complementary: we view the image as a superposition of waves. For a two-dimensional image, for example, think of the image as a summation of many waves propagating along different orientations. For a given orientation, the wave can be at different frequencies, and you have amplitude, frequency, and phase. If all these parameters are set correctly and you add all these waves together, you obtain the image. This is the basic, high-level idea‚Äîthe particle and wave perspectives of the image and of image reconstruction.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 13 - Slide6.txt", "file_path": "Lecture 13\\Texts\\Slide6.txt", "content": "Let me explain a little further. Take an arbitrary picture‚Äîsay, a portrait of our former president. You can decompose the picture into many, many small elements. For each element, it‚Äôs very simple: just a homogeneous pixel, a small square. Add these together. \n\nThe trick is really the amplitude: you need to make sure all these amplitudes are modulated nicely. Then, when you put them together, you have the perception of a picture or a natural scene, whatever. This is one way to represent a picture.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 13 - Slide7.txt", "file_path": "Lecture 13\\Texts\\Slide7.txt", "content": "Another way, as mentioned, is Fourier analysis. In a two-dimensional picture, you perform a Fourier analysis and obtain a Fourier spectrum.\n\nPick any point: the frequency is proportional to the distance from the origin to that point in the Fourier domain. In this example, you see the frequency, and you also have an amplitude proportional to the value at this particular point. That gives a particular wave propagating along that direction.\n\nFourier analysis uses all kinds of wave components: the DC component, horizontal waves, vertical waves, low frequency, high frequency, and waves that propagate in arbitrary directions‚Äîall these kinds of waves. The trick is the setting of the parameters. Once you find the coefficients, you add the Fourier components together, and then you can recover the image‚Äîin this case, Albert Einstein. \n\nSo there are two ways, and based on these two perspectives, we can have two approaches.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 13 - Slide8.txt", "file_path": "Lecture 13\\Texts\\Slide8.txt", "content": "We introduced two families of methods‚Äîalgebraic and analytic. Let‚Äôs begin with the algebraic approach and set up the measurement model it relies on.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 13 - Slide9.txt", "file_path": "Lecture 13\\Texts\\Slide9.txt", "content": "The measurement can be written in line-integral form or in ray-sum form. Let me explain.\n\nFirst, we have an incoming intensity, N i, and an output intensity, N o. The output is related to the input through linear attenuation. For a single element along the ray, the attenuation is mu times delta x, where delta x is the pixel size.\nWith multiple material components along the path, the incoming intensity is attenuated by the first pixel; the output of the first pixel becomes the input to the second pixel, and so on. Adding these effects together gives the standard relationship. This is a quick review of the earlier module.\n\nThe attenuated output equals the input multiplied by an exponential factor whose exponent is the negative sum of the little mu k values, each weighted by delta x. This is our imaging model or data model.\nWhat is known and what is unknown?\u000bKnown: N i, N o, and the step size delta x that we choose.\u000bUnknown: the set mu k, for k equals 1, 2, 3, ‚Ä¶, n.\n\nNormalize by taking a ratio and then a natural logarithm:\n‚ÄúNatural log of N i divided by N o equals the sum over k of mu k times delta x.‚Äù\nThis is a linear equation in the unknowns mu k, with delta x acting as a weight. Because N i and N o are measured, the right-hand side is known after the log. For each X-ray path, we obtain one linear equation. With many paths, we obtain a system of linear equations, often written compactly as A times x equals b. In this algebraic view, the image is treated as pixels, and each measurement provides a linear constraint on those pixel values.\nIf we take the limit as delta x becomes very small, the sum becomes an integral:\n‚ÄúThe integral along the ray of mu of x, d x, equals the natural log of N i over N o.‚Äù\n\nSo, in the discrete domain we speak of a ray-sum‚Äîa summation‚Äîand in the continuous domain we speak of a line integral‚Äîan integral along the X-ray path. Either way, X-ray measurements provide ray-sums or line integrals of the attenuation field. With this model in mind, we can now discuss why the resulting linear system can have a unique solution, at least heuristically.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 13 - Slide10.txt", "file_path": "Lecture 13\\Texts\\Slide10.txt", "content": "This is what I call the onion-peeling idea.\u000b\nThink of a picture and decompose it into a collection of pixels. The pixel does not have to be rectangular; here, I use triangular pixels. I perform the decomposition into many, many triangular pixels and then resolve the unknowns layer by layer. Start with the outermost unknowns, represented as a right triangle. This is a heuristic idea, so follow the logic.\n\nIn the limiting case, imagine the object support where an X-ray touches one molecule, an extremely small element. Take this as an example: you have a small material element. You know the incoming intensity, you know the attenuated intensity, and because the data are partitioned, you know the total path length. From these three knowns, you can resolve the only unknown, the attenuation coefficient mu‚Äîcall it mu-zero or mu-one‚Äîfor that pixel. By assumption, this pixel is homogeneous. With this simple argument, you now know mu for this right triangle. By the same argument, all the right-layer pixels can be directly measured with a prior flow of X-rays.\n\nOnce that is done, move to the next layer, colored green. Use a green X-ray. You know the incoming intensity and the attenuated intensity. You already know mu for the right element, and you also know mu for the current neighbor. Because you know these mu values, you can compute how the incoming intensity is attenuated and thus determine the incoming flux into the green pixel. You also know the attenuated intensity leaving this location. That attenuated intensity is the quantity just out of the green pixel but further attenuated by the right pixel. Using Beer‚Äôs law in reverse, you back out the intensity before that extra attenuation. Now you know the value here, you know the value there, and you know the total length, so you can resolve mu for this particular green triangular pixel. Likewise, each of the green elements is resolved.\n\nNext, move to the light-blue ray. At this point, the green and the right layers are known; only this one remaining element is unknown. The same heuristic applies.\nThis argument ties the relations without complicated mathematics. Heuristically, you can peel the image layer by layer and resolve the underlying image algebraically. When you solve the linear system equation pixel-wise, you are essentially doing exactly this: layer by layer. In parallel-beam geometry, you can send all possible rays and reorder them so you effectively view the object from the outermost layer inward, resolving all unknowns layer by layer. This is the key argument.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 13 - Slide11.txt", "file_path": "Lecture 13\\Texts\\Slide11.txt", "content": "So, mathematically, we can state a data sufficiency condition for two-dimensional image reconstruction. You have a cross-section, and you arbitrarily draw lines‚Äîthese are the X-ray paths. For any such line, we say you can find at least one source position. \n\nWhat does that mean? It means that along this line, an X-ray source sends rays in this direction, so the line integral, or ray-sum, is measured. It doesn‚Äôt mean you have every piece of information you might want. It means: if a line intersects the cross-section, then we have data on that line. That is the maximum amount of data you could have, and, under this simple understanding, that amount is sufficient.\n\nThis is the way I understand the subject and how I try to explain it to students. I like to give you a picture and some visual, geometrical ideas. If you feel confused, you can watch my lesson. Right now, I think there are about half a million viewers, a lot of likes, so I really hope your watch gives me a like‚Äîthat‚Äôs good.\n\nSome other heuristics are very important but not easy. For example, when you compute the coefficient, think of it as an inner product‚Äîa high-dimensional vector projected onto a basis function. That is a very important heuristic, but I doubt all of you fully understand it. If you don‚Äôt, please review. I hope, by the end of this month, to upload a newer version that will be much better than the rough draft, so please read it. Even if I wouldn‚Äôt test it again, you need to understand Fourier analysis so that you understand CT and MRI much better. Anyway, this is the idea of the data sufficiency condition.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 13 - Slide12.txt", "file_path": "Lecture 13\\Texts\\Slide12.txt", "content": "Okay. With X-rays, we can measure parallel-beam projections as shown here. Along each X-ray, each datum gives you one linear system equation. If you have a parallel beam at projection angle theta, and you have, say, one hundred rays or five hundred rays, then you have one hundred or five hundred linear system equations.\n\nOne view is not enough, because you cannot resolve superimposed structures‚Äîyou see two things on top of each other, and you do not know which one is on top and which one is beneath. So you need to keep changing theta.\nThe original function f of x, y is converted to a new two-dimensional function p of theta, t. Theta is the angle; t is the coordinate. Using X-ray measurements, you perform a physical or mathematical transform from f of x, y to p of theta, t.\n\nFor example, if you have a bright, small disk, it will trace a sinusoidal curve. That is why we call the data-domain representation a sinogram. We also call it a Radon transform‚ÄîRadon was a mathematician many years ago. So this is a projection, one view; the sinogram puts all the views together.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 13 - Slide13.txt", "file_path": "Lecture 13\\Texts\\Slide13.txt", "content": "Computed tomography was previously called computer-aided tomography, C-A-T‚Äîso we usually put a cute cat here. C-T is nothing but the inverse process. Once you have sinogram data, p of theta, t, the question is: given the measurements, how do you invert the process?\n\nWhat is the underlying image that explains this data? Once you have the data, you reconstruct the image. The inverse process goes from data to image. X-ray measurement goes from image to data. The tomographic algorithm goes the other way, from data to image, the underlying image.\n\nAnd how do you do it? The picture I gave you shows you can use an impending method, so you have a heuristic feeling.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 13 - Slide14.txt", "file_path": "Lecture 13\\Texts\\Slide14.txt", "content": "Now, let me give you a numerical example.\u000b\nIn practice, the image can be five hundred twelve by five hundred twelve, but for teaching purposes, I‚Äôll use a two by two image. This is a simple case, but the essential idea is already there.\n\nYou have pixel values one, two, three, four‚Äîvery simple‚Äîbut you don‚Äôt know these; this is just something I set up. What you are allowed to probe is with X-rays. I told you X-rays cannot pinpoint a single pixel. If you had a magic pen that could read out a point one pixel like a photograph, you would not need tomographic reconstruction‚Äîthat would be too simple.\n\nWith X-ray measurements, we can get some information. Suppose you shoot X-rays this way. As I explained, an X-ray measurement gives you a ray-sum. From the earlier slide: if you do not send X-rays through these two pixels, you will not be able to say what mu one and mu two are. But you do know the sum of these two pixels: mu one plus mu two equals seven. From this, you still do not know mu one and mu two, but you know the sum is seven. Likewise, take a vertical ray: you get mu two plus mu four equals four.\nWith X-ray measurements, you can write a number of equations. Solve the equations and you get the unknowns‚Äîthat‚Äôs the idea. Normally, for an n by n image, you have n by n unknowns. Here, for two by two, you have four unknowns, so you need to shoot four rays and get four measurements: four linear equations, four unknowns. It looks perfect, right? Not that simple.\n\nLook at this: if you shoot these two rays, the first two equations added together give a right-hand side of ten, and the last two equations added together are also ten. So if you subtract one equation from the sum, you get the remaining one. It‚Äôs a little tricky, but the point is that these four equations are not totally independent. From three of them you can derive the last one.\nThe trick is that ‚Äúnumber of equations equals number of unknowns‚Äù only works under the condition that each equation provides new information‚Äîthey must be independent. If they are not independent, you do not have enough equations to solve the problem.\n\nSo you should shoot a ray along the diagonal direction. Then you may get ‚Äúthis one plus this one plus this one plus this one,‚Äù and you will have enough independent equations to solve uniquely.\nAnyway, remember: ‚Äúnumber of equations equals number of unknowns‚Äù is under the assumption that all equations are independent. Otherwise, you do not have enough equations.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 13 - Slide15.txt", "file_path": "Lecture 13\\Texts\\Slide15.txt", "content": "Here, I‚Äôll still use two horizontal rays and two vertical rays to show how we solve linear system equations using iterative algorithms through trial and error. \n\nWhy explain an iterative algorithm? For a simple case like this, you can solve it directly using an analytic method. But when the number of unknowns is huge‚Äîlike millions or billions of equations‚Äîthe direct method will not work efficiently. You may not have enough computer memory, and other issues may arise. So you use an iterative algorithm to solve the situation. Also, the iterative algorithm I‚Äôm about to explain lets you impose prior knowledge, like non-negativity or smoothness. These topics are beyond the scope of this lecture.\n\nLet me give you the basic idea‚Äîhow you can do trial and error to solve a system of linear equations. As I told you, this is your underlying image. You have four measurements‚Äîtwo horizontal and two vertical‚Äîand you try to solve this system. The starting point: whenever you use an iterative algorithm, you need a starting point. Here, because I know nothing about the image, I choose a neutral guess and assume nothing in the field of view, so every pixel is zero. This is my starting point, or guess zero. It is a natural and unbiased starting point.\n\nFirst, if this guess were correct, then the vertical integrals must be zero and zero. Based on the assumption, I get these two estimated values: zero, zero. We call this a predicted or synthetic projection. It‚Äôs not that you must accept zero, zero just because I say so‚Äîyou can challenge it. You can say, ‚ÄúIf everything is zero, the vertical integrals must be zero, zero.‚Äù But the physical measurement says six and four. It is not zero, zero. How do we explain the contradiction?\nWe compare the measurement with the prediction. We see errors six and four‚Äîhere you measured six but predicted zero, so the error is six; there you measured four but predicted zero, so the error is four. This positive error indicates my initial guess underestimates the pixel values.\n\nAlong this ray, the real measurement is four, but I predicted zero. Clearly, there must be something in these two pixels. Their values should add to six along the first vertical ray. I do not know whether to give more to the first pixel or the second pixel, so to be fair, I evenly divide the error: six becomes three and three. I put the error back. After this redistribution, the two values add to six, so that vertical error is removed. Likewise, for the second vertical ray, four becomes two and two. After this, I am vertically consistent: along the first column, three plus three equals six, and along the second column, two plus two equals four, matching the measurements.\n\nNext, let‚Äôs double-check the horizontal integrals. Now the row sums are five and five. The measured horizontal sums are seven and three. Comparing again, the errors are plus two and minus two. A positive error means I still underestimate that row; a negative error means the true sum is less than my current estimate.\nI redistribute the errors: the plus two is decomposed into one and one and added back to the two pixels in the first row; the minus two is decomposed into minus one and minus one and subtracted from the two pixels in the second row. In this case, we are lucky‚Äîafter this second iteration, we obtain the correct result. Once we reach this state, the vertical and horizontal data are both perfectly explained. We are done‚Äîthis illustrates the idea.\n\nIn real situations, it is never this simple. You need many iterations and many unknowns go back and forth, and a well-designed iterative algorithm will ensure that after many iterations the solution converges. Sometimes the iterative process gives an oscillating solution, and you need regularization. But again, for this undergraduate-level course, notice the basic idea.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 13 - Slide16.txt", "file_path": "Lecture 13\\Texts\\Slide16.txt", "content": "So, in summary, the algebraic approach goes in the following steps.\nFirst, convert the data into line integrals to form a system of linear equations.\u000bThis is the first step.\n\nSecond, solve the system of linear equations to reconstruct the underlying image, like the iterative process I showed you.\nIf needed‚Äîalthough I didn‚Äôt explain the details‚Äîyou regularize the reconstruction with prior knowledge. For example, you know the CT attenuation coefficient, mu, determines attenuated X-ray intensity and cannot be negative. So, during the iterative process, if the current solution gives a negative value, you know it cannot be negative; you force the negative to zero. That is how prior knowledge regularizes the image reconstruction.\n\nThen you iteratively refine the intermediate image, or current guess, one cycle at a time until the outcome is satisfactory.\nHow do you know the outcome is satisfactory? One way is visual‚Äîthe image makes sense given your prior knowledge. Another way is data consistency‚Äîbased on your current image, you predict the projection data; if your prediction compares well with the measurement‚Äîvery close‚Äîyou say, okay, it is good enough. As far as data fitness is concerned, we are doing a good job.\n\nSo, that is the idea of the first algebraic approach. The first one is not very hard, but at least you are sure that we can do it.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 13 - Slide17.txt", "file_path": "Lecture 13\\Texts\\Slide17.txt", "content": "Next, the analytic approach involves the Fourier slice theorem‚Äîthat‚Äôs the key point‚Äîso we need Fourier analysis.\n\nIf you already understand Fourier analysis very well, you will have a good time here. If you are still confused about Fourier analysis, you may struggle a little, and I recommend a review. This is not an easy topic, but it is very cool. For the Fourier analysis, analytic approach, let me give you a heuristic explanation.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 13 - Slide18.txt", "file_path": "Lecture 13\\Texts\\Slide18.txt", "content": "From this point of view, you shoot X-rays, and you are not trying to get a result directly. This is a different perspective. You shoot parallel-beam X-rays, going this way and that way, and then you get line-integral measurements. What do such measurements do? They act like a probing wave.\n\nIn this case, think of the underlying image not as Albert Einstein or a cross-section of your chest. Think of the image as a summation of many waves. Because of Fourier analysis, you can always do this‚Äîyou reconstruct a bunch of waves.\nFor example, consider waves propagating horizontally, like the green wave. For any horizontal wave, let me make some comments so you understand why Fourier analysis works nicely. For such a horizontal wave, for all X-ray projections along oblique directions‚Äîthat is, any orientation that is not vertical‚Äîwe can say one thing: if you do the vertical line integral, the wave has a positive cycle and a negative cycle that cancel out. The wave just keeps oscillating. So if the projection orientation is horizontal or makes an oblique angle‚Äîany degree except vertical‚Äîall these ray-sums give you zero.\n\nGetting zero means you get no information about the underlying horizontal wave‚Äîexcept in one direction: the vertical direction. Along the vertical projection, the positive and negative cycles do not cancel, so only the vertical projection gives you critical information about a horizontal wave. Other directions cancel out.\n\nFrom the vertical projection, you obtain information about waves propagating horizontally. And Fourier analysis says the image decomposes into many waves along different orientations. So, to get horizontal waves, you use vertical projections. For waves at one-degree orientation, you need projections at ninety degrees plus one degree‚Äîthe orthogonal direction.\n\nIf you want to resolve all the waves, you need projection angles spanning zero to one-hundred-eighty degrees. This is the heuristic: you need orientations that probe each wave family. With the horizontal wave superimposed this way, you take the vertical projection; then, if you do Fourier analysis along the horizontal direction, you should recover all the wave information along that horizontal direction. Let me explain this a little better.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 13 - Slide19.txt", "file_path": "Lecture 13\\Texts\\Slide19.txt", "content": "In the so-called Fourier slice theorem, it basically says this: you have the parallel-beam projection. So p of theta‚Äîthis angle is theta. You get all these projections, and each projection carries wave information along its direction, the X-ray direction.\nThe information carried by this projection profile is waves along the p-axis. This p-axis makes an angle theta. It‚Äôs the same idea as before: a vertical projection carries wave information along the horizontal direction. So a projection at angle theta carries wave information along the p-axis, making angle theta.\n\nIf you perform a one-dimensional Fourier analysis, you get the Fourier spectrum. This green profile is the Fourier spectrum of the projection profile. The Fourier spectrum lies along the rho-axis, making the same angle theta. A point on this rho-axis gives a wave whose frequency is proportional to the distance from that point to the system origin. It is a wave propagating along the theta direction, orthogonal to the X-ray beam direction. Along this rho-axis, you have many points; these points represent unique two-dimensional waves propagating along this direction, making an angle theta.\n\nThis is heuristic. You need a one-dimensional Fourier transform to recover a radial line profile in Fourier space. To reconstruct a two-dimensional image, f of x, y, you need all the Fourier information, so the angle theta needs to go from zero to one hundred eighty degrees. When the rho-axis orientation changes from theta equals zero, then one, two, three, up to one hundred eighty degrees, the whole Fourier space is swept by the rho-axis. That means with one projection, you only measure information along one line. But if you change theta from zero to one hundred eighty degrees, all the data points in the Fourier space have been measured‚Äîyou have all the information. Then you can perform a two-dimensional inverse Fourier transform.\n\nThis is a geometrical perspective on how to reconstruct the image using Fourier analysis, or from a wave-analysis perspective. Just these few things‚Äîtwo slides. I hope you understand these two slides. Then we will take a minute's rest. I will show you my mathematics.\nStep by step, you will understand this geometrical wave argument a little better. OK. I think the algebraic perspective may be easier for you. The wave analysis‚Äîif you feel confused, feel free to ask me. Think about it. This is a very elegant way to solve the problem. The Fourier slice theorem is an important theorem for tomographic reconstruction.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 13 - Slide20.txt", "file_path": "Lecture 13\\Texts\\Slide20.txt", "content": "I uploaded a chapter by Clark in a CT book. It‚Äôs a very good one. When I was a fourth-year graduate student, I did a summer job with my supervisor. That summer, I read through his book and implemented the cone-beam reconstruction algorithm. In my view, this is still the best engineering book to explain CT principles. The green textbook explains back-projection, but not as clearly as Clark‚Äôs book. To give a better, deeper explanation, we will use Clark‚Äôs Chapter 3.\n\nNow the coordinate system is x, y. Draw a line making an angle theta; call this line the t-axis. The projection is defined by the perpendicular distance from a point to the t-axis. The line through the origin at that distance is t. It can be expressed as\u000bx cosine theta plus y sine theta equals t.\nThe unit directional vector along the t-axis has components cosine theta, sine theta. They didn‚Äôt draw it, but you can think of cosine theta and sine theta as the two components of the unit vector along the t-direction.\nTake an arbitrary point x, y. Regarding the point as a vector from the origin. Project this vector onto the t-axis. The resulting distance is t. This is really an inner product: one vector with components x, y, and the unit vector with components cosine theta, sine theta. The inner product equals t. As long as t is the same, all points that satisfy this equation lie along the same straight line at a distance t.\n\nYou then do a line integral along that line at an angle theta and distance t. The projection p of theta, t, has two variables, but for a given theta, it is a one-dimensional function of t.\nThis line integral can also be written as a double integral. You integrate the underlying image over the whole plane, and you place a delta function along that line so that only the values on the line contribute. In other words, the double integral with the delta function is nothing but the line integral along that direction. This gives a convenient two-dimensional notation for the projection.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 13 - Slide21.txt", "file_path": "Lecture 13\\Texts\\Slide21.txt", "content": "Then we can write the standard Fourier transform for the underlying function f of x, y. Equation seven is the two-dimensional Fourier transform‚Äîstraightforward. Let me write the one-dimensional Fourier transform‚Äîjust the definition‚Äîequation eight. \n\nI have the one-dimensional function, which is the projection profile p of theta, t. It has one variable. Then you perform the Fourier transform, and you get the one-dimensional Fourier transform with frequency variable w. Here, the two-dimensional frequency variables are u and v. These two are just definitions of the two-dimensional and one-dimensional Fourier transforms.\n\nNow let‚Äôs go a little further. Consider the simplest example, the simplest case, and we try to compute the Fourier analysis. Fourier analysis is a two-dimensional function of u, v, but we set v equal to zero. That means we only consider Fourier coefficients along the u-axis‚Äîthat‚Äôs just one line through the two-dimensional Fourier spectrum. Given v equals zero, this becomes a one-dimensional function. So equation seven becomes equation nine. With v equal to zero, you get this part.\n\nRearrange this a little bit, because now the kernel, the exponential function, does not depend on y. So the integral with respect to y can be grouped into the inner integral. You get this part. And you see that the expression in the bracket is nothing but a vertical integral. Because you have a two-dimensional function, you just do the line integral along the y-axis. So this part in the bracket is nothing but a vertical integral.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 13 - Slide22.txt", "file_path": "Lecture 13\\Texts\\Slide22.txt", "content": "So this is nothing but a vertical integral. A vertical integral is the projection profile when theta equals zero. So that is a vertical integral‚Äîyou get this.\n\nNow, in two-dimensional Fourier analysis, F of u, zero‚ÄîF of u, zero‚Äîthe vertical integral is p at theta equals zero of x. Put this part into the bracket, and then you have this expression.\nSo this is two-dimensional Fourier analysis along the u-axis when v equals zero‚Äîthis is the u-axis. And here is the vertical integral. The vertical integral is a function of x in the x‚Äìy plane. And this part is one-dimensional Fourier analysis with respect to the variable x. So this is the one-dimensional Fourier transform. You can write it as the one-dimensional Fourier transform when theta equals zero, and the variable is u.\n\nThis is the one-dimensional Fourier transform in the general case for an arbitrary angle theta. So this is a special case of what I called the Fourier slice theorem. If you have a vertical projection profile, and you perform one-dimensional Fourier analysis, you get a one-dimensional Fourier spectrum‚Äîyou got a one-dimensional Fourier spectrum. It is nothing but the projection profile in the two-dimensional Fourier-transform space along the u-axis. So this is a special case of the Fourier slice theorem.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 13 - Slide23.txt", "file_path": "Lecture 13\\Texts\\Slide23.txt", "content": "When theta equals zero, you get the two-dimensional Fourier spectrum profile as a one-dimensional profile along the u-axis. How do you get this special-case profile along the single line u? Set theta to zero. You obtain vertical integrals. Vertical integrals give you a one-dimensional signal. You perform a one-dimensional Fourier transform, and you get the spectrum. So you do the physical measurement to get this profile; you perform the one-dimensional Fourier transform; you get the projection profile along this line. In the case I just showed, this line corresponds to theta equals zero, along the u-axis.\n\nIn this way, you recover Fourier-space information along only one line. With an X-ray measurement, you recover Fourier-domain information. Ideally, you want all the Fourier information recovered. Then you perform the two-dimensional inverse Fourier transform and reconstruct the underlying image f of x, y. The Fourier slice theorem is not limited to theta equals zero. The theorem claims that for arbitrary theta, this holds true. For an arbitrary theta, you have an arbitrary one-dimensional projection profile. You perform the one-dimensional Fourier transform and recover the profile along the corresponding line. As explained earlier, if you keep theta changing from zero to one hundred eighty degrees, the line sweeps the whole Fourier space, and you recover all the information. That is the idea.\n\nNow the heuristic: if you have the vertical projection property proved as such, you can immediately understand that the general case must be true. Why? Because the two-dimensional Fourier transform has a rotation property. If you rotate the object by thirty degrees, the two-dimensional Fourier spectrum is also rotated by thirty degrees. Since the angle theta is arbitrary due to the rotation property, you can select the angle as you set up the system. So if you ask, ‚ÄúI have this projection profile; I perform a one-dimensional Fourier transform; will I get the same type of profile along the corresponding line?‚Äù The answer is yes. I can select my x-axis along that direction and my y-axis perpendicular to it. In this rotated x‚Äìy coordinate, I can use exactly what I explained. Therefore, the Fourier slice theorem must be true. You can understand this immediately from the rotation property of the two-dimensional Fourier transform.\n\nWe can also do it mathematically. We introduce a rotated coordinate system t, s. For u, v, we introduce t, s; and actually this t, s is better placed in the x‚Äìy space. So we move this to the x‚Äìy plane.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 13 - Slide24.txt", "file_path": "Lecture 13\\Texts\\Slide24.txt", "content": "You can link the t, s coordinates to x, y through a straightforward coordinate transformation. Then you can go through the mathematical derivations to show the Fourier slice theorem in the general case. The theta angle is the same in the u, v domain and in the x, y domain.\n\nWith this coordinate transform, p of theta, t, for angle theta as a function of t, can be expressed as a vertical projection in the t, s coordinate system. Here s is the vertical direction in the t, s system.\n\nNext, perform the one-dimensional Fourier transform. The exponential factor is e to the power minus j two pi w t. Then do the same trick as before: insert the definition of the projection profile for angle theta into the bracket. You get the resulting expression, pretty much like the simplest case we did.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 13 - Slide25.txt", "file_path": "Lecture 13\\Texts\\Slide25.txt", "content": "Then you change the transform back from the t, s system to the x, y coordinate system.\u000b\nYou can express it this way. Rearranging a little, it can be expressed as a two-dimensional Fourier transform, capital F. The u component is w cosine theta, and the v component is w sine theta. Because in the two-dimensional Fourier transform you have x u plus y v, the factor w is redistributed back, giving this relationship.\n\nS theta of w equals F of open parenthesis w cosine theta, w sine theta close parenthesis.\nThis is nothing but the general form of the Fourier slice theorem. This is the mathematical derivation. You can review it yourself and check Chapter 3 if needed.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 13 - Slide26.txt", "file_path": "Lecture 13\\Texts\\Slide26.txt", "content": "Let me visualize what we mentioned.\u000b\nYou have the underlying image f of x and y, and using X-ray measurements, you get the projection profile p of theta and t. For a given theta, you perform a one-dimensional Fourier transform with respect to t. You get a line profile, making the angle theta in the two-dimensional Fourier space.\n\nBut theta can be changed as you wish from zero to pi, that is, zero to one hundred eighty degrees. So you get a line for theta equals zero, a line for theta equals fifteen degrees, a line for theta equals eighty degrees, and a line for theta equals one hundred seventy degrees, for example. When theta changes from zero to one hundred eighty degrees, all these radial lines will fully cover the Fourier space u, v. All the values are measured this way.\n\nWhen you know f of u, v completely, you perform the inverse Fourier transform and recover f of x, y. So the analytic process here is completed using the Fourier transform, particularly in the form of the Fourier slice theorem. This is a Fourier imaging example: we explain measured data in the Fourier space. We try to fill the Fourier space completely, and then we can perform image reconstruction.\nThis is just a little more specialized idea of how you use the Fourier transform to do CT reconstruction.\u000b\nOkay.\nSo let me go a step deeper. This is the general idea. First, I explained the general idea, then I explained the Fourier slice theorem, a little more specifically.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 13 - Slide27.txt", "file_path": "Lecture 13\\Texts\\Slide27.txt", "content": "Now let‚Äôs go even deeper and give a specific algorithm called filtered backprojection. This is nothing but the inverse Fourier transform, not in the rectangular coordinate system, but in the polar coordinate system, because we keep changing theta. So we should represent the inverse Fourier transform in polar coordinates, so that what you measure in polar coordinates fits directly into the formula.\n\nLet me go through the mathematical steps. This is the inverse two-dimensional Fourier transform. If you know the two-dimensional Fourier spectrum, capital F of u, v, you perform the inverse transform, and you get f of x, y.\nMy motivation is to use polar coordinates. In polar coordinates, you have radial lines in the Fourier space. That radial variable is W‚Äîthis is really the rho I showed you before‚Äîand the polar angle is theta. So\u000bu equals W times cosine theta, and v equals W times sine theta. That is the polar-to-rectangular coordinate transformation.\n\nFor d u d v, the small differential area element, in polar coordinates, you need W dW d theta. The small area element d u d v becomes W dW d theta in polar form. Here, W is the radius. A small angle d theta gives an arc length W d theta. A small radial increment dW gives the thickness. Multiply them together and you get the small area element. This is just your calculus: W dW d theta.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 13 - Slide28.txt", "file_path": "Lecture 13\\Texts\\Slide28.txt", "content": "So you put the inverse Fourier transform in terms of the polar coordinate system. In polar coordinates, the angle theta goes a full circle, and the radius W goes from zero at the origin all the way to infinity. That covers the full space.\n\nThen we do a little trick. We decompose the full circle into two half-scans: from zero to pi, and from pi to two pi. For the second part, I write theta plus pi instead of just theta. That is why the interval from pi to two pi becomes zero to pi after the change of variables. So you get this form.\n\nFor parallel-beam geometry, the angular range of zero to one hundred eighty degrees is enough. If you scan from zero to three hundred sixty degrees, you simply double the information; you really need only half of it.\nMathematically, we handle this by changing the capital F of W, theta, plus one hundred eighty degrees back to a function of theta. Use the identities cosine of theta plus pi equals minus cosine theta, and sine of theta plus pi equals minus sine theta.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 13 - Slide29.txt", "file_path": "Lecture 13\\Texts\\Slide29.txt", "content": "Taking all these trivial transformations, here is the property we use and can easily verify in Fourier analysis: when the angle shifts by one hundred eighty degrees, you keep the same angle but reverse the sign of w. With this, you only need the angular range from zero to pi.\n\nNext, change W to minus W. Then the radial limit from zero to infinity becomes zero to minus infinity. Swap the limits, and the inner integral runs from minus infinity to plus infinity. Put everything together and you have:\n\u000b‚Äúf at x, y equals the integral from theta equals zero to pi, integral from w equals minus infinity to infinity, S theta of w times absolute value of w times e to the j two pi w t d w, d theta; with t equals x cosine theta plus y sine theta.‚Äù\nWhere t equals x cosine theta plus y sine theta.\nAll these are just mathematical details‚Äîif you get lost, review and you will be able to follow. The key point is that we end up with this formula. This formula is what we call filtered backprojection. If you got lost somewhere, it is not critical; you can review. But trust me for now‚Äîthrough these steps, you get this result. This is filtered backprojection.\n\nWhy ‚Äúfiltered‚Äù? Because here you see S of w‚Äîthe one-dimensional Fourier spectrum along the radial line‚Äîand it is multiplied by the absolute value of w. If you performed the inverse Fourier transform without the absolute value of w, you would go back to the original projection profile, the one-dimensional signal you measured. But with the additional factor absolute value of w, the original Fourier spectrum is modified.\nWhen w is small‚Äînear the origin‚Äîlow-frequency components are weighted small. When w is large‚Äîat high frequency‚Äîthe weighting is large. So S of w times absolute value of w is a high-pass filtering: the high-frequency components are elevated in proportion to absolute value of w. After this spectral modification, you perform the inverse Fourier transform and go back to the projection domain. Because of the high-frequency enhancement, this inverse transform is no longer the original projection profile. Instead, it is a modified, high-pass-filtered projection profile. It is not p theta of t anymore; it is q theta of t, the filtered projection profile. That is why we call it filtered.\n\nWhat do I mean by backprojection? \nYou filter first‚Äîthis one-dimensional filtration‚Äîand then you put the filtered values back into the field of view. According to the argument t equals x cosine theta plus y sine theta, for any x, y you retrieve the value q theta of t from your filtered projection profile and accumulate it back over angles. That step is the backprojection process. ‚ÄúFiltration‚Äù may be clear; ‚Äúbackprojection‚Äù can feel a little confusing, but it simply means smearing the filtered profile back across the image along the corresponding rays.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 30, "slide_filename": "Slide30.txt", "slide_annotation": "Lecture 13 - Slide30.txt", "file_path": "Lecture 13\\Texts\\Slide30.txt", "content": "So here we have this picture. The backprojection is visualized. This is the projection profile after filtration, so it is a filtered projection profile. This filtered projection profile, Q theta i of t, is really smeared back over the field of view. Think of this projection profile as being smeared back from this particular direction. You have many filtered projection profiles; you sweep all of them and smear them back over the field of view. Add them together to get the result.\n\nFor a given x, y, what is the contribution from a given Q theta? That is the question I want to explain. Suppose x, y is here. For this x, y‚Äîthe point in image space‚Äîhow much contribution do we get from a particular filtered projection profile, here Q theta i? You do this inner product: you get t. Spoken clearly: t equals x cosine theta i plus y sine theta i. That t is the distance between this line and the central line, both perpendicular to this direction, making an angle theta i. For any point along this line, the inner product gives the same t. So, as long as the pixel is on this line at distance t, you retrieve this value from the filtered projection profile. For all points on this line, t is the same. You compute t, you get the value, and you put that value back for any x, y on this line. That value is the same here, here, and here‚Äîall along the line. If that value is one, then along that line every point gets one. This is what I call smearing back.\n\nThat was for this theta, theta i. For another angle, you smear back in a different way. The process is linear‚Äîthe integral means summation. For each theta, you retrieve the value from Q theta, then add it back to the particular point x, y. For another angle you do the same. You can view it like this: from this filtered projection profile, you get the value here; along a different direction, another projection profile gives you another value through that same point x, y. So, heuristically, from the filtered projection domain, any profile is smeared back uniformly, its value covering all points along its line. All projection angles are added in the same way, and then you have the image value recovered. Think about that. I will show reconstructing examples in a few minutes, so you can get a better idea. OKay.\n\nNow, several slides with green buttons are about the reconstruction filter. Here, the reconstruction filter‚Äîa high-pass filter‚Äîis the absolute value of w. This filter does not have an inverse Fourier transform by itself, because the absolute value of w is not integrable. However, we can introduce a band-limited assumption: for a given projection profile, you have a maximum frequency, a maximum bandwidth W.\n\nSo the filtration really needs to be done with this truncated filter. In the original Fourier space, the ideal is the absolute value of omega extending to infinity. We assume a maximum bandwidth capital W; outside that, nothing is meaningful for the data. With this window, the spectrum becomes integrable. You truncate the otherwise divergent high-pass filter, then perform the inverse transform with respect to this truncated version, H of omega. You compute the inverse Fourier transform as the spatial-domain counterpart of the high-pass filter. You do the computation, and you get this part.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 31, "slide_filename": "Slide31.txt", "slide_annotation": "Lecture 13 - Slide31.txt", "file_path": "Lecture 13\\Texts\\Slide31.txt", "content": "Now we assume capital W is the maximum bandwidth. Because of that, we can use the sampling theorem. The projection profile can be expressed through the sampling kernel in terms of discrete sampling points P theta at k tau‚Äîwhere tau is the sampling step. \n\nLikewise, the filtering kernel, under the assumption of limited bandwidth capital W, can be expressed in terms of sampled values. So you get these two equations.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 32, "slide_filename": "Slide32.txt", "slide_annotation": "Lecture 13 - Slide32.txt", "file_path": "Lecture 13\\Texts\\Slide32.txt", "content": "So the continuous-domain expression can be expressed in terms of discrete data points. You get the filtered projection profile in terms of sampled data points. This is the band-limited high-pass filtering. \n\nYou can read more if you are interested, but anyway, these are practical implementation details about high-pass filtering for filtered backprojection. What I have explained here is only the two-dimensional case.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 33, "slide_filename": "Slide33.txt", "slide_annotation": "Lecture 13 - Slide33.txt", "file_path": "Lecture 13\\Texts\\Slide33.txt", "content": "And in the three-dimensional case, you have an extended, higher-dimensional version of the Fourier slice theorem. The mathematics is a little more complicated, but the essential idea is still the same.\n\nYou have a 3-D object. You view it as a superposition of 3-D waves‚Äînot 2-D waves‚Äîwith all kinds of orientations and frequencies. When you form a projection from one direction and superimpose contributions along that direction, you only get information for the wave components whose directional vector and frequency components lie in the plane orthogonal to the projection direction. \nIn other words, the 2-D Fourier transform of a 2-D projection gives you a central plane through the 3-D Fourier transform of the object, oriented perpendicular to the projection rays. This is really just the extension of the 2-D Fourier slice theorem.\n\nI don‚Äôt want to confuse you too much‚Äîagain, you see this nice green button.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 34, "slide_filename": "Slide34.txt", "slide_annotation": "Lecture 13 - Slide34.txt", "file_path": "Lecture 13\\Texts\\Slide34.txt", "content": "So, analytic approach: we convert X-ray data into Fourier space‚Äîequivalently, we could work in Radon space‚Äîbut here I explain the Fourier-space processing. We invert the Fourier-space transform according to a closed-form formula, like the one I derived. Filtered backprojection is an analytic formula; you do not need iterative reconstruction.\n\nIn the reconstruction process, some filtering or prior processing steps may be used. An iterative algorithm is very useful when the data are not complete‚Äîsay, some views are missing, projections are blocked by metal, or there are other imperfections in data acquisition. Analytic pros: you don‚Äôt need an iterative process; you have a formula‚Äîthat‚Äôs nice. But it assumes low noise and complete data.\n\nSo the two approaches‚Äîiterative and analytic‚Äîhave their strengths and weaknesses. Nowadays, iterative methods are often more popular when we need low-dose reconstruction or a very short acquisition time. And filtered backprojection, as I explained here, is just the mathematical formulas plus the geometrical pictures I mentioned.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 35, "slide_filename": "Slide35.txt", "slide_annotation": "Lecture 13 - Slide35.txt", "file_path": "Lecture 13\\Texts\\Slide35.txt", "content": "Now I‚Äôm going to show you some numerical examples at the end so you have a better understanding. \nThese examples let you see how what we learned works on real cases. After that, we‚Äôll take a quick look at a few clinical images.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 36, "slide_filename": "Slide36.txt", "slide_annotation": "Lecture 13 - Slide36.txt", "file_path": "Lecture 13\\Texts\\Slide36.txt", "content": "So the real example is a simple cross-section with a small bright disk. \n\nYou do one-dimensional projection data acquisition at an angle theta. You get this profile, and you see a peak due to the bright disk. This is at an angle theta. Theta can be changed from zero degrees to pi. As theta changes, the peak traces half of a sinusoidal curve in the sinogram. For this given view, the projection profile is here.\n\nWe have this example, but at different elevations‚Äîthat is, at different theta angles‚Äîyou have different data. So, if you do backprojection, what will happen? I explained a lot about backprojection: backprojection is the filtered projection profile smeared back over the field of view. Let me show you what happens. First, let‚Äôs not bother with any filtration. This is the projection profile, and I backproject it.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 37, "slide_filename": "Slide37.txt", "slide_annotation": "Lecture 13 - Slide37.txt", "file_path": "Lecture 13\\Texts\\Slide37.txt", "content": "So this is the projection profile, something like this. You do backprojection‚Äîyou smear the profile back along the original X-ray path over the field of view. The higher value here is put back, so along this line, the value is placed at every pixel on the line. Here, the value is a little low, so along this line, every point is a little dark. You find the pixels on that line, take the value, and smear it back.\n\nThis is from one direction. You do the same from every direction. The smear-back results are added together, because the integral with respect to d-theta is just a summation. You smear back one projection‚Äîyou get a picture like this. You take another projection‚Äîyou smear back the other way. You add the two together. Many projections add many contributions together.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 38, "slide_filename": "Slide38.txt", "slide_annotation": "Lecture 13 - Slide38.txt", "file_path": "Lecture 13\\Texts\\Slide38.txt", "content": "From one hundred eighty degrees, you add everything together. You get something not too much different from the truth, but it looks very blurred. You still see a region that is a little high, because this part of the projection is high. When you backproject along the original X-ray paths, that high value is placed along those lines.\n\nHowever, there are artifacts. In the smearing-back process, a high value here gets spread to places that are not actually high in the object. Backprojection alone smears the high values around. So, simply collecting projections and doing backprojection is not enough to recover an accurate image. That is why we really need to filter the projections.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 39, "slide_filename": "Slide39.txt", "slide_annotation": "Lecture 13 - Slide39.txt", "file_path": "Lecture 13\\Texts\\Slide39.txt", "content": "So you have a projection profile. You don‚Äôt just smear it back. First, apply a high-pass filter with kernel H. After filtering, you get a high-pass-filtered projection profile, called P-prime of theta, t. \n\nThen you perform backprojection. That‚Äôs filtered backprojection: filter each projection with H, and backproject every filtered projection profile.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 40, "slide_filename": "Slide40.txt", "slide_annotation": "Lecture 13 - Slide40.txt", "file_path": "Lecture 13\\Texts\\Slide40.txt", "content": "This is the sinogram. The filtering is one-dimensional along the horizontal direction, so a high-pass filter emphasizes edges. \n\nHere is the original sinogram; here is the filtered sinogram. Once it is filtered, we use these data for backprojection.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 41, "slide_filename": "Slide41.txt", "slide_annotation": "Lecture 13 - Slide41.txt", "file_path": "Lecture 13\\Texts\\Slide41.txt", "content": "Then you can recover the original image. This looks very close to the original. If you have many angles, with very small angular increments, and the detector spacing is very small, the result becomes closer and closer to the true image. \n\nThis is just a numerical example. The next few slides show a MATLAB implementation‚Äîalso your homework‚Äîso you can read through and try it yourself.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 42, "slide_filename": "Slide42.txt", "slide_annotation": "Lecture 13 - Slide42.txt", "file_path": "Lecture 13\\Texts\\Slide42.txt", "content": "MATLAB has toolbox commands that implement projection, filtering, and filtered backprojection. \n\nThe keyword is radon. You supply an image and theta. Theta can be a single value, which gives one projection, or an array, which gives many projections.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 43, "slide_filename": "Slide43.txt", "slide_annotation": "Lecture 13 - Slide43.txt", "file_path": "Lecture 13\\Texts\\Slide43.txt", "content": "The inverse transformation is done with iradon. The projection beams are laid out line by line, and the default line spacing is one pixel‚Äîone unit apart.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 44, "slide_filename": "Slide44.txt", "slide_annotation": "Lecture 13 - Slide44.txt", "file_path": "Lecture 13\\Texts\\Slide44.txt", "content": "The setup looks like this: parallel-beam geometry with sensors on one side, a source on the other, and the system rotating by the angle theta to collect projections.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 45, "slide_filename": "Slide45.txt", "slide_annotation": "Lecture 13 - Slide45.txt", "file_path": "Lecture 13\\Texts\\Slide45.txt", "content": "When you do projection, they basically decompose a single pixel into a two-by-two matrix, then shoot rays through all these centers into the projection domain. \n\nIf a ray hits the center of a detector bin, that detector takes the full value. If it hits the boundary between two detector bins, the value is divided equally. If it hits at an arbitrary location, linear interpolation is used.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 46, "slide_filename": "Slide46.txt", "slide_annotation": "Lecture 13 - Slide46.txt", "file_path": "Lecture 13\\Texts\\Slide46.txt", "content": "Here‚Äôs a sample Radon code you can run yourself. It generates an image with a white block, then produces the projection data. \n\nAfter that you apply the filtration. The red arrows simply point to figures that appear on the next slide.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 47, "slide_filename": "Slide47.txt", "slide_annotation": "Lecture 13 - Slide47.txt", "file_path": "Lecture 13\\Texts\\Slide47.txt", "content": "You get this projection profile‚Äîthe sinogram.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 48, "slide_filename": "Slide48.txt", "slide_annotation": "Lecture 13 - Slide48.txt", "file_path": "Lecture 13\\Texts\\Slide48.txt", "content": "Next, use the inverse Radon code, which essentially performs filtered backprojection. You can also choose linear interpolation, and if you set the filter to ‚Äúnone,‚Äù that means no filtering‚Äîjust simple backprojection.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 49, "slide_filename": "Slide49.txt", "slide_annotation": "Lecture 13 - Slide49.txt", "file_path": "Lecture 13\\Texts\\Slide49.txt", "content": "On the top row images, you can see the difference: the unfiltered backprojection is blurry, while the filtered backprojection looks much closer to the original image. This is the key to reconstructing CT images.\n\nThe bottom row shows some clinical examples. It feels like magic. Review the material; if you didn‚Äôt do the preview and you don‚Äôt review, you won‚Äôt catch all the tricks. But if you read carefully, you‚Äôll understand the secret: the X-ray machine sends beams you never see, yet your internal structure is clearly revealed. \n\nYou can resolve features down to about one-third of a millimeter, with every detail laid out by the algorithms I just explained. It‚Äôs an amazing achievement‚Äîinner vision with X-rays.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 13", "slide_number": 50, "slide_filename": "Slide50.txt", "slide_annotation": "Lecture 13 - Slide50.txt", "file_path": "Lecture 13\\Texts\\Slide50.txt", "content": "For homework, review what we did with the block, and do the same steps using an ellipse. \n\nWork through the MATLAB code. Some variable names may not be obvious from the slides, but if you open MATLAB Help or search online, everything is straightforward. Spend some time, get familiar, and see how filtered backprojection works for you. \n\nThat‚Äôs all for today‚Äîthank you.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 14", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 14 - Slide1.txt", "file_path": "Lecture 14\\Texts\\Slide1.txt", "content": "Good afternoon.\u000bToday, we will explain the system aspects of computed tomography. Since the CT data acquisition is a scanning process, a CT system is typically called a CT scanner.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 14 - Slide2.txt", "file_path": "Lecture 14\\Texts\\Slide2.txt", "content": "As you can see, the topic of this lecture is called CT scanner.\u000bNext time, we will have a MATLAB session.\u000bDuring that, we‚Äôll talk about filtered backprojection and related concepts.\u000bYou‚Äôll have the opportunity to review MATLAB commands and functions so you can perform filtered back projection.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 14 - Slide3.txt", "file_path": "Lecture 14\\Texts\\Slide3.txt", "content": "So, let‚Äôs look at the outline for today‚Äôs lecture.\n\nFirst, we‚Äôll talk about projection data truncation‚Äîthe issue when the X-ray projections don‚Äôt fully cover the object, which can cause artifacts.\n\nNext, we‚Äôll discuss different scanning modes used in CT technology.\n\nThen, we will move on to common image artifacts that can degrade image quality.\n\nFinally, we‚Äôll cover X-ray radiation dose, which is important because it can be harmful to patients if not properly managed.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 14 - Slide4.txt", "file_path": "Lecture 14\\Texts\\Slide4.txt", "content": "As we discussed in the previous lecture, when you collect X-ray projection data from an unimpeded perspective, you aim to reconstruct the image layer by layer, starting from the outermost shell.\n\nIt‚Äôs like peeling an onion, gradually revealing each layer beneath the surface.\nTo achieve this, you need to have X-ray measurements along every line passing through the region of interest.\nFor each X-ray, you want to record the line integral‚Äîthe total attenuation along that path.\nThis situation pertains to 2D image reconstruction.\n\nAlways keep in mind the principle of unimpeded data acquisition.\nWe say there must be at least one point on the X-ray source trajectory for every line passing through the object.\nIn other words, you need a measured projection corresponding to every line through the cross-section.\nThis ensures you have complete data for accurate reconstruction.\n\n\n.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 14 - Slide5.txt", "file_path": "Lecture 14\\Texts\\Slide5.txt", "content": "Now, in the 3D CT case, the X-ray source remains a point source. But instead of using a linear detector array, you use a planar or two-dimensional detector array.\n\nThis configuration results in a cone beam geometry originating from the point source.\nThe X-ray source emits radiation in a cone-shaped beam, which should be wide enough to cover the entire object.\nIn other words, the object must be fully contained within the cone beam to ensure complete data acquisition.\nThis concept is similar to the 2D case but extended to volumetric imaging.\n\nOnce you have all the rays, as shown here, you can rearrange, reorganize, or re-bin the X-rays into parallel beam geometry.\nIn parallel beam geometry, you can apply Fourier transform-based imaging methods.\nThe Fourier slice theorem allows the use of the one-dimensional Fourier transform on projections to recover the two-dimensional Fourier spectrum.\nEach projection recovers a profile in Fourier space, and by changing the viewing angle, you sample different radial lines sweeping over the entire Fourier domain.\n\nLikewise, in 3D imaging, an extended version of the Fourier slice theorem is required.\nThis involves applying Fourier transforms in higher dimensions to recover the 3D spatial frequency information of the object.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 14 - Slide6.txt", "file_path": "Lecture 14\\Texts\\Slide6.txt", "content": "The Fourier transform, as we learned, is the foundation of classic CT theory. It requires complete coverage of an entire cross-section or the whole object. When you have projection data, you can use the Fourier slice theorem in 2D or 3D, and then perform the inverse Fourier transform also in 2D or 3D. This classical framework is called the central dogma.\n\nThe whole cross-section or entire object must be fully covered by the X-ray beam. No matter the shape of the beam, you can always re-bin the data into parallel beam geometry. Then, the collected data can be inversely transformed back to the original image domain. That‚Äôs the fundamental way image reconstruction is performed.\n\nAs we learned, filtered back projection is essentially just the inverse Fourier transform represented in polar coordinates. This core concept explains the process pretty well. If you follow this, you should grasp the essential idea. In my opinion, the key question is: what is the difference between classic CT theory and modern CT theory?", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 14 - Slide7.txt", "file_path": "Lecture 14\\Texts\\Slide7.txt", "content": "This leads us to the issue of data truncation. If the data is incomplete, the Fourier slice theorem‚Äî which assumes a full projection profile for each one-dimensional Fourier transform‚Äîcan no longer be applied straightforwardly.\n\nWhat happens if part of the data is missing? If some portion of the projection profile is lost, the Fourier transformation cannot be performed fully. This is called the data truncation problem. Much of modern CT theory has been developed to address this data truncation problem, finding ways to reconstruct images even with incomplete data.\n\nBefore we delve into the types of data truncation, let me give you some context.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 14 - Slide8.txt", "file_path": "Lecture 14\\Texts\\Slide8.txt", "content": "First, let me show you the four generations of CT scanning geometry, often called generations or modes. This basically refers to how you systematically collect X-ray projection data.\n\nIn the first generation, you have a single point X-ray source and a single point detector. Each measurement records one line, one line integral. The setup moves by translation, producing one parallel beam projection in one direction. Then the entire apparatus rotates slightly to a new angular position, producing another parallel beam projection. This process depends on the viewing angle, often labeled theta. That‚Äôs the first generation.\nThe second generation improves data acquisition efficiency. Instead of measuring one line integral at a time, you use a small detector array with a narrow fan beam. This detector array collects multiple lines simultaneously as it translates, covering the cross-section completely. Each translation collects multiple parallel beam projections, depending on how many detector shells you have. For example, if you have five detector shells, you get five parallel beam projections per translation. After a small rotation, you repeat this process.\n\nThe third generation further improves efficiency dramatically. Instead of a few detector elements, you have hundreds or thousands along a linear or arc-shaped wide detector array. This is a full fan beam imaging geometry. The detector array continuously rotates to collect data. This configuration satisfies the data sufficiency condition: performing a full 360-degree scan ensures every line through the patient‚Äôs cross-section is measured somewhere, so the line integrals are known. This third-generation geometry is still very popular. Modern CT scanners mostly use this geometry but have expanded the one-dimensional detector arrays into multi-row or cone-beam configurations. We‚Äôll discuss those later.\n\nThe fourth generation differs by making the detector array a full stationary ring. Instead of rotating the detectors, only the X-ray source rotates around the patient. The detector ring stays fixed, continuously collecting signals. This fourth-generation geometry solves some mechanical problems associated with rotating the detector array.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 14 - Slide9.txt", "file_path": "Lecture 14\\Texts\\Slide9.txt", "content": "Next, we have electron beam CT, which uses a different approach. Instead of a mechanical X-ray tube rotation like earlier generations, this system uses an electron beam that‚Äôs electromagnetically steered to hit a fixed target shaped as multiple metallic arcs.\nEach arc produces X-rays upward, forming the scanning beam. The electron beam is steered by electromagnetic fields very rapidly without moving parts mechanically. This allows for much faster imaging speeds.\n\nThe driving force behind the evolution of scanning generations is increasing data acquisition efficiency: collecting the same amount of data in a shorter time to improve temporal resolution.\nCardiac imaging is the most challenging application for CT, since the heart beats fast, anywhere between 60 and 100 beats per minute. Tomographic reconstruction assumes the object is stationary, but the heart is constantly moving. So, we need to shorten the data acquisition time drastically.\n\nThe electron beam CT scanner was designed specifically for cardiac imaging due to its high speed. However, it is costly, and image noise is often higher. Additionally, the fourth-generation geometry faces problems with scattered signals affecting other detectors, which causes degradation in image quality.\nBecause of these limitations, the third-generation geometry remains more widely used today, and electron beam CT scanners are now mostly considered outdated.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 14 - Slide10.txt", "file_path": "Lecture 14\\Texts\\Slide10.txt", "content": "Defining CT scanning modes, or how many generations to count, varies depending on the source. Different textbooks categorize them differently. Usually, the first three generations are well established: the first generation, the second generation, and the third generation.\n\nHowever, beyond these, the classification becomes a matter of opinion and varies by source.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 14 - Slide11.txt", "file_path": "Lecture 14\\Texts\\Slide11.txt", "content": "Let's look at scanning modes. Here, you can visualize a diamond that helps you remember the progression of CT generations. The first generation uses a parallel beam, then a narrow fan beam in the second generation, followed by a wide fan beam with rotation in the third generation. The third generation features fan beam geometry with rotating detectors. Beyond that, we have the fourth and fifth generations. The next stages are called the sixth or seventh generation, which involve helical scanning.\n\nHelical scanning is quite an innovative, out-of-the-box idea. Traditionally, with one-dimensional detector arrays, imaging is done by rotating the source around the patient‚Äîoften a half circle or full circle‚Äîto collect sufficient data for reconstructing one cross-section using filtered back projection or iterative reconstruction algorithms. After reconstructing one slice, the patient table is translated slightly to image the next cross-section. This process is known as the ‚Äústep and shoot‚Äù mode.\n\nYou scan one cross-section, then move the patient, accelerate or decelerate the table, perform another full rotation, and repeat. This approach is time-consuming, especially because anatomical and pathological features exist in three dimensions, and patients can only hold still for limited times. Scanning slice by slice leads to low throughput and motion artifacts.\nSpiral scanning solves many of these issues. You ask the patient to remain still, often holding their breath. Then, the table moves continuously while the X-ray source rotates, and data acquisition happens simultaneously and continuously. From the patient‚Äôs perspective, the source follows a helical trajectory, which is why this mode is called helical imaging.\n\nHowever, helical scanning introduces data truncation issues. In second-generation scanning, the data may be transversely truncated due to the small detector size. In helical scanning, data is longitudinally truncated because the source follows a helical path within any transverse plane, meaning a full two-dimensional data set for image reconstruction is absent.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 14 - Slide12.txt", "file_path": "Lecture 14\\Texts\\Slide12.txt", "content": "This slide illustrates a spiral, single-slice CT scan, showing the imaging geometry. You see the helical path around the patient clearly. \n\nNow, the question is: how do we perform image reconstruction effectively despite this geometry? \n\nHow do we handle the challenges posed by the helical scan?", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 14 - Slide13.txt", "file_path": "Lecture 14\\Texts\\Slide13.txt", "content": "With helical scanning, when you want to reconstruct an XY cross-section, ideally, you‚Äôd have rays confined entirely within the XY plane. But the actual green helical scanning path deviates from this ideal.\n\nYou measure line integrals only along rays that coincide with this helical path. Immediately, the X-ray source moves out of the plane, producing rays above and below it. To reconstruct a cross-section, say in the XY plane, you need every line integral through this section, including along particular rays such as Ray A.\n\nIn practice, you can‚Äôt measure the line integral exactly along Ray A because the source never stays on that plane long enough. You only have rays above and below it. That‚Äôs the central data truncation problem in helical CT.\nEngineers found a practical solution. Helical scanning benefits from continuous table motion without stops or accelerations. The patient lies flat on the table, which moves steadily through the scanner. The patient simply stays still until done, making the process fast and smooth.\nTo compensate for missing data, we perform linear interpolation between the upper and lower rays, which are available. The assumption is that the data varies roughly linearly between these rays. This allows synthesis of an in-plane data set, which then can be used with filtered back-projection algorithms. This was the first trial approach.\n\nLater improvements involved helical scanning which uses opposite rays, not just the nearest rays in the same direction. Before reaching a certain position, you could use complementary rays shot from an opposite angle to improve interpolation.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 14 - Slide14.txt", "file_path": "Lecture 14\\Texts\\Slide14.txt", "content": "Using opposite rays that are closer to ray A reduces the distance between the measured and estimated line integrals. The narrower this gap, the better the accuracy you can expect.\n\nThis ‚Äúhalf-scan‚Äù interpolation enhances longitudinal resolution along the Z-axis compared to linear interpolation. Longitudinal resolution along the Z direction is often lower than in-plane resolution.\nBack when I just graduated and started work at Washington University Medical School in St. Louis, helical CT was introduced. Radiologists appreciated the improved temporal resolution it offered.\n\nHowever, they noticed motion blurring along the Z-axis caused by the interpolation process. Since the patient is continuously moved through the scanner, motion blur appears in the longitudinal direction, similar to a photo taken while running.\n\nAs they say, there‚Äôs no free lunch‚Äîyou gain temporal resolution but lose some longitudinal resolution due to motion blur.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 14 - Slide15.txt", "file_path": "Lecture 14\\Texts\\Slide15.txt", "content": "We analyzed helical CT scanning modes further. Traditional step-and-shoot CT uses multiple circular scans. For each scan, you reconstruct one CT slice, repeating many times for full coverage.\nIn helical CT, instead of discrete circular scans, you have one continuous helical acquisition. For each helical rotation, interpolation creates synthetic data points.\n\nYou can reconstruct images at many arbitrary longitudinal positions along the patient, generating a large number of slices computationally. This technique is called retrospective reconstruction.\nRetrospective reconstruction has a significant clinical advantage. For example, in tumor detection, a lesion may fall between two slices in traditional scanning and go undetected.\n\nWith retrospective reconstruction, densely sampled slices mean one slice will include the tumor, improving detection confidence and diagnostic accuracy.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 14 - Slide16.txt", "file_path": "Lecture 14\\Texts\\Slide16.txt", "content": "Next, we have partial volume averaging artifacts. For each slice, if a tumor only partially lies within that slice, you might not see it clearly. Small tumors can be especially difficult to detect when they occupy only half of a slice, and image noise further complicates clear visualization.\n\nHowever, with spiral CT, you can select longitudinal positions as densely as you want. This gives you a real opportunity to reconstruct images densely enough. One of the reconstructed slices is likely to contain the tumor fully, providing much better contrast resolution and visibility.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 14 - Slide17.txt", "file_path": "Lecture 14\\Texts\\Slide17.txt", "content": "Building on this heuristic idea, we performed a rigorous linear system analysis and published a journal paper in Medical Physics. We performed modeling the signal response of the CT system to compare the two scanning modes as I mentioned earlier in the lecture; that is, multiple scanning circles versus a spiral or helical scanning trajectory.\n\nOur key theoretical conclusion is that, for a given X-ray dose, meaning a fixed number of total incident photons, you can choose to scan in either the conventional step-and-shoot mode or helical mode to have multiple scanning circles or a single helical trajectory. Given the same X-ray dose, helical CT offers substantially better longitudinal resolution than conventional step-and-shoot.\nThis improvement comes from the inherent capability of helical CT to perform retrospective reconstruction‚Äîthat is, reconstructed overlapping slices along the longitudinal axis.\n\nIn our papers, we specifically recommended reconstructing roughly three to four slices per helical turn for the best results.\nThus, helical CT not only improves temporal resolution but also enhances longitudinal resolution, so there‚Äôs no loss in image quality. This important finding helps explain why helical scanning is so beneficial.\n\nSome believe that you gain temporal resolution with helical scanning but lose longitudinal resolution. However, with proper overlapping reconstruction, both temporal and longitudinal resolution can be improved. This is a key insight from our earlier work and kind of a free-lunch in clinical practice.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 14 - Slide18.txt", "file_path": "Lecture 14\\Texts\\Slide18.txt", "content": "Helical or spiral CT began with single-slice scanning using a linear detector array, capturing rays in two dimensions.\nExpanding this, cone beam helical spiral CT was developed. For cone beam spiral CT, the issue of data truncation is addressed more intricately.\nIf you slice through the patient in any plane, the actual X-ray beam forms a fan beam in that plane.\n\nThis fan beam is longitudinally truncated because the patient‚Äôs body is quite long, and the detector size limits the coverage. You cannot cover the whole body with a single cone beam at once.\n\nOften, you focus on smaller regions of interest like the heart or inner ear, so using a huge cone beam isn‚Äôt practical.\nConsequently, data truncation naturally occurs in helical cone beam scanning, making it an important consideration in advanced CT imaging.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 14 - Slide19.txt", "file_path": "Lecture 14\\Texts\\Slide19.txt", "content": "In the early 1990s, I wrote a paper called the General Cone Beam Reconstruction Algorithm. \n\nIt explains that for cone beam helical scanning‚Äîwhere the beam geometry is divergent‚Äîyou cannot simply apply linear interpolation to reconstruct images.\n\nThe mathematics involved is more complicated, requiring specialized algorithms to handle data truncation and beam geometry correctly.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 14 - Slide20.txt", "file_path": "Lecture 14\\Texts\\Slide20.txt", "content": "This particular paper became the first seminal work on helical cone beam scanning and has been widely cited.\n\nToday, multi-slice cone beam helical scanning is widely used, with between 100 and 200 million such CT scans performed worldwide each year. This number highlights how prevalent and important this technology has become.\n\nReflecting on that time, we didn‚Äôt file any patents on this work, which turned out to be a major missed opportunity. Had we filed for intellectual property, it could have been extremely valuable.\nHelical scanning combines multiple technologies, including slip ring transmission, X-ray sources, and sophisticated reconstruction algorithms. The software scanning model itself is crucial to performance.\n\nTo put it in perspective: even if each scan were worth only one cent, with a million scans annually, that‚Äôs a million dollars a year in potential revenue.\nSo, not securing patent protection was probably the biggest mistake we made. Nonetheless, our original paper still stands as the prototype of this important process and algorithm.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 14 - Slide21.txt", "file_path": "Lecture 14\\Texts\\Slide21.txt", "content": "Now, let‚Äôs discuss longitudinal data truncation from a single slice. When you perform one helical turn with a single slice, you have a parallel beam and adjacent rays, allowing you to perform interpolation.\n\nHowever, when you move to a wider angle cone beam, simple interpolation is no longer sufficient, and more advanced algorithms are needed. The first paper I wrote on cone beam helical scanning proposed an approximate algorithm for this challenge.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 14 - Slide22.txt", "file_path": "Lecture 14\\Texts\\Slide22.txt", "content": "It then took over a decade for the field to develop what is called the exact helical cone beam algorithm. This algorithm still uses the filtered backprojection framework but is mathematically exact.\n\nThe author, Dr. Katsevich, is a mathematician, so the paper is quite dense and challenging for most engineering students. Nonetheless, it demonstrates that modern CT theory can effectively handle longitudinally truncated data.\n\nEarlier, I mentioned that in the second generation, data truncation happens transversely, while helical CT mainly deals with longitudinal data truncation, which can be seen as a symmetric problem.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 14 - Slide23.txt", "file_path": "Lecture 14\\Texts\\Slide23.txt", "content": "There is also an interesting transverse data truncation problem.\nImagine your cross-section represents the green base, while the blue area is a smaller region of interest, or ROI.\nPerhaps you‚Äôre only interested in the heart, or in the ear if planning a cochlear implant.\n\nIn this case, the ROI is small. You might use a narrow fan beam targeted only through the ROI, then rotate as in third-generation fan beam geometry. This means the projection data is transversely truncated on both sides.\nWith truncation on both edges, the Fourier slice theorem no longer applies, making reconstruction very challenging.\nMathematically, this ‚Äúinterior problem‚Äù has no unique solution, meaning multiple structures could produce identical data, and you wouldn‚Äôt know which is correct.\n\nSeveral years ago, my group revisited and solved this problem, which we felt very proud of.\nWhile the classical interior problem‚Äôs non-uniqueness holds in unrestricted function spaces, we showed that introducing prior knowledge allows exact reconstruction. \n\nFor example, if you know a small sub-region of the ROI, such as air in the airway or blood in the aorta, you can use that known X-ray attenuation coefficient as a reference. This prior information enables theoretical, exact reconstruction within the ROI from truncated data, a method called interior tomography. Our work was well received, although reviewers raised important questions.\n\nOne student tested whether blood density varies among different races or sexes, finding it consistent across populations.\nThis allowed us to use blood density as a stable reference in cardiac imaging, so you only need to scan the heart, reducing radiation dose.\nLater, reviewers pointed out that contrast agents‚Äîlike iodine injected during CT angiography‚Äîchange blood density, so you can‚Äôt always rely on a fixed reference.\nThis challenge led us to develop a new solution.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 14 - Slide24.txt", "file_path": "Lecture 14\\Texts\\Slide24.txt", "content": "In the new approach, we assume there is no known sub-region.\nInstead, we model the region of interest as composed of finitely many sub-regions, each modeled by piecewise polynomials.\n\nThe boundaries of these sub-regions need not be known in advance. Each sub-region can be approximated by constant, quadratic, or more general multivariable polynomials.\nThis assumption matches the idea of band-limited signals from sampling theory.\nWith this sparsity-based model, where the ROI is decomposed into a small number of smooth regions, the interior reconstruction problem becomes solvable.\n\nFor example, in a chest CT scan, you can reconstruct the whole chest from all data or just from truncated data restricted to an ROI.\nHere is a real dataset scanned on a Siemens scanner demonstrating this.\nInterior tomography is no longer a mathematical curiosity but delivers practically good reconstruction results.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 14 - Slide25.txt", "file_path": "Lecture 14\\Texts\\Slide25.txt", "content": "Let me add some comparison between classic CT and interior tomography.\n\nAccording to classical theory, which we learned and which I explained in textbooks, you need to collect all data to satisfy the data sufficiency condition‚Äîusually a fan beam or cone beam that covers the entire cross-section or object.\nThe data must comprehensively cover the object for accurate reconstruction.\nInterior tomography, however, only requires partial data limited to the region of interest.\n\nThus, with filtered backprojection, classic CT reconstructs a global image, like a wholesale business covering the entire area.\nInterior tomography reconstructs a smaller, targeted area‚Äîlike retail shopping.\nThis opens up new opportunities, providing flexibility and precision.\nYou only need to shoot X-rays through the important region, reducing dose and focusing on clinical needs.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 14 - Slide26.txt", "file_path": "Lecture 14\\Texts\\Slide26.txt", "content": "I have given multiple presentations on interior tomography. One key idea is that interior tomography uses less data‚Äîless is more in many ways. Less data means a deeper understanding of imaging principles. We can handle larger objects, use less radiation dose, and achieve faster scanning.\n\nWhy is it faster? All scanning geometries, from first to helical generations, aim to speed up scanning. A cone beam is better than a fan beam because it allows parallel data acquisition. Interior tomography takes this even further by enabling higher-level parallelism with multiple X-ray sources and smaller detector arrays.\n\nIn the extreme, you could have many X-ray focal spots, possibly based on emerging technologies like carbon nanotube cold emission X-ray sources. These enable distributed X-ray cells with small detector elements arranged similarly to the fourth-generation geometry.\nAt any instant, such a system could collect multiple truncated projections. Handling data truncation like this allows reconstruction focused on small regions of interest, achieving the fastest possible tomographic imaging speed. You cannot go faster than the speed of light, which governs X-ray photon travel.\n\nWe filed for intellectual property on this concept at Virginia Tech. Later, I will discuss the potential of interior tomography in cardiac imaging.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 14 - Slide27.txt", "file_path": "Lecture 14\\Texts\\Slide27.txt", "content": "Now, let me explain CT scanner architecture. To reconstruct images, we rely on line integrals from X-ray projections, using filtered backprojection and Fourier transform-based reconstruction, or interior tomography for region-of-interest reconstruction.\nTo acquire this data, you need hardware: the CT scanner itself. This is a donut-shaped structure called the gantry, which is the framework holding all components.\n\nInside, you find X-ray sources and collimators that shape the beam and filter out low-energy X-rays. Low-energy photons won‚Äôt penetrate the patient and only add an unwanted dose.\nCollimators also reject scattered radiation to ensure the measured signal corresponds closely to the direct line integrals.\n\nThe scanner is a sophisticated device: you place the patient in the gantry, and from that, you acquire many images that resolve internal structures clearly.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 14 - Slide28.txt", "file_path": "Lecture 14\\Texts\\Slide28.txt", "content": "Let me show you a typical CT scanner layout. You have the X-ray tube, which is quite heavy‚Äîfor example, our lab‚Äôs donated GE tube is heavy.\nPowering the tube requires a high-voltage supply and a cooling system; scanning consumes energy and heats the tube.\nCooling is done with water or air to keep components at safe operating temperatures.\nThis entire setup is called the gantry. It includes the patient table where the patient lies, the detector array opposite the source, and collimators in front of the detectors.\n\nIn conventional CT, the source, detectors, and associated hardware are mounted on the rotating gantry.\nDuring scanning, the gantry physically rotates, typically half or a full circle, then reverses direction. This back-and-forth rotation is limited by cabling, which can tangle with repeated turns.\n\nFor helical scanning, slip ring technology is employed. The slip ring lets you rotate continuously without cables twisting or breaking.\nIt consists of rings with carefully designed contacts to transmit power to the X-ray source and data from the detectors.\nSlip ring technology is critical for continuous and fast scanning. It enables the transfer of data to the scanner‚Äôs computer or local storage during rotation.\nWhen designing a CT scanner, the goal is faster and faster imaging speed, with the most demanding application being cardiac imaging‚Äîthe holy grail of CT.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 14 - Slide29.txt", "file_path": "Lecture 14\\Texts\\Slide29.txt", "content": "At RPI, we have collaborated with GE Global Research Center and with Bruno Diem and others to develop optimized architectures for dedicated cardiac CT scanners.\n\nWe published a paper last year on this topic.\nDesigning a CT scanner is much like designing a building or room‚Äîyou consider architecture and optimal layout.\nOur team is currently preparing a grant proposal to advance this research. The proposal is due soon and covers aspects not addressed in previous reviews.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 30, "slide_filename": "Slide30.txt", "slide_annotation": "Lecture 14 - Slide30.txt", "file_path": "Lecture 14\\Texts\\Slide30.txt", "content": "I am especially excited about this project, collaborating with GE.\nCurrently, cardiac CT scanners mostly use third-generation geometry, but with cone beam helical scanning mode, which some call improved third generation or helical cone beam generation.\nThis system is already very fast, capable of rotating three to four times per second.\nThe mechanical acceleration involved is astonishing‚Äîmuch higher than what‚Äôs involved in launching a space shuttle.\nThe current best temporal and spatial resolution for cardiac CT is about 500 microns.\n\nBut according to cardiac surgeons I know, 100 micron resolution would solve many clinical challenges.\nImproving cardiac CT resolution requires higher imaging speed to freeze the heart‚Äôs rapid motion on this fine scale.\nHowever, as you increase rotation speed, the X-ray tube has less time to emit photons, limiting achievable image quality.\nOur out-of-the-box solution uses interior tomography. We designed a stationary ring scanner to boost X-ray flux by orders of magnitude.\n\nImagine a cardiac patient lying still while being scanned. You get immediate images.\nWhen higher resolution is needed, the patient or ring can be translated to focus precisely on the region of interest.\nThis stationary ring can move up, down, left, or right to align perfectly with conventional cardiac CT images.\nI believe this design is a promising way forward‚Äîan out-of-the-box solution for high-speed, high-resolution cardiac imaging.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 31, "slide_filename": "Slide31.txt", "slide_annotation": "Lecture 14 - Slide31.txt", "file_path": "Lecture 14\\Texts\\Slide31.txt", "content": "Now, let‚Äôs finish with the last two¬†sections. First, image artifacts. I‚Äôve mentioned¬†image artifacts¬†several times¬†throughout this¬†course. Tomographic images¬†are meant to¬†reveal the true¬†underlying anatomical¬†structure. However, for various¬†reasons, the¬†images sometimes¬†show structures¬†that aren‚Äôt actually¬†present. These¬†false structures¬†are what we call¬†image artifacts.\n\nIf there‚Äôs no real structure present, what you see¬†might just be¬†noise. Sometimes, artifacts show¬†up as streaks or stripes in¬†the image, but¬†obviously, there¬†are no stripes¬†inside the human¬†body. They are¬†just artifacts. We‚Äôll discuss¬†CT image quality¬†considerations¬†next, which include¬†how artifacts¬†impact the overall¬†image quality.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 32, "slide_filename": "Slide32.txt", "slide_annotation": "Lecture 14 - Slide32.txt", "file_path": "Lecture 14\\Texts\\Slide32.txt", "content": "Starting with data sampling: In modern CT scanners, such as those from GE or Siemens, a typical rotation collects about 1,000 projections‚Äîsometimes slightly fewer, depending on the manufacturer, like 800 or 790.\n\nAt each projection, the detector array works like a comb beam or multi-row detector, capturing thousands of data points along each ray path. For example, along one ray direction, you might have about 1,000 detector elements.\nAll these detector readings combine to define the scanner‚Äôs spatial resolution, often achieving sub-millimeter precision.\n\nIn the future, we are aiming to improve this further, reaching resolutions on the order of 100 microns. This is challenging because every wire and detector element has limits, so current strategies often focus on targeting smaller regions of interest.\nTo resolve fine details, the angular sampling intervals and detector element sizes must be small enough, consistent with Fourier analysis and sampling theory.\n\nIf the sampling rate is too low or the detector elements are too big, small features won‚Äôt be resolved clearly. For instance, with about 512 rays, you get fairly clear images. With fewer rays, small details become difficult to detect.\nAngular sampling is also critical. If you have nearly 1,000 views around the object, image quality is good. Using fewer views leads to aliasing artifacts and lower image quality.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 33, "slide_filename": "Slide33.txt", "slide_annotation": "Lecture 14 - Slide33.txt", "file_path": "Lecture 14\\Texts\\Slide33.txt", "content": "CT reconstruction relies on CT numbers, which are derived from Beer's law. Beer's law relates the incoming and attenuated X-ray intensities via a parameter called the linear attenuation coefficient, mu.\nData processing converts raw measurements into line integrals, which are then used to reconstruct mu values throughout the image.\n\nOn the CT monitor, you don‚Äôt see mu values directly but relative values compared to water, since the human body is mostly water.\nThus, CT numbers indicate how much more or less dense a region is than water.\nFor example, bone has a CT number over 1000, air has about -1000, and water is defined as zero. The range is often magnified by 1000 times for visualization.\n\nThis scale is sometimes called Hounsfield units, named after Sir Godfrey Hounsfield, the British engineer who developed the first CT scanner, which earned him a Nobel Prize for its revolutionary capability to reveal internal structures.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 34, "slide_filename": "Slide34.txt", "slide_annotation": "Lecture 14 - Slide34.txt", "file_path": "Lecture 14\\Texts\\Slide34.txt", "content": "Different body regions use different CT number windows for display. These windows are defined by minimum and maximum CT numbers, depending on the manufacturer or clinical application.\n\nFor instance, the full range of CT numbers might be mapped to 256 grayscale levels on a display, from white to black.\nThe human eye can typically distinguish about 50 shades within a window.\nIf too many shades are presented, subtle differences become difficult to see.\n\nFor lung imaging, for example, you might set the window maximum to about 250 Hounsfield units and the minimum to the air value.\nWithin this window, 256 gray levels help render tissues clearly. Anything outside the window appears clipped and is not visible.\nThis windowing is critical to tailoring visualization to different clinical needs.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 35, "slide_filename": "Slide35.txt", "slide_annotation": "Lecture 14 - Slide35.txt", "file_path": "Lecture 14\\Texts\\Slide35.txt", "content": "There are multiple resolution measures in CT. High-contrast resolution detects small objects adjacently positioned; for example, cardiac CT currently resolves about 500 microns.\nIf you want to detect even smaller details, down to 100 microns, you may need advanced methods like interior tomography that we‚Äôve discussed extensively.\n\nLow-contrast resolution differentiates subtle intensity differences. This is important because tumors may look very similar to surrounding tissue.\nNarrow window settings, chosen carefully with a mean and range, help highlight these subtle differences.\nIf the CT number difference is very small‚Äîsay 5 to 10 Hounsfield units‚Äîand noise is comparable, detecting such differences requires boosting contrast effectively.\n\nHence, low contrast resolution depends not just on hardware but also on image noise characteristics.\nTemporal resolution is crucial for dynamic organs like the heart. To get high-resolution cardiac images, both spatial and temporal resolution must be adequate.\nIf your detector resolves fine details but scanning speed is too slow, motion blurring will reduce effective spatial resolution.\nSo, achieving both fast scanning and high spatial resolution is essential.\n\nPhoton-counting detectors with elements as small as 55 microns are available, enabling very high-resolution imaging.\nCombining this with interior tomography concepts and distributed thousands of photon-counting detector elements, it‚Äôs feasible to build a ‚Äúmagic ring‚Äù scanner that focuses on small regions of interest with resolution down to 100 microns.\nThis is an area of active research that we continue to pursue.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 36, "slide_filename": "Slide36.txt", "slide_annotation": "Lecture 14 - Slide36.txt", "file_path": "Lecture 14\\Texts\\Slide36.txt", "content": "The noise you see in CT images is due to quantum effects and other random variations. Several factors influence this noise level, such as the tube current, often referred to as MAS. When the tube current is high, the X-ray tube emits more photons, so the statistical fluctuations decrease, which means less noise.\n\nImage resolution also plays a key role. Think of pixel or voxel size in a simple way. If you have a large pixel, it collects many photons, resulting in a lower noise level. But if you target a very small, high-resolution pixel or voxel, fewer photons pass through it, so you need even more photons overall to maintain good image quality.\n\nThis is why interior tomography is advantageous. Since it directs photons only through the small region of interest, it reduces unnecessary exposure while maintaining image quality for the target structure.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 37, "slide_filename": "Slide37.txt", "slide_annotation": "Lecture 14 - Slide37.txt", "file_path": "Lecture 14\\Texts\\Slide37.txt", "content": "Low contrast image resolution can be measured using resolution phantoms containing small test objects with subtle differences in density. These phantoms help to assess if the scanner and reconstruction algorithm can visualize objects with minor contrast differences.\n\nWhen contrast is poor, like between blood vessels and surrounding tissue, contrast agents such as iodine are injected. These agents highlight the vessels, making blood, tumors, and related structures more visible.\n\nThis approach significantly improves low-contrast resolution, enabling better diagnosis and visualization of soft tissues.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 38, "slide_filename": "Slide38.txt", "slide_annotation": "Lecture 14 - Slide38.txt", "file_path": "Lecture 14\\Texts\\Slide38.txt", "content": "Temporal resolution, so you see conventional CP scanning. You do one slice at a time. Then you\nmove the patient into the country by a small increment. Do again, patient, keep pressing. \n\nSo from slice to slice, you see some discontinuity. So this is not due to some pathological feature.\nIt's really due to motion artifacts that show the temporal resolution agent is good enough. But\nWith helical scanning, it's smoothed out, but you have some longitudinal blurring.\n\nHere is the earlier algorithm. Later, I mentioned overlapping reconstruction. You really can\nrecover an image longitudinally and a better resolution than conventional scanning. Anyway,\nso we wouldn't have time to go into details.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 39, "slide_filename": "Slide39.txt", "slide_annotation": "Lecture 14 - Slide39.txt", "file_path": "Lecture 14\\Texts\\Slide39.txt", "content": "Scattering artifacts occur due to scattered X-ray photons. Let me explain this with an example. Imagine sending an X-ray beam from the south direction through the body. Follow my cursor as I illustrate. The primary X-ray beam travels straight through a data element, creating the expected line integral for reconstruction.\n\nHowever, during measurement, some X-rays scatter in various directions. You might see scattering toward the right, left, or even backward directions. These scattered photons add a background signal that does not correspond to the direct transmission along the beam path.\nBecause of this scattered radiation, the detector picks up additional signals on top of the real primary X-rays. This extra background leads to artifacts in the image‚Äîusually appearing as low-frequency streaking or haze that reduces contrast and clarity.\nIf you correct for scattering components by estimating and subtracting them, you can restore the image quality close to the phantom reconstruction without scatter.\n\nOne common method to reduce scatter is using beam blockers placed strategically. These beam blockers function to block certain parts of the primary radiation, allowing measurement of scatter separately and improved image correction.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 40, "slide_filename": "Slide40.txt", "slide_annotation": "Lecture 14 - Slide40.txt", "file_path": "Lecture 14\\Texts\\Slide40.txt", "content": "Beam hardening artifacts occur when X-ray beams pass through an object, resulting in changes in beam energy distribution that affect image quality. Let me explain this in some detail.\n\nImagine an X-ray beam going through the body in a certain direction, forming a parallel beam projection. This is a real and common setup.\nWhen using a medical X-ray tube and current detector technology, both measure the attenuated signals, resulting in what we call the line integral. However, if you reconstruct the image based on the uncorrected projection profile, the reconstruction may look distorted, often represented by a dotted projection line in diagrams.\nWhy does this happen?\n\nIt‚Äôs due to beam hardening. X-ray beams are polychromatic, meaning they contain photons of various energies, forming a spectrum. Initially, the beam may have many lower-energy photons that get attenuated more as the beam penetrates the object.\nLower-energy photons have higher attenuation coefficients, so they are absorbed more readily in tissue.\nAs the beam progresses into the body, the relative amount of higher-energy photons increases because they penetrate deeper. This process ‚Äúhardens‚Äù the beam‚Äîincreasing its average energy.\n\nSince higher-energy photons are less easily blocked, the beam becomes more penetrating along longer paths.\nDespite this, many reconstruction algorithms assume a monochromatic beam, leading to errors.\nThis results in an underestimation of the linear attenuation coefficient mu because the beam appears to pass more easily than expected.\nConsequently, the computed line integrals and preprocessed data are based on a single energy assumption, but the reality is a changing, polychromatic beam.\n\nThis discrepancy causes characteristic artifacts in images, such as streaks and cupping.\nProper correction methods and models that account for beam hardening are necessary to reduce these artifacts and improve image quality.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 41, "slide_filename": "Slide41.txt", "slide_annotation": "Lecture 14 - Slide41.txt", "file_path": "Lecture 14\\Texts\\Slide41.txt", "content": "This is an example of CT images affected by beam hardening without any correction. You can see a darker strip in the image, which obscures details. After applying correction techniques, you can visualize structures like gray matter much more clearly.\n\nBeam hardening artifacts originate from the X-ray source and the polychromatic nature of the X-ray beam. When you use a photon-counting detector, it can measure X-ray signals at different energy levels separately.\n\nBy distinguishing between energies, photon-counting detectors effectively eliminate beam hardening artifacts. This technology significantly improves image quality by reducing these artifacts that cause distortions in conventional CT images.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 42, "slide_filename": "Slide42.txt", "slide_annotation": "Lecture 14 - Slide42.txt", "file_path": "Lecture 14\\Texts\\Slide42.txt", "content": "Now, let‚Äôs discuss volume averaging and various related artifacts. These artifacts are not difficult to understand conceptually, but effectively removing them can be challenging.\n\nIn CT imaging, volume averaging occurs because of finite detector element size and limitations in the reconstruction matrix‚Äôs resolution. These hardware elements are not small enough to capture features smaller than the detector size or the X-ray focal spot.\nImagine you have a fine detail‚Äîa small yellow cross in an image. But your image pixels are arranged in a grid, and this cross might fall entirely within a single pixel.\n\nThat one pixel will then represent an average of all the structures within it, so the small cross may simply blend into a larger, less distinct area.\nFor example, if the cross is in one pixel and that pixel appears as a light yellow block, you cannot resolve the cross‚Äôs true shape.\nThis phenomenon is called volume averaging.\n\nIn CT images, various factors cause blurring and averaging effects, causing images to look less sharp.\nDetector pixels have a limited size, and the X-ray source‚Äôs focal spot is not a perfect point but has an elliptical shape, which contributes to image blur.\n\nTo partially counteract this, image processing techniques like deblurring and deconvolution are applied, recovering some lost resolution.\nThis relates to our earlier study of linear systems and convolution in imaging.\nSo, volume averaging is a fundamental problem intrinsic to CT imaging, limiting the spatial resolution achievable.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 43, "slide_filename": "Slide43.txt", "slide_annotation": "Lecture 14 - Slide43.txt", "file_path": "Lecture 14\\Texts\\Slide43.txt", "content": "Next, we have what are called metal artifacts. Inside the human body, you may have metal implants like pacemakers or dental fillings. Metal is much denser than bone, so it can block X-rays effectively.\nBecause of this, X-rays cannot penetrate metal implants well. The attenuated signal behind the metal may be very noisy or even just noise without a clear signal.\n\nWhen you perform filtered backprojection reconstruction in such cases, the results are often poor, showing streaking artifacts.\nThis happens because the line integral passing through the metal is very long, with many photons blocked or scattered. Beam hardening and other effects further degrade the signal.\nAs a result, the detected signals are compromised with a low signal-to-noise ratio. When estimating line integrals, the data is inconsistent and inaccurate.\n\nFiltered backprojection on such corrupted data generates stripe-like artifacts in the images, which is a typical manifestation of metal artifacts in CT.\nReducing metal artifacts is an important research topic, and multiple algorithms have been developed, though it remains challenging.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 44, "slide_filename": "Slide44.txt", "slide_annotation": "Lecture 14 - Slide44.txt", "file_path": "Lecture 14\\Texts\\Slide44.txt", "content": "The last topic I‚Äôm going to explain¬†is motion artifacts. Imagine you¬†have a phantom¬†object, like¬†this one, viewed¬†at angle zero. The phantom¬†has a taiji pattern, a yin-yang shape.\nAs you slowly increase the projection¬†angle, the X-ray source rotates around the phantom.\nOver time, the data you collect changes. When the angle¬†reaches 90 degrees, the phantom appears compressed, shrunk to a¬†minimum in the¬†projection.\n\nAs the source continues rotating, at¬†180 degrees, the phantom appears expanded¬†again.\nThis back-and-forth pattern creates a sinusoidal oscillation in the data.\nThis example illustrates what happens¬†when the object¬†isn‚Äôt stationary, but moves periodically.\nYou gather projection data while the¬†object moves¬†according to¬†this periodic¬†motion.\nIf you then apply filtered backprojection¬†to reconstruct¬†images from this¬†data, you get¬†motion artifacts‚Äîblurring or distortion¬†caused by the¬†object‚Äôs movement¬†during acquisition.\n\nHowever, if you know the¬†object‚Äôs motion¬†trajectory, you¬†can incorporate¬†that information¬†into iterative¬†reconstruction¬†algorithms.\nWith this additional information, it¬†is possible to¬†recover accurate¬†images despite¬†motion.\nTomographic reconstruction remains feasible, provided you¬†have either explicit¬†knowledge of¬†motion or use¬†algorithms that¬†infer motion¬†implicitly.\nDue to time constraints in this lecture, we won‚Äôt delve¬†deeply into motion¬†artifact correction¬†algorithms.\nFor cardiac imaging, one way to reduce¬†motion artifacts¬†is through hardware: performing ultra-fast scanning so the heart doesn‚Äôt have¬†significant motion¬†during image¬†acquisition.\n\nThis instantaneous imaging ‚Äúfreezes‚Äù the cardiac¬†structure, minimizing¬†motion blur.\nAnother way is through smart computational¬†algorithms that¬†predict motion¬†and correct for¬†it during reconstruction.\nOften, cardiac CT patients are given¬†beta blockers¬†to slow the heart¬†rate, reducing¬†motion during¬†scanning.\nAdditionally, CT acquisition can be¬†synchronized¬†with the ECG¬†signal, coupling¬†image data with¬†specific cardiac¬†phases.\nThe ECG indicates when the heart is¬†largest or smallest. You can select¬†data segments¬†corresponding¬†to the heart‚Äôs maximum volume¬†and reconstruct¬†images at that¬†phase.\n\nSimilarly, you regroup data segments¬†for the minimum¬†volume phase¬†and reconstruct¬†images accordingly.\nThis approach allows reconstruction¬†of a beating¬†heart in different¬†cardiac phases.\nThis gives a rough idea of how motion¬†artifacts are¬†managed in dynamic¬†organ imaging.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 45, "slide_filename": "Slide45.txt", "slide_annotation": "Lecture 14 - Slide45.txt", "file_path": "Lecture 14\\Texts\\Slide45.txt", "content": "Let‚Äôs finish with some important points about X-ray radiation. X-rays are much more energetic than visible light. We experience visible light all the time‚Äîsunshine, for example‚Äîwhich is generally considered healthy.\nBut X-rays have much shorter wavelengths, roughly from 0.1 nanometers to 10 nanometers, and much higher energy.\nBecause X-ray photons carry so much energy, they can break molecular bonds in biological tissues. This damage can potentially cause genetic mutations, which are often harmful.\n\nIn the long term, such mutations may increase the risk of cancer or other genetic diseases.\nThere is some debate, though. Some argue that small doses of radiation might even be beneficial, similar to exercise, causing small stress that strengthens the body.\n\nIn this view, controlled, moderate radiation exposure acts like a vitamin, gently challenging the body to repair and strengthen itself.\nWhile that perspective exists in some articles, the general consensus is caution.\nAs CT imaging, especially advanced methods like helical CT for lung cancer screening, becomes more widespread, concerns about radiation dose increase.\n\nPatients receive higher cumulative doses, and this raises public health questions.\nSo, it is crucial to focus on how we can reduce radiation dose and how we measure it in the first place.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 46, "slide_filename": "Slide46.txt", "slide_annotation": "Lecture 14 - Slide46.txt", "file_path": "Lecture 14\\Texts\\Slide46.txt", "content": "Let me explain the basic concepts from your Green Book regarding radiation dose.\nX-ray radiation is a form of electromagnetic radiation, and when it passes through your body, energy is deposited into your tissues. This energy deposited is called the absorbed dose.\n\nThe absorbed dose is a physical measurement of energy per unit mass of tissue, expressed in grays. One gray equals one joule per kilogram.\nThis is the fundamental concept of radiation dose‚Äîthe energy your body absorbs from X-rays.\nHowever, from a health perspective, the effective dose equivalent is more relevant.\nTo calculate this, the human body is divided into multiple organs or regions because different organs have different sensitivities to radiation.\nSome tissues, like those involved in reproduction or rapidly regenerating cells, are more sensitive to radiation damage.\nTherefore, younger women, children, and reproductive organs require special caution to avoid excessive exposure.\nThis sensitivity is accounted for by weighting factors, denoted by W.\n\nThe dose equivalent, represented by H, combines the absorbed dose with factors related to the type and energy of radiation.\nHigher energy photons or particles generally cause more biological damage per unit of absorbed energy.\nFor example, X-rays, gamma rays, and alpha particles differ in their relative biological effectiveness.\nTo account for this, a quality factor, QF, is applied to quantify the biological impact of different radiation types.\n\nFor X-ray CT, the QF is normalized to one, but other radiation types have different values.\nThus, the effective dose equivalent is calculated using these weighted factors, resulting in a more accurate estimation of biological risk.\nThis calculation is summarized by a formula sometimes noted as equation 1.30 in radiation safety literature.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 47, "slide_filename": "Slide47.txt", "slide_annotation": "Lecture 14 - Slide47.txt", "file_path": "Lecture 14\\Texts\\Slide47.txt", "content": "Let me share with you some typical radiation dose values.\nFor example, in a breast exam, the physical energy absorbed is measured in grays. But when you take biological sensitivity into account and use standard formulas, the unit becomes the sievert, often abbreviated as Sv or sometimes called C-word.\n\nTo clarify, grays measure the pure physical energy deposited, while sieverts factor in biological effects, making them more meaningful for assessing health risk.\nBecause a sievert‚Äîor the C-word‚Äîis a relatively large unit, radiation dose is often expressed in millisieverts (mSv).\nFor a typical breast examination, the effective dose is quite low.\nFor high-dose CT scans, the effective dose can be in the range of three millisieverts.\nFor a full-body CT scan, it could be around ten millisieverts.\nThese values give you an idea of the radiation dose magnitude.\n\nIn CT scanning, another important metric is the CT Dose Index, or CTDI, which is used to estimate how much radiation dose is delivered.\nCTDI is not organ-specific; it measures the average dose over one full rotation of the scanner around the patient, providing a rough estimate of radiation exposure.\nLater, I will show you a figure to illustrate this concept visually.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 48, "slide_filename": "Slide48.txt", "slide_annotation": "Lecture 14 - Slide48.txt", "file_path": "Lecture 14\\Texts\\Slide48.txt", "content": "Before we dive deeper, let me give you a natural reference to help put radiation dose levels in perspective.\nIf you live in India, you receive an annual natural radiation dose of over 10 millisieverts‚Äîor 10 million microsieverts‚Äîwhich comes from natural sources like soil and cosmic rays.\n\nIn China, the natural background radiation exposure is roughly half that of India.\nIf you are concerned that radiation is harmful, then living in Toronto would be better, as the natural background radiation there is lower.\nIn the United States, the average natural background radiation dose is about three millisieverts per year.\n\nThese background levels are unavoidable since radiation comes from natural sources in the environment.\nUnderstanding these reference levels helps put medical radiation doses into context.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 49, "slide_filename": "Slide49.txt", "slide_annotation": "Lecture 14 - Slide49.txt", "file_path": "Lecture 14\\Texts\\Slide49.txt", "content": "Now, let me give you some comparisons to help you understand radiation doses better.\nFor example, a chest X-ray typically delivers a radiation dose of about 0.1 millisieverts.\nA whole-body CT scan, on the other hand, can deliver around 10 millisieverts.\nResearchers and medical professionals are working hard to reduce these doses.\n\nOur goal is to bring CT radiation doses down to about one millisievert or less per scan, which would be considered comparatively safe.\nTo put this in perspective, in your everyday life, you naturally receive about two millisieverts of radiation annually from environmental sources.\nSo, when you receive a CT scan with a dose around one millisievert, that‚Äôs roughly half of your average yearly natural exposure.\nYou can think of it like visiting a high natural radiation area, such as parts of India or other regions, where you might be exposed to somewhat higher background radiation if you stay longer.\n\nThere are other units you might encounter, such as M-R or similar, but the gray and sievert (or millisievert) remain the standard units for expressing radiation dose. This comparison helps warm you up and offers a bit of perspective on how these numbers relate to everyday life.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 50, "slide_filename": "Slide50.txt", "slide_annotation": "Lecture 14 - Slide50.txt", "file_path": "Lecture 14\\Texts\\Slide50.txt", "content": "Now, let me explain three key concepts related to radiation dose.\nFirst, the absorbed dose. This represents the energy deposited by ionizing radiation into the body‚Äôs tissues. It is a purely physical measure.\nSecond, the effective dose accounts for the sensitivity of different organs and tissues to radiation. This is where weighting factors come into play.\n\nFor example, certain organs involved in reproduction or rapid cell regeneration are much more sensitive to radiation. We want to protect children and younger women, especially from high doses. While adults may be less sensitive, it is still prudent to minimize exposure.\nThe dose equivalent combines dose with information about the type of radiation. Some radiation types cause more damage per unit energy than others.\n\nHowever, when discussing CT scans, the radiation type factor is effectively one, so it generally does not alter dose estimates.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 51, "slide_filename": "Slide51.txt", "slide_annotation": "Lecture 14 - Slide51.txt", "file_path": "Lecture 14\\Texts\\Slide51.txt", "content": "Now, let me explain the concept of the radiation dose profile, sometimes called the releasing profile.\n\nImagine you perform a circular CT scan. This scanning method reconstructs images within a single plane, called the XY plane.\nIdeally, the X-ray beam would be finely collimated so that all photons are confined within the slice thickness, represented as a neat rectangular region‚Äîsay, highlighted in yellow.\n\nHowever, in reality, the system is not perfect.\nYou have scattered photons, and the emission is not perfectly confined.\nTherefore, the actual radiation dose profile follows a curve, often shown as a blue curve.\nThis means that, even though you intend to irradiate only the slice thickness, some dose extends beyond this region.\nThis dose \"tail\" outside the intended slice area also contributes to the total patient radiation exposure.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 52, "slide_filename": "Slide52.txt", "slide_annotation": "Lecture 14 - Slide52.txt", "file_path": "Lecture 14\\Texts\\Slide52.txt", "content": "Now, let's talk about the CT dose index, or CTDI.\n\nSuppose the slice thickness is represented by¬†T.\nYou integrate the radiation dose measurements over the slice thickness, effectively summing the dose contributions from the slice and imaginary adjacent slices‚Äîsay, seven slices above and seven slices below.\nAll these doses are added together, and the total is then averaged by normalizing with the slice thickness¬†T.\nThis result is what we call the CT dose index, or CTDI.\n\nIt's a simple yet important concept, as it includes any additional leakage radiation in the dose computation.\nThe CTDI provides a standardized measure of radiation output from the CT scanner, reflecting dose distribution within the irradiated volume.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 53, "slide_filename": "Slide53.txt", "slide_annotation": "Lecture 14 - Slide53.txt", "file_path": "Lecture 14\\Texts\\Slide53.txt", "content": "Another way to think about radiation dose is through the dose profile measured over multiple circular scans.\n\nWhen you perform one circular scan, you get a certain dose distribution.\nIf you then do a second circular scan, you‚Äôll get a similar dose profile again.\nWhen multiple scans are performed, each scan adds its own dose profile.\n\nThese multiple dose profiles overlap, contributing to the total radiation dose the patient receives.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 54, "slide_filename": "Slide54.txt", "slide_annotation": "Lecture 14 - Slide54.txt", "file_path": "Lecture 14\\Texts\\Slide54.txt", "content": "This concept of multiple scan average dose, or MSAD, is defined by an integral calculation.\nRadiation dose from neighboring scans contributes to the dose in the current scan.\nThe number of adjacent scans considered is represented by¬†N.\n\nYou perform the integration over the radiation dose profile, then normalize it by the slice thickness.\nThis normalization accounts for the actual tissue volume receiving radiation.\nMSAD is another classic measure of radiation dose in CT.\nWith the rise of helical CT, there are new adaptations of these dose measurements, including for helical scanning and combined bimodal CT systems.\n\nHowever, the fundamental principle behind measuring radiation dose remains the same.\nAs I mentioned earlier, understanding how we measure the release dose is essential to managing patient exposure.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 55, "slide_filename": "Slide55.txt", "slide_annotation": "Lecture 14 - Slide55.txt", "file_path": "Lecture 14\\Texts\\Slide55.txt", "content": "To measure radiation dose, we use specialized instruments called dosimeters.\nI personally had the opportunity to work on radiation dose measurements for CT scanners during my time at Washington University School of Medicine.\n\nIt was engaging and valuable research work, and I frequently used dosimetry equipment to evaluate radiation output.\nThese dosimeters help assess the radiation delivered by the scanner, ensuring safety and proper calibration.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 56, "slide_filename": "Slide56.txt", "slide_annotation": "Lecture 14 - Slide56.txt", "file_path": "Lecture 14\\Texts\\Slide56.txt", "content": "Now, let‚Äôs talk about some general guidelines regarding CT dose.\n\nRadiation dose measured during multiple scans is roughly proportional to the tube current, or mA. Higher tube current means more photons emitted, resulting in a higher radiation dose.\nLonger scan times also increase dose, as the patient is exposed for a longer period.\nIncreasing the tube voltage, or kVp, raises photon energy, which also influences dose. Although higher energy photons are more penetrating, they tend to cause slightly higher patient dose.\n\nWhen you reduce slice thickness‚Äîthat is, make the X-ray beam narrower‚Äîyou increase spatial resolution. But thinner slices require more photons since the beam is less focused, which can slightly increase the dose due to some photon wastage.\nMany factors affect dose increase or decrease, including patient size or phantom size. For a standard head phantom, the dose at the center and periphery is roughly the same.\n\nFor larger phantoms, like the whole body, the central dose may be lower due to increased attenuation, with a higher dose in peripheral regions.\nIn extreme cases, like imagining irradiating the Earth, the very center would receive almost no photons, so the central dose would be very low, while peripheral areas get a higher dose.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 57, "slide_filename": "Slide57.txt", "slide_annotation": "Lecture 14 - Slide57.txt", "file_path": "Lecture 14\\Texts\\Slide57.txt", "content": "Let me share some evidence that supports this approach.\n\nSeveral experts, including myself a few years ago, co-authored a landmark article.\nThis article mapped out strategies and technologies aimed at achieving routine sub-millisievert CT scanning.\nIt addresses how low-dose CT scans can be effectively performed while maintaining diagnostic image quality.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 58, "slide_filename": "Slide58.txt", "slide_annotation": "Lecture 14 - Slide58.txt", "file_path": "Lecture 14\\Texts\\Slide58.txt", "content": "We have discussed various technologies to reduce radiation dose in CT scanning, including interior tomography and photon counting detectors.\n\nAdditionally, iterative reconstruction algorithms play a key role. Unlike traditional analytic algorithms, which do not account for image noise or statistical fluctuations, iterative methods model these factors explicitly.\nBy doing so, they improve image quality while allowing dose reduction.\nThose represent technological approaches to lowering patient radiation exposure.\n\nNow, I‚Äôd like to share something not in the slides‚Äîa more biological approach.\nIonizing radiation causes real damage through DNA breaks and other effects.\nSome medications or natural products contain antioxidants, which can help mitigate radiation-induced damage.\nFor example, orange juice and blueberries contain antioxidants.\nIf you search the literature, you‚Äôll find several studies suggesting that antioxidants may have protective effects against radiation damage.\nWhile this is not part of any clinical protocol, and doctors don‚Äôt routinely prescribe antioxidants before CT scans, I personally believe in their potential benefit.\n\nNext time I have a CT scan, I plan to drink two bottles of orange juice an hour beforehand.\nIf you find this interesting, you might want to do your own research and consider consuming antioxidants before imaging.\nOne of my students is setting up experiments to scientifically evaluate the protective effect of antioxidants, like orange juice, against radiation damage during CT scans.\n\nThis is an exciting area of ongoing research, and we hope to quantify the benefits more precisely in the future.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 59, "slide_filename": "Slide59.txt", "slide_annotation": "Lecture 14 - Slide59.txt", "file_path": "Lecture 14\\Texts\\Slide59.txt", "content": "Now, let's talk about the last slide and some course housekeeping.\nThese three questions in the green book are quite typical and not very difficult.\nRegarding homework, you have one week‚Äîseven working days‚Äîto complete it.\n\n\nOn the other hand, for a bit of fun and creativity, you can try designing a futuristic portable CT scanner in an autonomous vehicle‚Äîthink of it like something fancy and innovative.\n\nI believe future technology will enable very affordable, fully reconfigurable CT scanners.\nImagine robotic X-ray detectors integrated into cars‚Äîif you want a screening, you just call like ordering an Uber.\nThe robotic scanner arrives at your home, performs the scan, and you get your results without leaving the house.\nI think that will be incredibly cool and transformative.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 14", "slide_number": 60, "slide_filename": "Slide60.txt", "slide_annotation": "Lecture 14 - Slide60.txt", "file_path": "Lecture 14\\Texts\\Slide60.txt", "content": "Several years ago, we undertook an innovative project and received some news coverage about it.\n\nWe built a CT scanner that does not rotate the gantry as in conventional designs.\nInstead, the patient lies on a rotating table‚Äîliterally a sheet that turns‚Äîwhile the X-ray tube and detector remain fixed in place.\nThe gantry is simplified to just a tube and a detector, with no mechanical rotation during imaging.\nWe then perform the translation of the table and the scanning components.\n\nAlthough this setup limits the viewing angle compared to traditional systems, it still produces good-quality images.\nIf you like, you can think creatively or artistically about how this concept could be extended to robotic scanners deployed in autonomous vehicles or portable scanners for versatile use.\n\nThis is all just for fun and imagination.\nThat‚Äôs all for today.\nThank you for your attention.", "total_slides_in_lecture": 60}
{"lecture": "Lecture 15", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 15 - Slide1.txt", "file_path": "Lecture 15\\Texts\\Slide1.txt", "content": "Hello everyone, and welcome back.\u000b\nToday, we will give the last MatLab lecture to explore how it can be used to demonstrate CT image reconstruction, that you‚Äôve been learning in this course.\n\nThe image you see here is an example from GE‚Äôs ‚ÄúRevolution CT.‚Äù This technology allows us to capture extremely high-resolution images of the human body. In this case, you can see a detailed 3D view of the skull, including the blood vessels‚Äîarteries, veins, and even tiny capillaries.\n\nIf you‚Äôre curious, the link at the bottom of the slide will take you to the website where this example came from. They have several other fascinating images that show how CT technology is evolving to provide ever-sharper and more detailed views inside the human body. These advances are not just technical achievements‚Äîthey are vital for clinical use, helping doctors diagnose and treat patients with much greater precision.\n\nAs we go through this lecture, I‚Äôll also remind you that all the MATLAB code examples we‚Äôll be using are available on the LMS. If you‚Äôd like to follow along or try them yourself later, you‚Äôll find everything there.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 15 - Slide2.txt", "file_path": "Lecture 15\\Texts\\Slide2.txt", "content": "Let‚Äôs take a quick look at our course schedule to see where we stand.\u000b\nAs you can see, we‚Äôve been moving steadily through the topics‚Äîstarting with the basics, like introduction, systems, and Fourier series, and then building up to signal processing and MATLAB practice.\n\nCurrently, we are on track with the section on computed tomography, or CT. This means you already have the mathematical and signal processing foundation, and now we are applying it directly to medical imaging.\nSo yes, we are on schedule, and today‚Äôs focus on MATLAB and CT fits exactly into the larger flow of the course.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 15 - Slide3.txt", "file_path": "Lecture 15\\Texts\\Slide3.txt", "content": "Now, before we dive into today‚Äôs material, just a quick reminder.\u000b\nIf you‚Äôd like to follow along with the examples, please make sure you have MATLAB installed, along with the Signal Processing Toolbox and the Image Processing Toolbox. These are essential for running the code.\n\nIf you haven‚Äôt set this up yet, don‚Äôt worry‚Äîit‚Äôs simple. Just download the zip file I‚Äôve posted on LMS, extract all the files, and place them into your MATLAB folder. Once that‚Äôs done, you‚Äôll be ready to run the scripts, either as we go through them in class or later on your own time at home.\nAlright, with that setup in place, let‚Äôs move on to the theory of CT.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 15 - Slide4.txt", "file_path": "Lecture 15\\Texts\\Slide4.txt", "content": "Now, let‚Äôs step into the theory of CT.\u000b\nWhat I‚Äôll give you here is a general overview, much of which should feel like a review from our earlier classes.\n\nThe basic idea is this: CT uses X-ray projections taken from many different angles around the body. As the X-rays pass through, they are attenuated‚Äîthat means they are weakened‚Äîdepending on the different properties of the tissues they pass through. Dense tissues like bone attenuate more, while soft tissues attenuate less.\n\nBy collecting all these projections from multiple views, we build up enough information to start reconstructing what‚Äôs inside the body. That‚Äôs the foundation of computed tomography.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 15 - Slide5.txt", "file_path": "Lecture 15\\Texts\\Slide5.txt", "content": "So, let‚Äôs look a little closer at the general theory of CT.\n\nWhen X-rays pass through the body, different tissues attenuate the beam in different ways. For example, bone blocks attenuate X-rays much more strongly than muscle or soft tissue. This difference is exactly what gives us useful information.\n\nNow, the challenge is that each X-ray image we take is really just a 2D projection. You can see that in the picture on the left. If we only had this single projection, we would miss most of the information about what lies behind or to the side of the structures.\nSo what do we do? We take projections from many different angles‚Äîlike the images you see in the middle. Each projection adds a different piece of information.\n\nThen comes the key step: we solve what‚Äôs called an inverse problem. Mathematically, this is done using the Radon transform and its inverse. By combining all of those projections through the inverse Radon transform, we can reconstruct a detailed image of the body‚Äôs interior. \n\nThat‚Äôs what you see on the right: a 3D CT reconstruction where bones and tissues are highlighted in different colors.\nIn this class, to keep things simpler, we will mostly work with 2D imaging cases when applying the Radon transform in MATLAB. But remember, the same principle extends to full 3D CT in clinical practice.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 15 - Slide6.txt", "file_path": "Lecture 15\\Texts\\Slide6.txt", "content": "Now let‚Äôs talk about the resolution of CT reconstruction.\n\nTwo key factors determine how sharp or detailed your reconstructed image will be.\nFirst, the number of rays, or the number of X-rays passing through the object at each angle. This directly affects the radial component of spatial resolution‚Äîthat is, how finely we can distinguish details along a line from the center outward.\nSecond, the number of views, or the number of different angles, is often denoted by theta. This controls the circumferential component of resolution‚Äîhow well we can distinguish details as we go around the circle.\nOn the cartoon CT slice shown here, you can see both components: the radial resolution going outward from the center, and the circumferential resolution wrapping around the image.\n\nSo, to summarize: the more rays you use, and the more views you collect, the higher the resolution of your reconstruction. On the other hand, using fewer rays or fewer views leads to blurrier images with less detail.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 15 - Slide7.txt", "file_path": "Lecture 15\\Texts\\Slide7.txt", "content": "Now let‚Äôs talk about the Radon transform.\n\nThe Radon transform is essentially a way to represent X-ray projection data, and the result is something called a sinogram. A sinogram is just an image that shows how the projections vary as we rotate around the object.\nThis representation also tells us something about the characteristics of the sample. For example, objects that are closer to the center of the field of view show up as regions of higher amplitude in the sinogram.\n\nHere you can see the mathematical definition of the Radon transform. Don‚Äôt worry about memorizing the equation‚Äîwhat matters is understanding that it describes how the function f of x and y, our object, is projected along different angles, which we denote by theta.\nOn the left, you see the same setup we discussed earlier, with X-rays passing through the object at angle theta. And on the right, you see an example sinogram. Notice the bright, wavy line: that corresponds to the yellow circular object in the field of view. Because it‚Äôs closer to the center, its contribution to the sinogram has a higher amplitude, and it traces this curve as the angle changes.\n\nSo the sinogram is the bridge between the raw projection data we collect and the reconstructed CT image we want to build.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 15 - Slide8.txt", "file_path": "Lecture 15\\Texts\\Slide8.txt", "content": "So far, we‚Äôve looked at the Radon transform. But to actually reconstruct an image, we also need the inverse Radon transform. This process relies on something called the Fourier slice theorem.\n\nHere‚Äôs the idea in simpler terms. If we take the one-dimensional Fourier transform of a Radon transform projection profile at a specific angle phi, that result is the same as taking the two-dimensional Fourier transform of the object and looking along a line at that same angle phi.\n\nThat‚Äôs powerful because it means each projection we acquire gives us a slice, or a line, through the object‚Äôs 2D Fourier transform. When we combine all these slices from projections taken at many different angles, we can fill in the complete 2D Fourier transform of the object.\nAnd once we have the full 2D Fourier transform, we can simply apply the inverse two-dimensional Fourier transform to reconstruct the original image.\n\nThe diagram here shows this step by step. Each projection corresponds to one line in Fourier space. Change the angle, and you get another line, and then another. As we add up all these lines at different angles, the Fourier space is gradually filled. Finally, applying the inverse Fourier transform brings us back to a reconstructed image in real space.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 15 - Slide9.txt", "file_path": "Lecture 15\\Texts\\Slide9.txt", "content": "The next key idea is back-projection, which is the standard method for reconstructing CT slices.\n\nHere‚Äôs how it works: we start with the sinogram, which contains all the projection data. For each projection, we take its information and ‚Äúback-project‚Äù it‚Äîthat means we spread that projection back across the image space at the angle where it was acquired.\nLet‚Äôs look at this cartoon example. Imagine we have a large oval, and inside it are two smaller red ovals. If we take a projection from the top down, the sinogram shows two bright spots that correspond to those red ovals. When we back-project this data, we get a blurry reconstruction along that one angle.\n\nNow, let‚Äôs do the same from a side view. Again, the sinogram shows two high-amplitude areas, and when we back-project that projection, we get another blurry reconstruction, but this time along the side angle.\n\nAt this point, we have two back-projections. When we combine them, the overlap starts to suggest the locations of the two smaller ovals inside the larger one. Of course, with only two projections, the result is quite rough‚Äîin this case, it looks more like two squares.\nBut as we add more and more projections from many different angles, the combined back-projections gradually approximate the true shapes. With enough views, the reconstruction recovers the original sample very accurately.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 15 - Slide10.txt", "file_path": "Lecture 15\\Texts\\Slide10.txt", "content": "Now let‚Äôs move from simple back-projection to filtered back-projection, or FBP.\n\nAs we saw in the last example, back-projection is the standard way of reconstructing CT slices. We take the sinogram, back-project each view, and combine them. But here‚Äôs the problem: if we simply use unfiltered back-projection, the resulting image often looks blurry. That‚Äôs because the projections overlap in a way that smears out the details.\nTo fix this, we apply a filter in the sinogram space before back-projecting. Filtering helps sharpen the edges and preserve the fine details that would otherwise be lost.\n\nLook at the images here. In the top row, we have a sinogram and its back-projected image. Notice how the back-projection produces just a fuzzy, grayish blob with poorly defined boundaries.\n\nNow, in the bottom row, the sinogram has first been filtered. After reconstruction, the image is much clearer‚Äîyou can actually see the circular structure with sharp edges, and even a smaller circle inside it, which could represent something like a tumor.\nOn the right side, you see examples of different filters that can be applied, such as the Ram-Lak filter, the Shepp-Logan filter, and the Hamming filter. Each of these adjusts the frequency content of the sinogram differently, but they all serve the same purpose: to enhance contrast and reduce blurring in the final reconstructed image.\n\nSo in summary, filtered back-projection is one of the most widely used methods in CT, because it gives us images that are both accurate and sharp enough for clinical use.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 15 - Slide11.txt", "file_path": "Lecture 15\\Texts\\Slide11.txt", "content": "Here‚Äôs something pretty cool to look at‚Äîthis is filtered back-projection in progress.\nWhat you‚Äôre seeing here starts with the original sinogram. Then, as the algorithm processes each view, the back-projections are gradually added together. Step by step, the image begins to take shape.\n\nIt‚Äôs almost like watching the reconstruction unfold in real time. As more and more views are included, the overlapping lines come together, and the original phantom‚Äîor the sample image we started with‚Äîslowly reappears.\nThis gives you a visual sense of how CT actually builds up an image: not from a single snapshot, but from many projections combined through filtered back-projection.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 15 - Slide12.txt", "file_path": "Lecture 15\\Texts\\Slide12.txt", "content": "Now, how do we use MATLAB to demonstrate these CT principles?\u000bMATLAB gives us two distinct simulation paths: parallel-beam CT and fan-beam CT. We‚Äôll explore both, step by step.\n\nFor our demonstrations, we‚Äôll work with three simple phantoms‚Äîthat is, test images used to evaluate reconstruction methods:\nSquare-in-square phantom: a small bright square centered inside a larger dark square.\nCircle phantom with a nodule: a large gray circle with a small bright circle near the edge‚Äîthink of that dot as a mock ‚Äútumor.‚Äù\nShepp‚ÄìLogan phantom: a standard test object that comes with MATLAB. It‚Äôs widely used in our field, so researchers can compare results fairly‚Äîif two groups reconstruct the same Shepp‚ÄìLogan phantom, their images are directly comparable.\n\nWe‚Äôll first apply parallel-beam tools and then switch to fan-beam tools, so you can see how the algorithms behave on the same phantoms.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 15 - Slide13.txt", "file_path": "Lecture 15\\Texts\\Slide13.txt", "content": "Let‚Äôs begin with parallel-beam CT.\n\nIn MATLAB, the main tool for simulating this is the Radon transform function, simply called radon. This function takes an image‚Äîour phantom‚Äîand generates its parallel-beam projections at different rotation angles.\n\nIn a parallel-beam setup, the X-ray beams are arranged so that they travel in perfectly parallel lines as they pass through the object and reach the detector. Each projection corresponds to summing the values of the image pixels along those parallel paths.\n\nSo when you hear me say ‚Äúparallel-beam CT,‚Äù think of it as collecting straight, evenly spaced rays, rotating around the object, and storing all of that information in a sinogram. MATLAB‚Äôs radon function gives us exactly that.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 15 - Slide14.txt", "file_path": "Lecture 15\\Texts\\Slide14.txt", "content": "Now let‚Äôs go deeper into how MATLAB‚Äôs radon function works for parallel-beam CT.\n\nThink of the simulated beams as rays spaced one pixel apart. These beams are projected from the source, travel in straight parallel lines, pass through the object, and arrive at the detector on the other side.\nIn practice, both the beams and the detector array rotate together around the center of the image. The amount of rotation is controlled by the angle theta, which we provide as an input to the function. So by sweeping through many angles, we gather projections from all around the object.\n\nThe MATLAB function looks like this:\u000bR, x p equals radon I, theta;\nHere‚Äôs what each part means:\nR is the result‚Äîit contains the projection values, essentially the amplitudes of the rays.\nX p gives the detector positions, if you need to keep track of them.\nI is simply your input image, or phantom.\ntheta is the set of rotation angles you want to use.\n\nSo, in one line of code, MATLAB takes your image and simulates what the parallel-beam projections would look like at the specified angles.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 15 - Slide15.txt", "file_path": "Lecture 15\\Texts\\Slide15.txt", "content": "Let‚Äôs look at our first MATLAB example for parallel-beam CT.\n\nIf you‚Äôve downloaded the materials, the code is called E x 1 parallel dot m. You can try running it now or later on your own. Inside the script, at line 5, there‚Äôs a variable called p type. By changing the p-type to 1, 2, or 3, you can switch between the three phantoms: the square, the circle with a small circle inside, or the Shepp‚ÄìLogan phantom.\n\nHere‚Äôs what happens when you run the code. On the left, you see the image domain, in this case, just a simple white square. As the red arrow rotates, it represents the projection angle, which we call theta. Each angle contributes new data to the sinogram.\nOn the right, you see the sinogram domain. Notice how information is gradually added as theta changes. For the square, the sinogram has a symmetric, crisscrossing pattern.\n\nIf you switch to the second phantom‚Äîthe large circle with a small white dot‚Äîthe sinogram looks different. The small dot produces a wavy curve in the sinogram because the detector gets closer to and farther from the dot as it rotates.\n\nFinally, with the Shepp‚ÄìLogan phantom, the sinogram becomes much more complex. There are many structures inside that phantom, and each one leaves its signature on the sinogram. This complexity is much closer to what you would see in a real CT scan of the human brain, where multiple tissues and structures contribute overlapping patterns.\n\nSo the sinogram is really just another way of saying: here‚Äôs how the object looks from every possible angle.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 15 - Slide16.txt", "file_path": "Lecture 15\\Texts\\Slide16.txt", "content": "Now let‚Äôs try a simple experiment with the code.\n\nIn the script, there‚Äôs a parameter called the theta step. This sets how finely we sample the rotation angles. For example, if the step is 1, MATLAB computes projections at 0, 1, 2, 3 degrees, and so on, all the way to 180. But if we change the step to 5, then MATLAB only samples at 0, 5, 10, 15 degrees, and so on.\n\nHere‚Äôs what happens when we make that change. The sinogram becomes much coarser. You can see it here for the square phantom: instead of smooth, continuous curves, the sinogram looks blocky, because we‚Äôve skipped over many intermediate angles.\nWhat does this mean for reconstruction? It means lower resolution. If we try to reconstruct an image from this sinogram, the result will appear streaky or blocky, because we don‚Äôt have enough angular information to fill in the details.\n\nSo the takeaway is: to get a high-resolution reconstruction, we want a small theta step, which gives us many angular views. Fewer angles make the computation faster, but the image quality suffers.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 15 - Slide17.txt", "file_path": "Lecture 15\\Texts\\Slide17.txt", "content": "Now let‚Äôs see what happens if we change the maximum theta value.\n\nPreviously, we collected projections from 0 all the way up to 180 degrees. But what if we stop early‚Äîsay, at just 45 degrees?\nWhen we do that, the sinogram is essentially truncated. Instead of showing information from a full half-circle of projections, it only covers a small portion of the angles. In other words, we‚Äôve only scanned part of the sample.\n\nWhat does that mean for reconstruction? If we only gather projections from 0 to 45 degrees, then when we try to reconstruct the image, we‚Äôre missing most of the information. The result will be incomplete and inaccurate, because the algorithm never saw the object from the other directions.\n\nSo, limiting the maximum theta is like taking photos of a sculpture but only from one side‚Äîyou lose the complete picture. To get a full and accurate reconstruction, we need projections across the whole angular range.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 15 - Slide18.txt", "file_path": "Lecture 15\\Texts\\Slide18.txt", "content": "So far, we‚Äôve seen how to use the forward Radon transform with MATLAB‚Äôs radon function. But to actually reconstruct an image, we need the inverse Radon transform, which in MATLAB is implemented as the function iradon.\nThis function performs back-projection for parallel-beam sinograms, and it can also carry out filtered back-projection if we choose.\n\nHere‚Äôs the format of the function:\u000bI equals I radon R, theta, interp, filter;\nI is the reconstructed image, the output we want.\nR is the sinogram, the input data we‚Äôve collected.\ntheta is the set of rotation angles used.\nInterp is the interpolation method‚Äîthis tells MATLAB how to fill in values between samples.\nThe filter is the filter we want to apply for filtered back-projection.\n\nThe last two arguments, interpolation and filter, are optional. That means you could just write I radon R, theta to perform simple, unfiltered back-projection. But if you include a filter, such as Ram-Lak or Shepp‚ÄìLogan, MATLAB will automatically implement filtered back-projection, giving you a sharper and more accurate image.\n\nSo, in short, radon simulates the projections, while iradon reconstructs the image from those projections. Together, they form the foundation of CT simulation in MATLAB.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 15 - Slide19.txt", "file_path": "Lecture 15\\Texts\\Slide19.txt", "content": "Let‚Äôs walk through Example 2: Parallel-beam back-projection.\n\nThe code for this is called E x 2 parallel back dot m. Inside the script, line 32 runs a normal back-projection with no filter, and line 35 applies a filtered back-projection.\n\nIn this case, I‚Äôve included the Hamming filter as an example. But MATLAB also supports other filter types, such as Ram-Lak, Shepp‚ÄìLogan, Cosine, and Hann. You can even design your own filter and add it as an input if you want to experiment.\nNow let‚Äôs compare the results. On the left, you see the reconstruction of a simple white square using unfiltered back-projection. Notice how the image shows streaks and a kind of ‚Äúcross‚Äù pattern‚Äîthat‚Äôs the artifact caused by overlapping unfiltered projections.\nOn the right, you see the result with filtered back-projection. The square is much cleaner, with sharper edges and less background noise. The filter removes the smearing and helps reveal the actual structure.\n\nIf you change the phantom, such as using the Shepp‚ÄìLogan phantom, you may notice that different filters give slightly different results. Some filters are better for enhancing edges, while others help reduce blurriness or noise. This is why in practice, the choice of filter depends on what details are most important for your application.\n\nSo this example shows us the power of filtering‚Äîwithout it, back-projection images can be full of artifacts, but with it, we get reconstructions that are much more useful and accurate.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 15 - Slide20.txt", "file_path": "Lecture 15\\Texts\\Slide20.txt", "content": "Now, let‚Äôs take a closer look at how changing the maximum theta or the theta step affects reconstruction.\n\nBy default, our code uses a maximum theta of 180 degrees and a step size of 1 degree. But what happens if we adjust those values?\nOn the left, you see the case where max theta is 180 and the step is 5. Here, we‚Äôre still covering the full range of angles, but we‚Äôre sampling them more coarsely. As a result, the back-projected image shows more artifacts‚Äînotice the diagonal streaks in the unfiltered image. Filtering helps reduce some of these, but the square edges are still less crisp compared to the default case.\n\nOn the right, we have max theta reduced to 90 degrees with a step of 1. Now the issue is different: we‚Äôve cut the angular range in half, so the reconstruction is incomplete. The square looks more rounded at the edges, and you see stronger cross-like artifacts in the background.\n\nSo here‚Äôs the key takeaway:\nIncreasing the step size (fewer sampled angles) makes the reconstructions less defined.\nReducing the maximum angle (smaller angular coverage) lowers resolution and produces incomplete images.\nIn real CT scanning, both of these parameters matter. More angles mean better images but also more computation and radiation dose. Fewer angles save time and dose but reduce quality.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 15 - Slide21.txt", "file_path": "Lecture 15\\Texts\\Slide21.txt", "content": "Now let‚Äôs turn to the second type of CT geometry that MATLAB can simulate, which is fan-beam CT.\n\nUnlike parallel-beam CT, where all the X-ray beams are perfectly parallel, in fan-beam CT, the rays spread out from a single source point, forming a fan shape as they pass through the object. This setup is much closer to how real CT scanners work today, because the source rotates around the patient while the rays diverge outward.\n\nAs I mentioned in class, fan-beam geometry introduces some additional considerations compared to the parallel-beam case, but it also gives us more realistic simulations. MATLAB provides dedicated functions to handle fan-beam CT, which we‚Äôll look at in the next slides.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 15 - Slide22.txt", "file_path": "Lecture 15\\Texts\\Slide22.txt", "content": "Now let‚Äôs look at MATLAB‚Äôs fanbeam function.\n\nIn this geometry, the X-rays don‚Äôt travel in parallel lines like before. Instead, they spread out in a fan shape from a single point called the beam vertex. You can see that illustrated here: the source is at the top, and the rays diverge outward, passing through the sample.\nOn the opposite side, we have the detector array that records how much of the X-rays make it through.\n\nThere‚Äôs also an important parameter here called D. This is the distance from the fan-beam vertex‚Äîthat is, the source position‚Äîto the center of rotation of the sample. For something simple, like a square phantom, D might just be the length of the diagonal. For a human body, D would be the distance from the source to the center of the patient.\nAs in parallel-beam CT, both the source and detector rotate together around the object at different angles, which we denote by theta. By sweeping through all these angles, we collect the fan-beam projection data needed for reconstruction.\n\nSo, the fanbeam function in MATLAB allows us to simulate this geometry directly, bringing us closer to how modern CT scanners actually operate.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 15 - Slide23.txt", "file_path": "Lecture 15\\Texts\\Slide23.txt", "content": "Now let‚Äôs look at the fanbeam function in MATLAB in more detail.\n\nThis function calculates projection data for a specified fan-beam geometry. Unlike the parallel-beam case, here the rotation angles are fixed from 0 to 360 degrees. That means you can‚Äôt restrict the scan to only half a circle, like 0 to 180, or just 0 to 45. The fanbeam function always assumes a full rotation.\n\nHere‚Äôs what the function looks like:\nF1, sensor pos 1, fan rot angles 1] equals fan beam P, D, ‚ÄòFan Sensor Spacing', dsensor1;\nLet‚Äôs break it down:\nF1 is the fan-beam projection data, the main output you need.\nSensor pos 1 gives the positions of the detectors.\nFan rot angles 1 lists the rotation angles used.\nP is your input image, or phantom.\nD is the distance from the source vertex to the object‚Äôs center of rotation.\n‚ÄòFan Sensor Spacing' is a property name, which tells MATLAB you‚Äôre specifying the spacing between detectors.\nDsensor 1 is the actual spacing value.\n\nThose last two inputs are optional. You can run the function with just the image and the distance, and MATLAB will use default detector spacing. But if you want more control‚Äîsay, to change the resolution or field of view‚Äîyou can specify these additional arguments.\nSo in short: fanbeam gives you projection data over a full 360 degrees, with flexibility to adjust sensor spacing if needed.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 15 - Slide24.txt", "file_path": "Lecture 15\\Texts\\Slide24.txt", "content": "Now that we‚Äôve seen how MATLAB generates fan-beam projections, the next step is to reconstruct an image from them. For that, we use the I fan beam function.\n\nHere‚Äôs what happens under the hood: I fan beam first converts the fan-beam data into equivalent parallel-beam projections. Once that‚Äôs done, it applies the same filtered back-projection algorithm we studied earlier to perform the inverse Radon transform and reconstruct the image.\n\nThe function looks like this:\nI fan 1 equals I fan beam F1, D, ‚ÄòFan Sensor Spacing', dsensor 1);\nBreaking it down:\nIfan1 is the reconstructed image, the output.\nF1 is the sinogram data produced by the fanbeam function.\nD is the distance from the source to the object‚Äôs center.\n‚ÄòFan Sensor Spacing' is the property name that tells MATLAB you‚Äôre specifying detector spacing.\nD sensor 1 is the actual spacing value.\n\nThe last two inputs are optional‚Äîyou can let MATLAB use default spacing if you don‚Äôt need to adjust it.\nSo, just like with parallel-beam CT, we have a pair of functions: fan beam to simulate the projections, and I-fan beam to reconstruct the image. Together, they allow us to model fan-beam CT entirely within MATLAB.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 15 - Slide25.txt", "file_path": "Lecture 15\\Texts\\Slide25.txt", "content": "Now let‚Äôs move to Example 3: Fan-beam CT.\n\nIf you run the script E x 3 fan beam dot m, you‚Äôll see how MATLAB handles this geometry. Inside the code, at line 25, there‚Äôs a parameter called D. By convention, D is set to be slightly larger than half the diagonal length of the image. Then, at line 26, the D sensor value is set to 1, which specifies the spacing between detector elements.\n\nOn the left, you can see the fan-beam sinogram for a simple square phantom. On the right, you see the reconstruction using filtered back-projection. The square shape is clear and recognizable, but you may also notice small dots or speckles near the corners. These are reconstruction artifacts, and if you run this code on your own computer, you‚Äôll probably see them more clearly.\nFor more complex phantoms, the sinogram looks similar to what we saw in earlier examples, but extended over a full 0 to 360 degrees. \n\nThe reconstructions in those cases often show diagonal streaks or artifacts around the edges. Even so, the important structural information inside the phantom‚Äîlike the outlines of shapes‚Äîremains fairly accurate.\nSo, fan-beam CT in MATLAB gives us realistic results, but it also shows us that no reconstruction is perfect. Artifacts are always a part of the process, and learning how to recognize and minimize them is a big part of CT imaging.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 15 - Slide26.txt", "file_path": "Lecture 15\\Texts\\Slide26.txt", "content": "Now let‚Äôs experiment with sensor spacing in fan-beam CT.\n\nIn our code, this is the D sensor value on line 26. By default, we set it to 1. But what happens if we change it?\nOn the left, we set D sensor equals 0.5. This means the detectors are closer together, giving us finer sampling. The result is higher resolution‚Äîyou can clearly see the square shape, with only minor artifacts.\nIn the middle, we keep the D sensor equal to 1, the default value. The reconstruction is still decent, but not quite as sharp as when we used the smaller spacing.\nOn the right, we set D sensor equals 5. Now the detector spacing is wide, so we lose a lot of detail. The reconstruction looks blurred and distorted‚Äîthe square shape is almost unrecognizable, more like a rounded blob.\n\nSo the key point here is: resolution is directly related to detector spacing. Smaller spacing means higher resolution. Larger spacing means lower resolution.\nIn practice, of course, reducing sensor spacing increases data size and computational load. But if the spacing is too large, the resolution drops too much to be useful. Finding the right balance is part of designing and operating a CT system.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 15 - Slide27.txt", "file_path": "Lecture 15\\Texts\\Slide27.txt", "content": "Now let‚Äôs test what happens when we change the parameter D in our fan-beam simulation.\nRemember, D is the distance from the fan-beam source vertex to the center of the object. In our code, this is set in line 28. The default value is 191, which is roughly half the diagonal length of the image.\n\nHere‚Äôs what happens:\nOn the left, with D equals 191 (default), the reconstruction looks good. The square phantom is sharp, and the resolution is reasonable.\nIn the middle, when we increase D to 300, the resolution decreases. The edges become less clear, and faint artifacts start to appear in the background.\nOn the right, with D equals 500, the resolution drops significantly. The square no longer looks crisp‚Äîit appears distorted, and the background shows strong artifacts.\nSo the rule here is: the farther away the source is from the object, the lower the reconstruction resolution.\n\nThis simulation is useful not only for learning but also for system design. It shows that if we set the source too far from the patient, image resolution suffers. By keeping the source-object distance within a good range, we can achieve sharper reconstructions.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 15 - Slide28.txt", "file_path": "Lecture 15\\Texts\\Slide28.txt", "content": "That wraps up our MATLAB section for today.\u000b\nYou‚Äôll use MATLAB‚Äôs radon and iradon functions to work with an ellipse phantom‚Äîgenerate the projections, form the sinogram, and reconstruct the image.\n\nPlease make sure your submission clearly shows:\nthe input ellipse image,\nthe sinogram you generated, and\nThe reconstructed image (try both unfiltered and filtered back-projection, and note which filter you used).\nIf you run into issues, double-check that the Signal Processing and Image Processing Toolboxes are installed, and verify your angle settings and interpolation options.\n\nGreat work today. Next time, we‚Äôll build on this and discuss how to interpret artifacts and improve reconstruction quality.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 15 - Slide29.txt", "file_path": "Lecture 15\\Texts\\Slide29.txt", "content": "And finally, let me remind you about your second homework assignment.\nFrom the Green Book, please complete problems 1.14, 1.15, and 1.18. \n\nIn addition, there‚Äôs an optional project for those of you who want to take on something more creative. As part of the Art_X initiative, you can design a concept for a portable CT scanner‚Äîfor example, one mounted inside a self-driving car, or a CT scanner designed for a completely novel application.\n\nTo get inspired, I‚Äôve included two references here at the bottom of the slide, which showcase some innovative CT scanner designs. These papers, including some from Dr. Wang‚Äôs group, may give you ideas for how CT technology can be adapted beyond the traditional hospital setting.\nAgain, the design project is optional‚Äîbut the Green Book problems are required.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 15", "slide_number": 30, "slide_filename": "Slide30.txt", "slide_annotation": "Lecture 15 - Slide30.txt", "file_path": "Lecture 15\\Texts\\Slide30.txt", "content": "And with that, we‚Äôll wrap up today‚Äôs session.\nThank you all for joining the class and following along. We‚Äôve covered a lot‚Äîfrom the basic theory of CT, to the Radon transform, back-projection, filtered back-projection, and finally MATLAB implementations for both parallel-beam and fan-beam geometries.\n\nPlease remember to finish your homework assignments and, if you‚Äôre interested, explore the optional design project for something creative.\n\nThat‚Äôs all for today. Once again‚Äîthank you, and I look forward to seeing you next time.", "total_slides_in_lecture": 30}
{"lecture": "Lecture 16", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 16 - Slide1.txt", "file_path": "Lecture 16\\Texts\\Slide1.txt", "content": "Welcome to Lecture 16 on Nuclear Physics.\n\nIn this course, we focus on understanding nuclear physics in the context of biomedical imaging. The key to learn this material is to go through it more than once ‚Äî first by following the lecture, and then by reviewing the textbook or slides afterward. Repetition helps clarify the concepts and makes them easier to remember.\n\nNow, when you open the textbook chapter on nuclear medicine or nuclear imaging, you might feel that it contains an overwhelming amount of information. There are details about pharmaceuticals and diseases that may seem excessive. For our purpose, you don‚Äôt need to memorize those. Instead, we will focus on the physical principles, including the chemistry and instrumentation concepts, essential components, and particularly the mathematical modeling and quantitative relationships.\n\nThese are the core concepts you should master well. The clinical terms and long lists are good to know, but not our main focus.\nThroughout the slides, you‚Äôll notice visual markers such as red diamonds or green buttons. Again, these are meant to highlight what is most important and to guide your attention to the central knowledge in this lecture.‚Äù", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 16 - Slide2.txt", "file_path": "Lecture 16\\Texts\\Slide2.txt", "content": "As you can see from the course schedule, we are right on track. We‚Äôve already covered the earlier topics such as introduction, system concepts, Fourier series, and signal processing. More recently, we completed modules on CT reconstruction and MATLAB applications.\n\nToday‚Äôs lecture is focusing on nuclear physics. In the next lecture, we will explain SPECT and PET imaging principles and systems. After that, we will move on to MRI, ultrasound, and optical imaging, before wrapping up the course with the final exam.\n\nSo everything is proceeding according to plan.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 16 - Slide3.txt", "file_path": "Lecture 16\\Texts\\Slide3.txt", "content": "Now we begin the section on nuclear physics, which marks the start of our discussion on nuclear imaging as an imaging modality. As with every modality we cover, I‚Äôll first give you a general perspective ‚Äî a big picture introduction to provide background and context. After that, we‚Äôll focus on the key elements specific to this modality.\n\nIn this course, I want you to always pay attention to the outline. The outline is your roadmap. It shows what‚Äôs important and helps you remember the structure of the topic. As in earlier lectures, I use visual markers like the red diamond and the green button to highlight the most essential points. These are your hints for where to focus.\n\nNow, some of you may feel concerned about prerequisites, such as calculus or differential equations. Don‚Äôt let that discourage you. Today, it‚Äôs easier than ever to look up mathematical concepts ‚Äî a quick search can bring up explanations and definitions right away. What matters most is your willingness to learn. This field is highly interdisciplinary, and your goal should be to explore, connect, and build your own understanding of the knowledge you need.\n\nSo, as we step into nuclear physics, remember: our approach is not to memorize every detail, but to see the story. First, we build a general understanding, then we dive into specific components like radioactivity, tracer production, and data acquisition. Once you see the big picture, the details fall into place more naturally. Nuclear imaging is a fascinating field, and I hope you‚Äôll enjoy this journey as much as I do.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 16 - Slide4.txt", "file_path": "Lecture 16\\Texts\\Slide4.txt", "content": "Let‚Äôs begin with a bit of history to set the stage. Every imaging modality we study has been shaped by many talented researchers whose discoveries were recognized at the highest levels. Nuclear imaging is no exception. If you look back at the milestones in its history, you‚Äôll find several famous names, and among the most remarkable is Marie Curie.\n\nMarie Curie was awarded two Nobel Prizes for her groundbreaking work on radioactivity. In fact, element ninety-six, called curium, was named in her honor. Her influence extended to the next generation as well ‚Äî her daughter Ir√®ne became a distinguished nuclear physicist in her own right and received a Nobel Prize in 1935. Her other daughter, √àve, wrote a highly acclaimed biography of her mother.\n\nSo, when we talk about nuclear imaging today, we are building on a legacy that began with pioneers like Marie Curie. Their contributions laid the foundation for our understanding of radioactivity, which is central to this entire field.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 16 - Slide5.txt", "file_path": "Lecture 16\\Texts\\Slide5.txt", "content": "This is Michael Ter-Pogossian, a very important figure in nuclear physics and especially in nuclear imaging. He is recognized as one of the pioneers of positron emission tomography, or PET, which became the first truly functional brain imaging technology.\nPET was groundbreaking because, unlike conventional CT, which shows anatomy, PET allows us to evaluate the brain in action ‚Äî during mental processes. Later in this course, we will study PET in detail.\n\nI‚Äôd like to share a personal connection here. After finishing my PhD, I was hired at the Mallinckrodt Institute of Radiology at Washington University in St. Louis. I started as a medical physicist, doing imaging measurements and research. The leader of our medical physics group at that time was Professor Michael Ter-Pogossian. I was still young and didn‚Äôt fully realize how great he was ‚Äî I simply saw him as my immediate supervisor.\nLooking back, I recognize his remarkable contributions. Many people felt that he truly deserved a Nobel Prize for his invention of PET and his pioneering results. Unfortunately, he passed away in 1996 from heart disease. Beyond his scientific work, he was also the founding editor-in-chief of the IEEE Transactions on Medical Imaging, which remains the most important journal in our field.\n\nI remember him as not only brilliant but also humorous and down-to-earth. For example, he once told us that doing too much simulation is like having a simulated meal ‚Äî it might look convincing, but afterward, you‚Äôre still hungry. That kind of humor made him unforgettable.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 16 - Slide6.txt", "file_path": "Lecture 16\\Texts\\Slide6.txt", "content": "Here we see X-ray CT, which you are already familiar with from earlier lectures. Let me give you a quick review.\nIn conventional CT, the system typically uses an energy-integrating detector. This means that all of the incoming X-ray photons are combined, and the result is an averaged measurement over the entire X-ray spectrum. That gives us the grayscale images you know well, where tissues appear in shades from black to white.\n\nNow, with more advanced technology, we can use photon-counting detectors. These detectors are energy-sensitive, which means they can record not only the number of photons but also their energies. By sorting the detected photons into many energy bins, we can see the full X-ray spectrum, including characteristic features called K-edges for different materials.\n\nThis approach allows us to separate and identify materials such as gold, iodine, calcium, and fat. In other words, instead of just a grayscale image, we can create color-coded images that reflect the molecular composition of the tissue or phantom. This is called spectral, or molecular, CT.\nSo, with photon-counting CT, we move beyond structural imaging into the possibility of functional and molecular imaging ‚Äî a very exciting direction for the future.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 16 - Slide7.txt", "file_path": "Lecture 16\\Texts\\Slide7.txt", "content": "Today, we move into a very different story ‚Äî the basic idea of nuclear imaging. This principle is absolutely fundamental.\nJust like in X-ray imaging, we are still working with radiation. But here‚Äôs the key difference: in nuclear imaging, the radiation does not come from an external source outside the body. Instead, it comes from inside the patient, because we introduce a radioactive tracer into the body. That tracer emits gamma rays.\n\nNow, gamma rays and X-rays are very similar. Their energy ranges overlap, although gamma rays are usually considered to be slightly more energetic. Both are forms of electromagnetic radiation, part of the same spectrum. Importantly, like X-rays, gamma rays can penetrate the body.\nBut if we simply detect gamma rays without direction, we only get a collection of signals with no spatial meaning. To form an image, we need to know where each photon came from. This is why we use a collimator ‚Äî such as a pinhole collimator or a parallel-hole collimator. The collimator ensures that only photons traveling along certain directions are recorded, giving us the directional information we need for tomographic imaging.\nAfter passing through the collimator, the gamma rays strike a scintillation crystal. This crystal converts the high-energy gamma photons into flashes of visible light. Next, photomultiplier tubes take that light and convert it into electrical signals, amplifying the signal dramatically.\nThose electrical signals are then digitized and sent to a computer. One measurement gives us a single projection. By collecting many projections from different angles, we can reconstruct a tomographic image.\n\nIt‚Äôs important to understand what the resulting image represents. It is not showing us anatomical structure, like CT does. Instead, it shows the distribution of the radioactive tracer inside the body. That means the image reflects physiology ‚Äî the function of tissues ‚Äî rather than just anatomy. This is the key and very powerful idea behind nuclear imaging.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 16 - Slide8.txt", "file_path": "Lecture 16\\Texts\\Slide8.txt", "content": "This slide further illustrates the principle of nuclear imaging. Here we see the use of a parallel-hole collimator. The purpose of the collimator is to make sure that the signals we detect are aligned with the holes. That way, any photon recorded must have come from directly beneath that hole, and this gives us the spatial information we need. Without it, we would not know the photon‚Äôs origin.\n\nOnce the gamma photon passes through the collimator, it hits the scintillation crystal. The crystal converts that high-energy photon into visible light. Then the photomultiplier tubes convert the light into an electrical signal, which can be read by the computer for image formation.\nNow, notice what happens after a radioactive tracer is injected into the bloodstream. The tracer travels through the body and tends to accumulate in certain locations, depending on the physiology. In this example, for cardiac imaging, the tracer concentrates in the heart muscle. This allows us to see how it penetrates the cardiac chambers and the muscle wall.\nThe resulting image reflects the distribution of the tracer. Since the tracer participates in biochemical reactions, nuclear imaging provides insight into physiology ‚Äî the function of tissues ‚Äî rather than just anatomy.\n\nThis makes nuclear imaging highly complementary to X-ray imaging. While X-ray imaging uses an external source and shows structure, nuclear imaging relies on an internal radioactive tracer and shows biological function. Together, they give us a much more complete picture.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 16 - Slide9.txt", "file_path": "Lecture 16\\Texts\\Slide9.txt", "content": "The physical principles of nuclear imaging can be summarized as follows.\nFirst, we use an unstable isotope as a tracer molecule, also called a radionuclide. This tracer can be administered in different ways ‚Äî most commonly through intravenous injection, but in some cases, it can also be taken orally, for example, by drinking a small amount of radioactive liquid.\n\nOnce the tracer enters the body, it participates in normal metabolic processes. As it does so, it emits gamma rays from inside the body. These gamma rays are then detected, and the resulting measurements reflect the distribution of the tracer.\nWhat makes this so powerful is that the signal corresponds to metabolism and physiological function, rather than just anatomical structure. In other words, nuclear imaging allows us to see how tissues are working, not just how they look.\n\nThis sets the stage for us to compare nuclear imaging with X-ray imaging, and to highlight both the similarities and the important differences between them.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 16 - Slide10.txt", "file_path": "Lecture 16\\Texts\\Slide10.txt", "content": "Here, we compare X-ray CT with nuclear imaging by looking at their similarities and differences.\nOn the left, you see a CT scanner and CT images. On the right is a PET scanner, which represents nuclear imaging. Although the images shown here may not be perfectly matched, the idea is clear ‚Äî both modalities use radiation and produce tomographic images.\n\nLet‚Äôs begin with the similarities. \nFirst, gamma rays and X-rays have overlapping energy ranges. Their energies are high enough that, when passing through biological tissue, we can often treat them as traveling in straight lines. Of course, scattering occurs, but for image reconstruction, we assume straight-line geometry. That simplifies tomographic reconstruction.\n\nSecond, the data from both modalities can be expressed as line integrals. In CT, this is exactly the Radon transform. In nuclear imaging, it is a related but slightly different mathematical formulation. Still, both methods involve line integrals and support tomographic image reconstruction.\n\nThird, both CT and nuclear imaging generate two-dimensional slices, three-dimensional volumes, or even four-dimensional datasets when dynamics are included. So, in terms of imaging geometry and reconstruction, they share a strong common foundation.\n\nNow, let‚Äôs turn to the differences. In CT, the radiation source is external ‚Äî the X-ray tube outside the patient. In nuclear imaging, the source is internal ‚Äî a radioactive tracer introduced into the bloodstream. This tracer accumulates in certain regions, such as the heart or tumors, especially malignant tumors with a high blood supply.\nAnother difference is the type of information provided. CT primarily gives anatomical information ‚Äî structural details of organs and tissues. Nuclear imaging, by contrast, provides functional information, showing how tissues are metabolizing or reacting biologically. This makes nuclear imaging highly complementary to CT.\nThird, there is a difference in flux and resolution. CT uses a strong external X-ray beam, producing high photon flux and therefore high-resolution images. Nuclear imaging uses only a small injected amount of tracer, so the signal is weak. The data are noisier, and the spatial resolution is lower. Nuclear images may not look as sharp as CT images.\n\nFinally, there is sensitivity. CT detects differences in tissue density, but since most tissues are made of light elements, benign and malignant tumors can appear very similar until they grow large enough to change anatomy. That means CT may detect cancer only at later stages. Nuclear imaging, however, is far more sensitive. Because the tracer accumulates in areas of abnormal metabolism, nuclear imaging can reveal very small or early-stage tumors ‚Äî often before structural changes appear.\nThis sensitivity is one of the greatest strengths of nuclear imaging, making it an invaluable tool for early detection and treatment planning.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 16 - Slide11.txt", "file_path": "Lecture 16\\Texts\\Slide11.txt", "content": "CT and nuclear imaging are highly complementary. CT provides detailed anatomical information, while nuclear imaging shows functional and biochemical processes. So the natural idea is ‚Äî why not combine them?\nThat‚Äôs exactly what happened with the development of hybrid scanners. One of the pioneers in this effort was Simon Cherry, who helped establish PET-CT as a practical technology.\n\nThe major strength of nuclear imaging is its ability to label many types of metabolites. This makes it highly sensitive for studying biochemical and physiological processes inside the body. The limitation, of course, is that nuclear imaging by itself has relatively low resolution. That‚Äôs why it works best when combined with CT or MRI.\n\nCT or MRI gives us the anatomical background ‚Äî the structural map of tissues. The nuclear tracer distribution can then be precisely superimposed on that background, maximizing the information content. This combination allows us to see both anatomy and function in perfect alignment.\nPET-CT was the first successful hybrid scanner. Today, in oncology departments, it is rare to see a stand-alone PET scanner. Almost always, PET is integrated with CT. This avoids the problems of image registration that occur when separate scanners are used. Moving the patient between machines can change posture or positioning, making alignment difficult. With a single integrated scanner, those issues are eliminated.\n\nLater, PET-MRI was also developed, bringing together molecular imaging with the excellent soft-tissue contrast of MRI. And researchers have even envisioned the possibility of a ‚Äútri-modality‚Äù scanner ‚Äî PET, CT, and MRI all in one system. That would give us an even more powerful tool for comprehensive imaging.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 16 - Slide12.txt", "file_path": "Lecture 16\\Texts\\Slide12.txt", "content": "We have now finished the general perspective, so let‚Äôs move on to radioactivity.\n\nRadioactivity is explained in several sections of the textbook. The reading itself is not especially difficult, but the terminology can feel confusing, especially if you don‚Äôt have a strong background in chemistry. I‚Äôve had to go through these materials multiple times myself. Over time, it becomes clearer ‚Äî what we are really dealing with are physical and chemical phenomena at the atomic level.\nSo why does radioactivity occur? The reason is that certain isotopes are unstable. These isotopes undergo decay, releasing radiation in the process. The decay can follow different mechanisms, which we will study in detail.\n\nImportantly, radioactive decay follows an exponential curve. This behavior can be modeled mathematically using ordinary differential equations. Once you see the formulation, it‚Äôs not as difficult as it sounds. With some practice, the mathematics will become straightforward, and it provides a precise way to describe radioactive processes.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 16 - Slide13.txt", "file_path": "Lecture 16\\Texts\\Slide13.txt", "content": "This slide is mainly a review of basic nuclear physics concepts: the atomic number and the mass number.\nThe atomic number, denoted by Z, is simply the number of protons in the nucleus of an atom. The mass number, usually written as A, is the total number of nucleons, meaning the number of protons plus the number of neutrons. In most cases, the atomic weight is approximately equal to this mass number.\n\nFor example, in helium, the atomic number is 2, because there are two protons. The mass number is 4, because helium has two protons and two neutrons. When an atom is neutral, the number of electrons surrounding the nucleus matches the number of protons in the nucleus. This keeps the charge balanced.\nNow, once we understand atomic number and mass number, we can define what isotopes are.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 16 - Slide14.txt", "file_path": "Lecture 16\\Texts\\Slide14.txt", "content": "Now let‚Äôs talk about isotopes.\n\nIsotopes of a chemical element all share the same atomic number ‚Äî meaning they have the same number of protons. But they differ in their mass numbers because the number of neutrons is different. So, isotopes are the same element, but with different neutron counts.\nInside the nucleus, protons and neutrons are held together by the strong nuclear force. When the balance between protons and neutrons is within a stable range, the nucleus remains stable. But if the configuration becomes unbalanced ‚Äî for example, when there are too many neutrons or too many protons relative to each other ‚Äî the nucleus can become unstable.\n\nAn unstable nucleus will eventually change into a more stable state. This happens through radioactive decay, which follows an exponential curve. We‚Äôll explore that decay law in more detail and even derive it mathematically later on.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 16 - Slide15.txt", "file_path": "Lecture 16\\Texts\\Slide15.txt", "content": "Radioactive isotopes can emit gamma rays. Let‚Äôs look more closely at what that means.\n\nOn the electromagnetic spectrum, X-rays and gamma rays actually overlap in terms of energy. X-rays typically fall within a certain energy range, while gamma rays can extend both lower and higher. In medical imaging, gamma rays are often in the range of hundreds of kilo-electronvolts. For example, 511 kilo-electronvolts is a key energy level used in positron emission tomography, or PET. At lower energies, gamma rays are used in single-photon emission computed tomography, or SPECT.\n\nSo in terms of energy, X-rays and gamma rays are quite similar. The main difference lies in how they are generated. X-rays are produced by electron interactions ‚Äî for instance, when high-energy electrons strike a tungsten target, generating X-rays in the process. In contrast, gamma rays are produced by transitions inside the nucleus itself. When an unstable nucleus changes from a higher energy state to a lower energy state, it releases that excess energy in the form of a gamma photon.\n\nThis distinction is important: X-rays come from interactions involving electrons, while gamma rays come directly from nuclear transitions. Both are powerful tools in imaging, and as we move forward, we‚Äôll see in more detail how gamma rays are generated and used.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 16 - Slide16.txt", "file_path": "Lecture 16\\Texts\\Slide16.txt", "content": "Radioactive isotopes can decay through several different mechanisms, and it‚Äôs important to understand the main ones.\n\nIn your textbook, these are grouped into four basic types. The first is alpha decay, the second is beta decay, which includes both beta-minus and beta-plus, or positron, emission ‚Äî the third is gamma decay, and the fourth is electron capture.\nAn easy way to remember them is simply in order: alpha, beta, gamma, and then electron capture. Each of these processes describes a different way in which an unstable nucleus transforms into a more stable state, releasing radiation in the process.\n\nWe‚Äôll go into detail about each type of decay in the following slides.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 16 - Slide17.txt", "file_path": "Lecture 16\\Texts\\Slide17.txt", "content": "Let‚Äôs begin with alpha decay.\n\nIn alpha decay, an unstable isotope transforms into a daughter nucleus and emits an alpha particle. An alpha particle is essentially the nucleus of a helium atom ‚Äî two protons and two neutrons bound together.\nAlpha particles carry a relatively high energy, typically in the range of 3 to 7 mega-electronvolts. Because of this, they can cause severe tissue damage. However, they cannot travel far ‚Äî usually only a few millimeters in biological tissue.\n\nFor this reason, alpha radiation is not useful for imaging. It doesn‚Äôt penetrate deeply enough to form images. Instead, it has therapeutic applications. If we can direct alpha-emitting isotopes precisely to cancer cells, they can deliver very localized, highly destructive radiation that kills the tumor while sparing surrounding tissue.\n\nSo, while alpha decay is the first mechanism of radioactive decay we study, it is more relevant to radiation therapy than to nuclear imaging.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 16 - Slide18.txt", "file_path": "Lecture 16\\Texts\\Slide18.txt", "content": "The second major type of decay is called beta decay. There are two forms: beta-minus decay and beta-plus decay.\n\nIn beta-minus decay, a neutron inside the nucleus converts into a proton, releasing an electron ‚Äî which we call a beta-minus particle ‚Äî along with an antineutrino. This process changes the nucleus into a different element. Beta-minus particles can be damaging to tissue, and in some cases, this type of decay is used for therapy.\n\nIn beta-plus decay, a proton in the nucleus converts into a neutron, releasing a positron ‚Äî which is a positively charged electron ‚Äî along with a neutrino. The positron does not remain in the body for long. It quickly encounters a nearby electron, and when the two meet, they annihilate each other. This annihilation produces a pair of gamma photons.\n\nThese gamma photons do not emit randomly. They travel in opposite directions, almost exactly 180 degrees apart. This property is the basis for positron emission tomography, or PET. By detecting these pairs of photons and knowing they travel in straight, opposite directions, we can reconstruct images of where the tracer accumulated in the body.\n\nSome textbooks separate positron emission from beta decay and list it as a distinct process. Others group it under beta-plus decay. Either way, the important point is this: for imaging purposes, beta-plus decay and gamma decay are useful, while alpha decay and beta-minus decay are not. PET, in particular, relies on beta-plus decay and positron annihilation.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 16 - Slide19.txt", "file_path": "Lecture 16\\Texts\\Slide19.txt", "content": "Positron emission is another key decay mechanism, and it is especially important for nuclear imaging.\n\nHere‚Äôs what happens: inside an unstable nucleus, a proton transforms into a neutron. In the process, the nucleus emits a positron ‚Äî which is a positively charged electron ‚Äî along with a neutrino. The emitted positron travels only a short distance before it encounters an electron. Because they carry opposite charges, the positron and the electron are naturally attracted to one another. When they meet, they annihilate each other. This annihilation produces two gamma photons, each with an energy of 511 kilo-electronvolts. These photons travel in opposite directions, almost exactly 180 degrees apart.\n\nThis property is the basis for positron emission tomography, or PET. By detecting pairs of gamma photons and knowing they must lie along the same line, we can reconstruct the location where the positron annihilation occurred. That gives us an image of the tracer distribution inside the body.\n\nSo, compared with alpha and beta-minus decay, positron emission is extremely valuable for imaging ‚Äî it is the physical principle behind PET, one of the most important functional imaging modalities in medicine.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 16 - Slide20.txt", "file_path": "Lecture 16\\Texts\\Slide20.txt", "content": "Now let‚Äôs look at gamma decay.\n\nGamma decay occurs when a nucleus changes its internal configuration and releases the excess energy in the form of a gamma photon. This process does not change the number of protons or neutrons, so the element remains the same ‚Äî only the energy state of the nucleus changes.\nThere are two main categories. The first is immediate gamma decay. In this case, the nucleus drops from a higher energy state directly to a lower state, and a gamma photon is released right away.\n\nThe second category is more common and more useful in practice. Here, the nucleus does not release the gamma photon immediately. Instead, it first enters a metastable state. After a short delay, the gamma photon is released, sometimes along with other particles such as an antineutrino.\nA classic example of this process is technetium-99m, often abbreviated as Tc-99m. The ‚Äúm‚Äù stands for metastable. This isotope is widely used in nuclear medicine because of its convenient half-life and the gamma photons it emits, which are ideal for imaging.\n\nSo, gamma decay is highly relevant to nuclear imaging, especially in the form of isotopes like Tc-99m, which remains one of the most important tracers in clinical practice.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 16 - Slide21.txt", "file_path": "Lecture 16\\Texts\\Slide21.txt", "content": "The textbook explains gamma decay in more detail, and it makes an important point: no radionuclide decays solely by gamma emission. Instead, gamma rays appear as part of a decay scheme where an intermediate state ‚Äî called a metastable state ‚Äî is formed.\n\nA widely used example is technetium-99m, or Tc-99m. It is produced from molybdenum-99, which undergoes beta decay. The daughter nucleus, Tc-99m, is metastable. After a half-life of about six hours, it transitions to the stable form Tc-99, releasing a gamma photon in the process.\nThe gamma photon from Tc-99m has an energy of about 140 kilo-electronvolts. This energy is ideal for nuclear medicine imaging. If the photon energy is too low, below about 100 keV, most of the photons are absorbed in tissue and never reach the detector. If the energy is too high, above about 200 keV, the photons penetrate the collimator and reduce image quality. That is why the sweet spot for imaging is between 100 and 200 keV ‚Äî and Tc-99m fits perfectly in this range.\n\nThis is why Tc-99m has become the most widely used radionuclide in nuclear medicine, applied in more than 90 percent of diagnostic imaging studies. So now you‚Äôve seen the three main decay mechanisms: alpha, beta, and gamma. In the next step, we‚Äôll discuss the final one ‚Äî electron capture.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 16 - Slide22.txt", "file_path": "Lecture 16\\Texts\\Slide22.txt", "content": "The final type of decay we‚Äôll discuss is electron capture.\n\nIn electron capture, a proton in the nucleus is transformed into a neutron. This happens when the nucleus captures one of the atom‚Äôs own inner electrons, usually from the K-shell. By capturing this electron, the proton combines with it to form a neutron, and a neutrino is released.\nThe daughter nucleus is often left in an excited state. To release that excess energy, it emits X-rays or Auger electrons. In some cases, additional radiation is produced through what we call bremsstrahlung ‚Äî a German word meaning ‚Äúbraking radiation.‚Äù This process is similar to what we studied earlier in X-ray production, where electrons slow down and emit radiation.\n\nSo, electron capture does not directly produce a particle like an alpha or a beta. Instead, it changes the balance inside the nucleus and produces photons ‚Äî either X-rays or gamma rays ‚Äî along with other possible emissions. This mechanism adds to the classification of decay types, and it highlights again that photons can be generated in different ways. Whether they are called X-rays or gamma rays depends not on the photon itself, but on how it was produced.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 16 - Slide23.txt", "file_path": "Lecture 16\\Texts\\Slide23.txt", "content": "Here is another way of classifying radioactive decay. In total, you can think of five types.\n\nFirst, alpha decay, which typically occurs among the heavier elements. Second, beta-minus decay. Third, positron emission, or beta-plus decay. Fourth, electron capture. And finally, spontaneous fission.\nFor our purposes, the first four are the most important. Spontaneous fission is more of a nuclear physics phenomenon and is not especially relevant to medical imaging.\n\nSo, depending on which textbook you consult, you may see these processes grouped slightly differently. But for this course, just keep in mind the essential ones: alpha, beta, gamma, and electron capture. Within beta decay, remember there are two cases ‚Äî beta-minus and beta-plus.\nWith that, you now have a solid overview of the main decay mechanisms we will encounter in nuclear imaging.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 16 - Slide24.txt", "file_path": "Lecture 16\\Texts\\Slide24.txt", "content": "This table lists some of the most common radioactive tracers used in nuclear medicine. Each tracer has a characteristic half-life and a specific gamma-ray energy.\n\nFor example, technetium-99m has a half-life of about six hours and emits gamma rays at 140 kilo-electronvolts, which is ideal for imaging. Gallium-67 has a longer half-life of about three days and multiple gamma energies. Thallium-201, xenon-133, indium-111, and iodine isotopes are also widely used, each with its own half-lives and energy levels.\n\nYou don‚Äôt need to memorize the details of this table. Instead, just recognize that different radionuclides are available for different imaging applications, and their properties ‚Äî half-life and gamma energy ‚Äî determine how and where they are best used. This is general knowledge to give you a sense of the variety of tracers available in nuclear medicine.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 16 - Slide25.txt", "file_path": "Lecture 16\\Texts\\Slide25.txt", "content": "", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 16 - Slide26.txt", "file_path": "Lecture 16\\Texts\\Slide26.txt", "content": "Now let‚Äôs look at the mathematical details of radioactive decay.\nThe key point is that the rate of change of the number of radioactive nuclei, which we call capital N, is proportional to how much material is still present. \n\nIn symbols, this is written as:\n‚Äúdee N by dee t equals negative lambda times N.‚Äù\nHere, lambda is called the decay constant. The negative sign simply tells us that the number is going down over time.\nIf you solve this equation, the solution is an exponential curve. It looks like this:\n‚ÄúN of t equals N naught times e to the power of negative lambda times t.‚Äù\nHere, N of t is the number of nuclei at time t, and N naught is the starting number at time zero.\n\nNow, the half-life ‚Äî written as t one-half ‚Äî is the time it takes for the material to decay to one-half of its original amount. To find it, we set N equal to one-half of N naught in the formula. If you solve for time, you get:\n‚Äút one-half equals natural log of two, divided by lambda.‚Äù\nSo the half-life depends only on the decay constant.\n\nBut in medicine, we also have to think about biological clearance. When we inject a radioactive tracer, the body tries to remove it ‚Äî for example, through urine. That biological process also follows an exponential law, with its own half-life.\nWhen we put both processes together ‚Äî the physical decay and the biological clearance ‚Äî we get the effective half-life. \n\nThe formula is:\n‚ÄúOne over effective half-life equals one over the physical half-life plus one over the biological half-life.‚Äù\nThis is the equation you really need to remember. It tells us how long a tracer effectively stays in the body, and that is the number we use in nuclear medicine calculations.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 16 - Slide27.txt", "file_path": "Lecture 16\\Texts\\Slide27.txt", "content": "Now we move on to the third part of our discussion ‚Äî tracer production.\nSo far, we‚Äôve gone through the general perspective, and we‚Äôve talked about isotopes and how they decay through different mechanisms. Some of these mechanisms, like positron emission or gamma decay, produce gamma rays that we can detect for imaging purposes. We also saw how radioactive decay can be modeled mathematically. \n\nNow let‚Äôs ask the next question: how do we actually produce radioactive tracers?\nOne of the most practical, popular, and cost-effective methods is through something called a radionuclide generator. These generators are extremely important because they are the most widely used way to produce tracers for clinical nuclear medicine.\n\nThe process is often called ‚Äúmilking.‚Äù You‚Äôll soon see why this analogy makes sense. We can even describe this process mathematically. It looks a bit more complicated than simple decay, but really, it‚Äôs just a system of two equations ‚Äî one for the parent isotope and one for the daughter isotope. When you couple them together, you can model how tracers are produced over time.\nThat‚Äôs what we‚Äôll explore next.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 16 - Slide28.txt", "file_path": "Lecture 16\\Texts\\Slide28.txt", "content": "Earlier, we talked about the four main mechanisms of radioactive decay: alpha decay, beta decay, gamma decay, and electron capture. Within beta decay, remember there are two types ‚Äî beta minus and beta plus. And for imaging purposes, beta plus is the more important one.\nNow let‚Äôs switch perspective. Instead of asking how isotopes decay, we ask how do we produce these radioactive tracers in the first place? The good news is, this is also easy to remember ‚Äî there are four basic methods.\n\nAccording to your textbook, the four ways to produce radionuclides are:\nNeutron capture.\nNuclear fission.\nCharged-particle bombardment.\nAnd the use of radionuclide generators.\n\nSo again, four mechanisms for decay, and four methods for production ‚Äî a nice symmetry that helps you remember.\nAmong these, I want to highlight the fourth method ‚Äî radionuclide generators ‚Äî because it is the one most widely used in clinical practice. The first two methods require a nuclear reactor, and while they are very important in research and industrial production, they are not as accessible for everyday hospital use. Between those two, nuclear fission is now more popular and more cost-effective compared to neutron capture.\n\nBut for us, as medical imaging scientists and engineers, the radionuclide generator is the key player, and that‚Äôs the one we‚Äôll explore in detail next.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 16 - Slide29.txt", "file_path": "Lecture 16\\Texts\\Slide29.txt", "content": "Now let‚Äôs take a closer look at the first two methods: neutron capture and nuclear fission. Both of these require a nuclear reactor.\n\nA nuclear reactor is essentially a device that initiates and controls a sustained chain reaction. In a typical power plant, this chain reaction produces heat, which is then transferred into a working fluid like water or gas, and eventually used to generate electricity.\nBut for our purpose in nuclear medicine, reactors play a different role ‚Äî they are used to produce radionuclides. In neutron capture, a nucleus absorbs an extra neutron and becomes unstable, creating a useful isotope. In nuclear fission, a heavy nucleus splits into smaller nuclei, and among the products are medically important isotopes such as molybdenum-99, which is the precursor of technetium-99m.\n\nBetween the two, nuclear fission is more popular today because it is more efficient and cost-effective than neutron capture. Still, both methods remain central to the production of medical tracers ‚Äî though they are limited by the need for specialized, large-scale reactor facilities.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 30, "slide_filename": "Slide30.txt", "slide_annotation": "Lecture 16 - Slide30.txt", "file_path": "Lecture 16\\Texts\\Slide30.txt", "content": "Let‚Äôs now underline the second method, which is nuclear fission. The first method, neutron capture, is less commonly used today, so we‚Äôll focus more on fission.\n\nWhat happens in nuclear fission? The nucleus of a heavy atom, such as uranium-235, absorbs a high-energy particle like a neutron. This creates an unstable excited state. As a result, the nucleus splits into two lighter nuclei, along with a few free neutrons, gamma photons, and a large release of energy.\n\nOn the right-hand side, you can see the process illustrated. Uranium-235 absorbs a neutron and briefly becomes uranium-236. That unstable nucleus then splits into two smaller fragments, krypton-92 and barium-141, along with additional neutrons. Those free neutrons can go on to trigger further reactions, creating a chain process.\nThis principle is very powerful. It‚Äôs the same physics behind nuclear power plants and, unfortunately, also nuclear weapons. In our context, though, fission is important mainly because it can be used to produce medically useful radionuclides, such as molybdenum-99, which later decays into technetium-99m for imaging.\n\nThat said, in this course, we won‚Äôt emphasize the reactor-based methods too heavily. Instead, we‚Äôll focus more on the radionuclide generator, since that is what you are most likely to encounter in clinical imaging practice.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 31, "slide_filename": "Slide31.txt", "slide_annotation": "Lecture 16 - Slide31.txt", "file_path": "Lecture 16\\Texts\\Slide31.txt", "content": "The third method of producing radionuclides, and one that is especially important for nuclear imaging, is the use of a cyclotron. A cyclotron is a type of particle accelerator. It works by accelerating charged particles‚Äîsuch as protons‚Äîoutward in a spiral path under the influence of a strong magnetic field and a rapidly varying electric field.\n\nWhen these high-energy particles strike a target material, nuclear reactions occur, and radionuclides are produced. One of the most well-known examples is the production of fluorodeoxyglucose, or FDG, which is widely used in PET imaging.\nBecause PET imaging requires very short-lived tracers like fluorine-18, cyclotrons must be located relatively close to hospitals and imaging centers. This is one of the reasons PET imaging is more costly: it not only requires advanced scanners but also a dedicated cyclotron facility to generate the necessary tracers.\n\nSo, while nuclear reactors play a role in producing some radionuclides, cyclotrons are indispensable for producing the positron-emitting isotopes used in modern clinical imaging.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 32, "slide_filename": "Slide32.txt", "slide_annotation": "Lecture 16 - Slide32.txt", "file_path": "Lecture 16\\Texts\\Slide32.txt", "content": "Here you see a graphical illustration of a modern cyclotron system. On the right is the cyclotron itself, the particle accelerator that produces the radionuclides. In the middle is the biosynthesizer, which is used to process the raw radioactive material into a usable radiotracer‚Äîsuch as FDG for PET imaging. On the left is the computer terminal, where the entire operation is monitored and controlled.\n\nThis layout highlights the workflow: the cyclotron generates the radionuclide, the biosynthesizer prepares it into an injectable tracer, and the computer terminal ensures that everything runs safely and precisely. Together, these components allow us to produce short-lived tracers on demand for clinical use in nuclear imaging.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 33, "slide_filename": "Slide33.txt", "slide_annotation": "Lecture 16 - Slide33.txt", "file_path": "Lecture 16\\Texts\\Slide33.txt", "content": "The fourth method of producing radionuclides, and the one most relevant to clinical imaging today, is the use of radionuclide generators.\nA very well-known example is the technetium-99m generator. This system is sometimes nicknamed the ‚Äúmoly cow‚Äù because it is based on the parent isotope molybdenum-99, which naturally decays to technetium-99m. Just as a cow can be ‚Äúmilked‚Äù repeatedly, the generator can be eluted, or ‚Äúmilked,‚Äù each day to obtain fresh technetium-99m for imaging.\n\nTechnetium-99m is especially important because it is the most widely used radionuclide in single photon emission computed tomography, or SPECT. It provides the right balance of half-life, about six hours, and gamma ray energy, about 140 kilo-electron volts, making it very effective for diagnostic imaging while keeping the patient dose reasonable.\n\nThis approach is extremely cost-effective compared to building a nuclear reactor or cyclotron. That is why radionuclide generators remain a backbone of nuclear medicine practice worldwide.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 34, "slide_filename": "Slide34.txt", "slide_annotation": "Lecture 16 - Slide34.txt", "file_path": "Lecture 16\\Texts\\Slide34.txt", "content": "Let us now look more carefully at how the technetium generator actually works. This system operates through a two-step process.\n\nThe parent isotope is molybdenum-99, which is prepared and loaded into an alumina column inside the generator. This column is shielded with lead for safety. Over time, molybdenum-99 naturally decays to technetium-99m, the daughter isotope. Technetium-99m, however, binds only weakly to the alumina column. That means if we pass a saline solution through the column, it will wash the technetium-99m out, while leaving the molybdenum-99 behind. This daily washing process is often called ‚Äúmilking‚Äù the generator, just like milking a cow. Each elution provides a fresh supply of technetium-99m for clinical use.\n\nOnce obtained, the technetium-99m is injected into the patient‚Äôs body. Inside the body, it undergoes further decay to technetium-99, a stable isotope. Importantly, this decay is accompanied by the emission of gamma photons, which can then be detected by imaging devices such as a gamma camera.\n\nFrom a modeling perspective, this is a two-step decay chain. The parent, molybdenum-99, decays with its own half-life to produce technetium-99m. Then technetium-99m decays with a shorter half-life into stable technetium-99, releasing gamma radiation in the process. Each of these steps follows an exponential law, and when combined, they produce a dynamic interplay between parent, daughter, and final stable product.\nThis two-step process is what makes the technetium generator so effective‚Äîit provides a steady supply of a short-lived imaging isotope without requiring a reactor or cyclotron on-site.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 35, "slide_filename": "Slide35.txt", "slide_annotation": "Lecture 16 - Slide35.txt", "file_path": "Lecture 16\\Texts\\Slide35.txt", "content": "Now, let us connect the generator process to mathematical modeling. To properly describe the production and decay of technetium-99m, we need three first-order differential equations. These equations track the parent isotope molybdenum-99, the daughter isotope technetium-99m, and finally the stable granddaughter technetium-99.\n\nThe first equation describes the parent: it decays exponentially at a constant rate, as we already studied. The second equation describes the daughter, technetium-99m. This one is more interesting because it has two contributions: one positive term, which comes from the decay of the parent feeding into the daughter, and one negative term, which accounts for the daughter decaying further. Finally, the third equation describes the granddaughter, which only has a positive contribution coming from the decay of the daughter.\n\nIf you look closely, you will see that the daughter equation is not purely exponential‚Äîit is influenced by both the parent and its own decay. To solve this type of system, we use standard methods for ordinary differential equations. Specifically, the daughter solution is expressed as the sum of two parts: a homogeneous solution, which describes the natural decay, and a particular solution, which accounts for the contribution from the parent. These two components combine to give the real physical solution.\n\nThe important point here is not to get lost in the algebra, but to recognize the principle: exponential decay chains can be modeled systematically, and by solving these equations, we can predict the time course of technetium-99m production and availability in a generator. This is crucial in practice, because it tells us how much tracer will be available at a given time and when to perform elution for maximum yield.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 36, "slide_filename": "Slide36.txt", "slide_annotation": "Lecture 16 - Slide36.txt", "file_path": "Lecture 16\\Texts\\Slide36.txt", "content": "Now let‚Äôs make sense of the daily yield from a radionuclide generator. \n\nOn the left graph, the dashed line shows the parent isotope‚Äîmolybdenum-99‚Äîwhich slowly decays over time. The solid line shows the daughter isotope‚Äîtechnetium-99m‚Äîwhich gradually builds up as the parent decays. At first, the daughter accumulates rapidly, but eventually, a balance is reached. Why? Because as more daughter atoms appear, they also begin to decay at a faster rate. At some point, the rate of production from the parent equals the rate of decay of the daughter. That balance defines the maximum activity of the daughter.\n\nThis is the key point emphasized in the text: the effective decay rate of the daughter is governed not by its own half-life alone, but by the half-life of the parent. The system naturally seeks equilibrium, so the ratio between parent and daughter becomes constant.\nOn the right-hand graph, you see the practical outcome. Each spike represents an elution, where the daughter isotope is washed out of the generator for clinical use. Typically, this is done once every 24 hours. After each elution, the daughter begins to build up again from the parent, forming another peak. Over the course of days, however, the overall yield decreases because the parent itself is decaying exponentially.\n\nSo in practice, you get this repeating cycle: each day you harvest a certain amount of technetium-99m, use it for imaging, and then allow the generator to recharge. After about a week, the parent has decayed so much that the yield becomes too low, and the generator must be replaced.\nThis is the essence of milking a radionuclide generator‚Äîan elegant and cost-effective way to provide a steady supply of short-lived tracers for nuclear medicine.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 37, "slide_filename": "Slide37.txt", "slide_annotation": "Lecture 16 - Slide37.txt", "file_path": "Lecture 16\\Texts\\Slide37.txt", "content": "Now let‚Äôs look at how technetium-99m is actually used in medicine. This table lists some of the most common radiopharmaceuticals and their clinical applications. What you can see immediately is how versatile this isotope is‚Äîit can target almost every major system of the body.\n\nFor example, technetium-labeled macroaggregated albumin is used for pulmonary perfusion studies, helping us assess blood flow in the lungs. Diphosphonates labeled with technetium are widely applied in skeletal imaging, especially for detecting bone metastases. For the brain, compounds like glucoheptonate or HMPAO can be used to image tumors or measure brain perfusion. Sulfur colloid labeled with technetium allows us to visualize the liver, spleen, and even sentinel lymph nodes. DTPA is useful for renal function and pulmonary ventilation studies. And compounds like sestamibi are used to evaluate myocardial perfusion, making technetium imaging an essential tool in cardiology.\n\nSo what do we learn from this? With just a single isotope‚Äîtechnetium-99m‚Äîwe can design radiopharmaceuticals to study the lungs, bones, brain, liver, kidneys, heart, and more. This is one of the main reasons technetium-99m became the workhorse of nuclear medicine.\nWith that, we finish our discussion of tracer production. We have seen the different ways isotopes can be generated, particularly the third and fourth methods‚Äîcyclotrons for producing positron emitters used in PET imaging, and generators for producing technetium-99m used in SPECT imaging.\n\nNow, we move into the last part of today‚Äôs lecture: data acquisition. Here, we‚Äôll focus on two main approaches. The first is the gamma camera, which detects single gamma-ray photons and is central to SPECT, or single photon emission computed tomography. The second is coincidence detection, which captures pairs of gamma photons emitted simultaneously, and this is the basis of PET, or positron emission tomography.\nAs we go through this, keep in mind the difference: SPECT relies on detecting individual gamma photons, while PET relies on detecting photon pairs from positron annihilation. And this difference has profound implications for scanner design and image quality.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 38, "slide_filename": "Slide38.txt", "slide_annotation": "Lecture 16 - Slide38.txt", "file_path": "Lecture 16\\Texts\\Slide38.txt", "content": "So that concludes the third part, where we discussed radioactivity and tracer production. We learned about the four methods of producing radionuclides, with special focus on the third and fourth methods‚Äîcyclotrons for positron emission tomography, and radionuclide generators for single photon emission tomography. Together, these methods provide the backbone of nuclear medicine.\n\nNow let‚Äôs move on to the final part of today‚Äôs lecture: data acquisition. This is where the signals generated by radioactive decay are actually detected and transformed into images. There are two main approaches we need to understand. The first is the gamma camera, which is used in single-photon emission computed tomography, or SPECT. The gamma camera captures individual gamma-ray photons one by one. The second approach is coincidence detection, which is the principle behind positron emission tomography, or PET. In this case, we don‚Äôt just detect single photons; instead, we detect pairs of photons that are emitted simultaneously, traveling in opposite directions.\n\nWhy does this happen? Because when a positron encounters an electron, the two annihilate, producing a pair of gamma photons. Detecting these pairs in coincidence allows PET to provide highly accurate information about the location of the annihilation event inside the body.\nSo, in summary, SPECT relies on detecting single photons with a gamma camera, while PET relies on detecting photon pairs through coincidence detection. This distinction has a profound effect on how scanners are designed and on the quality of the images they produce.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 39, "slide_filename": "Slide39.txt", "slide_annotation": "Lecture 16 - Slide39.txt", "file_path": "Lecture 16\\Texts\\Slide39.txt", "content": "Now let‚Äôs look more carefully at the principle of the gamma camera, which is one of the most important devices in nuclear medicine imaging. I‚Äôve placed a red diamond here to highlight that this concept is essential.\n\nThe very first step in the system is the collimator. Without collimation, gamma photons would be arriving at the detector from all directions, and we wouldn‚Äôt know where they came from. The collimator, usually made of heavy metal such as lead, only allows photons traveling along certain directions to pass through. In this way, we can establish the line of response. Only photons traveling more or less straight along the openings are detected, while those coming from oblique angles are blocked. This is very similar in spirit to X-ray imaging, where each measurement corresponds to a line integral along a particular direction.\n\nThe choice of collimator is closely linked to photon energy. If the gamma rays have too low an energy, they will be absorbed within the body and never reach the detector. If the energy is too high, the collimator cannot stop the oblique photons, so the image contrast is lost. That is why, for clinical nuclear medicine, we typically work in the range of about 100 to 200 keV ‚Äî a compromise between tissue penetration and collimator effectiveness.\n\nOnce a photon passes the collimator, it strikes the scintillation crystal. This crystal, usually sodium iodide doped with thallium, emits a flash of visible light when it absorbs a gamma photon. That flash is extremely faint, so it is coupled to photomultiplier tubes. The photomultipliers amplify the light signal into an electrical pulse.\n\nThen, through a network ‚Äî sometimes called the Anger logic or Anger position circuit ‚Äî the system calculates the location where the photon entered the crystal. A pulse height analyzer checks the photon‚Äôs energy, ensuring we only accept photons in the desired energy window. Finally, the computer records the position and energy, gradually building up an image of tracer distribution inside the body.\nSo the key sequence is: collimator to select direction, scintillation crystal to convert gamma to light, photomultipliers to amplify the signal, electronics to analyze energy and position, and finally the computer to assemble the image.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 40, "slide_filename": "Slide40.txt", "slide_annotation": "Lecture 16 - Slide40.txt", "file_path": "Lecture 16\\Texts\\Slide40.txt", "content": "", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 41, "slide_filename": "Slide41.txt", "slide_annotation": "Lecture 16 - Slide41.txt", "file_path": "Lecture 16\\Texts\\Slide41.txt", "content": "Now, let‚Äôs take a closer look at the different collimator designs and how each one affects imaging performance.\n\nStarting with the parallel-hole collimator shown at the top left, this is the most commonly used design. In this case, the object and the image appear at the same scale. It‚Äôs simple, robust, and widely applied in routine nuclear medicine imaging.\n\nNext, on the top right, we have the diverging collimator. Here, the holes fan outward. The effect is that the image is smaller than the object. This type is useful when you need to fit a large object, like the entire body, onto a smaller detector field of view.\nAt the bottom left is the converging collimator. As the name suggests, the holes angle inward, producing a magnified image. This is helpful when you want to examine a small organ, like the heart or the thyroid, in greater detail while still using the same detector.\n\nFinally, at the bottom right, we see the pinhole collimator. This works like a camera obscura, creating a highly magnified but inverted image. Because the aperture is very small, spatial resolution can be excellent, but the downside is that sensitivity is poor‚Äîmost photons are blocked.\nThe key idea is always a trade-off. If you make the holes or apertures smaller, you gain higher spatial resolution, but you lose sensitivity and the images become noisier. If you make them larger, you collect more photons, but the resolution decreases. Designing collimators is therefore about finding the right balance between resolution, sensitivity, and the clinical question you are trying to answer.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 42, "slide_filename": "Slide42.txt", "slide_annotation": "Lecture 16 - Slide42.txt", "file_path": "Lecture 16\\Texts\\Slide42.txt", "content": "Now, let‚Äôs take a look at something a bit more advanced and exploratory.\n\nWe know that gamma rays are a form of high-energy electromagnetic radiation, very similar to X-rays, although they are produced differently. In X-ray science, researchers have developed special devices called polycapillary lenses. These lenses use bundles of tiny capillaries to guide and refocus X-rays, effectively bending them toward a focal point. The result is much higher spatial resolution, since the rays that scatter off-path are redirected and concentrated at the detector.\n\nYou can see the schematic here: the X-rays pass through the polycapillary optics, and instead of traveling in random directions, they are collected and focused. This technique has already been applied in confocal micro X-ray fluorescence, which allows elemental analysis within a very small probe volume‚Äîeven beneath the surface of a material.\n\nNow, the intriguing idea is whether similar optics could be applied to gamma rays in nuclear medicine. If we could refocus gamma rays in this way, we might overcome some of the limitations of mechanical collimators, which block most photons and reduce sensitivity. Polycapillary focusing would potentially increase both sensitivity and resolution, opening up exciting possibilities for future gamma-ray imaging.\n\nThat said, this is not part of the standard clinical toolbox‚Äîit‚Äôs more of a research direction. I included it here to show you that beyond traditional collimation, there are innovative approaches being explored to push the boundaries of nuclear imaging technology.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 43, "slide_filename": "Slide43.txt", "slide_annotation": "Lecture 16 - Slide43.txt", "file_path": "Lecture 16\\Texts\\Slide43.txt", "content": "When I visited companies or nuclear detector vendors, they often gave me these as small souvenirs‚Äîclear blocks that look just like glass. But these aren‚Äôt ordinary glass. They are specially manufactured scintillation crystals, the heart of gamma cameras.\n\nWhat makes them remarkable is that when an invisible gamma ray photon strikes the crystal, it produces a tiny flash of visible light. In other words, they convert high-energy radiation, which our eyes cannot see, into visible photons that we can detect.\n\nIf you continuously bombard the crystal with gamma rays, you‚Äôll see flashes of light‚Äîvery faint to the naked eye, but easily measurable with sensitive detectors. This conversion process is the critical first step in turning radioactive emissions inside the body into signals that we can ultimately form into an image.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 44, "slide_filename": "Slide44.txt", "slide_annotation": "Lecture 16 - Slide44.txt", "file_path": "Lecture 16\\Texts\\Slide44.txt", "content": "As I mentioned earlier, the flux of gamma rays coming out of the body is very low. That means the signals are extremely weak. To make them useful, we need to amplify them, and that‚Äôs where the photomultiplier tube, or PMT, comes in.\n\nHere‚Äôs how it works. When a gamma ray enters the scintillation crystal, it produces a small flash of visible light. That light then strikes the photocathode inside the PMT. The photocathode is a special surface that converts each photon of light into an electron.\nNow, a single electron is still a very small signal, so the PMT uses a series of plates called dynodes. Each time an electron strikes a dynode, it knocks loose multiple new electrons. By the time this process is repeated across several dynodes, the signal has been multiplied millions of times.\nAt the final stage, what started as just one or two photons of light has been amplified into a strong stream of electrons‚Äîa measurable current. This current is then sent out of the PMT as an analog signal, which can later be digitized for image reconstruction.\n\nSo the PMT performs two essential functions: it converts light into electrons, and then it magnifies that tiny signal step by step until it‚Äôs strong enough to record. Without this amplification process, the gamma camera simply wouldn‚Äôt be able to detect enough information to form an image.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 45, "slide_filename": "Slide45.txt", "slide_annotation": "Lecture 16 - Slide45.txt", "file_path": "Lecture 16\\Texts\\Slide45.txt", "content": "Now that we know how to detect a signal, the next challenge is to determine where that signal came from. Detection is handled by the photomultiplier tubes, but localization‚Äîfiguring out the exact position of the gamma ray interaction‚Äîis accomplished by what‚Äôs called the Anger network, named after Hal Anger, who invented the gamma camera.\n\nHere‚Äôs the idea. The photomultiplier tubes cannot be made infinitely small, so they are spaced apart. This means a single scintillation event‚Äîa flash of light in the crystal‚Äîwill usually be seen by multiple tubes at once, with some receiving stronger signals and others weaker.\nThe Anger network uses a carefully designed arrangement of resistors, as you see here, to spread out those signals. By analyzing the weighted distribution of the outputs, the system can calculate the event‚Äôs most likely position. In other words, the network acts like a mathematical averaging system: if one region receives stronger signals, the center of activity is estimated there.\n\nSo, although an individual photomultiplier tube only gives you partial information, the combination of many tubes together, processed through the Anger logic, gives a precise two-dimensional location of the gamma ray event. This way, every gamma ray photon detected is not only counted but also assigned a position in the image, allowing us to build up a full spatial map of radioactivity inside the body.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 46, "slide_filename": "Slide46.txt", "slide_annotation": "Lecture 16 - Slide46.txt", "file_path": "Lecture 16\\Texts\\Slide46.txt", "content": "Now that we understand how detection and localization work, let‚Äôs see how they come together in a full imaging system. The result is called planar scintigraphy. Essentially, this is a two-dimensional projection image, much like a standard X-ray radiograph, but instead of using transmitted X-rays, we are mapping the distribution of gamma rays emitted from inside the body.\n\nHere‚Äôs the sequence of events. First, the gamma rays from the patient pass through the collimator, which selects only those photons traveling in defined directions. Next, the gamma photons interact with the scintillation crystal, producing flashes of visible light. Those light signals are then converted into electrical pulses by the photomultiplier tubes.\n\nThe outputs are processed through the position logic circuit, which localizes each detected event in the x and y directions. At the same time, the pulse height analyzer examines the energy of each pulse. This step is important because scattered photons inside the body lose energy, and we don‚Äôt want to include them‚Äîthey would blur the image. By setting an energy threshold, the system can reject those lower-energy, scattered signals.\n\nFinally, the gating circuit can synchronize signal acquisition with physiological motion, such as the beating heart or breathing cycle. That way, the images can be captured at consistent phases, reducing motion artifacts.\nAll of these components work together so that each detected gamma photon is accurately positioned and validated, building up a sharp two-dimensional image of radiotracer distribution inside the patient.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 47, "slide_filename": "Slide47.txt", "slide_annotation": "Lecture 16 - Slide47.txt", "file_path": "Lecture 16\\Texts\\Slide47.txt", "content": "Now let‚Äôs take a closer look at the role of the pulse height analyzer by examining the gamma-ray energy spectrum.\n\nOn the left, you see the spectrum of technetium-99m when the camera is exposed without a patient in front of it. The dominant feature here is a sharp photopeak, centered around the expected emission energy. There are also smaller features, such as the iodine escape peak and signals from Compton scatter or lead X-rays. These arise from interactions within the detector itself.\n\nBut once you place a patient between the source and the detector, the spectrum changes, as shown on the right. Now the clean photopeak is broadened, and the lower-energy region is filled in. Why? Because inside the patient, photons undergo Compton scattering. Each scattering event deflects the photon and reduces its energy. The larger the scattering angle, the lower the photon‚Äôs energy when it finally emerges. As a result, the detector sees not only the original full-energy photons but also a cloud of degraded, lower-energy ones.\n\nThis is a problem because those scattered photons don‚Äôt carry accurate positional information‚Äîthey blur the image. The solution is to use the pulse height analyzer. Since the amplitude of the electronic signal from the photomultiplier tube is proportional to the photon‚Äôs energy, we can set acceptance windows. Only signals that fall within a narrow energy range around the true photopeak‚Äîfor example, around 140 keV for technetium-99m‚Äîare accepted. Signals that are too low, likely coming from scattered photons, are rejected.\n\nThis energy discrimination is what gives nuclear imaging its clarity, much like how anti-scatter grids improve X-ray imaging. By gating and filtering the signals in this way, we make sure that the image is formed primarily from primary, unscattered photons, preserving both resolution and contrast.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 48, "slide_filename": "Slide48.txt", "slide_annotation": "Lecture 16 - Slide48.txt", "file_path": "Lecture 16\\Texts\\Slide48.txt", "content": "The point spread function is a useful way to judge how well a gamma camera can resolve detail. The narrower the point spread function, the better the spatial resolution. Now, what actually determines resolution in nuclear imaging? It turns out there are several factors at play, each introducing its own kind of blurring.\n\nFirst, there‚Äôs the intrinsic resolution of the gamma camera itself. This is limited by the scintillation crystal and the electronics that localize the signal. A thicker crystal spreads the light more broadly, and that uncertainty shows up as blur in the final image.\nSecond, the geometry of the collimator plays a major role. Whether you use a parallel-hole, converging, diverging, or pinhole design, the length and diameter of the channels directly control how sharp or blurry the image will be. Longer, narrower channels improve resolution but reduce sensitivity, because many photons are blocked. Shorter, wider channels improve sensitivity but at the cost of blurrier images.\nThird, Compton scattering inside the patient adds another layer of uncertainty. The deeper the radioactive source lies within the body, the more gamma photons are likely to scatter before they escape. Scattered photons are misleading‚Äîthey point in the wrong direction, lowering contrast and degrading resolution.\n\nWhen we put all these effects together, the total system resolution can be expressed mathematically. Just as when you combine independent sources of random error, you add their variances, here too the variances of the three components‚Äîintrinsic resolution, collimator resolution, and Compton scatter‚Äîadd together. The overall resolution is then the square root of that sum. In other words,\nsystem resolution equals the square root of the sum of the squares of the three main contributions.\n\nThis is the fundamental idea: every component of the imaging chain blurs the signal a little bit, and together they define the ultimate sharpness‚Äîor fuzziness‚Äîof the nuclear medicine image.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 49, "slide_filename": "Slide49.txt", "slide_annotation": "Lecture 16 - Slide49.txt", "file_path": "Lecture 16\\Texts\\Slide49.txt", "content": "When we think about the point spread function, or PSF, we are asking: how does the imaging system represent a single point source of radiation? \n\nIdeally, if you had a perfect gamma camera, one point of activity in the patient would appear as one sharp point in the image.\nIn practice, because of the intrinsic resolution of the scintillation crystal, the photomultiplier tubes, and the electronics, the point source becomes blurred into a Gaussian-shaped distribution. That Gaussian curve is what you would expect from the use of the camera and collimator.\n\nHowever, once you place the patient between the tracer and the detector, Compton scattering adds a complication. Many photons scatter inside the body before reaching the detector, and those scattered photons broaden the distribution. Instead of a neat Gaussian, the PSF develops a long tail‚Äîphotons detected in places far from the true source location.\nThis long tail is undesirable because it reduces spatial resolution and creates image blur. That is why we use techniques like pulse height analysis to reject scattered photons, keeping only the primary gamma rays.\n\nSo the main lesson here is: the Gaussian-like PSF represents the best-case resolution of the gamma camera, but Compton scattering stretches the PSF, adding tails and degrading image quality. Understanding and controlling this spread is fundamental to improving nuclear medicine imaging.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 50, "slide_filename": "Slide50.txt", "slide_annotation": "Lecture 16 - Slide50.txt", "file_path": "Lecture 16\\Texts\\Slide50.txt", "content": "Now let‚Äôs look at another important issue in nuclear imaging systems‚Äîdead time.\n\nWhen a gamma photon interacts with the scintillation crystal, it produces a pulse of light. That pulse has a finite width; it takes some recovery time before the system is ready to register the next event. If two or more photons arrive too close together in time, the system cannot distinguish them. Instead of recording two events, it may only record one, or in some cases, none at all. This limitation is called dead time. It reflects the fact that every detector and its electronics need a short period to reset after each event. If the incoming rate of photons is too high, the detector simply cannot keep up.\n\nThink of it like catching candies falling from the ceiling. If they fall slowly, you can catch and count them one by one. But if they start raining down too quickly, you can‚Äôt separate them anymore‚Äîyou miss some. That‚Äôs exactly what happens here with gamma photons.\nMathematically, we can describe this with the true count rate, denoted by capital N, which is the number of gamma interactions that really happen. Then there‚Äôs the observed count rate, lowercase n, which is what the detector actually records. Because of dead time losses, n is always less than N.\n\nThe system dead time, usually written as the Greek letter tau, can be expressed as the difference between the reciprocal of these two rates:\u000btau equals one over n minus one over N. This formula tells us how much recovery time the system effectively needs between two detected events. In practical gamma cameras, dead time losses are usually not a big problem unless the injected activity is very high‚Äîbut it‚Äôs an important limitation to be aware of.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 51, "slide_filename": "Slide51.txt", "slide_annotation": "Lecture 16 - Slide51.txt", "file_path": "Lecture 16\\Texts\\Slide51.txt", "content": "Now let‚Äôs connect what we have learned to one of the most important applications of nuclear imaging‚ÄîFDG PET for cancer imaging.\n\nPET stands for positron emission tomography. Unlike CT or MRI, which give us structural information, PET provides functional imaging. It shows us how tissues are working metabolically inside the body. The key is the tracer we use. For PET, the most widely used tracer is fluorodeoxyglucose, or FDG. This is a glucose analogue that is produced in a cyclotron. Because FDG behaves like glucose, it is taken up by tissues according to their metabolic activity. Cancer cells are often much more metabolically active than normal cells, so they absorb more FDG. When we image the concentration of FDG, we are essentially mapping the regions of high glucose metabolism. This makes PET a very powerful tool for detecting tumors and also for evaluating whether cancer has metastasized, that is, spread to other sites in the body.\n\nThe physics behind PET is tied to beta-plus decay. FDG emits a positron, which quickly encounters a nearby electron. When the positron and electron meet, they annihilate, producing a pair of gamma photons moving in nearly opposite directions. These paired photons are detected simultaneously‚Äîthat‚Äôs the coincidence detection we discussed earlier.\n\nThis entire principle‚Äîpositron emission, annihilation, and detection of the paired photons‚Äîforms the physical and chemical foundation of PET imaging. Today, FDG PET accounts for about 90 percent of all PET scans, highlighting just how central this method is in cancer diagnosis and treatment planning.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 52, "slide_filename": "Slide52.txt", "slide_annotation": "Lecture 16 - Slide52.txt", "file_path": "Lecture 16\\Texts\\Slide52.txt", "content": "Now, let‚Äôs take a closer look at how coincidence detection works in PET imaging.\nWhen a positron emitted from the tracer annihilates with an electron, the process produces two gamma photons, each with an energy of 511 kilo‚Äìelectron volts, traveling in nearly opposite directions. The key is that both of these photons need to be detected at the same time to confirm a true event.\n\nHere is how the process works step by step. Each gamma photon first enters a detector crystal. Inside the crystal, the photon interacts and produces visible light, which is then amplified by a photomultiplier tube, giving us an electronic pulse. But not every pulse comes from the correct kind of photon‚Äîscattered photons, for example, may have lower energy. This is where the pulse height analyzer comes in. It checks the energy of each detected photon and only accepts those within a narrow energy window around 511 keV. This helps us reject scattered signals.\n\nNow, imagine we have two detectors on opposite sides. If one detector registers a photon and the other also registers a photon of the correct energy, and the two signals arrive within a very short time window‚Äîjust a few nanoseconds apart‚Äîthen the coincidence circuit recognizes them as a pair. This means they must have come from the same annihilation event along the line connecting the two detectors.\n\nSo, only when both detectors see valid 511 keV photons at nearly the same instant do we count it as a true PET event. This is what allows PET imaging to trace the exact line of response inside the body, leading to the powerful three-dimensional reconstructions that make PET such a valuable tool in clinical imaging.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 53, "slide_filename": "Slide53.txt", "slide_annotation": "Lecture 16 - Slide53.txt", "file_path": "Lecture 16\\Texts\\Slide53.txt", "content": "Now, here‚Äôs another way to visualize coincidence detection. In this diagram, we see detectors arranged around the patient, with a positron annihilation event taking place inside the body. From this event, two gamma photons are emitted in nearly opposite directions.\n\nEach detector continuously records signals as a function of time. Most of the time, what you see are background signals or small pulses. But every now and then, a true gamma photon from an annihilation event strikes the detector, and this produces a sharp pulse.\nThe key idea is that we are not looking at a single detector alone. Instead, we compare signals across pairs of detectors. If detector i and detector j, located opposite one another, both register a pulse at nearly the same instant, and if both pulses have the correct energy‚Äîaround 511 keV‚Äîthen we know with high confidence that a true annihilation event occurred along the line connecting those two detectors.\n\nSo, the system continuously searches for these paired pulses. The electronics are designed to be fast and complex enough to check every possible detector pair in real time. Whenever a coincidence is confirmed, the system draws what we call a line of response between the two detectors. That line is where the annihilation must have taken place. \n\nThis principle‚Äîdetecting paired pulses and building lines of response‚Äîis the foundation of PET imaging. By collecting many such lines from annihilations throughout the body, we can reconstruct a full three-dimensional image of tracer distribution inside the patient.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 54, "slide_filename": "Slide54.txt", "slide_annotation": "Lecture 16 - Slide54.txt", "file_path": "Lecture 16\\Texts\\Slide54.txt", "content": "Up to now, we‚Äôve said that coincidence detection tells us that an annihilation event must have occurred somewhere along the line connecting two detectors. But that‚Äôs still quite a lot of uncertainty‚Äîit could be anywhere on that line.\n\nNow, with time-of-flight detection, we can do better. The idea is straightforward: when the two gamma photons are produced, they leave the annihilation site at the same instant, traveling in opposite directions at the speed of light. If the annihilation happens exactly in the middle, the photons will reach both detectors at the same time. But if the event happens closer to one detector, the photon on that side will arrive slightly earlier.\n\nSo by measuring this tiny arrival time difference, we can estimate where along the line the event occurred. Of course, because our timing measurements are not infinitely precise, there is still some uncertainty. That‚Äôs why we don‚Äôt pinpoint a single location, but instead assign a probability distribution‚Äîcentered closer to one detector or the other, depending on the measured delay. The effect is illustrated here. Without time-of-flight information, the whole line is equally likely, so the back-projected signal is spread out. With time-of-flight, the probability is concentrated in a smaller region along that line. When you combine millions of such measurements, the reconstructed PET image is sharper, with better contrast and less noise.\n\nSo the concept is simple: coincidence detection gives us the line, and time-of-flight narrows it down to a region on that line where the paired emission happened. This is why modern PET scanners increasingly use time-of-flight technology‚Äîit significantly improves image quality without requiring extra radiation dose.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 16", "slide_number": 55, "slide_filename": "Slide55.txt", "slide_annotation": "Lecture 16 - Slide55.txt", "file_path": "Lecture 16\\Texts\\Slide55.txt", "content": "So, here are your exercises for this week. \n\nThese questions are designed to help you practice the core ideas we learned today: calculating radioactivity from decay counts, relating activity to half-life and the number of nuclei, analyzing how the signal-to-noise ratio changes with dose and scan time, and modeling the technetium generator with parent‚Äìdaughter decay equations. You are not limited to only these four questions. If you want to challenge yourself, feel free to explore more problems in the textbook. But these are the ones that will be graded.\n\nSince this is an online course, all discussions and clarifications will happen here. If anything is unclear, simply revisit the lecture materials, the slides, or the recorded explanation‚Äîwe‚Äôll keep everything accessible for you to review at your own pace.\n\nThat brings us to the end of today‚Äôs lecture. Thank you for studying along online, and I‚Äôll see you in the next session.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 17 - Slide1.txt", "file_path": "Lecture 17\\Texts\\Slide1.txt", "content": "Welcome to Lecture 17 on SPECT and PET. Today, we‚Äôre finishing the nuclear imaging section. Before I explain the content of this lecture, let‚Äôs get started by recalling why these modalities matter in biomedical imaging. SPECT stands for Single Photon Emission Computed Tomography, and PET means Positron Emission Tomography.\n\nBoth enable molecular imaging and are vital for diagnostics, functional evaluation. They use specific tracers and detectors, which we‚Äôll explore in detail today", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 17 - Slide2.txt", "file_path": "Lecture 17\\Texts\\Slide2.txt", "content": "As you can see from the course schedule, we are right on track. Today‚Äôs lecture focuses on SPECT and PET. After this, we will proceed to MRI, ultrasound, and optical imaging, before concluding the course with the final exam.\n\nSo far so good, and everything is proceeding according to plan.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 17 - Slide3.txt", "file_path": "Lecture 17\\Texts\\Slide3.txt", "content": "Let‚Äôs review a little bit and address some points that were confusing in the last lecture. Have you reviewed the slides and the book chapter? Have you reconsidered why the dynamic curve was shown on the previous slide? \n\nWe utilize a generator system to produce technetium-99m (Tc-99m) and molybdenum-99 (Mo-99), and we observe the dynamic range illustrated. I want to underline that the relationship between parent and daughter nuclides is constant‚Äîthe decay rate of the daughter is governed by the half-life of the parent. This is a core principle in nuclear medicine generators, particularly in SPECT imaging.\n\nThere are three equations we looked at previously. By solving these equations and plotting with MATLAB, you can reproduce the curves and understand the dynamics involved. Normally, we conduct what‚Äôs called a ‚Äúmilking operation‚Äù at peak activity. This produces the types of curves shown here. So when starting from the Mo-99 source, you continually elute Tc-99m for SPECT imaging.\n\nCan anyone tell me why the decay rate of the daughter, which is Tc-99m, is governed by the decay constant or rate of the parent? Do you see this in the equations, or is there something confusing about it? Why do we have this rate relationship? Here‚Äôs a way to think about it: When learning medical imaging, always try to simplify complicated concepts into simple analogies for better understanding.\n\nFor example, imagine parents regularly giving money to a daughter. The parents have a certain amount of fortune and give it at a certain rate. The daughter is very active and immediately spends anything she gets‚Äîwhether it‚Äôs on an iPhone or movies, or travel. So, the parents‚Äô money declines slowly over time, but the daughter always spends fast, as soon as she receives it. In this analogy, the spending rate is governed by the rate at which the parents give money. This heuristic gives us a simple mental picture: the rate at which the daughter ‚Äúdecays‚Äù or spends is determined by the parent‚Äôs supply rate.\n\nReturning to the technical aspect, if you look at equation 2.17 from the textbook, it may not be immediately clear. But if you break it down and consider the relationship between parent and daughter, it becomes much easier to understand why Tc-99m‚Äôs activity is governed by Mo-99‚Äôs decay rate.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 17 - Slide4.txt", "file_path": "Lecture 17\\Texts\\Slide4.txt", "content": "Now, let‚Äôs talk about dead time in nuclear imaging systems. If the efficiency of our system is high, and we inject a large dose of radiopharmaceutical, the number of gamma rays hitting the scintillation crystal can actually be more than the system can handle. This happens because the gamma camera needs some time to recover between events. If two scintillation events happen too close together, they can't both get recorded.\n\nThe total dead time, which we call \"tau,\" is given by this equation: tau equals capital N minus small n, divided by small n times capital N. So, let me say it again: tau equals capital N minus small n, divided by small n times capital N.\n\nHere, \"capital N\" is the true count rate, which means how many scintillations actually happen every second. ‚Äùsmall n\" is the observed count rate, so that‚Äôs how many the system actually records. If you take one divided by a small n, you get the average time spent on each counted event. If you take one divided by capital N, that‚Äôs what you'd have in an ideal system without any dead time. The difference between these two tells us how much measurement is lost because of dead time.\nOf course, what we want is for dead time‚Äîagain, \"tau\"‚Äîto be zero, so every real event is counted. But in reality, as count rates go up, the number of missed events goes up too.\n\nAlways try to use simple ideas to help you understand, and don‚Äôt get lost in complicated formulas. For example, if a student brings me a huge program and says the results are wrong, I tell them, break it into small pieces‚Äîeach piece with clear inputs and outputs. Debug those parts one by one. This makes understanding the whole problem much easier and helps you find mistakes faster.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 17 - Slide5.txt", "file_path": "Lecture 17\\Texts\\Slide5.txt", "content": "Let‚Äôs try to keep a simple, heuristic picture in your mind. Having a clear mental image is key to understanding any concept well. Now, let‚Äôs talk about our outline and what we‚Äôll cover today. We‚Äôll go through single-photon imaging, computed tomography, and positron emission imaging. These are the two main nuclear imaging modalities for tomographic reconstruction.\n\nSo, with single photon imaging, we deal with gamma rays coming from the patient, and with positron emission, we‚Äôre focused on positrons. The positron doesn‚Äôt stay around for long‚Äîit quickly finds a nearby electron. When a positron meets an electron, we use a big word called annihilation, and together they create a pair of gamma-ray photons that shoot out in opposite directions at the speed of light.\n\nSo you have two kinds of imaging. With single photon emission computed tomography, or SPECT, we use single photons‚Äîpretty obvious from the name. And with positron emission tomography, or PET, it‚Äôs actually two photons created from one positron and one electron. Both types of imaging are deeply governed by statistics.\nAs background, I really want us to review statistical distributions. This part will be brief‚Äîjust a rapid overview. If you want to dig into details, you can do that later. After the statistics part, we‚Äôll talk about data models. For either single-photon or positron imaging, you need a mathematical model for how you collect data. Last time, I explained the physical concepts and chemical mechanisms, and we talked about collimation. But with formulas, you can actually write computer programs to do image reconstruction based on these models.\n\nSo the data modeling part is about writing down the equations. Given a distribution, what measurements do you expect to see? Once you have a forward model, you want to invert the process. That lets you recover the underlying distribution‚Äîthe actual tomographic images. Now, these images aren‚Äôt anatomical details or structures, but they show the distribution of radioactive features that you introduced into the patient.\nTo do this, we need to compensate for attenuation, and we have two options: deterministic and statistical reconstruction. We‚Äôll talk about those later. Then, finally, I‚Äôll discuss the scanner architecture. You know, in a CT scanner, you have X-ray tubes, X-ray detectors, high voltage, a transformer, a cooling system, a slip ring, and so on‚Äîall packed inside the gantry. A nuclear scanner might look similar on the outside, but inside, it‚Äôs quite different, and I‚Äôll explain how.\n\nI‚Äôll also mention correction methods, especially scatter correction and how we deal with random counts‚Äîhow we detect them and remove them. These corrections are much different from how CT works. Finally, I‚Äôll mention a bit about the Fully3D conference, which focuses on advanced topics in medical imaging reconstruction, with an emphasis on X-ray CT and related techniques. We‚Äôre organizing that meeting, and it‚Äôs a good chance to learn more. If time permits, I‚Äôll show you today‚Äôs homework questions and give comments. That‚Äôs pretty much the plan‚Äîjust to give you the big picture, so you can see where each part fits in.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 17 - Slide6.txt", "file_path": "Lecture 17\\Texts\\Slide6.txt", "content": "Statistical distributions are very important, especially for understanding noise and random variables over different intervals. The simplest case is the uniform distribution. With a uniform distribution, every point in the interval, like zero to one or zero to five, has the same chance of being picked. Every number in that range is equally likely. That‚Äôs one simple kind of distribution.\n\nNext, we have the Gaussian distribution, and after that, the Poisson distribution. The Gaussian distribution, also called the normal distribution, is your classic bell-shaped curve. You usually describe it with a mean‚Äîcalled mu‚Äîand a standard deviation, sigma. If mu is zero and sigma is one, that‚Äôs the standard normal distribution. Mathematically, you see a scaling factor and an exponential factor involving mu and sigma. This type of distribution is used everywhere in engineering, noise measurement, estimation, and more.\nIf you have lots of independent random variables, each contributing a bit‚Äîmaybe some are uniform, maybe some are not‚Äîwhen you put them all together, the outcome will tend to look like a Gaussian curve. This is a fundamental result from statistics, and it‚Äôs why you see the bell-shaped curve over and over in reality‚Äîlots of independent factors add up to give something that looks Gaussian.\n\nThe Poisson distribution is related to the Gaussian, but looks very different, especially with small means. The Poisson describes random variables that give you counts‚Äîlike one, two, three, and so on‚Äîwith different probabilities for each count. It‚Äôs controlled by the mean, which we call lambda. If that mean is small, you get a very asymmetric distribution. But as lambda grows larger, the curve starts to look more and more like a Gaussian‚Äîeventually, if lambda is really big, the Poisson distribution and the Gaussian distribution look nearly the same.\n\nFor the Gaussian, you have mean and variance, with variance noted as sigma squared. For Poisson, both mean and variance are just lambda. You can actually derive the Gaussian by combining lots of individual random distributions‚Äîthrough convolution‚Äîand prove it‚Äôs a bell-shaped curve.\nSo, in summary, we‚Äôre going to see both Gaussian and Poisson noise often in imaging, and understanding how they behave is key to handling measurement and image reconstruction accurately.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 17 - Slide7.txt", "file_path": "Lecture 17\\Texts\\Slide7.txt", "content": "And Poisson noise, Poisson distribution, you would need some high school statistics, you learned permutations and combinations. And this permutation, you just see the different arrangement, roughly speaking, a combination, you need to just remove those, those essentially the same arrangement like ABC and ACB. \n\nIn terms of combination, you think it's the same thing because this is same three individuals are involved in this activity. So you need just the distinction between permutation and combination. So you can review this slide, you will recall what you learned.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 17 - Slide8.txt", "file_path": "Lecture 17\\Texts\\Slide8.txt", "content": "And by nominal formula, basically, you have a series of events. And for an easy event, it's a probability stake. And say you're a toaster coin, and it's a certain probability P, and you see one result. And the other possibility will be Q. Q equals 1 minus P.\n\nSo you keep doing multiple experiments. And you know how many times you see high, how many times you see tails, this is a good model. And like the example shown here, if you purchase 10 computers. And individually, you know the defective rate is 2%. If you buy 10 computers in a row, what's the chance you get to buy the computer, then you can use by nominal formula. Basically, something like a toast coin. And the probability for you to see one side may not be exactly half. And in general, you can call it P and Q, Q equal to 1 minus P. And the personal distribution is derived from this binomial distribution. One, the number n is very big.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 17 - Slide9.txt", "file_path": "Lecture 17\\Texts\\Slide9.txt", "content": "Let‚Äôs go through the math a bit. I put a green button on this slide to help you remember the key idea: to understand Poisson and Gaussian noise, you need a good grip on the binomial formula, but also on permutations and combinations. When you have a large number of events, you can use some analytic steps to show that the distribution will approach the Poisson form.\n\nFor example, in gamma-ray emission‚Äîwhether it‚Äôs single gamma-ray emission or paired photons‚Äîthe process follows the Poisson distribution model, which uses the parameter lambda. So what does this mean in practice? Let‚Äôs say you‚Äôre counting gamma-ray photons during a scan. The whole time period gets cut into many small intervals. In each tiny interval, the probability that you get a photon is small, call that P, and the probability that you don‚Äôt is Q, which is just one minus P. Whether you get a photon or not in any interval is random; there‚Äôs no way to predict it exactly.\n\nThis randomness is at the heart of nuclear physics. The emission is not deterministic‚Äîit just follows probabilities. Einstein famously said, ‚ÄúGod does not play dice,‚Äù but some would argue that God indeed lets randomness play a role in the universe. In our context, the emission of gamma-ray photons is governed by pure chance, following the Poisson distribution. When you zoom in to those tiny intervals, you‚Äôre really dealing with just the possibility‚ÄîP or Q‚Äîof a gamma-ray being emitted, and it‚Äôs entirely a game of probability.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 17 - Slide10.txt", "file_path": "Lecture 17\\Texts\\Slide10.txt", "content": "Let me just sum up: this is a statistical model, called the Poisson model, and you should know a little bit about it, because it also applies to X-ray emission. When you hit tungsten with an electron beam, you get a certain number of X-ray photons coming out, and that X-ray emission also follows the Poisson distribution.\n\nWhen you detect X-ray photons, sometimes you use photon-counting detectors, which are the latest technology. But most common X-ray detectors are still current-based or energy-integrating types. In traditional X-ray detection, besides Poisson noise, you also get electronic noise from the semiconductor detector design. That electronic noise is modeled by the Gaussian distribution.\n\nSo for X-ray CT data acquisition, the full statistical model is actually a combination‚ÄîPoisson noise plus Gaussian noise. This is just a side note to help you see the big picture. Now, let‚Äôs look at data models for single photon emission and paired photon emission, and how these statistical concepts fit in.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 17 - Slide11.txt", "file_path": "Lecture 17\\Texts\\Slide11.txt", "content": "Let‚Äôs look at this slide‚Äîyou see a diamond symbol, so pay attention. For nuclear emission, the energy of gamma-ray photons is usually in the range of about 60 to 600 keV, and most importantly, about ninety percent of applications use the isotope technetium-99m. This isotope, Tc-99m, has an energy of 140 keV. Now, keep that number in mind‚Äîsometimes I forget, but it‚Äôs 140 keV, just a little higher than the energy used for X-rays. X-ray energy is usually between 80 keV and up to around 130 keV, so this radiotracer energy is actually quite comparable to the X-ray photon energy range. Now, for positron emission, you‚Äôll notice in the homework there's a question about the gamma-ray energy produced by positron emission. The answer is 511 keV, which is much higher.\n\nFor single photon emission, here‚Äôs how we model it. Let‚Äôs say the source is at position \"a.\" At location \"a\" in the body, you have a certain amount of radiotracer‚Äîa concentration or count of radioactive atoms‚Äîdenoted as N(a). You collimate the gamma-ray flux at this spot, and you look down that line to see how much signal flows toward the detector. Now, the gamma-ray can be emitted in any direction, so a scaling factor is applied, proportional to the size of that mechanical collimator. At the detector, you measure how many gamma-ray photons you can receive.\n\nIf the gamma-ray source were in a vacuum, you‚Äôd get just your initial concentration, scaled by the geometric factor‚Äîthat‚Äôs determined by how big the collimator opening is. In reality, the radiotracer is inside the human body, so the photons move from \"a\" toward the gamma camera detector at position \"d.\" Along the way, the photons encounter tissue and undergo attenuation. For gamma-ray photons at 140 keV, their energy is just above the high end used for CT x-rays, but the physics is similar‚Äîthey‚Äôre tiny waves.\nThe formula you see uses an exponential factor, e to the negative integral from \"a\" to \"d\" of mu(s) ds. Here, mu is the attenuation coefficient, which depends on position in the body. This exponential term describes how the signal from the source gets weighted by attenuation‚Äîit's a line integral, but not a full line like in CT, more of a partial line integral. This is what makes SPECT reconstruction different from X-ray CT reconstruction‚Äîso this data model is straightforward but very important.\n\nNow, let‚Äôs look at paired photon emission. Here, an event occurs: a positron is emitted, finds a nearby electron, and annihilates. That results in two gamma-ray photons flying off in opposite directions. One gamma-ray photon travels from position \"a\" to detector d1, the other goes from \"a\" to detector d2. Both photons are attenuated along their paths. The attenuation factor for each path acts like a probability‚Äîthe chance that the photon actually reaches the detector.\nFor a paired event to be registered, both photons have to hit their detectors at the same time‚Äîso the total probability is the product of two individual probabilities. Both exponential attenuation factors multiply together, so in the exponent, you just add up the two integrals‚Äîfrom d1 to a, and from a to d2. Together, these cover the entire path from d1 to d2, resulting in a traditional line integral. And the nice thing is, this line integral does not depend on the original source position \"a\"‚Äîit's only about the line between d1 and d2.\n\nSo, you have two models here: one where the weighting depends on the source location, and one where the weighting‚Äîthe response along the line‚Äîremains constant, regardless of where the gamma source is. For paired photon events, the formula has constant weight; for single photon events, the weight changes based on where the radiotracer is located. And if the radiotracer moves, the weighting factor changes with it. Any questions?", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 17 - Slide12.txt", "file_path": "Lecture 17\\Texts\\Slide12.txt", "content": "Let‚Äôs keep going and look further into this constant weight idea. You‚Äôll notice I put a green button here, and what I want to show is how you can eliminate the location dependence‚Äîmeaning the weighting factor no longer depends on the source position \"a.\"\n\nWhen you have two factors, each one representing a probability, you multiply them together. The source position cancels out in the exponent when you do this multiplication. You can actually apply this same concept to SPECT imaging. In SPECT, which uses single photon emission, if you put detectors on both sides and take two measurements‚Äîone on each side‚Äîyou can multiply those measurements together.\n\nWhen you multiply the measured signals, you end up with the concentration of the radiotracer squared‚Äîit appears as a square here because you measured along both paths. Technically, you‚Äôre combining two partial attenuation factors: one from \"a\" to detector d1, and one from \"a\" to detector d2. When you multiply them, the exponentials add up, and the weighting factor becomes constant with respect to the source location.\n\nSo it seems, at first glance, like SPECT imaging can have a constant weighting factor for these combined measurements, just like paired photon emission in PET, which already has constant weighting. But, be careful‚Äîreal life is not always this simple. There are practical limitations and differences in how signals behave, so always question whether these theoretical simplifications hold up in practice.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 17 - Slide13.txt", "file_path": "Lecture 17\\Texts\\Slide13.txt", "content": "Let‚Äôs look closer‚Äîif you have only one radiotracer concentration at a single source, it‚Äôs pretty simple. But when you have two sources, or more specifically, two gamma resources, things get complicated. If you try to multiply the measurements, like I showed on the previous slide, you end up expanding two terms from each measurement. That gives you four terms in total.\n\nThe first two terms are nice‚Äîthey give you a constant weighting factor, and the line integral of the concentrations squared. That part would make things look easy, as long as the weighting factor stays constant. But the problem is, you also get cross-product terms when expanding. These cross terms involve measurements from one source to the detector, multiplied by measurements from the second source to the other detector, so you get combinations like from d1 to a and from d2 to b, where a and b are different source positions.\n\nThese cross terms are messy and make modeling in SPECT much more complicated. The cross terms mean the data model for SPECT includes weighting factors that still depend on the source positions, even when you know the attenuation background. This partial attenuation messes up the tidy modeling you get with paired photon events in PET; for PET or paired emission, the modeling is straightforward, and you get constant weighting. That‚Äôs why, for SPECT, you can‚Äôt just ignore these source-position-dependent factors‚Äîthey‚Äôre built into the math and must be dealt with when reconstructing images.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 17 - Slide14.txt", "file_path": "Lecture 17\\Texts\\Slide14.txt", "content": "When we use a ring detector in PET, the key is to perform coincidence detection. This means we‚Äôre looking for events where two detectors on opposite sides register gamma photons at the same time‚Äîthose photons come from an annihilation happening somewhere inside the ring. You can determine a line integral along the green line shown here, or along the blue line for different angles. By rotating the ring or changing the projection angle, you can collect projections from multiple directions. \n\nWhen you arrange all these projections by angle, you get what‚Äôs called a sinogram, very similar to what you see in CT. As long as you know the attenuation and apply the correct compensation factors, you can accumulate your data into these line integrals. That‚Äôs the essential idea behind PET and SPECT data acquisition‚Äîgrouping your measurements along lines of response and sorting them by angle into a sinogram.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 17 - Slide15.txt", "file_path": "Lecture 17\\Texts\\Slide15.txt", "content": "Earlier PET scanners used planar imaging mode, which meant you had mechanical collimators defining the imaging planes. These collimators would shape the radioactivity that could reach the detectors, so the ring only collected signals from a thin slice, or plane, of the patient. The detectors were arranged in rings around the patient, and you‚Äôd have different parts of the ring recording data for the left and right sides. \n\nModern PET scanners, though, have removed these mechanical collimators. Instead, you use coincidence detection in three dimensions. This 3D orientation means you can detect gamma photons from many directions, and you‚Äôre not limited by a physical collimator‚Äîanywhere in the field of view, you can pick up valid annihilation events.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 17 - Slide16.txt", "file_path": "Lecture 17\\Texts\\Slide16.txt", "content": "Visually, you see that 3D PET systems have a clear benefit because they don‚Äôt use mechanical collimators. With no collimators blocking the gamma rays, you can capture a much greater number of photons, which raises the system‚Äôs sensitivity and improves image quality. The drawback is that, because the system has a wide opening, it can also pick up more scattered and random events. These unwanted signals can interfere with the data and reduce the quality of your images. \n\nLater, we‚Äôll cover what scattered and random events actually are, and how we try to remove them in post-processing. For 2D PET imaging, where you have strict collimation, only photons from a certain plane are detected, and scattered photons from other directions are blocked. But in 3D PET, you let in more data, and while that's a benefit for sensitivity, it can also make noise and unwanted signals more of a problem.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 17 - Slide17.txt", "file_path": "Lecture 17\\Texts\\Slide17.txt", "content": "When we talk about PET or paired photon emission models, the weighting factor for attenuation is constant along a given line of response. That means when you do an integral over all source distributions, you can pull this constant weighting factor outside the integral‚Äîit doesn't depend on the source position. \n\nYou‚Äôll see how this works more clearly in the next slide. So, let‚Äôs shift our focus to image reconstruction, where these ideas become important.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 17 - Slide18.txt", "file_path": "Lecture 17\\Texts\\Slide18.txt", "content": "Now we‚Äôre talking about the mathematical model for image reconstruction‚Äîthe core idea is how to represent our data. For PET reconstruction, we usually assume that the number of detected photons is large, so the probability averages out and the measurements closely reflect the mean values. \n\nThat lets us use a deterministic model for reconstruction. For a given line path or line of response, the measured data will primarily reflect the radiotracer concentration along that line, so we can treat the whole reconstruction process as solving for concentrations along known paths.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 17 - Slide19.txt", "file_path": "Lecture 17\\Texts\\Slide19.txt", "content": "There‚Äôs a lot of research going on in this area. I should put another green button here for curiosity‚Äôs sake‚Äîbecause some of these ideas can be solved analytically when you assume the attenuation coefficient, mu, is constant. You can use Fourier analysis, closed-form solutions, or algebraic approaches, like solving systems of linear equations.\n\nJust to point out, for those who are curious, there‚Äôs a paper shown here about using analytic techniques and the Fourier slice theorem for SPECT. These methods are more complicated than CT, but they show that there are options for solving the reconstruction problem with closed-form solutions or advanced mathematics. Next, let‚Äôs look at deterministic PET reconstruction.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 17 - Slide20.txt", "file_path": "Lecture 17\\Texts\\Slide20.txt", "content": "In PET, you see that the data model has a uniform weighting factor multiplied by the concentration at each location. In previous slides, I mentioned how, when you measure along a longer line, you need to take account of the location, but for PET, you measure all coincident detections. Effectively, you sum or integrate all the individual concentrations, called lambda. The important point is that the attenuation factor remains constant‚Äîit doesn‚Äôt depend on the source position, so it can be factored out.\n\nIf the attenuation coefficient, mu, is known, then it becomes an unknown factor in your measurements. But once you know mu‚Äîlike if you assume the patient‚Äôs body is mostly water, making mu equal to the value for water, or if you measure mu with a dedicated transmission scan‚Äîyou can normalize your measurements, and you end up with a clean line integral. With lots of gamma photons, the measurements really do reflect the mean, and you can use deterministic reconstruction, like filtered backprojection.\n\nIf you know mu from a special transmission scan or by assuming it‚Äôs the value for water, then your reconstruction is straightforward. An interesting twist is the time-of-flight approach, which uses the difference in arrival times of photons at the detectors to estimate where the annihilation occurred. By measuring these time differences, you can gain additional information about where the event happened‚Äîclose to one detector or the other. This time-of-flight technique is advanced, and although we don‚Äôt have time to fully cover it here, just know it‚Äôs one way to estimate mu and improve PET reconstructions.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 17 - Slide21.txt", "file_path": "Lecture 17\\Texts\\Slide21.txt", "content": "Let me explain attenuation correction and why it‚Äôs so important for PET-CT imaging. When you want to measure the attenuation coefficient, or mu, for PET, you often use an individual CT scan to map out the mu distribution inside the patient‚Äôs body.\n\nBut here‚Äôs a key point: the photon energy in PET is much different from CT. PET uses gamma photons at 511 keV, while X-ray CT scans use tube voltages that go up to about 140 kVp. The actual X-ray beam covers a spectrum from around 30 keV to 140 keV, but all these energies can be summed up as a single effective energy, usually around 70 keV. So, when you measure attenuation with CT, the coefficient you get is really for that effective energy, about 70 keV.\n\nFor PET attenuation correction, you need the mu value at 511 keV, because those more energetic gamma photons interact differently with tissue. To bridge this gap, you use a piecewise linear mapping. That means for any CT-measured attenuation coefficient at 70 keV, you estimate the corresponding coefficient for PET at 511 keV. This mapping follows a pretty reliable, piecewise linear relationship: tissues that have low attenuation at CT energies also have low attenuation at PET energies, and vice versa.\n\nWhy do we combine PET and CT? The CT scan provides anatomical structure, while the PET scan gives functional information. Having both types of data in a single framework is powerful‚Äîyou get two kinds of complementary information. But beyond that, CT also provides the key information needed for attenuation correction in PET. With the mu values from CT, you can calculate the scaling factor, making it possible to convert PET data into proper line integrals for accurate image reconstruction.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 17 - Slide22.txt", "file_path": "Lecture 17\\Texts\\Slide22.txt", "content": "Once you convert PET data into line integrals, you can remove the attenuation factor, and that will make PET imaging more accurate. Quantitative information can be extracted this way.\n\nLike I said here, the PET result without attenuation correction shows a very dark side. But when you apply the correct attenuation coefficient, you see a much more uniform appearance. The result is more accurate for concentrations‚Äîfor example, in the liver‚Äîmaking the measurements statistically meaningful. That‚Äôs one reason I covered the deterministic reconstruction process first. Now I‚Äôm giving you a high-level idea about statistical reconstruction. So, if you have lambda‚Äîcapital lambda‚Äîas the image to be estimated, here‚Äôs how the statistical approach works. We are given noisy measurements, and we want to estimate the image from those measurements. The measured quantity, called q, is what actually comes from the system, whether you use mechanical collimators or coincidence detection. The measurement is very noisy because the gamma-ray tracer concentration isn‚Äôt perfect, and the decay rate is reasonable, but the noise is not normal anymore. It‚Äôs not the normal noise you see in CT reconstruction. The underlying source distribution can be modeled statistically; the image represents the source distribution, broken down into small pixels, where each pixel acts like a small light bulb.\n\nSo far, we‚Äôve talked about image reconstruction for both SPECT and PET. This assumes your data model is good enough‚Äîmeaning you have a large number of gamma photons captured. In that situation, the measurements reflect the actual statistical mean, so you get a formula‚Äîthe data model.\nThe data model is tricky for single photon emission and different for paired photon emission because the weighting factor changes. For single photon emission, the weighting factor fundamentally depends on the location of the active source element. The weighting factor always depends on location.\nI explained that this location dependence cannot be easily removed. When you try to multiply paired measurements together, cross terms appear and you can't easily get rid of the spatial dependence.\n\nBut for PET photon emission, you‚Äôre actually able to account for the single source distribution, with a probability factor in the formula. If you have multiple radioactive source points along the same line, you just form the line integral by adding them up. The attenuation factor for any radioactive element along that line is the same.\nSo you can factor it out. As long as you know mu‚Äîthe attenuation coefficient‚Äîyou know the attenuation factor, and you get a neat line integral. Then you use filtered back projection, just like in CT. This is a very nice feature for image reconstruction. If you haven‚Äôt got the point yet, review the slides‚ÄîI‚Äôve been busy these past few days and haven‚Äôt uploaded this part yet, but I‚Äôll refine it and upload it soon. It‚Äôs important to review the slides and follow the argument‚Äîthis way, you‚Äôll truly understand the data model for SPECT and PET. That‚Äôs key. Once you know the model, for PET in particular, you can use filtered back projection.\n\nThat‚Äôs why nuclear tomography comes after CT, because you need to have some background knowledge from CT. For SPECT imaging, you have a location-dependent attenuation factor. But if you know mu, and if you measure it‚Äîlike in an individual transmission scan‚Äîyou can still convert it into the attenuation rate for your data.\nYou can turn each measurement into something that forms a line integral‚Äîa sum. Then you‚Äôll have a system of linear equations, which is just like CT‚Äîa system of linear equations to solve. You can use a real high-performance computer for this. Linear algebra gives you the tools to measure and solve these equations, and we know how to do that both numerically and analytically.\n\nBut what happens if you don‚Äôt have enough flux? That‚Äôs a big problem. I mentioned it the other day: X-ray flux is high, but for nuclear imaging, it‚Äôs much lower. That‚Äôs because you‚Äôre introducing radioisotopes. The number of emitted gamma-ray photons isn‚Äôt ideal, because you don‚Äôt want to introduce too much radioactive material into the human body. You need to use the minimum amount for safety. On the other hand, radioisotopes are incredibly sensitive. Any signal you detect comes from the radioisotopes, so you get very high sensitivity.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 17 - Slide23.txt", "file_path": "Lecture 17\\Texts\\Slide23.txt", "content": "We begin with the idea that each gamma ray measurement is modeled independently. The underlying framework is a plasma model. Consider an image made up of pixels. Each pixel, along with its neighbors, contributes independently‚Äîmeaning the measurement at one pixel does not directly depend on another. To capture this mathematically, we partition time into small intervals‚Äîfor example, 10 milliseconds each. One interval, the next interval, and another further away all behave independently. This independence is crucial because it allows us to express the probability of the entire measurement as the product of many small probabilities. Each detector, each pixel, each time interval contributes its own probability, and multiplying them together gives the overall likelihood.\n\nThis setup naturally leads to probabilistic modeling. Bayes‚Äô rule, which many of us first learned in high school, becomes central here. Imagine a patient comes in, and we perform data acquisition using a camera or a PET scanner. The raw measurements are then organized into sinograms in 2D or 3D. To understand error and uncertainty, we rely on the power of mathematical notation, which is both compact and expressive. In fact, the measured counts can be modeled as random variables following a Poisson distribution. The problem can then be phrased as: given the measured data, what is the most likely underlying image, denoted by capital lambda (Œõ)?\nThis probability depends on several factors. First, assume the true image is known, along with the source distribution and any background activity. If we also know the imaging geometry, then we can predict what each detector should record. For example, a detector may be expected to receive around 3,000 gamma photons during a five-minute scan. The difficulty is that in reality, the image is not known‚Äîwe only have the measured data. That‚Äôs why we must also consider prior knowledge about the image distribution. For younger patients without cancer, we may expect little to no radioactive uptake in major organs. For older patients, there may be a higher probability of uptake, for instance, in the colon if there is a tumor. These expectations form a prior distribution on the image.\n\nHowever, in many cases, especially when data is scarce, we may not want to assume anything specific. In that case, we take the prior distribution to be uniform, meaning every image is equally likely. This constant prior does not affect the statistical estimation, since it cancels out in maximization.\nFrom here, two key concepts arise: maximum a posteriori (MAP) estimation and maximum likelihood (ML) estimation. MAP seeks the image that maximizes the posterior probability, combining both the likelihood of the measured data given the image and the prior probability of the image itself. If we assume the prior is uniform, MAP reduces to maximum likelihood estimation. In ML, we search for the image that makes the measured data most probable.\n\nThe process is iterative: we try candidate images, compute the probability that they would have produced the measured data, and adjust until we find one with the highest probability. That image is then reported as our reconstruction. In summary, if the prior distribution is non-informative, MAP and ML give the same result. Otherwise, MAP incorporates prior knowledge to guide the estimation.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 17 - Slide24.txt", "file_path": "Lecture 17\\Texts\\Slide24.txt", "content": "The IMIL approach is often used because it naturally incorporates statistical knowledge. To explain this more simply, let us recall the idea of CT reconstruction. Imagine you have multiple views of an object. One view looks like this, another view looks different. Now, you try to reconstruct the underlying image. Suppose the underlying image is a star. When you trace the star vertically, the profile does not look right. When you trace it horizontally, again, it does not match. Then, you try using a different model‚Äîsay, an ellipse. If you trace the ellipse horizontally, you obtain a profile that matches the measured data more closely. This suggests that the elliptical model fits best. In this case, you would report the ellipse as the reconstructed image.\n\nThis is essentially what maximum likelihood estimation does: you keep trying different underlying images, generate simulated measurements, and compare them to the actual data. When the simulated measurements maximize the probability of matching the real ones, that candidate is taken as the true image. This approach is fundamentally different from deterministic CT reconstruction methods. Instead of assuming the data is exact, it operates within a statistical framework.\nIn this framework, measurements are modeled with a Poisson distribution. Because of the independence of measurements, the overall likelihood can be expressed as a product of individual probabilities. By taking the natural logarithm, this product is converted into a summation, making the formulation simpler and more practical to compute. The key point is that the statistical model accounts for the Poisson noise inherent in the data, which deterministic methods ignore.\n\nTo see the advantage, consider a phantom experiment. If you introduce radioactive tracers into the phantom and reconstruct with filtered back projection, you are assuming no noise‚Äîthat every measurement is an exact line integral. But in reality, measurements always have statistical fluctuations. This mismatch leads to streaking artifacts and random noise in the reconstructed images. By contrast, maximum likelihood reconstruction explains the fluctuations more accurately within the statistical model. As a result, the reconstructed images show fewer artifacts and improved quality, as demonstrated in the phantom example.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 17 - Slide25.txt", "file_path": "Lecture 17\\Texts\\Slide25.txt", "content": "This example shows patient reconstruction results. When using filtered back projection, the images clearly display streak artifacts and significant noise. These issues appear across all views‚Äîcoronal, transaxial, and sagittal. Overall, the images produced by filtered back projection are noisy and contain strong artifacts that reduce clarity and diagnostic value.\n\nIn contrast, maximum likelihood iterative reconstruction directly incorporates statistical knowledge into the reconstruction process. This approach produces images that are both visually and quantitatively improved. Noise is greatly reduced, artifacts are minimized, and the resulting images are more accurate and reliable for clinical use.\nThe side-by-side comparison highlights the advantage of maximum likelihood iterative reconstruction over traditional filtered back projection. In real patient data, the iterative approach delivers superior image quality, both in appearance and in diagnostic accuracy.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 17 - Slide26.txt", "file_path": "Lecture 17\\Texts\\Slide26.txt", "content": "Let‚Äôs talk again about PET-CT, which we‚Äôve mentioned several times. First, the CT image provides structural and anatomical information at a high spatial resolution. It shows the fine details and allows us to see different tissues clearly. On the PET image, you see the distribution of the radiotracer, and in this case, there is a lot of accumulation in one organ. This means the vasculature there is rich, actively taking in nutrients and glucose, which is a sign of rapid tumor growth. The radiotracer‚Äîoften a form of glucose‚Äîis picked up as the tumor cells consume more nutrients than normal tissues. All these radiotracer molecules gather in regions of heightened metabolism, typically where tumors are present.\n\nWhen you superimpose the PET image over the CT image, you get excellent anatomical context. CT provides what's called the linear attenuation coefficient, and this is used to correct for tissue attenuation in PET. If you use only a uniform attenuation correction, the lungs‚Äîwhich actually have very low tissue density‚Äîcan be misrepresented. Uniform correction assumes the same background for all regions, so in the lung, this leads to overestimation. In reality, the lungs attenuate the PET signal much less. If you apply uniform correction, you falsely report higher activity in the lungs than is accurate, simply because the tissue is less dense.\nCT solves this by providing individualized, tomographic detail for each region. The CT number for the lung is low, and when mapped to attenuation at 511 keV‚Äîthe PET photon energy‚Äîyou get a precise, much lower correction for lung tissue. This ensures the activity measurements from PET-CT are more accurate.\n\nLooking at the slide, you see areas where the tumor is intensely bright due to high tracer uptake‚Äîthese signals are not good news because they reflect significant tumor activity. But after treatment, a follow-up PET-CT scan can show decreased brightness, meaning reduced metabolic activity and, therefore, an effective response to treatment.\nFinally, while CT is very good for showing anatomical detail, such as bone and tissue interfaces, it is not sensitive to soft tissue contrast. CT excels at showing dense structures, like bone, but metabolic changes and soft tissue differences are much better visualized by PET. The combination of PET and CT gives you both anatomical layout and functional information, making it a powerful tool in medical imaging.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 17 - Slide27.txt", "file_path": "Lecture 17\\Texts\\Slide27.txt", "content": "Now let‚Äôs look at PET-MRI. MRI provides excellent soft tissue contrast, which is clear here‚Äîstructures within the organs are visible in great detail. However, MRI is not ideal for imaging bone structures, and some areas, like those in the lungs, can be less distinct compared to CT. We'll be diving deeper into MRI starting from the next lecture, and I‚Äôve already prepared a recorded session because there may be time constraints. I encourage you all to preview the MRI material; it‚Äôs a different modality altogether. MRI takes longer to acquire data, and its spatial resolution depends on the specific imaging agent and scan parameters, especially for maximum coverage of larger anatomical areas.\n\nBy combining PET with MRI‚Äîas shown here‚Äîyou get unique diagnostic information. PET provides molecular and metabolic data, while MRI supplies detailed anatomical views, especially for the brain and soft tissue tumors. This fusion enables more precise diagnosis and assessment of diseases like cancer and neurological disorders. For brain imaging and soft tissue tumors, PET-MRI truly offers cutting-edge capability, making it a highly valuable technology in modern medical imaging.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 17 - Slide28.txt", "file_path": "Lecture 17\\Texts\\Slide28.txt", "content": "Let‚Äôs move into the fourth part, where we discuss system architecture and scatter correction in SPECT and PET imaging. \n\nThis topic is pretty straightforward. We focus on how these imaging systems are built and the basic principles behind scatter correction, which is a major technical consideration for accurate image reconstruction.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 17 - Slide29.txt", "file_path": "Lecture 17\\Texts\\Slide29.txt", "content": "On this slide, you see a great picture of a SPECT scanner. The SPECT camera system is held together by a sturdy structure, and it‚Äôs designed to rotate around the patient. This rotation allows the scanner to collect data from multiple orientations, which is conceptually similar to how CT scanners work. \n\nBy rotating the detector and collimators around the patient, we get signals from different angles, enabling a full three-dimensional reconstruction. The system also allows for standard longitudinal scanning, which is what you typically see in practical clinical use.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 30, "slide_filename": "Slide30.txt", "slide_annotation": "Lecture 17 - Slide30.txt", "file_path": "Lecture 17\\Texts\\Slide30.txt", "content": "Now, building on the idea used in PET-CT, we can talk about SPECT-CT systems. In this setup, there can be two or even three SPECT cameras working at the same time to gather more data. If you use only one planar gamma detector, a lot of photons‚Äîespecially those coming horizontally and downward‚Äîwill be wasted, and their information won‚Äôt be captured. \n\nBut with triple detectors, you can use gamma photons much more efficiently. The CT part of the system gives you precise anatomical information and helps provide accurate attenuation correction for the SPECT data. This means better quantification, improved image quality, and superior diagnostic capability through combined anatomical and functional information.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 31, "slide_filename": "Slide31.txt", "slide_annotation": "Lecture 17 - Slide31.txt", "file_path": "Lecture 17\\Texts\\Slide31.txt", "content": "In pilot imaging, you work with a detector ring. This setup can be used for both human studies and small-animal imaging. Small animals are especially important in pharmaceutical research, since drug companies rely on them to test potential new drugs. The development of a drug typically passes through several phases: starting with cell cultures, then moving to small-animal studies, followed by preclinical trials, clinical trials, and eventually, market approval. Out of hundreds of drug candidates tested, only a very small fraction ever reach the market. This long, costly process is one reason why drug development is so expensive.\n\nThe pilot imaging ring itself consists of gamma detectors and the associated electronic circuitry. These detectors not only capture signals but also record timing information. When multiple gamma photons are detected within the same time window, the system identifies them as coming from the same event and reports a coincidence along a line of response. In this way, the line of response corresponds to the radio-tracer distribution along that line, which is essentially a line integral. The attenuation coefficient in this formulation is treated as independent and can be factored out.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 32, "slide_filename": "Slide32.txt", "slide_annotation": "Lecture 17 - Slide32.txt", "file_path": "Lecture 17\\Texts\\Slide32.txt", "content": "This is a big picture overview. If you look at the components in detail, multiple figures highlight the power and structure of each unit. You start with a scintillation crystal at the front of each module. The scintillation crystal's job is to convert incoming gamma photons into visible light. This visible light is then captured by a photomultiplier tube, or PMT. The crystal and PMT work together‚Äîwhen the scintillation crystal creates visible light, the photomultiplier tube multiplies and amplifies that light signal, converting it into an electrical current. After the signal passes through the tube, it goes on to electronic circuits that process the electrical impulse.\n\nDepending on the amplitude of that impulse, you can determine the energy of the interacting gamma photon. One unit like this‚Äîconsisting of an array of crystals and a set of PMTs‚Äîis called a \"block.\" Several blocks combined form a \"bucket,\" which can include hundreds of crystals. Multiple buckets are arranged together to create the full detector ring, which collects all the imaging data at once. There are additional engineering details in how these elements are assembled and how the signals are managed, but at its core, this system is designed to efficiently detect and process gamma events for reliable image reconstruction.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 33, "slide_filename": "Slide33.txt", "slide_annotation": "Lecture 17 - Slide33.txt", "file_path": "Lecture 17\\Texts\\Slide33.txt", "content": "Now let's talk about the desirable properties of crystal materials used in PET scanners. I'll go through these quickly. First, the crystal should have a high density, which gives it a large effective cross-section for interacting with gamma rays. Next, you want a large effective atomic number‚Äîthis increases gamma-ray detection efficiency by boosting photoelectric absorption. The decay time should be short; you need the crystal to release its light energy very quickly. If the decay time is too long, then each photon event can overlap in time, causing loss in timing resolution and poor image quality.\n\nA high light yield is also important‚Äîthe crystal must convert as much gamma energy as possible into visible light, maximizing signal for efficient detection. Ideally, the wavelength of the emitted light should be near 400 nanometers, since photomultiplier tubes are most sensitive in this range, making the system more efficient. Additionally, the refractive index should be close to 1.5 to ensure optimal optical coupling between the crystal and the photomultiplier tube.\nThe crystal must also be non-hygroscopic, meaning it should not absorb moisture from the air. If it does, it can degrade and turn into powder, which would make it unusable for detector fabrication. All these properties are critical for reliable, high-performance medical imaging with PET scanners.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 34, "slide_filename": "Slide34.txt", "slide_annotation": "Lecture 17 - Slide34.txt", "file_path": "Lecture 17\\Texts\\Slide34.txt", "content": "All of these are considered desirable properties for PET detector crystals. The table lists the main candidate materials used in PET scanners. You don't need to memorize all the details or specific values here‚Äîin clinical practice, it‚Äôs more important to understand the general differences and why a particular material might be chosen for a given system. \n\nWhat matters is knowing that these materials differ in their density, decay time, emission intensity, atomic number, refractive index, and whether they are hygroscopic. Each property affects how well the crystal works for detecting gamma rays and producing high-quality PET images.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 35, "slide_filename": "Slide35.txt", "slide_annotation": "Lecture 17 - Slide35.txt", "file_path": "Lecture 17\\Texts\\Slide35.txt", "content": "There aren't many engineering details here, but this is mainly for those working in detector development. The key components for PET detectors start with the scintillation crystals, which are organized into blocks. Each block is coupled with multiple photomultiplier tubes in the background‚Äîtypically four per block. These blocks are then assembled into a complete detector ring that surrounds the patient. As the PET system operates, the crystals detect incoming gamma photons, and the photomultiplier tubes convert the light produced by these interactions into electrical signals.\n\nThe system‚Äôs processing unit precisely monitors the electrical signals to determine exactly when and where events have occurred, and at what energy. The main gamma photon from a PET scan carries 511 keV energy. But if there is multiple scattering, such as Compton scattering inside the body, energy is lost and can complicate the signal. The electronics and coincidence processing unit integrate all detected events, and finally, this data is sent to the computer system.\nAfter data acquisition, image reconstruction begins‚Äîoften with methods such as filtered back projection‚Äîto produce cross-sectional images. All of these steps come together to make PET imaging possible, from crystal blocks and photomultiplier tubes to data integration and final image creation.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 36, "slide_filename": "Slide36.txt", "slide_annotation": "Lecture 17 - Slide36.txt", "file_path": "Lecture 17\\Texts\\Slide36.txt", "content": "The technical advancement in PET imaging has been driven largely by improvements in detector technology. Over the years, we‚Äôve seen a progression from NaI detectors to BGO, and now to LSO, with each milestone year bringing significant gains in image quality. \n\nThese different generations of detectors reflect long-term innovation in materials, engineering, and system design. As you can see, each new detector technology has resulted in clearer, higher-resolution images. This slide is just for your information; I won‚Äôt spend too much time on it, but it‚Äôs important to recognize how advancements in detector design have directly impacted the quality of PET images throughout the history of the field.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 37, "slide_filename": "Slide37.txt", "slide_annotation": "Lecture 17 - Slide37.txt", "file_path": "Lecture 17\\Texts\\Slide37.txt", "content": "For a system to work at its best, noise, scatter, and random events must be estimated and removed. This slide is a nice summary. A true event means you have a single annihilation, then two photons are emitted and detected by opposite detectors, and captured by the circuit. That's what you really want‚Äîthese true coincidences are the signals that contribute to accurate image reconstruction.\n\nScatter events have different mechanisms. Here‚Äôs an example: a photon is emitted from an event, but gets scattered away before it reaches the detector. The system may report a coincidence along this scattered path, but it‚Äôs not the true line of response. This isn‚Äôt the real signal we need, so the scattered photon event should be removed‚Äîjust like in CT, where scattering also occurs.\n\nRandom events are different. In this case, two gamma photons are generated from different events in the body, and one photon is detected in the same time window as another photon from a separate event. The system reports a coincidence, but along a line that doesn‚Äôt correspond to either true event‚Äîthis is not accurate, and should be avoided if possible.\n\nIn 3D PET imaging, you can see that scattered and random events become more frequent, mainly because sensitivity increases due to the wider aperture and more possibilities for 3D data acquisition. So, it‚Äôs important to maximize sensitivity while minimizing scatter and random events to maintain image quality.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 38, "slide_filename": "Slide38.txt", "slide_annotation": "Lecture 17 - Slide38.txt", "file_path": "Lecture 17\\Texts\\Slide38.txt", "content": "How can you estimate the scattered background? This is actually simple. You set the energy window around the main photopeak. That's the main window right at the peak, and you collect counts there. Then, you also set windows on the left and right‚Äîthese are the low and high-energy side windows. The idea is that you know the primary gamma photon should be at 511 keV, so its energy falls in the main window.\n\nIf there are scattered photons, their energy will be lower than 511 keV, so you‚Äôll see them in the lower energy window. If there are random events or noise, these could appear in the higher energy window. By collecting data in both the lower and higher side windows, you have a way to estimate the background scattering.\nYou then assign weights to the counts in the lower and higher windows‚Äîthis weighted combination gives you an estimate of the scattered background in the main window. With that estimate, you subtract the scattered contribution from the main window, giving you the corrected signal. This process lets you measure and remove the scatter components, so your final image is much cleaner and more accurate.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 39, "slide_filename": "Slide39.txt", "slide_annotation": "Lecture 17 - Slide39.txt", "file_path": "Lecture 17\\Texts\\Slide39.txt", "content": "So this shows you the result. This is the real measurement, and you can see the scattered background as it appears in the data. When you remove the scattered background, you get these corrected measurements, or corrected reconstructions. \n\nThis demonstrates how the result changes by addressing the scattering problem. Scatter correction is a key point in improving measurement accuracy and image quality.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 40, "slide_filename": "Slide40.txt", "slide_annotation": "Lecture 17 - Slide40.txt", "file_path": "Lecture 17\\Texts\\Slide40.txt", "content": "And for random correction, this is a very clever idea. If you have random events being recorded, they introduce noise into your data. For usual data processing, these random events really complicate things. But we can address this purposely: for each part of the detector, we introduce a delay.\n\nIf it‚Äôs a random coincidence, then the probability of the random event with this result delay is the same as without the delay. This is the key statement‚Äîthis is the important idea. By introducing a delay in the coincidence detection, the probability line for incident and delayed results is the same, because the random coincidences depend on the average gamma-ray activity, which is assumed to be more or less constant over time. The delay itself doesn't change that rate.\n\nYou can then use the measured random events from the delayed window to directly estimate and subtract out the random event contribution from your true data. This approach gives you higher-quality, more quantitative results. That‚Äôs the whole idea behind random correction", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 41, "slide_filename": "Slide41.txt", "slide_annotation": "Lecture 17 - Slide41.txt", "file_path": "Lecture 17\\Texts\\Slide41.txt", "content": "There are multiple high-order effects in PET imaging, and this is what I‚Äôm showing here with these figures. Ideally, you want to detect coincidences‚Äîby definition, in a given time window, you should only have two gamma-ray photons captured. If your time window is short enough, you get a well-controlled count rate, typically up to 100,000 per second, which is the principle behind coincidence detection. However, sometimes within a short time interval, you might have three gamma-ray photons reported simultaneously. How does this happen? For example, the first annihilation event is detected near detector two, so its gamma photon reaches that detector and is recorded. \n\nWithin that same time window, another event occurs closer to detector thirteen, and its sister photon travels a longer path. Before the photon from the second event reaches detector five, the third photon from the first event might reach detector ten. So, within the same interval, three photons can be detected as coincidences.\nWhenever you get triple coincidences‚Äîthree detected photons in the same resolving window‚Äîthey introduce confusion, and you simply discard that data. The system is designed to expect only pairs of coincident photons. If you see this higher-order event, you just throw it away, focusing only on true doubles.\n\nBut confusing situations remain: it‚Äôs hard to know which events should be assigned to which lines of response. That's where the correction factor formula comes in. For every true coincidence measurement, you also need to account for the data lost due to triple or higher-order coincidences. The formula shown here calculates the correction factor‚Äîcombining both the estimated loss from discarded high-order events and the true measurement‚Äîto help restore your measured data to the real level. Sometimes, higher-order events may be very rare, but if your time window is wide enough, you might encounter even more photons simultaneously, which complicates things further. In practice, these cases are always discarded, and the formula allows you to estimate the amount of lost data. You then use this correction factor to scale your results back to the true count, improving accuracy in quantitative PET measurements.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 42, "slide_filename": "Slide42.txt", "slide_annotation": "Lecture 17 - Slide42.txt", "file_path": "Lecture 17\\Texts\\Slide42.txt", "content": "So, graphically, you can see this process clearly. The total counts‚Äîthe overall measured signal or image‚Äîactually contain two main components. These two components are scatter background and random counts, along with a variety of other effects and oddities. You can estimate the random count by using a delayed window, and triple coincidences can be corrected as well. There are many correction methods, but the most important thing to understand from this lecture is to have a rough idea of how these corrections work.\n\nAll these corrections‚Äîincluding attenuation correction, scatter correction, random correction, and corrections for dead time‚Äîare essential to refine the measured signal and make sure your results reflect the true activity as accurately as possible.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 43, "slide_filename": "Slide43.txt", "slide_annotation": "Lecture 17 - Slide43.txt", "file_path": "Lecture 17\\Texts\\Slide43.txt", "content": "The goal is to make your line integral data accurately reflect the true line integrals through the body. This allows you to reconstruct images that truly represent the concentration of radiotracer in the organ of interest. You can do this for each organ and at each time point during the PET imaging session. By plotting these time curves, you can assess perfusion in different regions and extract physiologically relevant parameters. \n\nThis process gives you more diagnostic information and helps to interpret the results accurately. These steps‚Äîattenuation correction, scatter and random correction, and dead time correction‚Äîare all essential for quantitative PET imaging. They work together to ensure that your final images and time-activity curves are both accurate and clinically meaningful, which is the central principle behind quantitative PET analysis.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 44, "slide_filename": "Slide44.txt", "slide_annotation": "Lecture 17 - Slide44.txt", "file_path": "Lecture 17\\Texts\\Slide44.txt", "content": "I want to summarize PET and SPECT imaging with this slide. The key point is that the data models are different, and the underlying technology relies on gamma rays, but at very different energies. SPECT uses lower-energy gamma rays, typically around 140 keV. Because of this lower energy, you can use a mechanical collimator‚Äîhigh-density metals effectively block unwanted photons, shaping the beam. But in PET, the gamma ray energy is much higher, at 511 keV. Mechanical collimation simply doesn‚Äôt work because the energy is too great for physical collimators, so PET uses electronic collimation instead.\n\nWith mechanical collimation, a large portion of useful gamma-ray photons is rejected, reducing sensitivity. By contrast, electronic collimation in PET does not block any useful gamma photons. This means PET sensitivity is fundamentally higher than SPECT sensitivity.\n\nAnother key feature is time information. In PET, two photons resulting from an annihilation are emitted simultaneously. The system records the precise timing, making time-resolved imaging possible and increasing sensitivity. In SPECT, we don‚Äôt know precisely when the radiotracer emits its gamma photons inside the body, so there‚Äôs no available time scale‚Äîjust as in standard X-ray imaging, where we can‚Äôt control the emission timing for each photon. Even if you try to time the arrival of X-ray photons at a detector, you don‚Äôt have a proper time reference. With PET, the emission process gives you this timing, allowing sensitive and advanced time-resolved imaging. This is a crucial, elegant concept behind PET technology.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 45, "slide_filename": "Slide45.txt", "slide_annotation": "Lecture 17 - Slide45.txt", "file_path": "Lecture 17\\Texts\\Slide45.txt", "content": "The leading researchers in this field have developed a very large PET detector known as a whole body PET. By making the detector long enough to cover the entire body, the sensitivity is dramatically increased, allowing for even greater sensitivity than conventional PET systems. \n\nWith this approach, you can obtain a much more sensitive and comprehensive view of the entire body in a single scan. This technology offers the opportunity to capture detailed, global images of tracer distribution, potentially improving diagnostic capabilities and expanding what can be studied. It's a very interesting and innovative idea that's advancing the field of molecular imaging.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 46, "slide_filename": "Slide46.txt", "slide_annotation": "Lecture 17 - Slide46.txt", "file_path": "Lecture 17\\Texts\\Slide46.txt", "content": "When you perform whole-body PET and also whole-body CT, you need to consider the practical challenges and differences. Whole-body CT uses a significant amount of radiation, since it scans the entire patient with X-rays. In contrast, with whole-body PET, as shown in the figure, there is no space to easily position the X-ray components completely around the patient in the same way as traditional CT.\n\nHowever, researchers are actively thinking about creative solutions, and we actually encourage students to work on the possibility of achieving full-body CT simultaneously with PET, even when scanners are large enough to scan the whole body at once. This might require using new, out-of-the-box ideas‚Äîpossibly even involving MRI technology‚Äîinstead of conventional techniques. These are exciting areas for innovation and research, and we can discuss some of those cutting-edge possibilities later on in the course.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 47, "slide_filename": "Slide47.txt", "slide_annotation": "Lecture 17 - Slide47.txt", "file_path": "Lecture 17\\Texts\\Slide47.txt", "content": "Another point I want to highlight, as indicated by the green button here, involves using multiple pinholes with a scintillation camera. Traditionally, you would use a single pinhole to capture one image at a time. But in this pioneering work, as shown in the paper, they introduced the concept of using several pinholes‚Äîseven in this case. Each pinhole forms a distinct image, producing multiple views of the target simultaneously; for example, you get a double-cone image from one pinhole‚Äôs geometry, and with seven pinholes, you get seven images at once.\n\nThis approach was used to image the heart, allowing for seven simultaneous views, which improved accuracy by acquiring several perspectives at a single time point. Although the reconstructions from those seven views weren‚Äôt fully impressive with the technology available back then, the idea remains promising. There is real potential: if you collect seven views from one direction and another seven from an orthogonal direction, then with modern signal processing, you may achieve image reconstructions far superior to those early results. This opens up interesting possibilities for future multi-view tomography and advanced image reconstruction techniques.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 48, "slide_filename": "Slide48.txt", "slide_annotation": "Lecture 17 - Slide48.txt", "file_path": "Lecture 17\\Texts\\Slide48.txt", "content": "Anyway, so this is just for your information. The slide summarizes key concepts for SPECT and PET imaging: statistical distributions, how data is modeled with single and paired emissions, various reconstruction methods including attenuation compensation, deterministic, and statistical reconstruction, as well as the architecture of SPECT and PET scanners and scatter correction. \n\nFinally, it mentions the fully 3D approaches and highlights the importance of the conference and community in advancing this field.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 49, "slide_filename": "Slide49.txt", "slide_annotation": "Lecture 17 - Slide49.txt", "file_path": "Lecture 17\\Texts\\Slide49.txt", "content": "And we had a fully 3D meeting in China. This conference is dedicated to advances in fully three-dimensional image reconstruction in radiology and nuclear medicine, bringing together experts, researchers, and students from around the world to share the latest developments and ideas. \n\nIt‚Äôs a valuable opportunity to connect with the community and stay current in this rapidly evolving field.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 50, "slide_filename": "Slide50.txt", "slide_annotation": "Lecture 17 - Slide50.txt", "file_path": "Lecture 17\\Texts\\Slide50.txt", "content": "So this is the website for the Fully3D 2017 meeting, which was held in Xi‚Äôan, China from June 18 to 23, 2017. If you are interested, you can take a look for more information about the conference, its location, and the history of Xi‚Äôan as China‚Äôs oldest ancient capital and the starting point of the Silk Road. It‚Äôs also home to the famous Terracotta Army, known as the eighth wonder of the world.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 51, "slide_filename": "Slide51.txt", "slide_annotation": "Lecture 17 - Slide51.txt", "file_path": "Lecture 17\\Texts\\Slide51.txt", "content": "And in the center of the city, there is this Bell Tower. Every morning, the bell would be rung so life would be synchronized. This is the heart of the city. We enjoyed very much the tour during the conference.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 52, "slide_filename": "Slide52.txt", "slide_annotation": "Lecture 17 - Slide52.txt", "file_path": "Lecture 17\\Texts\\Slide52.txt", "content": "So the interesting thing is that I designed a logo specifically for the Fully 3D meeting. The logo is modeled after the iconic Bell Tower in the city, making it a distinctive symbol that connects the event to the cultural heart of Xi‚Äôan.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 53, "slide_filename": "Slide53.txt", "slide_annotation": "Lecture 17 - Slide53.txt", "file_path": "Lecture 17\\Texts\\Slide53.txt", "content": "Why does a logo look like this? The reason is clear: the meeting covers the full spectrum of CT, SPECT, and PET reconstructions. The logo visually represents these modalities together. For CT, you see the cone-beam geometry at the top, which is standard in modern CT scanning. \n\nFor SPECT, there‚Äôs a model with a collimator, a layer of crystals, and photomultiplier tubes‚Äîthese components are essential for forming SPECT images. At the base is the PET detector ring, which is fundamental for PET. By including these technical elements, the logo symbolizes the integration of all three imaging modalities, reflecting the focus and breadth of the Fully 3D meeting.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 54, "slide_filename": "Slide54.txt", "slide_annotation": "Lecture 17 - Slide54.txt", "file_path": "Lecture 17\\Texts\\Slide54.txt", "content": "Also, you have this movie of the 3D logo that brings together all the elements for Fully3D‚Äô17 and represents the spirit of the meeting.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 17", "slide_number": 55, "slide_filename": "Slide55.txt", "slide_annotation": "Lecture 17 - Slide55.txt", "file_path": "Lecture 17\\Texts\\Slide55.txt", "content": "For today‚Äôs homework, I have listed five problems. You are welcome to do more if you wish.\n\nThe first problem is about isosensitive imaging, which I briefly introduced earlier. It involves taking two views and compensating in a specific way. The key idea is that if you assume a simple binary case with only one source distribution, you can multiply the views to achieve compensation. However, remember that this is not a universal solution‚Äîit applies only under limited assumptions.\n\nThe second question asks how many total counts are necessary to achieve 1 percent uniformity in a SPECT image, given a 128 by 128 data matrix. This problem is fairly straightforward. You can solve it using the Poisson distribution along with basic sensitivity concepts. The answer should follow directly from these principles.\nThe third exercise focuses on reducing statistical noise in an image. Consider what happens if you double the total imaging time, and separately, what happens if you double the mass of tracer injected. In each case, determine by what factor the noise in the image would be reduced.\nThe fourth problem asks you to compute the energy of gamma photons produced from positron emission. The correct result should be 511 keV. To calculate this, apply the formula E equal m c square If needed, look up the rest mass of the electron and positron.\n\nFinally, the fifth question addresses artificially high levels of radioactivity observed in PET lung images. You are asked to suggest one possible mechanism by which this effect could occur. These exercises are designed to help you practice applying statistical principles, imaging physics, and basic quantitative reasoning to nuclear medicine problems.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 18 - Slide1.txt", "file_path": "Lecture 18\\Texts\\Slide1.txt", "content": "Today, we‚Äôre going to begin a new chapter on the physical principles of Magnetic Resonance Imaging, or MRI. This is Lecture 18 in our series.\n\nMRI is one of the most powerful tools in modern biomedical imaging, and in this session, we‚Äôll look at the physics closely that makes MRI work. We‚Äôll build up from the physical foundations including Maxwell‚Äôs equations, through the concepts of magnetization and precession, all the way to how signals are generated and detected. Along the way, we‚Äôll also discuss key parameters, such as proton density, T1 and T2, and how they enable image contrasts.\nOkay, let‚Äôs get started with MRI physics.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 18 - Slide2.txt", "file_path": "Lecture 18\\Texts\\Slide2.txt", "content": "So, we are still on schedule. If you look at the course outline, we have already covered topics such as Fourier series, signal processing, CT reconstruction, and nuclear imaging. \n\nNow, we are moving into MRI materials. Today‚Äôs focus is on MRI physics, which is the foundation for understanding how MRI systems generate and detect signals. This will prepare us for the next lectures, where we will go deeper into MRI techniques, systems, and applications.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 18 - Slide3.txt", "file_path": "Lecture 18\\Texts\\Slide3.txt", "content": "Let me start by giving you both a preview and a review. The preview is like a big picture of what MRI physics is about. I will provide you with some basic knowledge and introductory ideas that will help you understand the more detailed content later. \n\nSo first, let‚Äôs take a look at this overall framework. We will begin with the physical foundation, then move on to signal generation, and finally discuss signal decay. This roadmap will guide us through today‚Äôs lecture.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 18 - Slide4.txt", "file_path": "Lecture 18\\Texts\\Slide4.txt", "content": "First, let‚Äôs step back and look at the big picture. Up to this point, we have studied CT and nuclear imaging. These are already used together in hybrid imaging. CT provides us with detailed structural and anatomical information, while nuclear imaging methods, such as PET and SPECT, provide functional information by tracking radioactive tracers that participate in biochemical reactions within the body.\n\nBecause these two types of information are highly complementary, combining them makes the results much more powerful. That is why PET/CT scanners have become standard in many hospitals, especially in radiation oncology. More recently, companies like Siemens and GE have developed PET/MRI systems. However, CT and MRI have not yet been commercially combined, although research is moving in that direction.\n\nThe long-term vision is to bring CT, MRI, and nuclear imaging together into one unified system, giving us structural, functional, and biochemical information at the same time. This is the major trend in imaging‚Äîmoving from individual modalities to hybrid and, eventually, fully integrated scanners.\nNow, with that context, let us move to our next imaging modality, MRI, which is unique in that it can provide both anatomical and functional information in a single technique.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 18 - Slide5.txt", "file_path": "Lecture 18\\Texts\\Slide5.txt", "content": "Now, let‚Äôs go through some important milestones. MRI is unique because it can provide two types of information. On one hand, it gives us anatomical information, very much like CT, by showing the structure of soft tissues. On the other hand, it can measure blood oxygenation levels, which adds functional information. That combination is what makes MRI so powerful.\nThe story begins in 1946, when Bloch at Stanford and Purcell at Harvard discovered nuclear magnetic resonance. For this work, they received the Nobel Prize in Physics in 1952. At that time, NMR was mainly used to measure signals from a whole sample.\nThen in 1973, Lauterbur at Stony Brook University introduced the idea of magnetic resonance imaging. The key innovation was to use gradient magnetic fields, which made it possible to turn NMR from a bulk measurement into a tomographic imaging technique. Now we could form images with pixels and voxels instead of just an overall signal. This breakthrough eventually earned Lauterbur and colleagues the Nobel Prize in Medicine in 2003.\nBy the late 1970s, the first human MRI images were produced. In the early 1980s, commercial MRI systems became available. Then, in 1993, functional MRI was developed, allowing us to look at brain activity through blood oxygenation changes.\nAnd now, we are in the era of large-scale brain initiatives, where MRI continues to be at the center of neuroscience and clinical imaging.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 18 - Slide6.txt", "file_path": "Lecture 18\\Texts\\Slide6.txt", "content": "Here is the very general idea behind how MRI works. At the center of the system, we have a large superconducting magnet. This magnet creates a very strong, stable magnetic field. Around it, we place gradient coils, which allow us to vary the magnetic field in different directions and make tomographic, or slice-by-slice, imaging possible.\n\nIn addition, we use radiofrequency coils, or RF coils. These coils send radio waves to disturb the alignment of the protons in the body. When the protons are tipped away from alignment, they begin to precess, and in doing so, they emit their own radiofrequency signals. The RF coils then detect these signals.\n\nWith the help of signal processing and mathematical reconstruction, we turn these detected signals into detailed images. Right now, it may sound abstract, but as we go deeper into the lecture, we will carefully unfold each of these components. For now, just keep in mind the big picture: a strong magnet, gradient coils for spatial encoding, RF coils for excitation and detection, and signal reconstruction to form the final images.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 18 - Slide7.txt", "file_path": "Lecture 18\\Texts\\Slide7.txt", "content": "To get a rough idea of how MRI works, let‚Äôs look at it step by step. \n\nMagnetic resonance imaging‚ÄîMRI for short‚Äîcan be modeled in the following way. First, consider this conducting loop of wire. The magnetic dipole moment is defined as the current going through the wire, times the cross-sectional area enclosed by the wire, times a unit vector perpendicular to that surface area. So the magnetic dipole moment points in the perpendicular direction, like this.\n\nNow, a proton is believed to spin on its axis at the subatomic level. Because it‚Äôs spinning, the magnetic dipole moment can be thought of as the sum of many tiny circulating currents. Whenever you take a dipole moment and place it in an external magnetic field, the dipole experiences a torque. That torque is given by the equation: tau equals mu cross B. In words: the torque equals the magnetic dipole moment crossed with the magnetic field.\nWhat this means is that the dipole moment will tend to align with the magnetic field. Imagine here is the B-field, and here is the dipole moment. When you take the cross product, you get a torque out of the board. That torque causes the dipole to rotate in this direction. The dipole will try to line up with the external B-field, but it won‚Äôt line up completely. The angular momentum of the spinning top‚Äîso to speak‚Äîprevents complete alignment. Instead, it undergoes precession, just like a spinning top wobbling around a vertical axis.\nSo, think of the proton as a spinning top. It rotates on its axis, but also precesses about an axis that is parallel to the external magnetic field.\nNow, let‚Äôs introduce radiation. We emit an electromagnetic wave laterally, toward the precession axis. For simplicity, I‚Äôve drawn only the magnetic component, oscillating in and out of the board as it travels laterally. If the angular frequency of this radiation equals the angular frequency of precession, resonance occurs.\n\nWe define the Larmor angular frequency:\u000bomega L equals gamma times B-naught.\nHere gamma is the gyromagnetic ratio. For a proton, gamma is about two-point-sixty-seven times ten to the eighth radians per second per tesla. To convert from radians per second to hertz, we divide by two-pi. So,\ntwo-pi f L equals gamma times B-naught.\nOr equivalently:\u000bf L equals gamma over two-pi, times B-naught.\nNumerically, this works out to about forty-two megahertz per tesla.\nSo, the frequency of the applied radiation matches the precession frequency of the proton. Whenever the proton comes around to a certain point, the applied B-field is right there to interact with it. The torque tends to flatten the precession, step by step, as the proton keeps rotating.\nWhen the radiation is turned off, the proton flips back to its original motion, precessing around the external magnetic field axis. This is the fundamental mechanism of magnetic resonance. Historically, it was called nuclear magnetic resonance, but later changed to simply magnetic resonance‚Äîmostly for public perception, since ‚Äúnuclear‚Äù tends to worry people.\n\nNow let‚Äôs take a group of hydrogen atoms. The body contains a large amount of hydrogen, because we are mostly water‚ÄîH2O. Initially, the dipole moments of hydrogen protons are randomly oriented. When we apply an external magnetic field, the protons tend to align with the field, precessing around its axis.\nWhen we emit radiation laterally, as we discussed before, the protons flatten out. Then, when the radiation is turned off, the protons flip back. Think of this like a spring being released. As they flip back, they emit radiation. That emitted radiation can be detected and processed to form an image.\nIn 1973, a breakthrough came from Paul Lauterbur‚Äîhe showed that you could actually use this principle to create images. The idea was: take a subject, place them inside a tube with a strong superconducting magnet, on the order of five tesla. The magnetic field is aligned along the axis of the tube, so all the protons line up and precess.\n\nThen, apply a secondary magnetic field that produces a gradient along the axis of the tube. That means the magnetic field is slightly stronger at one end, weaker at the other. Because the Larmor frequency depends on the field strength, different positions along the tube correspond to different precession frequencies.\nFor example, at a position one-point-three meters down the tube, the magnetic field might be B1, slightly larger than B0. Substituting B1 into the equation, you find a different resonance frequency. By tuning the applied radiation frequency, only protons at that slice will resonate and emit radiation. This gives spatial localization.\nEngineers extended this idea. By using different gradient coils‚Äîlike the saddle coil shown here‚Äîthey can create gradients not only along the Z-axis, but also in the X and Y directions. That way, we can localize signals to small cubes of tissue, called voxels, inside the body.\n\nFinally, only tissues containing hydrogen‚Äîmeaning water and fat‚Äîproduce signal. That‚Äôs why MRI works so well for imaging soft tissues in the human body.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 18 - Slide8.txt", "file_path": "Lecture 18\\Texts\\Slide8.txt", "content": "So that gives us a general idea of how MRI works. Areas in the body that contain a lot of water, like soft tissues, produce strong signals, which show up bright in the image. Areas with little water, like bone, produce weak signals, so they appear dark. That is the basic contrast mechanism in MRI.\nThe short lecture we just saw is an excellent example of how to present these key principles clearly. I encourage you to revisit it after you study the details, because once you learn the full physics of MRI, you will be able to understand nearly all of it.\n\nNow, let‚Äôs review some of the mathematical foundations. One important concept is the dot product. If we have two vectors, v and w, their dot product is equal to the magnitude of v times the magnitude of w times the cosine of the angle between them. Geometrically, the dot product measures how much one vector projects onto another. You can think of it as relating to the area formed by the parallelogram between the two vectors.\n\nThis is a very elegant mathematical operation, and it also has a counterpart called the cross product, which we will look at next.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 18 - Slide9.txt", "file_path": "Lecture 18\\Texts\\Slide9.txt", "content": "Now let‚Äôs look at the cross product. When you take two vectors, say vector a and vector b, their cross product is not a scalar like the dot product, but instead a new vector. The magnitude of this vector is equal to the length of a times the length of b times the sine of the angle between them. The direction of the result is perpendicular to the plane formed by a and b, determined by the right-hand rule.\n\nSo while the dot product gives us a projection, the cross product gives us an orthogonal vector that carries both magnitude and direction. This operation is not just a mathematical trick‚Äîit has deep physical meaning. In MRI physics, cross products appear naturally when we describe torques, angular momentum, and precession, making them essential to understanding the behavior of magnetic moments in an external field.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 18 - Slide10.txt", "file_path": "Lecture 18\\Texts\\Slide10.txt", "content": "Here we define torque and see how it relates to angular momentum. Linear momentum is straightforward: it is mass times velocity. According to Newton‚Äôs second law, the time derivative of linear momentum is force. That means if you want to change momentum, you need to apply a force, which changes velocity.\nNow, when we move from linear motion to rotational motion, we deal with angular momentum. Angular momentum is defined as the cross product of the position vector r and the linear momentum p. So, L equals r cross p.\n\nTo change angular momentum, we need torque. Torque is also defined through a cross product: r cross F, the position vector crossed with the applied force. Just as force is the reason linear momentum changes, torque is the reason angular momentum changes.\nThis is really just an extension of Newton‚Äôs second law into rotational dynamics. The derivative of angular momentum with respect to time equals torque. In other words, torque is the rotational analog of force.\n\nThis connection shows why the cross product is essential. It is not only a mathematical operation but also the key to describing how forces lead to changes in rotation. This principle is central in understanding precession and magnetization, which we will soon connect back to MRI physics.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 18 - Slide11.txt", "file_path": "Lecture 18\\Texts\\Slide11.txt", "content": "Let‚Äôs quickly review some high school physics to set the stage. Electric charges interact through forces. If you place two charges in space, the nature of those charges determines how they behave. Like charges, meaning both positive or both negative, repel each other. Opposite charges, one positive and one negative, attract each other.\n\nThe strength of this interaction is given by Coulomb‚Äôs law. The force is proportional to the product of the two charges, divided by the square of the distance between them. So the closer the charges, the stronger the force.\nThis simple idea about electric forces will be important when we connect electricity and magnetism, and eventually link these basic interactions to the physics of MRI.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 18 - Slide12.txt", "file_path": "Lecture 18\\Texts\\Slide12.txt", "content": "Here we see the behavior of magnetic poles. Just like electric charges, magnets also interact depending on their polarity. Every magnet has two poles: a north pole and a south pole. Like poles repel each other, while opposite poles attract.\n\nThis is a simple but powerful analogy. You can think of it like teamwork: if two people have the same role, they may push against each other, but if they have complementary skills, they come together to form a stronger team. In the same way, the north and south poles naturally attract and stabilize one another.\nThese basic principles of magnetism, combined with the behavior of electric charges, are the foundation for understanding the electromagnetic interactions that drive MRI physics.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 18 - Slide13.txt", "file_path": "Lecture 18\\Texts\\Slide13.txt", "content": "The Earth itself has a magnetic field, but it is actually quite weak. The Earth‚Äôs field is about half a Gauss. To put this into perspective, one Tesla equals ten thousand Gauss. So the Earth‚Äôs field is only about fifty microtesla, which is tiny compared to what we use in MRI.\n\nMRI scanners rely on magnets that are tens of thousands of times stronger. Clinical MRI systems typically operate at one and a half Tesla, three Tesla, or even higher in research settings. These strong fields are essential for aligning enough protons in the body to generate measurable signals.\nSo while the Earth‚Äôs magnetic field is important for navigation and protecting us from solar radiation, it is far too weak for imaging. For MRI, we need very powerful superconducting magnets. And now, let‚Äôs connect electric and magnetic forces together, because in physics, they are two sides of the same coin.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 18 - Slide14.txt", "file_path": "Lecture 18\\Texts\\Slide14.txt", "content": "Electricity and magnetism are deeply connected. Whenever an electric current flows through a wire, it generates a magnetic field around that wire. If you coil the wire into loops, the magnetic fields from each loop add together, creating a much stronger overall magnetic field.\nThis is the principle behind an electromagnet. By sending current through a coil, we can generate powerful magnetic fields, and the more loops of wire we use, the stronger the field becomes.\n\nThis connection between electricity and magnetism is fundamental to MRI. The giant superconducting magnets used in MRI are essentially coils carrying current, producing extremely strong and stable magnetic fields needed for imaging.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 18 - Slide15.txt", "file_path": "Lecture 18\\Texts\\Slide15.txt", "content": "So far, we have seen that an electric current can generate a magnetic field. But the reverse is also true: a changing magnetic field can generate an electric current.\nHere‚Äôs how it works. Imagine you have a conducting loop. If nothing changes in the magnetic field passing through the loop, no current flows. But if you insert a magnet into the loop, or pull it out, the magnetic field inside the loop changes. This change induces a current in the loop.\n\nThis is Faraday‚Äôs law of electromagnetic induction. The induced current always acts to oppose the change that caused it, a principle known as Lenz‚Äôs law. So if you try to increase the magnetic field, the induced current will create a field that resists that increase. If you decrease the magnetic field, the induced current will create a field that resists the decrease.\nThis interplay between electricity and magnetism is fundamental for MRI. The scanner excites protons using magnetic and radiofrequency fields, and then relies on induced currents in coils to detect the returning signals.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 18 - Slide16.txt", "file_path": "Lecture 18\\Texts\\Slide16.txt", "content": "Now, let‚Äôs bring everything together with Maxwell. Alongside Newton and Einstein, James Clerk Maxwell is regarded as one of the greatest physicists. His major contribution was formulating the four Maxwell equations, which elegantly describe how electric and magnetic fields behave and interact.\n\nI don‚Äôt expect you to master these equations right now, but it is important to recognize their role. They unite everything we have talked about: charges producing electric fields, magnetic fields produced by currents, and the deep connection between electricity and magnetism.\nIn these equations, the dot product is used to describe divergence‚Äîhow field lines flow in or out of a point. The cross product is used to describe curl‚Äîhow field lines twist or circulate. With just these two mathematical operations, Maxwell captured the full behavior of electromagnetic fields.\n\nSo at a basic level, remember this: Maxwell‚Äôs equations are the foundation of all electromagnetic interactions. They are the framework that underlies MRI physics and many other technologies in modern science and engineering.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 18 - Slide17.txt", "file_path": "Lecture 18\\Texts\\Slide17.txt", "content": "Here we come to the Lorentz force, a very fundamental concept. The equation says that the total force on a charged particle is equal to the electric force plus the magnetic force. Mathematically, F equals q times E plus q times v cross B. Here, q is the charge, E is the electric field, v is the velocity of the particle, and B is the magnetic field.\n\nThis means that when an electric charge moves through a magnetic field, it experiences a force. The direction of that force is given by the cross product, which makes it perpendicular to both the velocity and the magnetic field.\nAt a deeper level, the Lorentz force can actually be derived from Maxwell‚Äôs equations, showing that Maxwell‚Äôs framework is completely self-contained. Together, these equations govern all of electromagnetism: electric and magnetic fields, waves, and interactions.\n\nFor us, the important point is to become familiar with these concepts‚ÄîMaxwell‚Äôs equations, electromagnetic interactions, and the Lorentz force‚Äîbecause they are the physical foundation for MRI signal generation and detection.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 18 - Slide18.txt", "file_path": "Lecture 18\\Texts\\Slide18.txt", "content": "That wraps up the first part of our discussion. Next, we will move more systematically, following closely along with the structure in your textbook.\nIn the first part, we covered some foundational physics: angular momentum, magnetic moments from a quantum mechanical perspective, magnetization in the classical sense, and the fascinating but at first mysterious idea of precession. These are the building blocks.\n\nThe magnetic moment represents the behavior of individual protons at the quantum level. Magnetization describes how many of these tiny moments add up into a collective vector that we can measure in bulk. And precession explains how that vector behaves in an external magnetic field.\nTogether, these ideas form the physical foundation that allows us to understand signal generation, relaxation, and ultimately the imaging sequences that make MRI possible.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 18 - Slide19.txt", "file_path": "Lecture 18\\Texts\\Slide19.txt", "content": "Let‚Äôs quickly revisit some high school chemistry to ground ourselves. Here is the basic atomic structure. In the outer shell, we have electrons moving around. In reality, their positions are described by probabilities, but we often picture them as orbiting the nucleus.\nInside the nucleus, we find two main types of particles: protons and neutrons. Protons carry positive charges, while neutrons are electrically neutral. Since all the protons are positively charged, they would naturally repel each other. But they are held together by the strong nuclear force, which balances out the electromagnetic repulsion and creates a stable nucleus.\n\nThis balance of forces is what allows atoms to exist in a stable form. And it is precisely these nuclear properties‚Äîespecially the behavior of protons‚Äîthat are central to MRI, because the hydrogen nucleus, which is just a single proton, is the main source of signal in magnetic resonance imaging.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 18 - Slide20.txt", "file_path": "Lecture 18\\Texts\\Slide20.txt", "content": "Now let‚Äôs focus on the proton itself. A proton is a positively charged particle, and one of its most important properties is spin. You can think of spin as being similar to a tiny top rotating on its axis. Because the proton is both spinning and charged, it behaves like a little current loop. And any current loop produces a magnetic field.\n\nThis is why we say a spinning proton has a magnetic moment. You can picture it as a miniature bar magnet, with a north pole and a south pole.\nIf we take just one proton‚Äîsay, from a hydrogen atom in a water molecule‚Äîit acts like a tiny magnet. But in the body, we have countless protons, and in the absence of an external magnetic field, their magnetic moments point in random directions. As a result, they cancel out, and there is no overall magnetic effect.\nHowever, when we apply a strong external magnetic field, something very interesting happens. These magnetic moments begin to align, and that alignment is what \n\nMRI relies on generating signals. We will come to that point soon.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 18 - Slide21.txt", "file_path": "Lecture 18\\Texts\\Slide21.txt", "content": "Here we arrive at two key concepts you must know: angular momentum, usually written as P, and the magnetic moment, written as miu.\n\nFrom mechanics, angular momentum describes the rotation of a particle‚Äîin this case, the proton. You can think of it as a spinning top. Just as torque can change the angular momentum of a top, external influences can change the angular momentum of a proton.\nBut unlike a simple spinning top, the proton also carries an electric charge. Because of this, it has not only mechanical properties but also electromagnetic ones. Its magnetic behavior is summarized by the magnetic moment miu.\n\nThe beautiful part is that these two quantities‚Äîangular momentum and magnetic moment‚Äîare directly linked. The relationship is miu equals gamma times P, where gamma is a constant called the gyromagnetic ratio.\nQuantum mechanics adds one more layer: angular momentum cannot take on any arbitrary value but is quantized. It is expressed in terms of the spin quantum number, I. For a proton, I equals one-half, so the angular momentum follows a specific formula.\n\nThe main takeaway is this: the proton‚Äôs mechanical rotation and its electromagnetic behavior are tied together in a very simple and elegant way. This link forms the basis of how MRI exploits proton spins to generate signals.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 18 - Slide22.txt", "file_path": "Lecture 18\\Texts\\Slide22.txt", "content": "When a proton with a magnetic moment is placed in a strong external magnetic field, its behavior becomes quantized. Classically, you might imagine that the magnetic moment could point in any direction. But quantum mechanics tells us that only certain orientations are allowed.\n\nSpecifically, we focus on the longitudinal component of the magnetic moment, meaning the part aligned with the external field. For the proton, this component can take on only two values: one parallel to the field and one antiparallel. These values are determined by the nuclear magnetic quantum number, m I, which for a proton can be plus one-half or minus one-half.\n\nSo, the z-component of the magnetic moment, miu z, equals plus or minus gamma times Planck‚Äôs constant divided by four pi. This means the magnetic moment is never perfectly aligned‚Äîit is always tilted slightly. But it is restricted to these two possible states.\nThis quantum restriction, the fact that the magnetic moment can only take on discrete orientations, is what gives rise to the fundamental energy levels we use in MRI.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 18 - Slide23.txt", "file_path": "Lecture 18\\Texts\\Slide23.txt", "content": "When we place spinning protons, each with a magnetic moment, into a strong external magnetic field, their energy levels split. This phenomenon is called the Zeeman effect.\n\nThe magnetic moment of a proton can only align in one of two possible ways with respect to the external field: parallel or antiparallel. In the parallel case, the magnetic moment lines up with the field, and this state has lower energy. In the antiparallel case, the magnetic moment points against the field, and this state has higher energy.\n\nSo, instead of a continuous range of orientations, quantum mechanics restricts protons to just these two discrete states. The energy difference between them is proportional to the strength of the magnetic field.\nThis is similar to a mechanical analogy: imagine trying to rotate a bar magnet in a strong external field. If you align it with the field, that is the stable, low-energy position. If you force it to oppose the field, that requires work and corresponds to a higher-energy state.\n\nThis splitting of energy levels is fundamental in MRI. It is the small difference in populations between protons in the lower-energy parallel state and the higher-energy antiparallel state that generates the net magnetization we rely on to produce signals.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 18 - Slide24.txt", "file_path": "Lecture 18\\Texts\\Slide24.txt", "content": "When protons are placed in a strong magnetic field, they split into two energy states: the parallel state, which has lower energy, and the antiparallel state, which has higher energy. This is the Zeeman effect.\n\nThe energy of each state can be expressed as:\nE equals negative mu z times B naught.\nHere, mu z is the longitudinal component of the magnetic moment, and B naught is the strength of the external magnetic field.\nBecause mu z can take on two values, plus gamma h over four pi, or minus gamma h over four pi, the energies are:\nE equals plus or minus gamma times h times B naught, divided by four pi.\nThe difference between these two levels is what we call delta E, and it is given by:\nDelta E equals gamma times h times B naught, divided by two pi.\nThis difference is critical because it sets the resonance condition. Only when the energy of the applied radiofrequency matches this delta E can protons flip between the two states.\n\nAt body temperature, slightly more protons are in the lower energy parallel state compared to the higher energy antiparallel state. This small imbalance is what produces the net magnetization that we detect in MRI.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 18 - Slide25.txt", "file_path": "Lecture 18\\Texts\\Slide25.txt", "content": "Now, let‚Äôs bring in some mathematics through the Boltzmann distribution, which describes how protons split between the two energy states.\n\nWe can write the ratio of the number of protons in the anti-parallel state to the number in the parallel state as:\n‚ÄúN anti-parallel divided by N parallel equals the exponential of negative delta E over k T.‚Äù\nHere, delta E is the energy difference between the two states, k is the Boltzmann constant, and T is the absolute temperature in kelvins.\n\nSubstituting the expression for delta E, we get:\n‚ÄúN anti-parallel divided by N parallel equals the exponential of negative gamma times h times B-zero, divided by two pi k T.‚Äù\nWith a first-order approximation, this becomes:\n‚ÄúN anti-parallel over N parallel is approximately equal to one minus gamma h B-zero over two pi k T.‚Äù\n\nWhat does this mean in practice? It means the populations of the two states are nearly equal, with only a tiny excess in the parallel, or lower-energy state.\nThe net MRI signal comes from this small population difference. If we take the difference in numbers between the parallel and anti-parallel states, we get:\n‚ÄúDelta N equals N s times gamma h B-zero divided by four pi k T.‚Äù\nHere, N s is the total number of protons in the body. The key point is that this difference is extremely small. For example, at a magnetic field strength of one and a half tesla, out of about one million protons, only a handful contribute to the net signal.\n\nSo, although the individual imbalance is tiny, the collective contribution of billions and billions of protons is what makes MRI signals measurable.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 18 - Slide26.txt", "file_path": "Lecture 18\\Texts\\Slide26.txt", "content": "Now, all of these models we‚Äôve discussed, including the Boltzmann distribution, set the stage for the next key idea: precession.\n\nWhen a magnetic moment sits inside a strong external magnetic field, it doesn‚Äôt simply align once and stay fixed. Instead, it undergoes a motion very much like a spinning top. The top leans and slowly wobbles around the vertical axis. In the same way, the magnetic moment traces out a precession around the magnetic field direction. We‚Äôll go into more detail on this soon.\n\nTo explain why this happens, we need to go back to Newton‚Äôs second law‚Äînot just in the familiar form, force equals mass times acceleration, but in the rotational context. In rotations, the central quantity is angular momentum.\nThe law tells us that the time derivative of angular momentum, written as dL over dt, is equal to torque. So, torque is the rotational analog of force. Just as force changes linear momentum, torque changes angular momentum.\nThe derivation on this slide shows the details. If you‚Äôre interested in the full proof, you can follow it step by step. If not, you can simply trust the key result: the rate of change of angular momentum equals torque.\n\nThis is the foundation we need to understand precession in MRI physics.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 18 - Slide27.txt", "file_path": "Lecture 18\\Texts\\Slide27.txt", "content": "This is a classic example of precession, and it is the same principle that applies in MRI when the magnetic moment of protons precesses in a magnetic field.\nThink of a spinning top. The top‚Äôs axis of rotation points in one direction, representing the angular momentum. If no external force acts on it, the top will keep spinning steadily, just as a moving object keeps moving in a straight line.\n\nNow, if a force is applied ‚Äî here, gravity acting at the center of mass ‚Äî it produces a torque. Torque is given by the cross product of the lever arm vector r and the force vector F. In this case, r cross F points into the page. That means the torque is perpendicular to the angular momentum.\nWhen the torque is perpendicular, it doesn‚Äôt stop the motion or flip the top over directly. Instead, it causes a small change, delta L, that is also perpendicular to the original angular momentum. This sideways change makes the tip of the angular momentum vector trace out a circle.\nSo instead of falling over, the spinning top precesses‚Äîit wobbles around in a circular path.\n\nAnd that is the key insight: whenever a spinning object with angular momentum experiences a torque perpendicular to it, the result is precession. In MRI, the proton‚Äôs magnetic moment behaves just like this spinning top, precessing around the direction of the external magnetic field.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 18 - Slide28.txt", "file_path": "Lecture 18\\Texts\\Slide28.txt", "content": "This idea is not too hard to grasp if we think about orbital motion.\nTake the example of the Moon orbiting around the Earth, or a satellite orbiting a planet. You might wonder: why doesn‚Äôt it just fall straight down? The answer is that there is always a centripetal force pointing inward, toward the center of the orbit.\n\nThat centripetal force doesn‚Äôt change the magnitude of the velocity, but it does change its direction. Because the force is always perpendicular to the velocity vector, it continuously ‚Äúbends‚Äù the path, keeping the satellite in circular motion rather than letting it fly off in a straight line.\nMathematically, the centripetal force is equal to mass times velocity squared, divided by the radius: F equals m v squared over r.\n\nNow connect this to angular momentum. When the change in a vector is always perpendicular to the vector itself, the result is circular motion. In the orbital case, the velocity vector keeps changing direction, while in angular momentum, the same principle applies: torque causes a perpendicular change in angular momentum, and so the angular momentum vector itself traces out a circle.\n\nThis is the key point to understand precession: a perpendicular change leads to circular motion.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 18 - Slide29.txt", "file_path": "Lecture 18\\Texts\\Slide29.txt", "content": "When a magnetic moment is placed in an external magnetic field, it experiences a torque. This torque does not flip the magnetic moment directly into alignment with the field. Instead, it causes the moment to precess, meaning it rotates around the axis of the magnetic field, very much like a spinning top under the influence of gravity.\n\nThe rate of this precession is called the Larmor frequency. The Larmor frequency depends on two things: the strength of the magnetic field and a constant called the gyromagnetic ratio, which is unique for each type of nucleus.\nMathematically, we write the Larmor frequency as omega zero equals gamma times B zero. Here, omega zero represents the precession frequency, gamma is the gyromagnetic ratio, and B zero is the strength of the magnetic field.\n\nThis relationship tells us that the stronger the field, the faster the precession. And this precession frequency is the key to MRI, because it sets the frequency at which protons respond to radiofrequency pulses and generate the measurable MRI signal.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 30, "slide_filename": "Slide30.txt", "slide_annotation": "Lecture 18 - Slide30.txt", "file_path": "Lecture 18\\Texts\\Slide30.txt", "content": "Now let‚Äôs carefully look at how we can compute the precessional frequency.\nWhen a magnetic moment, which we call mu, is placed at an angle to the external magnetic field B, it experiences a torque. This torque can be written as mu cross B zero. That torque is what drives the angular momentum to change over time.\nMathematically, we can say: torque equals the derivative of angular momentum with respect to time. In other words, dP by dt equals mu cross B zero.\n\nHere, P is the angular momentum. Because protons both have mass and positive charge, their spinning motion not only produces angular momentum but also generates a tiny current loop. That current loop is associated with a magnetic moment. And so, whenever the magnetic moment is not aligned with the magnetic field, it feels a torque.\nNow, to connect the pieces: magnetic moment and angular momentum are directly proportional. That means the change in angular momentum is tied to the change in magnetic moment.\nTo see how this becomes precision, consider a very small angular change, which we call d phi. For such small changes, the sine of d phi can be approximated as just d phi. With this, we can relate torque to the rate of change of the angular position.\nSo the angular frequency of precession, written as omega, is equal to the torque divided by the magnitude of angular momentum times sine theta. If you expand the cross product, you eventually find that omega equals gamma times B zero.\n\nHere, gamma is the gyromagnetic ratio, a constant specific to the nucleus.\nThis final result is extremely important: the precessional frequency is directly proportional to the strength of the magnetic field. In a stronger field, the spins precess faster. In a weaker field, they precess more slowly. This proportionality is the foundation of magnetic resonance.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 31, "slide_filename": "Slide31.txt", "slide_annotation": "Lecture 18 - Slide31.txt", "file_path": "Lecture 18\\Texts\\Slide31.txt", "content": "It‚Äôs often much easier to demonstrate precision than to explain it with equations.\nHere you see a gyroscope. Its circular motion around the stand is precession. This motion results from the combination of two effects: the downward force of gravity and the angular momentum around the spinning axis.\n\nNotice how the wheel does not simply fall. Instead, because of torque, it precesses around the point where it is supported. If I increase the downward force‚Äîfor example, by adding the weight of a wrench to the axle‚Äîyou can see that the precession frequency increases.\n\nThis is the same principle that applies in MRI. The frequency of precession is directly linked to the strength of the magnetic field. Stronger forces or fields lead to faster precession. We call this frequency encoding, and it is the foundation for how MRI gathers spatial information.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 32, "slide_filename": "Slide32.txt", "slide_annotation": "Lecture 18 - Slide32.txt", "file_path": "Lecture 18\\Texts\\Slide32.txt", "content": "Now we move from individual protons to the idea of magnetization.\nEach proton carries a magnetic moment, and as we have discussed, these moments precess around the external magnetic field. In the population of protons, slightly more occupy the lower-energy state, aligned parallel to the magnetic field, while slightly fewer are in the higher-energy, anti-parallel state. This imbalance, even though very small, is crucial.\n\nBecause the magnetic moments are vectors, those that are opposite to each other will cancel out in the transverse plane. What remains is a small excess of protons in the parallel state, and when we add up all of their contributions, we obtain a single net vector pointing along the direction of the magnetic field. This is what we call the net magnetization vector, often denoted as M.\nFrom this point on, instead of worrying about the behavior of individual protons, we can focus on this collective magnetization vector. We can treat it as a classical vector that can tilt, rotate, or precess, just like a spinning top. This makes it much easier to understand the physics and to design MRI pulse sequences.\n\nSo, magnetization is the bridge from the microscopic quantum world of spins to the macroscopic signals we use in MRI.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 33, "slide_filename": "Slide33.txt", "slide_annotation": "Lecture 18 - Slide33.txt", "file_path": "Lecture 18\\Texts\\Slide33.txt", "content": "Now we bring everything together and look at the steady state of magnetization.\n\nEarlier, we talked about the tiny surplus of protons in the lower-energy parallel state compared with those in the higher-energy anti-parallel state. That difference is very small, but when multiplied by the enormous total number of protons in the body, it adds up to a measurable effect.\nFor a single proton, the magnetic moment can be expressed as gamma times h-bar over two. But now, instead of just one proton, we have on the order of 10\nto the power of 23 protons, and only a small fraction contributes to the net imbalance. Multiplying that tiny difference by the large population gives us the bulk magnetization.\n\nWe call this net magnetization M naught. It represents the collective contribution of all the protons and points along the direction of the external field B naught.\nIn this steady state, the magnetization only has a longitudinal component. That means M z equals M naught, while the transverse components M x and M y are both zero.\n\nEven though individual protons are still precessing and flipping between parallel and anti-parallel orientations, when you take the average, the only surviving component is along the z-axis. This is what we refer to as the steady-state magnetization.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 34, "slide_filename": "Slide34.txt", "slide_annotation": "Lecture 18 - Slide34.txt", "file_path": "Lecture 18\\Texts\\Slide34.txt", "content": "So far, we‚Äôve spent quite some time on the physical foundation. That‚Äôs good, because now you have a solid grounding in the physics behind MRI ‚Äî angular momentum, magnetic moment, precession, and magnetization. All of this is essential because without that base, the next steps would be very difficult to follow.\n\nNow we‚Äôre ready to move forward and talk about how the MRI signal is actually generated. This is the transition point ‚Äî from theory into application. The key question becomes: once the protons are aligned and precessing in the magnetic field, how do we disturb them in a controlled way and measure the response?\nThat‚Äôs where the radiofrequency pulse comes in. By applying a carefully tuned RF pulse, we can perturb the magnetization vector and create a measurable signal. In the upcoming slides, I‚Äôll explain this process step by step ‚Äî beginning with RF perturbation, then the Bloch equations that describe the dynamics, and finally, how we detect the free induction decay signal.\n\nSo, let‚Äôs pay attention to the next slide ‚Äî this is where MRI really comes alive.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 35, "slide_filename": "Slide35.txt", "slide_annotation": "Lecture 18 - Slide35.txt", "file_path": "Lecture 18\\Texts\\Slide35.txt", "content": "So here is the key motivation for MR signal generation. We have this net magnetization vector, which is called M-naught. It reflects the water and lipid content in the body, because that‚Äôs where most of the hydrogen protons are found. That sounds great, but the important question is: how do we actually measure M-naught?\nIf you just leave M-naught alone in its steady state, it aligns with the main magnetic field and stays constant in time. A static field like this will not generate any measurable electrical signal. Remember, electromagnetic induction only works when the magnetic field is changing. A constant magnetization produces nothing in our detection coil.\n\nThe key idea is this: if we can flip M-naught away from its alignment with the main field, then it will gain a transverse component. That transverse part does not just sit there ‚Äî it precesses, meaning it rotates around the axis of the main magnetic field. This rotating transverse magnetization produces an alternating magnetic field. An alternating field is exactly what we need, because it induces an electrical signal in the detection coil.\n\nThis was the breakthrough that launched nuclear magnetic resonance. It was discovered in nineteen forty-six by Felix Bloch at Stanford and Edward Purcell at Harvard, and their work was recognized with the Nobel Prize in Physics in nineteen fifty-two.\nSo the essential trick is: flip the net magnetization, create a transverse component, let it precess, and detect the alternating signal that comes out. This is the foundation of MRI signal generation.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 36, "slide_filename": "Slide36.txt", "slide_annotation": "Lecture 18 - Slide36.txt", "file_path": "Lecture 18\\Texts\\Slide36.txt", "content": "When the magnetization vector is in steady state, it is balanced. No magnetic field is changing, so no signal can be detected. But once we flip it, we break that balance. Now there is a transverse component of magnetization. This transverse part does not stay still ‚Äî it undergoes circular motion, what we call precession. And this precessing transverse component produces an alternating magnetic field, which can then be measured through electromagnetic induction, as described by Maxwell‚Äôs equations.\n\nSo how do we flip the magnetization to break the balance? To answer that, we look at the energy difference between the parallel state and the anti-parallel state of the protons. Earlier, we derived this difference and called it delta E.\nThis energy difference must be supplied by an incoming electromagnetic wave. And the energy carried by a photon is equal to Planck‚Äôs constant times the frequency. That means the frequency of the applied radiofrequency wave has to exactly match the energy gap between the two spin states.\nWhen you rearrange the formulas, you find that the resonant frequency, written as f, is equal to gamma times B-zero divided by two pi. In terms of angular frequency, written as omega, the result is omega equals gamma times B-zero.\n\nThis is a fundamental result. It tells us that the Larmor precession frequency of the proton is identical to the frequency of the radiofrequency field we must apply to drive transitions between the parallel and the anti-parallel states.\nSo, from the quantum mechanical point of view, resonance occurs when the radiofrequency wave carries exactly the right energy to match that gap ‚Äî and that is the basis of nuclear magnetic resonance.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 37, "slide_filename": "Slide37.txt", "slide_annotation": "Lecture 18 - Slide37.txt", "file_path": "Lecture 18\\Texts\\Slide37.txt", "content": "We can now look at RF excitation from the quantum mechanical point of view.\u000bImagine we have two energy levels: a lower energy state and a higher energy state. When we send in a radiofrequency wave at the resonance frequency, that wave carries just the right amount of energy to promote a proton from the lower state up to the higher state.\n\nBut once the RF excitation stops, those protons in the higher energy state cannot stay there forever. They naturally relax back down to the lower energy state, which is the more stable configuration. As they do so, the excess energy is released. Importantly, the released energy is in the form of the same radiofrequency signal that was used to excite them.\n\nThis emitted signal is what we detect externally. It is the key step that allows us to convert the microscopic spin transitions of hydrogen protons into a measurable macroscopic MR signal.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 38, "slide_filename": "Slide38.txt", "slide_annotation": "Lecture 18 - Slide38.txt", "file_path": "Lecture 18\\Texts\\Slide38.txt", "content": "Now, let‚Äôs look at the classical view of RF excitation.\u000bWe have our net magnetization vector, which represents the collective magnetic moment of all protons. In the steady state, this vector points along the z-axis, aligned with the static magnetic field B-zero.\n\nWhen we apply an additional magnetic field, called B-one, along the x-axis, something interesting happens. The cross product between the magnetization vector and B-one produces a torque. This torque acts in the direction perpendicular to both vectors, and it causes the magnetization to tip away from the z-axis.\nBut remember, the system is still precessing around the main magnetic field. So the magnetization doesn‚Äôt simply tilt once and stop‚Äîit spirals away from the z-axis, tracing out a cone. The longer we apply the B-one field, the further the vector tips, increasing the flip angle.\n\nThis picture is very convenient to describe using the concept of a rotating frame. In that frame, the B-one field looks stationary, and the magnetization vector simply rotates around it. That‚Äôs why we often describe RF excitation as ‚Äúflipping‚Äù the magnetization into the transverse plane.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 39, "slide_filename": "Slide39.txt", "slide_annotation": "Lecture 18 - Slide39.txt", "file_path": "Lecture 18\\Texts\\Slide39.txt", "content": "In the laboratory frame, the magnetization vector doesn‚Äôt just tip smoothly into the transverse plane. Instead, it traces out a spiral path, because while it is flipping down, it is also precessing around the z-axis at the Larmor frequency. So what we observe looks like a helical trajectory.\n\nBut there is a much simpler way to describe this motion. Instead of staying in the laboratory coordinate system, we can shift into a rotating frame of reference. This is a coordinate system that rotates at exactly the Larmor frequency.\n\nIn this rotating frame, the spiral disappears. The magnetization no longer seems to precess‚Äîit simply tilts, or ‚Äúflips,‚Äù into the transverse plane. This makes the description of RF excitation much more intuitive.\n\nThat‚Äôs why, throughout MRI physics, we often switch between the laboratory frame and the rotating frame. The physics is the same, but in the rotating frame, the mathematics and the visualization become much simpler.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 40, "slide_filename": "Slide40.txt", "slide_annotation": "Lecture 18 - Slide40.txt", "file_path": "Lecture 18\\Texts\\Slide40.txt", "content": "Think about this in terms of a revolving door. If you just walk up and push the door randomly, sometimes your push helps it move, and sometimes it cancels out‚Äîbecause your force is not always aligned in the right way.\nBut if you keep pushing at exactly the right rhythm, always perpendicular to the door, you will steadily drive it around in a smooth motion. That is resonance: applying a force at the right frequency so that every push adds up.\n\nIn MRI, the same idea holds. The RF signal has to be applied at the exact precessional frequency‚Äîthe Larmor frequency. If it matches, then every cycle of the RF field keeps nudging the spins in the same direction. In the rotating frame, this no longer looks like an oscillating wave. Instead, it looks like a steady force, a constant torque, applied to the magnetization vector.\nThat constant torque is what tips the magnetization away from the z-axis. It is what allows us to rotate the vector from its equilibrium position into the transverse plane, or even further.\n\nSo the revolving door is a nice analogy: push in rhythm with the rotation, and the system responds strongly. Push out of rhythm, and the effect cancels out.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 41, "slide_filename": "Slide41.txt", "slide_annotation": "Lecture 18 - Slide41.txt", "file_path": "Lecture 18\\Texts\\Slide41.txt", "content": "We describe the change of the magnetization vector M of t with time.\u000bThe Bloch equation is:\ndM of t over d t equals M of t cross gamma times B of t, minus R times the difference of M of t and M zero.\nHere:\ngamma is the gyromagnetic ratio,\nB of t is the magnetic field,\nM zero is the equilibrium magnetization,\nand R is the relaxation operator that encodes T1 and T2 processes.\nNow, if we expand this into components, we get three equations:\nFor the z component:\u000bdM z over d t equals gamma times the quantity M x times B y, minus M y times B x, minus the difference M z minus M zero divided by T1.\nFor the x component:\u000bdM x over d t equals gamma times the quantity M y times B z, minus M z times B y, minus M x divided by T2.\nFor the y component:\u000bdM y over d t equals gamma times the quantity M z times B x, minus M x times B z, minus M y divided by T2.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 42, "slide_filename": "Slide42.txt", "slide_annotation": "Lecture 18 - Slide42.txt", "file_path": "Lecture 18\\Texts\\Slide42.txt", "content": "Now let‚Äôs see how the Bloch equation works in practice.\nFirst, consider precession. If the magnetization vector, capital M, is not parallel to the magnetic field B, then M will precess around B. On the other hand, if M is perfectly parallel to B, then the precession radius is zero. In that case, you do not see any visible precession, even though the vector is still technically circling like a single dot.\n\nNext, think about what happens when we flip M. Suppose M is rotated away from the z-axis into the transverse plane. Now the magnetization has a horizontal component, which we call M x y. This component will oscillate and produce an alternating magnetic field. That is the field that induces a measurable signal in the nearby coil.\nPhysically, you can imagine the magnetization vector as a magnetic bar. If you keep flipping or rotating this bar, the field it produces also keeps changing. That changing field drives current in the coil, which is exactly what we detect as the MR signal. The amplitude of this signal depends on proton density: the more protons available, the stronger the signal.\n\nNow, after the excitation pulse ends, the system will gradually return to equilibrium. The z-component of magnetization, M z, will recover to its steady-state value M zero. This recovery is governed by the T1 relaxation process, sometimes called spin-lattice relaxation. At the same time, the transverse component M x y will decay toward zero. This loss of coherence is governed by T2 relaxation, also called spin-spin relaxation.\nSo, to summarize:\nT1 describes how the longitudinal magnetization M z returns to equilibrium.\nT2 describes how the transverse magnetization M x y decays to zero.\n\nBoth of these effects are essential in shaping the MR signal we actually record.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 43, "slide_filename": "Slide43.txt", "slide_annotation": "Lecture 18 - Slide43.txt", "file_path": "Lecture 18\\Texts\\Slide43.txt", "content": "Now let‚Äôs talk about how the MR signal is actually detected.\n‚Äò\nWhen the magnetization vector, capital M, is flipped away from the z-axis, it develops a transverse component, which we call M x y. This transverse magnetization does not remain static ‚Äî it oscillates at the resonance frequency. That oscillation induces a sinusoidal current in the radiofrequency coil, exactly as predicted by Faraday‚Äôs law of electromagnetic induction.\n\nThe strength of this induced signal depends directly on the size of M x y. The larger the transverse component, the stronger the oscillating magnetic field, and therefore the stronger the detected voltage in the coil.\nThe maximum possible signal is reached when the flip angle is ninety degrees. At this angle, the entire magnetization vector lies in the x-y plane, meaning that M x y is equal to M naught, the full equilibrium magnetization. This is why a ninety-degree pulse is often used in MRI ‚Äî it gives the largest possible signal for detection.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 44, "slide_filename": "Slide44.txt", "slide_annotation": "Lecture 18 - Slide44.txt", "file_path": "Lecture 18\\Texts\\Slide44.txt", "content": "Once the radiofrequency pulse is turned off, the magnetization vector begins to relax back toward the main magnetic field, capital B naught. In this process, the excess energy that was absorbed is released.\nAs the system relaxes, the transverse magnetization ‚Äî that is, M x y ‚Äî begins to decay. This decay produces a signal that gradually decreases to zero. We call this the free induction decay, or FID.\n\nThe FID is the fundamental nuclear magnetic resonance signal. It oscillates at the resonance frequency, and its strength is proportional to the local proton density. In other words, the more protons you have in a region, the stronger the signal you record.\n\nThis decay curve, shaped by relaxation mechanisms T1 and T2, contains the raw information that MRI uses. By capturing and analyzing this signal, we can reconstruct images that reveal tissue structure and composition.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 45, "slide_filename": "Slide45.txt", "slide_annotation": "Lecture 18 - Slide45.txt", "file_path": "Lecture 18\\Texts\\Slide45.txt", "content": "Now that we have covered how MRI signals are generated and detected, the next step is to understand how those signals decay. This part is crucial, because the decay mechanisms ‚Äî specifically T1 and T2 relaxation ‚Äî are what ultimately give us image contrast.\n\nSo, in the final section of this lecture, we will look more closely at the physics behind T1 and T2. We will also see how these relaxation times can be measured, and how pulse sequences such as inversion recovery and spin echo are designed to take advantage of them.\n\nUp to this point, our discussion has focused on the overall process of signal generation and detection. But to move from signals to images, we need to understand relaxation. T1 and T2 are the keys that translate raw MRI physics into meaningful image contrast.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 46, "slide_filename": "Slide46.txt", "slide_annotation": "Lecture 18 - Slide46.txt", "file_path": "Lecture 18\\Texts\\Slide46.txt", "content": "So now let‚Äôs ask: why does the MR signal decay?\nThe answer is very much like the rolling ball analogy. A ball on the ground slows down because of resistance and friction. In MRI, once we flip the magnetization into precession, it cannot stay there forever. Over time, it will naturally return to its steady state, aligned with the main magnetic field, B naught.\n\nThis return happens through different relaxation mechanisms:\nT1 relaxation, also called longitudinal or spin-lattice relaxation, is when flipped nuclei realign with the main magnetic field. The energy they lose is given back to the surrounding tissue as thermal energy.\nT2 relaxation, also called transverse or spin-spin relaxation, is when the nuclei gradually lose phase coherence with one another. Even though they were flipped together, local differences in their environment cause them to fall out of step.\nT2-star relaxation is an additional effect that comes from imperfections in the magnetic field itself. The field is never perfectly uniform, and small local variations cause the spins to dephase even faster. This effect is especially important in functional MRI, where susceptibility differences are actually used to generate contrast.\nThe key point is that the MR signal we detect ‚Äî the NMR signal ‚Äî is proportional to proton density, but reduced by these T1, T2, and T2-star factors.\n\nAnd this is what gives MRI its powerful contrasts. For example:\nT1 weighting helps us distinguish gray matter from white matter.\nT2 weighting highlights tissues and fluid, such as cerebrospinal fluid.\nT2-star weighting is especially useful for functional MRI, because it is sensitive to magnetic susceptibility changes, such as those caused by blood oxygenation.\nSo, signal decay is not a nuisance. In fact, it‚Äôs what makes MRI such a versatile imaging tool.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 47, "slide_filename": "Slide47.txt", "slide_annotation": "Lecture 18 - Slide47.txt", "file_path": "Lecture 18\\Texts\\Slide47.txt", "content": "Now let‚Äôs take a closer look at the T1 effect.\nT1 describes how the longitudinal magnetization, that is, the component along the z-axis, gradually returns to its equilibrium value after being disturbed by a radiofrequency pulse.\n\nOn the left side of the figure, we see what happens after a ninety-degree pulse. In this case, the longitudinal magnetization, which we call M z, has been tipped entirely into the transverse plane, so it starts at zero. Over time, it recovers back toward its equilibrium value, which we call M naught, and it does so in an exponential fashion.\n\nOn the right side, we see the case of a one-hundred-eighty-degree pulse. Here, the longitudinal magnetization is flipped completely upside down, starting at negative M naught. From there, it also recovers exponentially back to M naught.\nThis exponential recovery is the hallmark of T1 relaxation. The recovery speed depends on the tissue type, because different tissues exchange energy with their environment‚Äîthe so-called spin-lattice interaction‚Äîat different rates.\n\nSo, to summarize: T1 tells us how quickly a tissue‚Äôs magnetization realigns with the main magnetic field, B zero. And these differences in T1 times are one of the key sources of image contrast in MRI.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 48, "slide_filename": "Slide48.txt", "slide_annotation": "Lecture 18 - Slide48.txt", "file_path": "Lecture 18\\Texts\\Slide48.txt", "content": "Now let‚Äôs talk about the T2 effect, which is also called transverse relaxation or dephasing.\nAfter we apply a ninety-degree pulse, the net magnetization lies entirely in the transverse plane, along the x-y plane. At that initial moment, all of the tiny magnetic moments of individual protons are lined up together, pointing in the same direction.\n\nBut over time, each of those protons experiences slightly different local magnetic fields. Some of them precess a little faster, and others precess a little slower. As a result, they gradually drift out of phase with one another.\nWhen that happens, their contributions begin to cancel out. The net transverse magnetization shrinks, even though each proton is still spinning.\nOn the graph at the bottom, you can see this process clearly. The transverse magnetization starts at its maximum value and then decays exponentially toward zero as the moments become more and more dephased.\n\nThis is the essence of the T2 relaxation process: it measures how quickly the spins lose phase coherence in the transverse plane, leading to signal decay.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 49, "slide_filename": "Slide49.txt", "slide_annotation": "Lecture 18 - Slide49.txt", "file_path": "Lecture 18\\Texts\\Slide49.txt", "content": "At this stage, we can summarize how T1 and T2 relaxation describe the time evolution of magnetization in MRI.\n\nFirst, consider the longitudinal component, which we usually call M z. After a radiofrequency pulse, it does not instantly return to equilibrium. Instead, it recovers gradually, following an exponential curve. Mathematically, we say that M z of time equals M naught multiplied by one minus e to the power of negative t over T1. In plain words, this means the recovery depends on a time constant, T1, which tells us how quickly the spins realign with the main magnetic field.\n\nNext, for the transverse components, which are M x and M y, these decay over time. Their rate of decay is described by the time constant T2. The equations are written as d M x over d t equals minus M x divided by T2, and d M y over d t equals minus M y divided by T2. In other words, both x and y components shrink exponentially toward zero, at a rate determined by T2.\n\nSo, together, T1 and T2 give us a semi-quantitative model for MRI. T1 governs how fast the longitudinal magnetization recovers, and T2 governs how fast the transverse magnetization disappears. Both processes follow exponential curves, but with different time constants.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 50, "slide_filename": "Slide50.txt", "slide_annotation": "Lecture 18 - Slide50.txt", "file_path": "Lecture 18\\Texts\\Slide50.txt", "content": "Here we have a summary table of T1 and T2 relaxation times for different tissues, measured at a field strength of one point five Tesla.\n\nFor fat, the T1 relaxation time is about two hundred sixty milliseconds, and the T2 is about eighty milliseconds.\nFor muscle, T1 is around eight hundred seventy milliseconds, while T2 is much shorter, about forty-five milliseconds.\nIn the brain, gray matter has a T1 of about nine hundred milliseconds and a T2 of about one hundred milliseconds. White matter is slightly shorter, with a T1 of about seven hundred eighty milliseconds and a T2 of about ninety milliseconds.\nThe liver shows a T1 of about five hundred milliseconds and a T2 of about forty milliseconds.\nFinally, cerebrospinal fluid has the longest times by far: a T1 of about two thousand four hundred milliseconds and a T2 of about one hundred sixty milliseconds.\n\nThese values illustrate a key point: different tissues have different relaxation times, and that difference is what gives MRI its powerful ability to generate contrast between tissues.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 51, "slide_filename": "Slide51.txt", "slide_annotation": "Lecture 18 - Slide51.txt", "file_path": "Lecture 18\\Texts\\Slide51.txt", "content": "Now let‚Äôs distinguish between T2, T2 prime, and T2 star.\nThe loss of phase coherence in the transverse magnetization can arise from two different mechanisms. The first is the so-called pure T2 decay, which comes from intrinsic spin‚Äìspin interactions inside the tissue. This is a physiological and biological effect, and it represents the fundamental limit of how long spins can stay in phase with one another.\n\nThe second contribution comes from spatial variations in the magnetic field strength inside the body. In practice, no magnet is perfectly uniform. There are always small imperfections in the main field, B zero. In addition, different tissues have slightly different magnetic susceptibilities, especially at boundaries between air and tissue, or bone and tissue. These local variations cause spins to accumulate extra phase shifts, further accelerating the loss of coherence. This additional contribution is represented by T2 prime.\nWhen we put these together, the overall transverse relaxation time we observe is called T2 star. Mathematically, the rate of decay of T2 star is the sum of the rates of T2 and T2 prime. That is, one over T2 star equals one over T2 plus one over T2 prime.\n\nSo in summary:\nT2 reflects fundamental spin‚Äìspin interactions.\nT2 prime reflects field inhomogeneity and susceptibility effects.\nT2 star combines both, and it is the effective decay time we actually measure in many MRI sequences.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 52, "slide_filename": "Slide52.txt", "slide_annotation": "Lecture 18 - Slide52.txt", "file_path": "Lecture 18\\Texts\\Slide52.txt", "content": "Now, let‚Äôs look at how T1 can be measured using the inversion recovery method.\nThe idea is to start with a one-hundred-eighty-degree pulse, which flips the net magnetization completely upside down along the negative Z-axis. Once it is inverted, we allow it to recover toward equilibrium during a delay time, which we call tau. During this period, the longitudinal magnetization gradually regrows toward its steady state, following the exponential recovery curve we discussed earlier.\n\nAt the end of the delay, we apply a ninety-degree pulse. This flips whatever longitudinal magnetization has recovered into the transverse plane, where it produces a measurable signal. By repeating this process with different values of tau, we can track how the magnetization regrows over time.\nMathematically, the detected signal follows the expression:\u000bS of tau equals M zero times the quantity one minus two times e to the power of minus tau over T1.\n\nBy fitting this recovery curve, we obtain the T1 relaxation time. In practice, this method gives a very robust way to measure T1 across different tissues, and it is the basis for many T1-weighted imaging protocols in MRI.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 53, "slide_filename": "Slide53.txt", "slide_annotation": "Lecture 18 - Slide53.txt", "file_path": "Lecture 18\\Texts\\Slide53.txt", "content": "Now let‚Äôs visualize what‚Äôs happening in an inversion recovery sequence.\nWe start with the net magnetization pointing up along the Z-axis at its equilibrium value, M‚ÇÄ. Then, we apply a one-hundred-eighty-degree pulse, which flips it completely upside down, to minus M‚ÇÄ. From this inverted state, the magnetization begins to relax back toward equilibrium. Over time, the longitudinal component regrows along the Z-axis following the T1 recovery curve.\nAt some chosen inversion time, which we call TI, we apply a ninety-degree pulse. This rotates whatever longitudinal magnetization has recovered into the transverse plane. Once in the transverse plane, it generates a measurable free induction decay, or FID signal, that is subject to T2 star decay.\nBy repeating this process with different inversion times and recording the signals, we can map out the full recovery curve. Then, using a logarithmic fit, we extract the T1 relaxation time.\nSo, in short, this diagram shows how the one-hundred-eighty-degree inversion, the recovery period, and the ninety-degree readout pulse together make it possible to measure T1 directly.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 54, "slide_filename": "Slide54.txt", "slide_annotation": "Lecture 18 - Slide54.txt", "file_path": "Lecture 18\\Texts\\Slide54.txt", "content": "Now let‚Äôs look at how we measure T2 relaxation using the spin-echo method.\nWe begin with a ninety-degree pulse that tips the net magnetization into the transverse plane. Right away, the spins begin to dephase because of small differences in their local magnetic environments. Faster spins move ahead, slower spins lag, and the overall signal decays quickly with a time constant that looks like T2 star.\n\nBut here is the clever trick: at a certain time, we apply a one-hundred-eighty-degree pulse. This flips all the spins over, so that the ones that were leading are now behind, and the ones that were lagging are now in front. As time continues, these spins re-converge, and at a later time, they come back into phase. When that happens, we see the spins add constructively and produce a strong signal, known as the spin echo.\nThe height of this echo decays with the true T2 relaxation time, not with T2 star. By repeating the sequence with different time delays, we can track how quickly the echo signal diminishes, and from that, extract the T2 constant.\n\nThis technique is both simple and powerful: it cancels out static inhomogeneities in the magnetic field and isolates the true spin-spin relaxation, which is exactly what T2 represents.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 18", "slide_number": 55, "slide_filename": "Slide55.txt", "slide_annotation": "Lecture 18 - Slide55.txt", "file_path": "Lecture 18\\Texts\\Slide55.txt", "content": "To wrap up today‚Äôs lecture, here is your homework assignment.\n\nFor problem 4.3, you will calculate the effects of different pulse sequences on thermal equilibrium magnetization. Your final answers should include the x, y, and z components of magnetization. The cases include a ninety-degree pulse about the x-axis, an eighty-degree pulse about the x-axis, two consecutive ninety-degree pulses about x and y, and finally, two consecutive eighty-degree pulses about x and y.\n\nFor problem 4.4, you will decide whether each statement is true or false and provide a brief explanation. These questions cover recovery of magnetization, the behavior of the static field B0, relaxation from the transverse to the longitudinal axis, and the interpretation of a short T1 relaxation time.\nThe due date is one working week from today.\n\nThank you for following along ‚Äî we‚Äôve covered a lot of ground, from the Bloch equations to T1 and T2 relaxation and even spin echo sequences. This sets the stage for more advanced imaging concepts in our next lecture.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 19 - Slide1.txt", "file_path": "Lecture 19\\Texts\\Slide1.txt", "content": "In this lecture, we move into the second part of MRI imaging. The first part, as you saw in the video, focused on the underlying physical principles. Now, we will explain how MRI imaging is performed, based on those physical models and principles.\n\nIt‚Äôs best to begin by reading the chapter first. You may naturally have questions, but that‚Äôs exactly why you attend this lecture. Without that preparation, it‚Äôs often difficult to build a solid understanding. MRI is quite tricky‚Äîyou need to carefully think about the concepts, the relationships, and revisit them several times. Hence, an iterative approach should be used to preview, discuss, and review, again and again.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 19 - Slide2.txt", "file_path": "Lecture 19\\Texts\\Slide2.txt", "content": "Again, let us see where we are.  We started with medical physics‚Äîspecifically MRI physics‚Äîin the first lecture. Now, we move to MR imaging. In our next lecture, we‚Äôll talk about MRI pulse sequences, which will give us a more specific look at how MRI is applied in practice.\n\nHowever, today, our goal is to first revisit what we have learned so far.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 19 - Slide3.txt", "file_path": "Lecture 19\\Texts\\Slide3.txt", "content": "The first part of today‚Äôs lecture is a review, just in case you missed something earlier. Even if you understood it the first time, it‚Äôs always good to refresh these key concepts.\n\nThe key point is to model and understand T1 and T2, which are critical relaxation parameters. Along with these, proton density is also an essential factor, and fortunately, that one is fairly easy to understand. Think of it this way: you have a collection of small magnets, whose density is proportional to proton density, and together they form a large magnetization vector, usually called the big M vector. Once you begin to manipulate this M vector, you immediately see the effects of T1 and T2.\n\nT1 and T2 are intrinsic properties of tissues. They vary with tissue type, and they are very important for functional and even molecular imaging. You need to understand how the MRI signal changes under the influence of T1 and T2, and how these quantities can be measured. When you do this properly, you can create multi-contrast MRI images, such as proton-density-weighted, T1-weighted, and T2-weighted images.\n\nThis is much more complicated than what we encounter in X-ray imaging. In X-ray, the main parameter is the linear attenuation coefficient. Even though this coefficient is energy-dependent, it still gives us a straightforward measure: it tells us how many photons, or what fraction of photons, will be attenuated when an X-ray beam passes through a given thickness of a given tissue type.\n\nBy contrast, T1 and T2 are not so simple. They require more imagination‚Äîespecially geometrical imagination‚Äîto grasp.\nNow, the remaining two parts of today‚Äôs lecture‚Äîslice selection and sampling in k-space‚Äîare where MRI becomes truly powerful. These steps allow us to extract MRI signals from the sample, or from the patient, in a spatially specific way. That‚Äôs how we generate cross-sectional or even volumetric images of the human body.\n\nIn summary: today we will begin by revisiting T1 and T2 modeling and measurement, and then move on to slice selection and k-space sampling. These latter two are really the emphasis of this lecture. In the previous session, we talked about MRI imaging principles. Now we will extend that foundation and see how it translates into actual imaging techniques.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 19 - Slide4.txt", "file_path": "Lecture 19\\Texts\\Slide4.txt", "content": "So, T1 and T2 relaxation, this is kind of a review. T1 and T2 are quite different.\n\nT1: We talk about the recovery of the M vector along the z-axis ‚Äî that‚Äôs the main field direction. T1 time is defined as sixty-three percent recovery of our original M vector, which we call M-naught. This is about spin‚Äìlattice interaction, something that involves energy.\nWhen you turn the M vector into a certain direction, like a ninety-degree direction, the energy you used ‚Äî injected into the tissue ‚Äî will eventually be lost due to spin‚Äìlattice interaction. And it will gradually return to the steady state. The M vector wants to be aligned along the main field direction, which is the B-naught field, or the z direction.\n\nOn the other hand, the T2 parameter talks about the dephasing effect. The M vector is not just an individual thing ‚Äî it‚Äôs really a collective phenomenon. All of the small magnets, the spinning protons, are very small magnets. Altogether, they form the big M vector.\nIf you flip the M vector, say by ninety degrees, from the z direction into the y or x direction, this big M vector at first points all in one direction. But it is made of many small magnets, and some of those spins are faster, some are slower, because of inhomogeneity in the B-naught field or in the local field. The local field ‚Äî ions, molecules, molecular motion ‚Äî all these things contribute to that inhomogeneity.\n\nOriginally, all the small magnets line up along the x-axis. If you apply a ninety-degree pulse with respect to the y-axis, you flip the M vector along the x-axis. But with time, this will dephase. Some processes are a little faster, some slower. And after a long enough time, all these small magnets, or spinning protons, are pointing in different directions.\n\nThe overall field is no longer this original M-naught. Instead, it becomes effectively zero. The decay is modeled as an exponential curve ‚Äî something like what you see on the slide. So, this is recovery, and this is decay. That is the T1, T2 story.\nLet me give you a little more mathematical detail so you understand why both the recovery and the dephasing follow an exponential curve. One is growing, the other is decaying. It‚Äôs something like this.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 19 - Slide5.txt", "file_path": "Lecture 19\\Texts\\Slide5.txt", "content": "So, the change rate, whether it is growth or decay, is described by an ordinary first-order differential equation. The first derivative ‚Äî the change rate ‚Äî is proportional to the original quantity.\n\nWhen we talk about the change of y, the rate of change is proportional to y. You keep seeing this kind of first-order differential equation in multiple imaging modalities. For example, in X-ray imaging, you have the initial beam intensity. It will be attenuated. The amount of attenuation ‚Äî the change of the X-ray flux ‚Äî is proportional to the initial flux, which we call I-naught.\n\nSimilarly, here in MRI, with y, we are talking about the initial M vector, which is M-naught. So we have this relationship.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 19 - Slide6.txt", "file_path": "Lecture 19\\Texts\\Slide6.txt", "content": "Whenever you have a governing relationship like this, you can go through some mathematical steps.\n\nWith the first derivative like this, you just rearrange it a little bit. So you have:\u000b(one over y) d-y equals k d-t.\nThen, integrate both sides of the equation. On the left, you get ln absolute y, and on the right, you get k t plus C.\n\nNow, exponentiate both sides. That gives:\u000by equals A e to the k t, where A is a constant.\n\nThis shows that whenever the process is described by a first-order differential equation, the solution must be of this form: y is exponential growth or exponential decay. In other words, a constant multiplied by an exponential function, with index k t, where t is time.\n\nSo, this explains why you see exponential decay, exponential growth, or attenuation in earlier imaging modalities like X-ray imaging, and why you see the same behavior here in MRI.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 19 - Slide7.txt", "file_path": "Lecture 19\\Texts\\Slide7.txt", "content": "Now, for T2, we need to look in more detail. T2 has two components, and it describes dephasing. The first dephasing is what we call the pure T2 effect, or spin‚Äìspin interaction. This happens because all the spinning protons are close to each other, and you do not have an absolutely uniform distribution. If you look microscopically, the spins interact with one another, and there are fluctuations in the local magnetic field. These effects are called pure T2, shown here in red, and they are due to spin‚Äìspin interaction. This behavior in soft tissue is physiological, and it cannot really be changed ‚Äî it is a natural biological phenomenon.\n\nOn the other hand, when you place a patient, or even a small animal, in an external magnetic field ‚Äî the B-naught field ‚Äî we usually describe that field as uniform. But in reality, the B-naught field is never perfectly homogeneous. There will always be some inhomogeneity. This inhomogeneity is fixed: when you set up the B-naught field, some regions are slightly weaker, while other regions are slightly stronger. So, in one location, the spins process a little slower, and in another location, they process a little faster. That difference also contributes to dephasing.\n\nSo, both biological spin‚Äìspin interaction inside the tissue, and physical inhomogeneity of the external magnetic field, contribute to the same dephasing effect, but for different reasons. The external contribution is fixed and constant, while the biological part cannot really be corrected.\nWhen we combine these two, we call the result T2 star. The external field inhomogeneity alone is called T2 plus. So, in summary: T2 star is the combined effect of external field inhomogeneity and biological spin‚Äìspin interaction. And importantly, there is a reciprocal relationship between them.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 19 - Slide8.txt", "file_path": "Lecture 19\\Texts\\Slide8.txt", "content": "Let‚Äôs explain the reciprocal relationship. We start with a constant, K. This K describes the strength of dephasing. If dephasing is due to spin‚Äìspin interaction, we can call it K1. If dephasing is due to external magnetic field inhomogeneity, we can call it K2. So, the total rate of dephasing is due to both K1 plus K2. Then, the time constant is simply the reciprocal of that total: one over K1 plus K2. That‚Äôs where the reciprocal relationship comes in.\n\nIf you still feel unclear, let‚Äôs connect it to something we already studied: nuclear imaging. When we studied nuclear decay, we said that a radioactive tracer inside the body disappears through two processes. One is physical radioactive decay, which happens naturally over time. The other is biological clearance, where the tracer leaves the body through circulation, mainly through urination. So, there are two mechanisms removing the tracer ‚Äî one biological and the other physical ‚Äî and together, they follow the same reciprocal rule.\n\nIn that case, we use the decay constant, lambda. The overall decay rate is lambda-one plus lambda-two, and the corresponding half-life is the reciprocal of lambda. So, just like in nuclear decay, where both physical and biological processes add together, in MRI, both spin‚Äìspin interaction and external inhomogeneity add together. Don‚Äôt be confused if you see these different equations ‚Äî equation two-point-four for nuclear medicine and equation four-point-two-nine for MRI ‚Äî because mathematically, they are describing the same idea.\n\nTo summarize: when we talk about T2, we really mean three different things. One is the biological T2, or pure spin‚Äìspin relaxation. The second is T2 plus, from external field inhomogeneity. And the third is T2 star, the combined effect of both. Now we have T1 and T2. The question is: how do we measure them?", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 19 - Slide9.txt", "file_path": "Lecture 19\\Texts\\Slide9.txt", "content": "To measure T1, we often use a technique called inversion recovery. Here‚Äôs how it works. First, we apply a one-hundred-eighty-degree pulse. This flips the magnetization vector completely upside down. At this moment, there are no transverse components. Remember, only transverse components induce a signal in the nearby coil. Without a transverse component, if you place a coil close to the patient, there is no signal to measure.\n\nNow, after a time delay, tau, we apply a ninety-degree pulse. This rotates some of the magnetization into the transverse plane. And once that happens, the coil can detect a signal. This is the signal you see here. Notice that the signal itself also decays. Why? Because of dephasing. And ask yourself ‚Äî is this dephasing due to T2, or due to T2 star? The answer is T2 star, because it includes both spin‚Äìspin interaction and the effect of field inhomogeneity.\n\nSo, this is the inversion recovery method: a one-eighty-degree pulse, followed by a time delay, and then a ninety-degree pulse that produces a measurable signal. The key question we then ask is, what is the amplitude of that signal?", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 19 - Slide10.txt", "file_path": "Lecture 19\\Texts\\Slide10.txt", "content": "This really depends on tau, the delay time, before you apply the ninety-degree pulse. And this is illustrated here.\n\nLet‚Äôs review carefully. First, you apply a one-hundred-eighty-degree pulse, and the magnetization vector M-naught becomes minus M-naught. If you don‚Äôt do anything, this remains negative. Originally, you had a positive M-naught, but after the one-eighty pulse, it became negative M-naught. This negative magnetization will gradually recover back toward the original M-naught, returning to the normal, stable equilibrium. If you wait long enough, this negative value will rise, reach zero, and then continue climbing until it becomes fully positive again.\n\nSo it all depends on tau ‚Äî the delay. At first, the amplitude is negative, then it grows smaller in magnitude, reaches zero, and then becomes positive, growing larger and larger toward M-naught. Now, if at some point during this recovery you apply a ninety-degree pulse, flipping it into the transverse plane ‚Äî along the y-axis ‚Äî you create a transverse component, and that can be measured. At this moment, you will have a signal.\n\nBut this signal is subject to T2 star decay, so what you measure is a free induction decay, or F-I-D signal. By repeating this process with different delay times, different values of tau, you can measure the signal amplitude as a function of tau-n. Then, you can plot this relationship. By arranging the equation properly, you get an exponential form with an index of minus tau-n divided by T1. If you take the logarithm, you can obtain a straight line. The slope of that line is minus one over T1, and from this slope, you can determine the T1 relaxation time.\n\nNow, be careful: if you directly take the log, you will not immediately get T1. You need to rearrange the equation. But one important point is that M-naught is known, because you can measure it directly. That‚Äôs why this works.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 19 - Slide11.txt", "file_path": "Lecture 19\\Texts\\Slide11.txt", "content": "Now, how do we measure T2? The most important and representative pulse sequence is the spin echo. Spin echo is a very clever design. It allows us to measure T2 directly. If you don‚Äôt use spin echo, the signal decay you observe will be governed by T2 star, which includes both biological effects and external field inhomogeneities. But what we want is the pure T2, which reflects tissue physiology and pathology.\n\nPure T2 is a biological effect, while T2 star is dominated by external field imperfections. Since your patient‚Äôs physiological status should not depend on imperfections in the external field, we really want to measure T2.\n\nSo how do we do it? Spin echo. First, you let the spins dephase for a short time. Then you apply a one-hundred-eighty-degree pulse. What happens? Some spins are lagging ‚Äî they have a phase delay. Other spins are moving faster ‚Äî they have an advanced phase. After the one-eighty-degree flip, the lagging spins continue to lag, and the faster spins continue to advance. But because their positions have been reversed by the flip, they will eventually meet again and refocus at a certain point.\n\nAt that refocusing point, the signal strength grows again, giving you the spin echo. This is a brilliant design. And in fact, you can repeat it multiple times for more echoes if you like.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 19 - Slide12.txt", "file_path": "Lecture 19\\Texts\\Slide12.txt", "content": "So here‚Äôs another way to see the spin echo.\n\nYou start with the magnetization vector. Apply a ninety-degree pulse, and now you have transverse components. These transverse components begin to dephase ‚Äî some spins are slower, shown in red, and some spins are faster, shown in black.\n\nNext, you apply a one-hundred-eighty-degree pulse. This instantly flips the distribution. The red, slower spins are moved to the position where the black, faster spins were, and the black spins take the place of the red ones. But remember, the red spins are inherently slower. So even though they were moved ahead, they continue to process slowly. The black spins are inherently faster, so even though they were moved behind, they continue to process quickly.\n\nNow, after the same amount of time passes, both groups realign again. The faster black spins catch up, and the slower red spins fall into place. At that moment, all the spins point together again, aligned in the same direction.\n\nBecause they are all aligned, their contributions add up constructively, and the transverse magnetization reaches its maximum. This produces a measurable signal in the nearby coils ‚Äî the spin echo signal.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 19 - Slide13.txt", "file_path": "Lecture 19\\Texts\\Slide13.txt", "content": "Here‚Äôs an even simpler way to explain spin echo. Imagine you have a group of students lining up at a race track. At the starting point, they are all together, right at the start line. After the signal, they all run toward the finish line. Some students are faster, and some are slower. So after a certain time, they will have covered different distances.\n\nNow, at that moment, we ask them all to turn around and keep running, but in the opposite direction. The faster runners, who had gone farther ahead, now have a longer distance to come back. The slower runners, who didn‚Äôt go as far, have a shorter distance to cover. But because they are slower, their speed matches the distance they need to make up.\n\nAfter the same amount of time, both the fast and slow runners arrive back at the starting line together. They all realign again. This is the basic idea behind spin echo. By using this trick, we can measure T1, the pure T2, without the interference of the external field inhomogeneity.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 19 - Slide14.txt", "file_path": "Lecture 19\\Texts\\Slide14.txt", "content": "So this spin echo sequence is a very clever design. And from it, we get two very important parameters.\n\nThe first one is the echo time, which we write as T-E. This is defined as the time from the ninety-degree pulse to the echo peak. In practice, you apply a ninety-degree pulse, then a one-hundred-eighty-degree pulse, and the time from the ninety-degree to the one-eighty is T-E divided by two. Then, after the one-eighty pulse, you wait another T-E divided by two, and that‚Äôs when the spin echo appears.\nThe spin echo amplitude reflects the decay due to pure T-two, independent of external field inhomogeneity. That‚Äôs the significance of T-E.\n\nThe second parameter is the repetition time, or T-R. After the first sequence, you can wait for a time T-R, and then repeat the same trick, ninety degrees, one-eighty degrees, another spin echo. But in order for the system to be in a stable state, T-R has to be long enough so that the magnetization vector M-naught has fully recovered.\n\nIf, for example, T-R is chosen as two thousand milliseconds, that may not be long enough for full recovery. In that case, the longitudinal magnetization is still less than the original M-naught, and the signal will be reduced.\nSo this is why T-E and T-R are significant. By definition, T-E should always be less than T-R.\n\nNow, this also connects to imaging. At the top, when we look at the spin echo for the whole sample, we get one signal that represents the entire patient or sample as a whole. But in imaging, we want more than that. We want to know the values of T-one and T-two for each pixel or voxel, not just the whole object. That requires us to encode spatial information. We‚Äôll get into that starting with the next slide.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 19 - Slide15.txt", "file_path": "Lecture 19\\Texts\\Slide15.txt", "content": "So now we move to the next part: being spatially specific.\nIn MRI, we don‚Äôt just want a signal from the whole sample. We want to isolate a particular slice. Not every layer, but just one slice. And not only that ‚Äî within that slice, we also want to know the signal from each location.\n\nThat means we need to encode each pixel in a way that makes its signal slightly different from every other pixel. Only then can we reconstruct an image that shows the T1 and T2 values for each voxel.\nThe first step in this process is slice selection. We begin by defining which slice we want to image.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 19 - Slide16.txt", "file_path": "Lecture 19\\Texts\\Slide16.txt", "content": "So this is about the processional frequency on B. In the textbook, we actually mentioned this in the previous lecture..\n\nBut let‚Äôs focus on the conclusion. The derivation goes through all these steps, and if you take the time, you will understand the details.\nThe key point is this: the processional frequency is proportional to the local magnetic field. That means if the overall field is stronger, the M vector ‚Äî or you can think of it as a spinning top ‚Äî will process faster.\n\nSo just follow the picture here. The last line is very clear: the angular frequency of precession, omega, is proportional to the local magnetic field. If we call the main field B-naught, then omega is proportional to B-naught. Later on, if you change B-naught to B-prime, omega will be proportional to B-prime.\nAnd then you multiply by a physical constant, called gamma.\n\n So, in short, omega equals gamma times B.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 19 - Slide17.txt", "file_path": "Lecture 19\\Texts\\Slide17.txt", "content": "Now, if you want to explore a little more detail, you can check the derivation for this mechanical precession of motion.\nIn this case, the angular frequency is proportional to the gravitational force. I‚Äôve marked it in green on the slide. This is basically a physics analogy using a spinning top and torque.\n\nIf you‚Äôre interested, you can read it. Otherwise, don‚Äôt worry too much ‚Äî it‚Äôs not essential for our discussion of MRI.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 19 - Slide18.txt", "file_path": "Lecture 19\\Texts\\Slide18.txt", "content": "Now here comes the great idea ‚Äî introducing a gradient field on top of the background field B-naught. This was such an important idea that it led to a Nobel Prize.\nSo why is this so important? Because this is the key step that allows us to encode spatial information.\n\nIn the main field B-naught, the processional frequency of all spins would be the same. That would not allow us to distinguish one location from another.\nBut if we apply a gradient field, we change the situation. The magnetic field strength will now depend on position. For example, the larger the z-coordinate, the stronger the field. As a result, the processional frequency will also depend on position.\n\nSo now, instead of all spins having the same frequency, their frequency varies with location. At z equals zero, the frequency is the original B-naught frequency. But away from zero, it becomes stronger or weaker depending on the gradient.\nThis is the concept of a field gradient.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 19 - Slide19.txt", "file_path": "Lecture 19\\Texts\\Slide19.txt", "content": "Once we have a field gradient, we can apply an external rotating magnetic field to push the M vector at its resonant frequency ‚Äî that is, its processional frequency.\n\nNow think back to the analogy from the previous lecture: remember the photograph of a lady pushing a rotating door? That‚Äôs the same idea. You have to push the door at just the right time to keep it moving smoothly. Similarly, you have to push the spins at just the right frequency to change their orientation.\nAnd even earlier, we watched a short video where the instructor explained how an MRI works in less than ten minutes. He said you simply send in an RF frequency signal. That RF signal, or RF pulse, is a form of electromagnetic wave. And electromagnetic waves are oscillating magnetic fields.\n\nSo when you send in an RF pulse whose frequency is in resonance with the processional frequency of the spins, you are effectively pushing those tiny spinning tops ‚Äî the protons ‚Äî in a coordinated way. With each push, you tilt the magnetization vector a little more, and eventually you flip the M vector from its original orientation along the z-axis into one of the horizontal axes, like the x-axis or y-axis.\n\nNow, here‚Äôs the key: you don‚Äôt want to flip every proton in the body. You only want to flip the spins in a specific slice of tissue. To do that, you must use both the gradient field and the bandwidth of your RF pulse together.\nThe frequency range of the RF pulse is centered at some frequency, call it omega-s, and it spans from omega-s minus delta-omega-s to omega-s plus delta-omega-s. So it has a certain bandwidth.\n\nIn the time domain, this looks like a rectangular pulse ‚Äî a signal that is ‚Äúon‚Äù for a finite duration. When you take the Fourier transform, that rectangular pulse in the time domain becomes a sinc-shaped function in the frequency domain. So, in practice, even though you are sending an RF signal in the time domain that looks like a sinc function, in the frequency domain it corresponds to a rectangular function.\n\nThis rectangular frequency band means that only those spins whose processional frequencies fall within that band will be excited. Now recall, because of the gradient field, the processional frequency depends on z-location. So different positions along the z-axis correspond to different resonance frequencies.\nTherefore, when you send in your RF pulse with its defined bandwidth, only the spins whose z-positions correspond to that frequency band will resonate and be flipped. Spins outside that band will not be affected ‚Äî they simply won‚Äôt respond, because the RF pulse is not in resonance with them.\n\nSo, the slice thickness directly corresponds to the frequency range of the RF pulse under the influence of the gradient field. If you make the bandwidth larger, the slice thickness increases. If you make the bandwidth smaller, the slice thickness decreases.\nTo summarize: you apply a gradient field to make the processional frequency vary with z-location. Then you send in an RF pulse with a defined frequency band. Only those spins in the matching z-range ‚Äî that slice ‚Äî are excited and flipped. Everything outside remains untouched. That is how slice selection in MRI works", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 19 - Slide20.txt", "file_path": "Lecture 19\\Texts\\Slide20.txt", "content": "Graphically, we can show this as you see on the slide.\nThe gradient field creates a range of resonance frequencies along the z-axis. When you send an RF pulse with a defined frequency range, only the spins within that frequency range ‚Äî and therefore within a specific z-range ‚Äî are excited.\n\nThat z-range corresponds to the slice thickness. So this is how we select a slice in MRI.\nWhat we‚Äôve explained here is the principle for slice selection along the z-axis.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 19 - Slide21.txt", "file_path": "Lecture 19\\Texts\\Slide21.txt", "content": "Now, let‚Äôs look at slice selection in more detail. In your textbook, you‚Äôll find this figure, labeled as 4.16. Here, you see that slice selection can be performed using gradients along the x-axis, the y-axis, or the z-axis. That means you can select a coronal slice, an axial slice, or a sagittal slice, depending on which gradient you apply.\nAnd the beauty of MRI is that you are not limited to just these orthogonal planes. With the right gradient combinations, you can select slices at any orientation you like, even oblique slices at odd angles. This is one of the great flexibilities of MRI imaging.\n\nNow let‚Äôs think about why this matters. Before we introduced gradient fields, all we could do was measure T1 and T2 signals from the whole sample ‚Äî essentially, we had no spatial specificity. We were only getting the overall relaxation properties averaged over the entire tissue volume. But now, with slice selection, we can be much more specific.\n\nWe can tell the scanner: ‚ÄúExcite this slice only.‚Äù That means the signals we collect come from just that defined region. All the other spins, those outside of the slice ‚Äî whether they‚Äôre above, below, or to the side ‚Äî will not contribute. Because remember, resonance is only defined for those spins whose frequencies match the selected RF bandwidth, which corresponds to the gradient-defined slice.\n\nSo in practice, the signals from outside the slice, those spins located elsewhere, remain silent. Only the spins inside that slice range are excited and measured. This is the principle of slice selection in MRI.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 19 - Slide22.txt", "file_path": "Lecture 19\\Texts\\Slide22.txt", "content": "Now, there is an important technical subtlety about slice selection that we need to address.\nWhen we apply a gradient field, by definition, it is not homogeneous. Instead, it varies linearly across space. But remember, any deviation from a homogeneous magnetic field introduces a problem: it causes dephasing.\n\nLet‚Äôs break this down. On the right-hand side of the diagram, you see the gradient field. On one side, the magnetic field is a little stronger. On the other side, it is a little weaker. Stronger fields correspond to faster precession. Weaker fields correspond to slower precession.\nSo when you look across the entire slice, the spins are no longer processing together in sync. Some spins are ahead, some are behind. If you just add all these signals together, they partially cancel each other out. The result is a substantial reduction in signal strength. That‚Äôs the dephasing problem.\nSo how do we fix it? We use what‚Äôs called a rephasing gradient.\n\nHere‚Äôs the idea: within the slice, after you‚Äôve applied the slice-select gradient, the spins have accumulated different phases. That is described mathematically in your textbook by equation 4.41. The phase shift depends on the z-position and on the gradient strength.\nBut notice the formula includes tau over 2, not tau. Why is it tau divided by 2? Well, think about it this way: during the full pulse duration tau, the spins at the top of the slice accumulate the maximum phase, and the spins at the bottom accumulate the opposite phase. The middle spins are only halfway through. So if you take the average phase across the slice, it corresponds to tau divided by 2. That‚Äôs why this factor appears.\n\nNow, to correct for this unwanted dephasing, we apply a second gradient ‚Äî but this time with opposite polarity. We call it the rephasing gradient. It lasts for a short time, tau-ref, and its strength is chosen so that the overall effect cancels the earlier phase spread.\nMathematically, the product of gradient strength and time for this rephasing pulse must equal tau over 2 times the original gradient strength. That‚Äôs equation 4.42 in your textbook.\nThe result is that the spins, which had drifted apart in phase, are brought back into alignment. This cancels out the artificial dephasing caused by the slice-selection gradient itself.\n\nSo now, within the slice, you‚Äôre left with a clean signal. The signal strength, mathematically, is proportional to the number of spins inside the slice. That‚Äôs why the formula is written as a double integral over x and y within the slice. It represents the total proton density summed across the slice area.\nThis is a big improvement. Without slice selection, we had a triple integral over x, y, and z ‚Äî the entire volume. But now, we‚Äôve reduced it to just two dimensions, a double integral. This means our signal comes only from the selected slice, not the whole volume.\n\nThat‚Äôs a huge step forward. But it‚Äôs still not the final goal. Because in imaging, we don‚Äôt just want the sum over the entire slice. We want to know the signal at each individual pixel or voxel. We want to resolve spatial detail ‚Äî what is the proton density, the T1, or the T2 value at each location. That‚Äôs what we‚Äôll cover in the next part.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 19 - Slide23.txt", "file_path": "Lecture 19\\Texts\\Slide23.txt", "content": "Now we move to the next major step: sampling in k-space.\n\nThis is probably the most important concept in MRI. And I‚Äôll be honest, it‚Äôs also one of the trickiest. So I strongly encourage you to review this multiple times, both in the textbook and in these lectures.\nThe idea is this: when we add gradient encoding ‚Äî both phase encoding and frequency encoding ‚Äî the MRI signal we measure naturally corresponds to a Fourier transform of the object we‚Äôre imaging.\n\nSo the scanner doesn‚Äôt measure an image directly. Instead, it measures data in k-space, which is essentially the Fourier space of the image. Then, by applying the Fourier transform, we reconstruct the actual image in spatial coordinates.\nThis concept is formalized in what we call the k-space theorem. And as you‚Äôll see, the k-space theorem in MRI is directly analogous to the Fourier slice theorem we studied earlier for X-ray computed tomography.\n\nSo, in summary: slice selection gave us spatial specificity along one axis. Rephasing gradients fixed the signal within that slice. And now, sampling in k-space gives us the ability to encode and reconstruct every pixel inside the slice. This is the final piece that allows us to turn raw MR signals into detailed images.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 19 - Slide24.txt", "file_path": "Lecture 19\\Texts\\Slide24.txt", "content": "Up to this point, we‚Äôve been talking about signals coming from a whole slice. But instead of just getting a collective signal from all the protons in the selected slice, our goal is to resolve individual locations inside that slice.\n\nWe want to know: what is the proton density, often called rho, at a given pixel location? What is the T1 relaxation time for that pixel? What is the T2 relaxation time for that pixel? Once you know all this information, pixel by pixel, you can construct a cross-sectional image. And that is the true purpose of tomographic imaging.\n\nSo here‚Äôs the challenge. Let‚Äôs pause for a moment and think carefully. We have an equation ‚Äî you can see it here ‚Äî which tells us that the signal is proportional to the sum over all spins inside the slice. That‚Äôs already an improvement. It‚Äôs much more specific than before, when we were averaging over the whole volume. Now, the signal comes only from one slice.\n\nBut here‚Äôs the puzzle: how can we take this equation, which only tells us the total signal from the slice, and somehow extract the spatial information pixel by pixel? How can we single out, for example, the value of rho, or T1, or T2, at a particular coordinate ‚Äî say, x naught, y naught?\nI encourage you to really think about this problem. Imagine you are designing the experiment yourself. How would you go about resolving the spatial information? \n\nHow would you separate the contributions from different locations within the slice?\nThere are actually multiple ways to approach this. The method I‚Äôm going to explain to you is very elegant and very efficient, but it‚Äôs not the only possibility. So think independently: if you were faced with this challenge, how would you do it?\n\nThe solution we use in MRI is to introduce phase encoding and frequency encoding. These tricks allow us to separate the signals spatially. When you see how it works, you‚Äôll realize it‚Äôs both clever and practical. And that‚Äôs what we‚Äôll now begin to explain.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 19 - Slide25.txt", "file_path": "Lecture 19\\Texts\\Slide25.txt", "content": "Let‚Äôs start with phase encoding.\n\nSuppose you do nothing special ‚Äî what happens? All the signals from the slice come out at the same frequency, and they just add together. You can‚Äôt tell which signal came from which location. It‚Äôs like hearing a classroom of students speaking at once, all in the same pitch. You know there are many voices, but you can‚Äôt separate them.\nSo how do we make progress? We apply a gradient encoding along one direction of the slice. This gradient is turned on for a certain amount of time. What happens then?\n\nAt the beginning, before the gradient is applied, all the spins are in phase. But once the gradient is turned on, the magnetic field is no longer the same everywhere. At one location, say at the top, the field is a little weaker. At another location, maybe lower down, the field is stronger.\nBecause precession frequency depends on field strength, the spins at different locations accumulate different amounts of phase shift over that same time interval.\nWhere the field is weak, the spins process more slowly.\nWhere the field is strong, the spins process more quickly.\nAnd since the gradient is linear, the phase difference grows proportionally with position.\n\nSo, after the gradient pulse, spins at the top may have accumulated only a small phase change, while spins at the bottom may have accumulated a large phase change, maybe even 180 degrees.\nNow, when you collect the signal, you know that spins with little or no phase shift must have come from one end of the slice, and spins with larger phase shifts must have come from the other end.\n\nThis is the essence of phase encoding. You haven‚Äôt fully reconstructed the image yet, but you now have row-wise spatial information. It‚Äôs like being able to say, ‚ÄúThat sound came from the front row, or the middle row, or the back row,‚Äù even if you don‚Äôt yet know exactly which student was speaking.\nPhase encoding is the first step toward resolving spatial information inside the slice.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 19 - Slide26.txt", "file_path": "Lecture 19\\Texts\\Slide26.txt", "content": "Now let‚Äôs connect this with the equations from your textbook.\n\nAs explained in the book, the phase factor is determined by the local processional angular frequency and the total time for which the phase-encoding gradient is applied. Mathematically, it‚Äôs the product of frequency and time. But remember, the local processional frequency is proportional to the local magnetic field. And the local magnetic field is not uniform ‚Äî it‚Äôs linearly changing because of the applied gradient.\n\nSo we can write the local field as gamma times G y, times y. That is, the gradient strength G y, multiplied by the coordinate y, times the gyromagnetic ratio gamma.\nThis means the accumulated phase factor depends directly on position y. Spins at different y locations accumulate different phase shifts during the same encoding time. So after phase encoding, if you collect the signal, it still comes from the same slice, but now it‚Äôs modulated by this spatially dependent phase factor. That gives you extra information.\n\nTo put it simply, now you can resolve the signal line by line across the slice. You‚Äôre no longer dealing with one lumped signal. Instead, you can tell which ‚Äúrow‚Äù in the slice contributed to the measured data. Of course, this is not yet a full tomographic image. What we want is pixel-wise resolution ‚Äî knowing the value at every point in the slice. Phase encoding alone only gives us line-by-line information. To complete the picture, we‚Äôll need to add frequency encoding on top of it. That‚Äôs what we‚Äôll discuss next.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 19 - Slide27.txt", "file_path": "Lecture 19\\Texts\\Slide27.txt", "content": "The next important idea in MRI is called frequency encoding. After we finish phase encoding, we do not immediately record any signal. At that stage, the information is already encoded into the spins as phase shifts, but the signal is not read out yet. The next step is to apply what we call the frequency-encoding gradient, often denoted as G-sub-s, or simply the readout gradient. Once this gradient is applied, we begin recording the MR signal.\n\nWhat happens here is that the local magnetic field is no longer homogeneous. Instead, because of the gradient, the magnetic field strength changes linearly across the imaging plane. On the right-hand side, the field is stronger, and on the left-hand side, the field is weaker. This change in field strength directly affects the processional frequency of the spins. Spins that sit in regions of higher field will process faster, while those in lower field regions will process more slowly. This is a very clever way to extract spatial information from the signal.\n\nNow, when we measure the signal, if we see a component oscillating at a very high frequency, we know that signal must have come from spins located on the right-hand side of the image, where the field is strongest. Conversely, if the signal oscillates at a lower frequency, we know it must have come from the left-hand side, where the field is weaker. In other words, the frequency of the signal directly encodes the spatial position of the spins along the direction of the gradient.\n\nSo, the field is no longer uniform across the entire plane. Instead, the gradient introduces a mapping between position and frequency. The faster oscillations in the measured signal correspond to spins located toward the right side, while the slower oscillations correspond to spins on the left side. All of the low-frequency signals come from the left-hand side of the slice, and all of the high-frequency signals come from the right-hand side. By combining this frequency information with the phase information we already stored in the earlier phase-encoding step, we can now determine not only which row the signal came from, but also which column. In this way, we can resolve the precise pixel location of the signal within the slice.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 19 - Slide28.txt", "file_path": "Lecture 19\\Texts\\Slide28.txt", "content": "Now let‚Äôs put everything together. The phase angle that you see here carries important spatial meaning. Spins at the top of the slice accumulate a smaller phase angle, while spins at the bottom accumulate a larger phase angle. So when you combine phase encoding with frequency encoding, you can start to separate the signals point by point within the slice. This is the basic idea of how we move from bulk signals to localized, pixel-specific information.\n\nYou may wonder, why do we need two more gradient fields? At the very beginning, we already used a slice-selection gradient ‚Äî let‚Äôs say along the z-direction ‚Äî to pick out one slice from the entire volume. But that only gives us localization in one dimension. To fully resolve the slice, we need to further encode along the other two directions: the y-direction for phase encoding, and the x-direction for frequency encoding. By applying gradients step by step, we gradually eliminate one dimension at a time. Slice selection removes the z-dimension, phase encoding handles the y-dimension, and frequency encoding takes care of the x-dimension. The end result is three-dimensional, point-specific localization of the MR signal. This is the foundation of tomographic imaging.\n\nMathematically, we can describe this process using the equations shown here. If we only apply frequency encoding without phase encoding, the acquired signal is expressed as an integral over the slice that includes a factor of ‚Äúe to the minus j gamma G-x times x times t.‚Äù This captures spatial frequency information along the x-direction. But when we combine both phase encoding and frequency encoding, the signal equation becomes the one you see at the bottom of the slide. Here, you have an additional factor of ‚Äúe to the minus j gamma G-y times y times tau p-e.‚Äù This accounts for the phase encoding gradient applied in the y-direction for a certain time tau p-e.\n\nNow, looking at this combined equation, you can see it resembles a Fourier transformation. You have terms in both the x and y directions ‚Äî frequency terms in x, and phase terms in y. Together, they allow us to map spatial coordinates into measurable signal space. This is the mathematical backbone of MRI imaging. We will go deeper into this Fourier interpretation later in the lecture. But for now, recognize that the combination of slice selection, phase encoding, and frequency encoding provides us with true point-wise information. Graphically, as shown here, this is exactly how the imaging sequence works.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 19 - Slide29.txt", "file_path": "Lecture 19\\Texts\\Slide29.txt", "content": "So here we come to the full sequence that combines slice selection, phase encoding, and frequency encoding. The process begins with a ninety-degree pulse. This radiofrequency pulse flips the magnetization vector, the big M vector, into the x-y plane. Once it is in the x-y plane, the transverse components begin to process, and these processing components induce an alternating magnetic field around the patient or the sample. That alternating field is what we measure as the MR signal.\n\nNow, you want to make sure that you are not flipping every spin in the body. You only want to flip the spins within a single slice. That is where slice selection comes in. During the ninety-degree pulse, you apply a slice selection gradient. This gradient ensures that only the spins that are resonant within that slice frequency range are flipped into the x-y plane. Spins outside the slice are not flipped, and so they do not contribute to the signal. This is the first major step ‚Äî slice selection removes one dimension, narrowing the signal to a single slice.\n\nNext, you apply phase encoding. On the slide, you see these multiple horizontal lines under the label ‚ÄúG-phase.‚Äù This is because phase encoding must be repeated multiple times with different gradient strengths. You can start from a minimum value, then gradually increase step by step, covering a full range of gradient amplitudes. Each phase encoding step imprints a different spatially dependent phase shift onto the spins along the y-direction. Why do we need so many lines? The reason is the Fourier transformation. To fully reconstruct an image, we need information along two degrees of freedom ‚Äî one for the phase dimension and one for the frequency dimension. Each phase encoding line is like collecting one row of data in Fourier space, which we call k-space.\n\nAfter that, we apply frequency encoding, shown here as ‚ÄúG-freq.‚Äù Frequency encoding happens during the readout. With this gradient on, different spatial locations along the x-direction correspond to different precession frequencies. When you record the signal, you collect a series of oscillations that map into the x-axis of k-space. Each readout provides you with N data points along this horizontal axis.\n\nSo, putting everything together: for each phase encoding step, you collect one frequency-encoded readout with N data points. After repeating this process with many different phase encoding steps, you end up with a full grid of data ‚Äî N by N points. Each row comes from one phase encoding, and each column comes from the frequency encoding readout. Altogether, this maps the entire cross-section of the slice into k-space. Then, by applying a two-dimensional Fourier transform, you reconstruct the final image.\n\nThis is the essence of Fourier imaging, or what we call k-space formulation in MRI. Slice selection removes one dimension, phase encoding handles the y-dimension, and frequency encoding handles the x-dimension. The result is a complete two-dimensional image of the selected slice, pixel by pixel.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 30, "slide_filename": "Slide30.txt", "slide_annotation": "Lecture 19 - Slide30.txt", "file_path": "Lecture 19\\Texts\\Slide30.txt", "content": "So, if we call the whole term for frequency encoding, G x times t, simply k x, and we call the phase-encoding term k y, then the previous formula we derived can be rewritten in this k-space format. The signal is proportional to the factors k x times x and k y times y, and this immediately shows that the signal we measure is essentially a Fourier transformation. For any fixed time point during readout, the time t sets a specific value of k x, while k y is determined by the strength of the gradient G y and the duration of the phase-encoding step, tau p e. In other words, the frequency-encoding factor is traced out by letting time run during readout, which sweeps continuously along one line in k-space, while the phase encoding only gives one value of k y per line, and this is why we need multiple phase-encoding steps. By repeating the experiment with many different gradient strengths for G y, we can step through multiple values of k y, gradually filling the two-dimensional k-space. This is the basis of Fourier imaging.\n\nOn the slide, you can see the formal definitions written explicitly: k x equals gamma over two pi times G x times t, and k y equals gamma over two pi times G y times tau p e. Substituting these into the signal equation, we get that S of k x, k y is proportional to the double integral over the slice of rho of x comma y times e to the minus j two pi times open parenthesis k x x plus k y y close parenthesis d x d y. This is shown as equation four point four nine, and it is the central mathematical statement of Fourier MRI. Notice that in your textbook, the two pi factor is missing inside the exponential, and this is actually a common oversight in many texts, including mine and in my lectures. In reality, the two pi must be included to match the conventional definition of the Fourier transform, so keep that in mind as a small but important detail.\n\nThe essential point here is that, through phase encoding and frequency encoding, the data we measure in MRI are already Fourier components. This is very different from X-ray imaging, where the Fourier slice theorem has to be used: there, each projection profile must be one-dimensionally transformed to draw a radial line in Fourier space, and extra processing is needed. In MRI, the signals themselves directly populate k-space, which means the Fourier information is collected naturally as part of the measurement. Each phase-encoding step contributes to the vertical direction in k-space, and the continuous readout under the frequency-encoding gradient sweeps across the horizontal direction. By combining both, we directly fill k-space, and after applying the inverse Fourier transform, we reconstruct the final image.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 31, "slide_filename": "Slide31.txt", "slide_annotation": "Lecture 19 - Slide31.txt", "file_path": "Lecture 19\\Texts\\Slide31.txt", "content": "So now let‚Äôs talk about how the data are actually collected in the k-space. When time t equals zero, the k value is also zero, which means you are sitting right at the center of k-space. As time increases during the frequency encoding readout, the trajectory moves gradually toward the right-hand side along the k x direction. In this way, you begin to cover half of the Fourier domain. Now remember, for real-valued functions, the Fourier transform has a very useful symmetry property: if you only collect data from one half of k-space, the other half can be inferred mathematically. This symmetry reduces the amount of data we need, but in practice, MRI systems still often collect both halves to account for noise and to achieve cleaner reconstructions.\n\nThis diagram shows how the data lines are filled. Each horizontal line corresponds to a readout under the frequency-encoding gradient, which traces across k x. After finishing one line, you step the phase-encoding gradient to a new value, moving up or down in the k y direction, and then record another frequency-encoded line. So, for example, line one at the bottom corresponds to the maximum negative value of the phase-encoding gradient. Then you step gradually upward, line by line, until you reach the maximum positive value of k y. The spacing between these lines is delta k y, and the spacing along the frequency-encoding direction is delta k x. By repeating this process, you fill up the entire k-space grid point by point, line by line.\n\nThis is why we call it k-space: each coordinate, k x and k y, corresponds to a Fourier component of the image. Once the k-space is fully sampled, an inverse Fourier transform is applied, and that directly reconstructs the image in the spatial domain. So the essential idea is very simple but very powerful: MRI does not record the image directly. Instead, it records data in k-space, which is really the Fourier space, and then uses the inverse transform to bring everything back into an image we can see.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 32, "slide_filename": "Slide32.txt", "slide_annotation": "Lecture 19 - Slide32.txt", "file_path": "Lecture 19\\Texts\\Slide32.txt", "content": "And here you see another important point: full coverage in k-space. If we look at this figure, you can see how we can slightly modify the pulse sequence to make sure the entire k-space is covered symmetrically. Normally, during frequency encoding, you simply start your data collection right away. That means your measurement begins at time t equal to zero, so the k x trajectory also starts at zero and moves only into the positive side. This means, by default, you are not covering the negative part of k-space.\n\nSo how do we fix this? Before the regular frequency encoding and data acquisition, we apply what is called a dephasing gradient. This is shown here as G dephase. By applying this dephasing pulse, we effectively shift the starting point of the k x trajectory into the negative region. In other words, instead of starting at zero, all the frequency components now begin with a negative k x value. Then, as time increases during the readout, the trajectory sweeps across k-space and eventually moves into the positive side. By doing this, you cover the entire rectangular region of k-space, centered around the origin.\nThis is important because, according to the Nyquist sampling theorem, you need to cover the full bandwidth of frequencies to accurately reconstruct the image. If your sampling region is too narrow, you will miss some Fourier components, and that leads to aliasing or artifacts in the reconstructed image. By carefully choosing the dephasing and rephasing gradients, you make sure that this rectangular region in k-space is wide enough to include all of the significant Fourier components of the object.\n\nSo the essential idea here is that MRI imaging works in k-space, and by adjusting the gradient pulses ‚Äî dephasing first, then rephasing with frequency encoding ‚Äî you can guarantee full coverage. This is how the k-space theorem is applied in MRI. And remember, in our setup, phase encoding is along the vertical direction, while frequency encoding is along the horizontal direction. Together, they fill the k-space grid, ensuring that the reconstructed image contains all the spatial information of the object.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 33, "slide_filename": "Slide33.txt", "slide_annotation": "Lecture 19 - Slide33.txt", "file_path": "Lecture 19\\Texts\\Slide33.txt", "content": "Now let me explain this idea more generally. Here, tau, shown in green, is the time you spend applying the phase-encoding gradient. During this phase encoding, you introduce a phase difference across the spins, but you are not yet collecting any signal. After that, you let time t run, shown in red, and that is when you actually begin collecting the MR signal. This is the formulation with both phase encoding and frequency encoding included.\n\nWe first perform phase encoding longitudinally, then we do frequency encoding horizontally. But in fact, you can do this in different ways. You can apply gradients in both the vertical and horizontal directions simultaneously. When you do this, during the time tau, you move the starting phase point diagonally in k-space, depending on the relative strengths of the gradients. If you then start collecting signal immediately, the k-space trajectory begins from that shifted point. Alternatively, you might first move the starting point horizontally by applying one gradient, then switch to a vertical gradient and move to another position, and only then begin collecting the signal. Each different choice changes the trajectory you trace in k-space.\n\nFor example, if you use an x gradient for frequency encoding, then as time t increases from zero, the k x value changes continuously toward positive values. If instead you first apply a horizontal gradient as a phase-encoding step, the initial point shifts along the k x axis. Then, by applying a vertical gradient, you move the starting point vertically. Once you begin signal acquisition, you trace a horizontal or vertical line in k-space. Finally, if you apply both gradients at once and immediately collect the signal, you start at the origin and move along a straight line at a slope determined by the ratio of the two gradients.\n\nThe essential idea is that whenever you collect a signal with a gradient on, the frequency of precession is shifted, and that frequency determines the trajectory in k-space. If you apply a gradient without collecting a signal, the spins accumulate phase instead. By combining these options, you can navigate k-space in virtually any pattern. This flexibility is what allows MRI to encode spatial information and build an image slice by slice, pixel by pixel.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 34, "slide_filename": "Slide34.txt", "slide_annotation": "Lecture 19 - Slide34.txt", "file_path": "Lecture 19\\Texts\\Slide34.txt", "content": "At this point, you might be wondering ‚Äî have you understood this, or are you confused? Honestly, if you have never studied this carefully before, it is very natural to be confused. Even for me, when I first studied this, I had to go through it several times, very carefully, before I felt I understood it clearly.\n\nSo let‚Äôs check ourselves. If you basically follow the logical flow so far, you can feel confident. If you are quite confused, that‚Äôs also expected ‚Äî because these concepts are tricky and require repeated review. If you‚Äôre somewhere in between, that‚Äôs normal too. The important thing is not to get discouraged. What I will do now is go over the same ideas again, but from a slightly different angle, with more detail and more illustrations, so that the picture becomes clearer.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 35, "slide_filename": "Slide35.txt", "slide_annotation": "Lecture 19 - Slide35.txt", "file_path": "Lecture 19\\Texts\\Slide35.txt", "content": "Now let‚Äôs revisit spin echo with another illustration. Spin echo is a very clever but also highly simplified signal model. In this figure, you can see the sequence step by step. You begin with the magnetization vector aligned along the z-axis. Then you apply a 90-degree pulse, which flips the vector into the transverse plane ‚Äî here shown along the y-axis, though it could just as well be flipped along the x-axis depending on how the pulse is applied.\n\nOnce in the transverse plane, the spins begin to dephase because of local field inhomogeneities. Some spins process slightly faster, and others slightly slower. Over time, this produces a spread in phase, as shown here. Then comes the magic step: a 180-degree pulse is applied. This pulse flips the spins over, so that those that were lagging are now ahead, and those that were ahead are now behind. From that point forward, the phase differences begin to refocus. After the same amount of time has passed, the spins come back into alignment, forming an echo.\n\nThis is the echo signal that can be detected by the coil. Importantly, this echo arises from the selected slice, assuming you have already applied slice selection. The equation shown here, equation 4.43, expresses the signal mathematically as being proportional to the integral of the proton density over the slice. In other words, the echo represents the combined contribution of all the spins in that slice, after refocusing has occurred.\nSo spin echo illustrates in a very clear way how we can recover useful signal despite dephasing, and how gradients and RF pulses can be combined to control the evolution of magnetization in MRI.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 36, "slide_filename": "Slide36.txt", "slide_annotation": "Lecture 19 - Slide36.txt", "file_path": "Lecture 19\\Texts\\Slide36.txt", "content": "And now, let‚Äôs talk about the actual signal we can measure in MRI, which is called the free induction decay, or FID. After applying a ninety-degree RF pulse, the net magnetization vector is flipped into the transverse plane. This transverse component of magnetization is constantly rotating around the main magnetic field, B zero. Because of this precession, the changing magnetic field induces an alternating electromagnetic field. And as you learned in high school physics, an alternating magnetic field induces an electric current in a nearby coil. So if we place a receiver coil close to the patient, that coil will detect this alternating signal. \n\nHowever, the signal does not last forever. It decays over time, and this decay is due to the T2-star relaxation. The spins gradually lose phase coherence, and the signal amplitude decreases exponentially. What we record is not a constant sine wave of fixed amplitude, but a damped oscillation that fades away with time. This decaying signal, directly measured from the whole sample, or from a whole slice if slice selection is applied, is the free induction decay, or FID. This is the fundamental NMR signal we detect in MRI.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 37, "slide_filename": "Slide37.txt", "slide_annotation": "Lecture 19 - Slide37.txt", "file_path": "Lecture 19\\Texts\\Slide37.txt", "content": "Now, let‚Äôs look at how we time the acquisition of this signal. Two very important parameters in MRI are TE and TR. TE stands for echo time, which is the time from the application of the RF pulse to the moment we collect the peak of the echo signal. TR stands for repetition time, which is the time between successive RF excitations. You see in this diagram that we often use a ninety-degree pulse followed by a one-eighty-degree refocusing pulse to generate a spin echo. The echo forms at time TE. TR is measured from one excitation pulse to the next. \n\nThese two parameters, TE and TR, are under our control. They are not properties of the tissue itself but technical choices we make during the imaging sequence. By carefully selecting TE and TR, we can emphasize different tissue contrasts, highlight T1 weighting, T2 weighting, or proton density weighting. And at the same time, we can also balance image quality with scan time. If TR is long, the magnetization has more time to recover, giving us strong signals, but the scan will take longer. If TR is short, scanning is faster, but some magnetization may not have fully recovered, which reduces the signal. TE works similarly: shorter echo times reduce T2 contrast but keep signal strong, while longer echo times allow more T2 differences to emerge but at the cost of weaker signals. These parameters, TE and TR, are the main levers we adjust to create different kinds of MRI contrast.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 38, "slide_filename": "Slide38.txt", "slide_annotation": "Lecture 19 - Slide38.txt", "file_path": "Lecture 19\\Texts\\Slide38.txt", "content": "And I said that you can collect a signal with a nearby coil. Then, if you place another coil, you can also get a signal. Remember, the horizontal component of the magnetization is a rotational vector. So the signal actually has two parts. It depends both on amplitude and on phase. The signal detected along the x-axis and the signal detected along the y-axis naturally have a ninety-degree phase difference, because the magnetization keeps rotating. That makes the signal a complex-valued quantity.\n\nSo, let‚Äôs describe it mathematically. You can record Sx, which is modulated as M zero. After a ninety-degree pulse, this transverse signal is subject to spin echo formation and also subject to T2 decay. The Sx term represents the amplitude and the frequency content along the x-axis. Likewise, you have a Sy component, which is just the complementary sine term. Together, these two components give you both cosine and sine contributions. If we put them into compact form, we can write the signal as a complex number: a real part and an imaginary part. This gives us a very compact, complex-valued signal representation. In this notation, I have chosen to place the imaginary part corresponding to Sy. This is just a convention. The coding system can be chosen differently. Here, I simply make Sy the imaginary component. But for physical measurement, both the real part and the imaginary part are equally meaningful. We use complex notation mainly for convenience.\n\nThis idea is similar to what we saw in Fourier series and Fourier transforms, where using a complex exponential makes the notation compact, but the actual measured signal is real. Any complex form can always be converted back into a purely real form if needed.\nNow, the processional frequency is omega zero, determined by the main field B zero. The transverse magnetization keeps processing at this angular frequency omega zero. If we use a rotating frame‚Äîmeaning we mentally rotate our coordinate system at the same frequency as the spins‚Äîthen the signal appears stationary. In the rotating frame, the oscillatory part is removed, and the signal just decays exponentially with T2. This makes the description much simpler.\nSo, this is the signal in the rotating frame: stationary, but still decaying, due to T2 relaxation. And remember, T two decay is a physiological property. We cannot control it; it reflects interactions within the tissue itself.\n\nNow, what about repetition? The MRI sequence is repeated after a certain repetition time, TR. If TR is not long enough, the longitudinal magnetization, M zero, has not fully recovered. T1 relaxation determines how much of the original longitudinal magnetization is available at the start of the next cycle. So, after TR, the signal amplitude depends on both T1 recovery and T2 decay.\nThe total signal at time t is therefore a combination of these factors: proton density, hidden inside M zero; T1 recovery, which determines how much longitudinal magnetization has returned; and T2 decay, which determines how fast the transverse signal fades. Together, these biological properties determine tissue contrast.\nOn top of that, we have technical parameters: B zero, the main magnetic field; TR, the repetition time; and TE, the echo time. These are under our control. They don‚Äôt reflect patient pathology, but our technical choices in the scan.\n\nNow, why do we need repetition in the first place? Recall that in phase encoding, each time you collect data, you only get one line in k-space. Frequency encoding samples along that line, but since k-space is two-dimensional, you need multiple phase-encoding steps. That means repeating the sequence many times. This is very much like CT scanning, where helical scanning or fan-beam scanning requires repeated projections. In MRI, k-space must be filled line by line, and that requires repetition.\n\nIf TR is very long, M zero is fully recovered between repetitions. That gives you clean, strong signals, but scanning takes longer. If TR is too short, M zero hasn‚Äôt fully recovered, so the signal is weaker. But shorter TR also means faster scanning. So there is a tradeoff: longer TR improves image quality, shorter TR reduces scan time. The goal is always to balance scan speed with image quality for the clinical application.\n\nFinally, both TR and TE can be adjusted to emphasize different tissue properties. A short TR and short TE give you T1-weighted images. A long TR and long TE give you T2-weighted images. Proton density weighting lies somewhere in between. So, this signal model for a ninety-degree pulse captures both the biological properties of tissue‚Äîproton density, T1, and T2‚Äîand the technical factors‚ÄîB zero, TR, TE, and time. Together, they shape the MRI signal and ultimately determine the image contrast.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 39, "slide_filename": "Slide39.txt", "slide_annotation": "Lecture 19 - Slide39.txt", "file_path": "Lecture 19\\Texts\\Slide39.txt", "content": "And you can use short TR or long TR. You can use short TE or long TE. And with different combinations, your image can reflect more about rho, about T1, or about T2. So with different combinations, you measure different parameters. This is not like CT, where you only measure the attenuation coefficient, mu. In MRI, you have T1, you have T2, and you have rho, the proton density. If you just do one measurement, one kind of image, then the image really reflects all of these parameters mixed together. But if you purposely want to emphasize one of the three parameters, you need to select the proper technical parameters.\n\nSo first, let me explain what I mean by long TR. TR is the time for repetition. If you wait long enough after each excitation, the m vector will return to the original position along the z-axis. So M0 aligns with the z-axis. That is what we mean by long TR. Long TR means the magnetization has had enough time to fully recover. Short TR means you do not give the system enough time, so the full value of M0 has not yet been recovered along the z-axis. That‚Äôs the meaning of long TR and short TR.\nNow let‚Äôs talk about TE, the echo time. What do we mean by short TE? Short TE means the echo time is very short. The spin echo is collected quickly, so the flipped M vector doesn‚Äôt have enough time to dephase. Because the echo is measured soon after flipping, the dephasing is not serious. So M xy, the transverse component after the 90-degree flip, has had very little time to dephase. That means there is no significant T2 effect. If you measure the signal with a short TE, you cannot tell how much T2 contributes, because T2 hasn‚Äôt had enough time to show its effect.\n\nNow, if you use a long TR, then the magnetization has fully recovered. That means you have no information about T1, because T1 does not play a role when you wait long enough. If you use short TR, the recovery along the z-axis is incomplete. In that case, the effect you observe depends on the degree of recovery of the longitudinal component, so you see the T1 effect. That‚Äôs why short TR with short TE is called T1-weighted imaging.\nNext, if you use long TE, the echo time is long enough for T2 decay to take effect. And if you also use long TR, so the longitudinal magnetization has fully recovered, then there is no T1 effect. In this case, the signal is dominated by T2 decay, so it is T2-weighted. That is shown here with the green arrow.\n\nNow, if you use long TR and short TE together, then there is no T1 effect, and no T2 effect. In that case, the signal is proportional to rho, the proton density, because you are basically measuring the full magnetization without either T1 or T2 altering it. That is called rho-weighted imaging.\nThere is also a combination shown here with short TR and long TE, but that is not possible in practice. Remember, TR is the repetition time, and TE is the echo time within that repetition. TE normally should not be longer than TR. So this case does not exist physically.\n\nSo, to summarize: short TR and short TE give you T1-weighted imaging, long TR and long TE give you T2-weighted imaging, and long TR with short TE gives you rho-weighted imaging. Again, this takes time for you to fully understand. And if this is your first time seeing it, it is natural to feel a little confused.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 40, "slide_filename": "Slide40.txt", "slide_annotation": "Lecture 19 - Slide40.txt", "file_path": "Lecture 19\\Texts\\Slide40.txt", "content": "Now, let‚Äôs take a look at some real examples of brain images. On the same slide, you can see three different types of images: rho-weighted, T1-weighted, and T2-weighted. In MRI, rho here means proton density weighting, not P-weighted. In the brain, there are mainly five important tissue types that we care about: grey matter, white matter, cerebrospinal fluid or CSF, blood, and water. If you look carefully at the table, you‚Äôll notice that the T1 and T2 relaxation times are quite different for each tissue. \n\nFor example, grey matter has a T1 of about 1 second and a T2 of about 0.1 seconds, while CSF has a much longer T1 of around 2 seconds and a T2 of around 0.25 seconds. Water has even longer values, T1 about 4.7 seconds and T2 about 3.5 seconds. These differences are the key to MRI contrast. The very same slice of the brain can look dramatically different depending on whether you emphasize proton density, T1, or T2 weighting. \n\nSo, depending on your imaging parameters, you can choose to highlight different tissue characteristics. This is something CT cannot do, because CT only measures the attenuation coefficient mu. But MRI gives us much richer information because of rho, T1, and T2. And for a special case, we can use Gx gradient encoding to separate frequencies along the x-axis. That allows us to further refine how we differentiate tissue types based on their signal behavior.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 41, "slide_filename": "Slide41.txt", "slide_annotation": "Lecture 19 - Slide41.txt", "file_path": "Lecture 19\\Texts\\Slide41.txt", "content": "Here we are looking at a special case: Gx gradient encoding. That means we apply a linear gradient along the x-axis, so that the precession frequency depends directly on the coordinate x. In this case, the signal we observe combines several effects: proton density, T1 recovery, and T2 decay. \n\nYou can see the equation here: the amplitude of the local signal depends on rho, multiplied by the term for partial recovery due to T1, and multiplied again by the decay due to T2. This gives the overall signal strength at each location. Then, with the Gx gradient, we impose a position-dependent frequency shift. That‚Äôs why the exponential factor here involves Gx times x. \n\nSo, summarizing: the total signal is complex, but you can think of it as having two parts ‚Äî the amplitude of the local signal, determined by rho, T1, and T2, and the frequency of the local signal, determined by the gradient. This separation is very useful because it tells us what portion of the signal comes from tissue properties, and what portion comes from our encoding scheme. This prepares us for data collection in k-space.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 42, "slide_filename": "Slide42.txt", "slide_annotation": "Lecture 19 - Slide42.txt", "file_path": "Lecture 19\\Texts\\Slide42.txt", "content": "Now let‚Äôs move this into the k-space picture. We define a function f of x and y, which includes rho, the proton density, multiplied by the T1 recovery term and the T2 decay term. This function describes the true tissue properties in space. Then we define kx as gamma over 2 pi, times Gx, times t minus TE. So as time increases, kx traces out a trajectory in Fourier space. The signal of t, the thing we actually measure, is the Fourier transform of f of x and y, evaluated along the kx axis. \n\nSo in this case, we are scanning a single line in k-space, the horizontal line, because the gradient field is only along x. \n\nThis is a very special case, but it makes the principle clear: our signal directly corresponds to the Fourier components of the object we are imaging. That‚Äôs why MRI data is often first collected in k-space and then reconstructed by inverse Fourier transform to get the final image.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 43, "slide_filename": "Slide43.txt", "slide_annotation": "Lecture 19 - Slide43.txt", "file_path": "Lecture 19\\Texts\\Slide43.txt", "content": "Now let‚Äôs consider the general case. Instead of just Gx, we allow general gradients along x, y, and z. These gradients can vary with time, and their effects accumulate as the spins process. The local precession frequency is determined by the inner product of the gradient vector G of t and the position vector r. When we integrate this over time, we get a phase factor that builds up gradually. This leads us to define k of t as gamma over 2 pi times the time integral of G of tau d tau. \n\nIn other words, k of t is a trajectory through Fourier space, controlled entirely by our choice of gradient waveforms. The measured signal s of t is then the Fourier transform of the object‚Äôs spin density, f of x, y, z, evaluated along this k-space trajectory. This is the k-space theorem. It tells us that by designing gradient waveforms, we can sample any path we want in k-space ‚Äî straight lines, zig-zags, spirals, anything. That‚Äôs why MRI is so flexible. This general Fourier space formulation is extremely powerful because it connects what we measure directly to the spatial distribution of spins inside the patient. And again, by inverse Fourier transforming the collected data, we reconstruct the image.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 44, "slide_filename": "Slide44.txt", "slide_annotation": "Lecture 19 - Slide44.txt", "file_path": "Lecture 19\\Texts\\Slide44.txt", "content": "So at this point, I need to emphasize that this formulation is only approximate. Where exactly do the approximations come in? There are really three places. \n\nFirst, if you look at this expression, you see that the function depends on time, t. But when we talk about the k-space theorem, we treat it as if the whole function is fixed in space, something like F of x, y, z, and not time-varying. In reality, there is a time dependence here, and when we ignore that, we are making an approximation. \n\nSecond, you see this vector r, the positional vector. We treat it as if it is constant, fixed in space. But in reality, spins may move. For example, spins in blood keep moving. Patients may also move. Even molecular motion introduces some effects. But in this model, we ignore those motions and assume r is constant. That is another approximation. And third, the factor related to signal decay. \n\nIn practice, the signal we measure is subject to T2 star decay. But in this approximation, we assume that the readout is fast enough, so that no significant T2 star decay occurs during acquisition. That is a third approximation. Under these assumptions, the formulation reduces neatly to a Fourier transformation, which makes image reconstruction straightforward. But if you want very accurate imaging, all these neglected factors must be included. And once you include them, the reconstruction is no longer a simple Fourier transform. It becomes much more complex. \n\nIn fact, this is an area where machine learning and advanced reconstruction methods can play a big role. In the last part of the course, I will mention how modern approaches are being used to solve these challenges.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 45, "slide_filename": "Slide45.txt", "slide_annotation": "Lecture 19 - Slide45.txt", "file_path": "Lecture 19\\Texts\\Slide45.txt", "content": "Now, let me give you a simple example to make this clearer. Under the three approximations we just discussed, we can measure the Fourier space information. The figure on the left shows what is recorded in k-space. Remember, the Fourier transform is a complex function. It has both amplitude and phase. But what we display here is only the amplitude. It looks something like this, a distribution of intensities in k-space. \n\nThen, if you apply an inverse Fourier transformation, you obtain the image on the right, which is a cross-sectional image of the brain. So with MRI, by collecting data in k-space and applying an inverse Fourier transform, we can reconstruct detailed cross-sectional anatomy. I think this is really an amazing achievement ‚Äî especially when you compare it with X-ray CT. MRI provides a completely different way of obtaining cross-sectional images, not from attenuation, but directly from signals in Fourier space.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 46, "slide_filename": "Slide46.txt", "slide_annotation": "Lecture 19 - Slide46.txt", "file_path": "Lecture 19\\Texts\\Slide46.txt", "content": "So now we come to the final part of this lecture. You see, we have obtained cross-sectional images using MRI. The last slides summarize the key points and also point to the MRI scanners themselves. I will not go into the engineering details of scanners here, since these are mostly descriptive and you can easily read them from the textbook. \n\nThe textbook straightforwardly explains the scanner components. If you read carefully, you will find it both easy and interesting to understand. You will also find additional material about spin echo sequences, gradient echo, and slice selection, which can deepen your understanding. These are not required for the core lecture, but they are excellent supplemental resources if you want to learn more. And finally, you will have some homework related to this lecture, so that you can review the key ideas and practice the concepts we discussed.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 47, "slide_filename": "Slide47.txt", "slide_annotation": "Lecture 19 - Slide47.txt", "file_path": "Lecture 19\\Texts\\Slide47.txt", "content": "This diagram shows a block representation of how an MRI system works, starting from signal generation to image display. At the heart of the scanner, you have the patient placed inside the bore surrounded by gradient coils and the RF, or radiofrequency coil. The RF coil serves two purposes ‚Äî it transmits the excitation pulses into the body, and it also receives the weak signals emitted back from the tissue. To generate those pulses, a frequency synthesizer and an RF amplifier create the correct radiofrequency signal. That signal is then sent through a transmit‚Äìreceive switch, which ensures that the same RF coil can switch roles between sending and receiving.\n\nOnce the RF coil receives the returning signal, it is very weak. So the signal first goes through a preamplifier to boost it. Then, it passes through a receiver blanking unit to protect the electronics during transmission. After that, the signal is routed to an RF amplifier and a demodulator, followed by a quadrature mixer, which separates the signal into its real and imaginary components. These components represent the complex-valued MRI signal.\n\nThe analog signal is then converted into a digital form by an analog-to-digital, or A/D, converter. From there, the digital data is stored temporarily in RAM memory and controlled by the host computer. The computer performs the essential reconstruction step ‚Äî the two-dimensional inverse Fourier transform, or 2-D IFFT. This is what converts raw k-space data into an actual image that can be displayed.\n\nOn the control side of the system, there are waveform generator timing boards and gradient amplifiers. These control the gradient coils inside the scanner, which are responsible for slice selection, phase encoding, and frequency encoding. The pulse programmer and software interface, also controlled by the computer, dictate exactly when and how these gradients and RF pulses are applied. Together, this timing sequence ensures precise spatial encoding of the MRI signal, allowing us to reconstruct high-resolution images.\n\nSo overall, this diagram gives us a complete picture: from the RF system and gradient amplifiers, to signal detection, digitization, Fourier reconstruction, and finally, the image display. It ties together all the engineering components that make MRI imaging possible.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 48, "slide_filename": "Slide48.txt", "slide_annotation": "Lecture 19 - Slide48.txt", "file_path": "Lecture 19\\Texts\\Slide48.txt", "content": "This 3D rendering gives us a cutaway view of how an MRI scanner is structured and how a patient fits inside the system. At the center, you see the patient lying on the patient table, which slides into the bore of the scanner. Surrounding the patient are several key components. Closest to the body are the radio frequency coils. These coils are responsible for transmitting the RF pulses that flip the spins inside the tissue, and also for receiving the weak signals emitted back from the patient. Without these coils, no imaging would be possible.\n\nJust outside the RF coils are the gradient coils. These coils are used to create small, linearly varying magnetic fields on top of the main magnetic field. By turning the gradient coils on and off in different directions, the scanner performs slice selection, phase encoding, and frequency encoding, which are the core steps that allow spatial localization of the MRI signal.\n\nEncasing the gradient coils is the large superconducting magnet, shown here as the main cylindrical structure of the scanner. This magnet produces the strong, uniform field we call B-zero, typically one and a half Tesla, three Tesla, or even higher in advanced systems. The magnet is what aligns the spins in the body, creating the initial conditions necessary for MRI.\n\nAll of this hardware is integrated into the scanner body, which you see as the outer blue casing. The patient is positioned on the table and moved smoothly into the bore for imaging. This design allows precise alignment of the body part of interest with the center of the magnet, where the field is most uniform.\nSo in summary, this diagram shows the layered structure of an MRI scanner: the patient table for positioning, the RF coils for transmit and receive, the gradient coils for spatial encoding, and the large magnet that provides the powerful, stable field at the heart of the system.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 49, "slide_filename": "Slide49.txt", "slide_annotation": "Lecture 19 - Slide49.txt", "file_path": "Lecture 19\\Texts\\Slide49.txt", "content": "Here we see a modern clinical MRI scanner. This particular system is the Sparkler 1 point 5 Tesla MRI, which was made through a joint venture between Philips and Neusoft in 2008. What you notice first is the large cylindrical magnet housing, which contains the superconducting magnet that generates the powerful and stable magnetic field, known as B-zero. That main field is what aligns the spins inside the patient‚Äôs body and makes MRI possible.\n\nAt the front of the scanner is the bore opening, with the patient table extending outward. The table is designed to move smoothly into the bore, positioning the patient precisely at the isocenter of the magnet, where the magnetic field is most uniform. Inside the bore, hidden from view in this picture, are the gradient coils and the radio frequency coils, which together perform the tasks of spatial encoding and signal detection. The control panels you see on either side allow the operator to manage some of the basic functions and safety interlocks.\n\nThis specific 1 point 5 Tesla strength is one of the most common clinical field strengths worldwide, balancing good image quality with manageable cost and patient comfort. It represents an industry standard for many diagnostic applications, including brain, spine, and body imaging. So this photo gives you a sense of what an actual MRI scanner looks like in the clinic, as compared to the system diagrams and cutaway renderings we discussed earlier.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 50, "slide_filename": "Slide50.txt", "slide_annotation": "Lecture 19 - Slide50.txt", "file_path": "Lecture 19\\Texts\\Slide50.txt", "content": "Here we see a PET-MRI scanner, which is an advanced hybrid imaging system. On the left side of the slide, there is a schematic diagram showing how the different components are arranged, and on the right side, you see an actual Siemens PET-MRI machine.\n\nThe MRI part of the scanner is built around the large cylindrical magnet, just like a conventional MRI. The magnet shielding coil, the primary magnet coil, and the gradient coils are all stacked concentrically, forming the core structure of the system. These generate the strong static field and the spatially varying gradient fields needed for magnetic resonance imaging. Inside this same system, PET detector modules are integrated. These include layers such as the PET camera, avalanche photodiodes, LSO crystals, and associated electronics like pre-amplifiers and driver boards. Together, these PET modules detect the gamma rays emitted by radioactive tracers inside the patient‚Äôs body.\n\nAn RF head coil is also shown in the diagram, which is part of the MRI subsystem, used to transmit radiofrequency pulses and receive the MR signal. The magnet cryostat, shown as the outer casing, maintains the superconducting coils at very low temperatures so that the magnetic field stays stable.\nOn the right-hand side, the Siemens clinical PET-MRI scanner looks very much like a conventional MRI machine. But the key difference is that the PET detectors are integrated into the bore, allowing simultaneous acquisition of MRI data and PET data. This means that while MRI provides excellent soft-tissue contrast and anatomical detail, PET simultaneously provides functional and metabolic information. The result is a powerful hybrid modality that combines structural and functional imaging in a single scan.\n\nThis PET-MRI approach is especially important in neurology, oncology, and cardiology, where you want to see not just anatomy but also how tissues are functioning or metabolizing in real time.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 51, "slide_filename": "Slide51.txt", "slide_annotation": "Lecture 19 - Slide51.txt", "file_path": "Lecture 19\\Texts\\Slide51.txt", "content": "This photo shows a preclinical MRI scanner, specifically the Bruker BioSpec Avance III 94/20 system, which operates at 9.4 Tesla. Unlike clinical MRI scanners that are designed for human patients, preclinical MRI systems are mainly used for small animals such as mice, rats, or rabbits. These scanners are critical in biomedical research because they allow scientists to study disease models, monitor treatment effects, and investigate basic biological processes in a controlled setting before moving to human trials.\n\nThe scanner looks similar in shape to a clinical MRI, with a large cylindrical magnet and a bore in the center, but the bore is much smaller since it is designed for animal studies. The extremely high magnetic field strength of 9.4 Tesla gives very high resolution and sensitivity, allowing researchers to visualize fine structural details that would not be possible at lower field strengths. With this scanner, scientists can measure anatomical structures, functional activity, and even molecular processes, all non-invasively.\n\nPreclinical MRI scanners like this one play a vital role in translational medicine, serving as a bridge between laboratory discoveries and clinical applications. By studying animal models of diseases such as cancer, stroke, or neurodegenerative disorders, researchers can optimize imaging protocols, test new therapies, and gather critical data before applying these methods in human patients.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 52, "slide_filename": "Slide52.txt", "slide_annotation": "Lecture 19 - Slide52.txt", "file_path": "Lecture 19\\Texts\\Slide52.txt", "content": "His photo shows a modern CT-MRI guided radiation therapy system. In radiation oncology, precise targeting of tumors is critical because you want to deliver a very high radiation dose to cancer cells while sparing as much of the surrounding healthy tissue as possible. Traditionally, CT has been used for radiation planning because CT provides good information on patient anatomy and electron density, which are needed to calculate radiation dose. However, CT has limited soft-tissue contrast, which means tumors in organs like the brain, liver, or pelvis may be difficult to outline clearly.\n\nBy combining MRI with CT in radiation therapy, doctors can take advantage of the strengths of both modalities. MRI provides excellent soft tissue contrast, allowing the tumor to be visualized more clearly, while CT provides the necessary density information for radiation dose calculation. This hybrid approach makes the targeting more accurate and safer.\n\nIn this photo, you see the treatment couch where the patient lies, with immobilization devices to keep the patient still. The large circular gantry houses the imaging and radiation components. During treatment, the system acquires imaging data to confirm the tumor‚Äôs position, and then the linear accelerator delivers a precisely shaped beam of radiation to the tumor. This technology represents a powerful integration of imaging and therapy ‚Äî using CT and MRI together to guide radiation treatment in real time, which improves tumor control and reduces side effects for patients.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 53, "slide_filename": "Slide53.txt", "slide_annotation": "Lecture 19 - Slide53.txt", "file_path": "Lecture 19\\Texts\\Slide53.txt", "content": "This slide shows a potential MRI-guided radiation therapy design proposed by Xun Jia and colleagues. What you see here is a concept where the MRI magnet is split into two halves, with a separation of about 70 centimeters. That gap allows room for radiation beams to pass through while still maintaining the magnetic field needed for MRI imaging. The design also includes a couch that can rotate by plus or minus twenty-five degrees, giving flexibility in patient positioning and beam delivery angles.\n\nThe circular ring you see provides both mechanical stability and magnet shielding. This is very important because strong magnetic fields need to be contained and stabilized to prevent interference with surrounding equipment and to protect patient safety. In addition to the MRI magnet, the design also considers adding a cone-beam CT, or CBCT, mounted on the same ring. Although not shown in this illustration, the CBCT would provide complementary X-ray imaging, making this a hybrid system that combines MRI‚Äôs excellent soft tissue contrast with CT‚Äôs accuracy for dose calculation.\n\nClearly, this figure illustrates an innovative idea in radiation oncology ‚Äî a split-magnet MRI system, integrated into a rotating gantry, designed to guide precise radiation treatment while maintaining real-time MRI imaging of the tumor.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 54, "slide_filename": "Slide54.txt", "slide_annotation": "Lecture 19 - Slide54.txt", "file_path": "Lecture 19\\Texts\\Slide54.txt", "content": "This slide is a list of extra resources for you to explore outside the lecture. These links are short videos that explain some of the key concepts we discussed in more detail. \n\nThe first group of links is about the spin echo. Spin echo is one of the most fundamental sequences in MRI, so watching those demonstrations will help reinforce how a ninety-degree pulse and a one-eighty-degree pulse work together to refocus the signal. The second group of links is about the gradient echo. Gradient echo is a variation where gradients are used to generate echoes instead of a one-eighty-degree pulse. These videos will show you the difference in how the signal is produced. Finally, the last link is about slice selection. As we discussed, slice selection is how we target a specific region of the body by combining a magnetic gradient with the RF pulse.\n\nNote that although these videos are not required, they are helpful if you want to review the concepts visually. They can give you an extra layer of understanding, especially if you are new to MRI physics.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 19", "slide_number": 55, "slide_filename": "Slide55.txt", "slide_annotation": "Lecture 19 - Slide55.txt", "file_path": "Lecture 19\\Texts\\Slide55.txt", "content": "This slide gives you the homework assignment for this section. You can see that it has three problems, numbered 4.7 through 4.9. In problem 4.7, you are asked to look at the five frequencies shown in the figure above and then state the order of the T1 and T2 relaxation times. For example, you might write something like T1 of brain is greater than T1 of CSF, which is greater than T1 of aqueous humor. The goal here is to compare how different tissues relax under different frequencies.\n\nIn problem 4.8, you are working with hydrogen nuclei in fat and water. You are told that the T2 value of fat is 100 milliseconds and the T2 value of water is 500 milliseconds. In a spin echo experiment, you need to calculate the delay time between the ninety-degree pulse and the one-eighty-degree pulse that maximizes the difference in signal between fat and water. This problem makes you think about how relaxation differences translate into image contrast.\n\nFinally, in problem 4.9, you are asked to write an expression for the longitudinal magnetization, M z, as a function of time after a one-eighty-degree pulse. You should then determine the time at which M z becomes zero. After that, you need to plot what happens if, instead of a one-eighty-degree pulse, you apply a one-thirty-five-degree pulse. This problem connects the mathematical model to how pulse angle changes affect the signal.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 20 - Slide1.txt", "file_path": "Lecture 20\\Texts\\Slide1.txt", "content": "Hello my students!\nToday we‚Äôll wrap up our MRI part by explaining pulse sequences. Although the title highlights pulse sequences, we‚Äôll also review the overall architecture of an MRI scanner and a few related concepts that tie everything together.\nToward the end of the lecture, I‚Äôll introduce some recent brain-initiative projects and a few emerging ideas in MRI research.\u000bThe main emphasis today follows the green book chapter, so I strongly encourage you to read that section carefully. If you fully understand the ideas we cover in this lecture, you‚Äôll find the final exam easier.\nOf course, I‚Äôll also share a few extra details and insights that go beyond the textbook‚Äîthese will help you develop a deeper and more practical understanding of MRI as a whole.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 20 - Slide2.txt", "file_path": "Lecture 20\\Texts\\Slide2.txt", "content": "Let‚Äôs take a quick look at our course schedule to see where we stand. After the MRI part, we have two more major imaging modalities: ultrasound and optical imaging, which are quite different but have their unique utilities.  Finally, I will give you a very brief overall of the emerging area called ‚Äúdeep imaging‚Äù or ‚Äúdeep reconstruction‚Äù.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 20 - Slide3.txt", "file_path": "Lecture 20\\Texts\\Slide3.txt", "content": "Last time, I asked you to read the textbook chapter on MRI. By now, many of you have probably gone through the green book and have some familiarity with the hardware components of an MRI system. Let‚Äôs review these ideas together so you can build a stronger, more intuitive understanding.\n\nMRI imaging relies on both hardware and software. The hardware provides the physical foundation‚Äîmagnets, gradient coils, and radio-frequency coils‚Äîwhile the software defines the pulse sequences, the inversion schemes, and the reconstruction algorithms that convert raw signals into images. Together, all these parts form what we call an MRI scanner.\nAt the heart of the scanner is a strong, stationary magnetic field, which we denote as B-zero. Inside this B-zero field, all the tiny magnetic moments in the body‚Äîwhat we call the magnetization vector, or M-vector‚Äînaturally align along the direction of B-zero, which we usually define as the z-axis or G-direction.\n\nNext, we introduce a radio-frequency field, called B-one. This is a rotating magnetic field applied perpendicular to B-zero. Because of this rotation, the B-one field exerts a torque on the M-vector, causing it to flip from its resting position toward the x-y plane. In other words, it moves the system from a stable, low-energy state to a higher-energy state.\nOnce magnetization has been flipped into the x-y plane, the spins begin to process, producing an alternating electromagnetic field. If we place detection coils near the sample‚Äîor the patient‚Äîthis changing field induces an electrical signal known as the free-induction decay, or FID. That is the fundamental signal we detect in MRI.\n\nAfter excitation, the M-vector gradually relaxes back to its original, low-energy position aligned with the B-zero field. This relaxation process releases energy, and we can monitor it to extract physical and biological information.\n\nNow, this M-vector is not a single magnet; it represents the collective effect of countless microscopic spins‚Äîsome pointing slightly up, some slightly down. The small imbalance between them adds up to produce a measurable net magnetization.\nIf we only used B-zero and B-one, we would get information about the entire sample as a whole‚Äîbut no spatial localization. To achieve tomographic imaging, we need gradient coils that generate linearly varying magnetic fields, denoted G-x, G-y, and G-z. By controlling these gradients, we can selectively encode spatial information: G-z is typically used for slice selection, G-x for frequency encoding, and G-y for phase encoding. Together, these allow us to map the detected signals into k-space‚Äîa spatial-frequency domain that follows the Fourier-slice theorem. From there, we apply Fourier transforms and inverse Fourier transforms to reconstruct the final tomographic image.\n\nSo, the hardware provides the data, and the mathematics‚Äîmainly Fourier analysis‚Äîturns that data into images. This is a high-level overview of how MRI works. If you‚Äôve already reviewed the textbook section on MRI scanner architecture, you should now recognize the key components we‚Äôre talking about.\nLet‚Äôs move on and look at these elements in more detail.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 20 - Slide4.txt", "file_path": "Lecture 20\\Texts\\Slide4.txt", "content": "Now let‚Äôs take a closer look at the signal model for a ninety-degree pulse. When the magnetization vector ‚Äî the M vector ‚Äî is flipped by ninety degrees, it moves entirely into the X-Y plane. This gives us the maximum signal, because the transverse magnetization is now at its full strength.\n\nOnce in the X-Y plane, the M vector begins to rotate, or process, around the main magnetic field. This motion induces an electric current in the nearby quadrature coils, and that‚Äôs the MRI signal we detect. Because the magnetization both rotates and decays, the signal has two parts ‚Äî an amplitude and a phase ‚Äî so we describe it as a phasor.\n\nMathematically, we can write the two components like this:\u000bS-x of t equals M-naught times e to the power of negative t over T-two, times cosine of omega-naught times t.\u000bS-x of t equals M-naught times e to the power of negative t over T-two, times sine of omega-naught times t.\n\nIf we combine these two into one complex expression, we get: S of t equals S-x of t plus i times S-y of t, which is equal to M-naught times e to the power of negative t over T-two times e to the power of negative i omega-naught t. This compact form represents both the magnitude and the phase of the signal together. If you prefer real numbers, you could keep the sine and cosine forms separately, but the complex expression makes the math much easier‚Äîespecially once we introduce phase encoding and frequency encoding later.\n\nIn the rotating reference frame, we remove the fast rotation caused by the main field B-zero, so the signal simplifies to: S of t equals M-naught times e to the power of negative t over T-two. This describes the exponential decay of the transverse magnetization‚Äîwhat we call T-two relaxation.\nWhen the sequence is repeated after a repetition time, or T-r, the magnetization recovers along the Z-axis following this relation: M-z of T-r equals M-naught times the quantity one minus e to the power of negative T-r over T-one.\n\nNow if we combine both effects‚ÄîT-one recovery and T-two decay‚Äîthe total signal becomes: S of t equals M-naught times the quantity one minus e to the power of negative T-r over T-one, all multiplied by e to the power of negative t over T-two. This signal depends on two groups of factors: biological parameters ‚Äî proton density rho, T-one, and T-two ‚Äî and technical parameters such as B-zero, T-r, T-e, and time t.\n\nWhat‚Äôs elegant here is how physics and mathematics come together. The sine and cosine functions we use are direct solutions of Maxwell‚Äôs equations, which describe electromagnetic fields. And when we add phase and frequency encoding, those same sinusoidal functions naturally form a Fourier representation‚Äîthe mathematical tool we use to reconstruct MRI images.\n\nSo, Maxwell‚Äôs equations describe the physical interaction, and Fourier analysis provides the mathematical framework. Together, they create what we call the K-space theorem‚Äîa beautiful harmony between physics and mathematics that makes MRI imaging possible.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 20 - Slide5.txt", "file_path": "Lecture 20\\Texts\\Slide5.txt", "content": "Now that‚Äôs really fascinating. Let‚Äôs take a look at the overall MRI system diagram to understand how all these components work together.\nAt the very top level, the MRI scanner begins with a strong, static magnetic field, denoted as B-zero. This field provides the main alignment for all magnetic moments inside the body.\n\nTo this, we add a rotating magnetic field, called B-one, which is generated by the radio-frequency coil, or R-F coil for short. ‚ÄúR-F‚Äù stands for radio frequency, which corresponds to the Larmor frequency‚Äîthe processional frequency of the spins.\nWhen the system applies this B-one field, the spins are excited, and the resulting signal is induced in the same coil placed nearby. Because the coil operates at the same resonant frequency, it can both transmit and receive the signal effectively.\n\nNow, let‚Äôs trace the signal path through the system. Everything begins at the host computer, which contains the control software. The computer sends pulse sequence commands to a pulse programmer or software interface. These commands are then passed to a waveform generator and timing boards, which create precise control signals for the gradient amplifiers and the frequency synthesizer.\n\nThe frequency synthesizer defines the exact frequency of the R-F pulse, and the R-F amplifier boosts that signal to a level strong enough to drive the R-F coil. Through the transmit‚Äìreceive switch, the system alternates between sending energy into the body and listening for the returning signal.\n\nOn the receiving side, the weak magnetic resonance signal first passes through a preamplifier, which increases its strength without distorting it. Then it goes through receiver blanking‚Äîa brief muting step to prevent overload when the transmitter is active. Next, the signal passes through another R-F amplifier, a demodulator, and a quadrature mixer, which separate the real and imaginary components of the complex signal.\n\nThis analog signal is then converted into digital form using an A‚ÄìD converter, meaning analog to digital converter, and the data are stored in the system‚Äôs memory, or RAM. From there, the computer performs a two-dimensional inverse fast Fourier transform, often abbreviated as two-D I-F-F-T, to reconstruct the spatial image from the frequency-domain data, also called k-space.\n\nFinally, the reconstructed image is displayed on the computer screen.\nSo, to summarize ‚Äî you have the B-zero field providing magnetic alignment, the B-one field for excitation and signal detection, and the gradient fields for spatial encoding. All these are coordinated by computer software that carefully synchronizes every pulse and acquisition step.\nThis is a high-level overview of how an MRI system operates‚Äîfrom the initial excitation pulse all the way to the final image reconstruction.\nNext, we‚Äôll dive a bit deeper into some of the hardware details behind these components.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 20 - Slide6.txt", "file_path": "Lecture 20\\Texts\\Slide6.txt", "content": "Now let‚Äôs talk about how the MRI system actually detects the signal. You don‚Äôt need to worry too much about the mathematical details here‚Äîthink of this as a conceptual overview. The signal we receive from the patient contains both a real part and an imaginary part, and we often combine them into a complex form to make the analysis simpler.\n\nWhen we first learned about Fourier series, we started with real functions‚Äîthings like sine and cosine. Later, we introduced complex notation, and suddenly the whole expression became much more compact, taking only about one-third of the space. That‚Äôs exactly what we‚Äôre doing here‚Äîit‚Äôs purely a mathematical convenience.\n\nSo, we have two components: the real signal, written as S-R of t, and the imaginary signal, written as S-I of t. When we combine them, the total signal can be expressed as: S of omega equals the integral from negative infinity to positive infinity of the quantity S-R of t plus j times S-I of t, multiplied by e to the power of negative j omega t, integrated with respect to t. This simply represents the Fourier transform of our signal, showing how it behaves across different frequencies.\n\nNow, in practical terms, the signal detected by the MRI coil is an oscillating voltage. For a typical 1.5-tesla scanner, this voltage oscillates at around 63.9 megahertz‚Äîthat‚Äôs an extremely high frequency‚Äîand its strength is only a few microvolts. Because it‚Äôs so high and so weak, it‚Äôs very difficult to digitize the signal directly.\n\nTo make digitization possible, we first have to demodulate the signal‚Äîin other words, we shift it from that very high Larmor frequency down to a much lower intermediate frequency, usually around 10.7 megahertz.\nHere‚Äôs how that works: The analog signal coming from the preamplifier is sent into a demodulator, then through a quadrature mixer. The quadrature mixer separates the signal into two parts‚Äîoften labeled ‚ÄúI‚Äù for the in-phase component, and ‚ÄúQ‚Äù for the quadrature, or ninety-degree phase-shifted, component. These two parts correspond to the real and imaginary signals we discussed earlier.\n\nAfter that, the signals pass through filters and variable-gain amplifiers to remove noise and adjust their strength. Finally, they go into two analog-to-digital converters, often called A‚ÄìD converters, where the continuous analog signal is converted into discrete digital samples.\nOnce this conversion is done, the signal enters the digital domain‚Äîfrom there, the computer can perform all kinds of mathematical operations, such as Fourier transforms, image reconstruction, and signal averaging.\n\nSo, to summarize: the goal of this detection process is to convert a weak, high-frequency, analog signal into a lower-frequency, digital signal that can be handled by a computer. This step bridges the physical world of magnetic resonance with the digital world of image formation.\nThese are primarily electrical engineering concepts, but understanding them gives you a complete picture of how MRI signal acquisition really works.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 20 - Slide7.txt", "file_path": "Lecture 20\\Texts\\Slide7.txt", "content": "Now, let‚Äôs talk about the background magnetic field, often referred to as B-zero. There are three main ways to generate this magnetic field inside an MRI system.\n\nThe mainstream approach, used in nearly all modern MRI scanners, is to use a superconducting magnet. This magnet operates in an extremely cold environment‚Äîclose to absolute zero‚Äîso that the wire inside becomes superconductive, meaning it has zero electrical resistance. That‚Äôs quite remarkable, because once current starts flowing in a superconducting loop, it can continue indefinitely without any energy loss.\nIf a superconducting magnet is not used, the second option is a permanent magnet. Permanent magnets can be very effective and relatively inexpensive when the total field strength is low‚Äîtypically below about zero point three five tesla. These are often used in open MRI systems or low-end scanners, where high field strength is not essential.\n\nThe third method is to use electromagnets made from copper coils, where electric current is continuously injected to generate the magnetic field. However, this method has major drawbacks: running current through the coils produces heat, which leads to noise and energy loss, making it inefficient and less stable for imaging.\n\nSo, among the three methods, the superconducting magnet remains the primary choice for modern MRI scanners. It provides a strong, stable, and uniform B-zero field, which is essential for producing high-quality images.\n\nIf you look at the cross-sectional illustration in your textbook, you‚Äôll see how the magnet is constructed. At the center, we have the room-temperature magnet bore, where the patient lies. Surrounding it are layers containing the superconducting windings, which are immersed in liquid helium to keep them cold enough to maintain superconductivity. Outside that, there are layers of vacuum insulation and liquid nitrogen to minimize heat transfer from the environment.\n\nThe resulting magnetic field, B-zero, is oriented along the Z direction in our coordinate system, with X and Y lying perpendicular to it.\n\nSo, to summarize:\u000bSuperconducting magnets are the standard in modern MRI systems,\u000bPermanent magnets are used for low-field or open designs, and\u000bResistive magnets using copper coils are mostly historical or experimental.\nThese are the key ways we generate the powerful and stable B-zero field that makes MRI imaging possible.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 20 - Slide8.txt", "file_path": "Lecture 20\\Texts\\Slide8.txt", "content": "If you look closely, you can see that the entire system is built like a layered cylinder. At the center is the main winding ‚Äî a set of coils made from a special metal alloy. When this material is cooled to an extremely low temperature, its electrical resistance suddenly disappears. That‚Äôs what we call superconductivity.\nNormally, in any electrical circuit, a conductor always has some resistance ‚Äî no matter how small. But once the temperature drops low enough, the resistance becomes exactly zero. That‚Äôs an amazing physical phenomenon. In fact, a professor from our own physics department many years ago did pioneering work on superfluidity and superconductivity, and his research earned a Nobel Prize. So, this topic has deep scientific roots right here on campus.\n\nIn this system, liquid helium is filled into the helium vessel to cool the main windings to superconducting temperatures. The outer layers include vacuum insulation and cold shields ‚Äî both inner and outer ‚Äî to prevent heat from entering the chamber. There are also shim coils used to fine-tune the uniformity of the magnetic field inside the bore. The entire setup is housed inside a metal casing to maintain stability and safety.\n\nWhen everything is working properly, a large current circulates continuously through the superconducting wire without any energy loss. That‚Äôs the key advantage ‚Äî it‚Äôs energy efficient and can generate an extremely strong and stable magnetic field.\n\nNow, if you look at the first link provided on the slide, it explains more about this superconductive design in detail. The second link, written partly in English and partly in Chinese, was authored by one of my former postdoctoral fellows, Dr. Wei. He proposed a theoretical interpretation of superconductivity and superfluidity from a quantum-mechanical perspective.\n\nAs you may recall, in quantum mechanics, matter is treated as both a wave and a particle. According to the standard theory, the probability wave that describes where a particle might be found is always non-negative ‚Äî it can be zero or positive, but never negative. Dr. Wei‚Äôs extension of the theory suggests that, under certain quantum conditions, the probability wave could actually take on negative values. That would mean a particle might seem to disappear in one region and reappear elsewhere instantly ‚Äî a phenomenon sometimes described as quantum teleportation.\n\nOf course, whether this interpretation is physically real is still open to debate, but it offers a fascinating theoretical connection between quantum behavior and macroscopic phenomena like superconductivity and superfluidity.\n\nSo, to summarize: in a superconducting magnet, the absence of resistance allows a persistent current to flow indefinitely, producing a strong and stable B-zero field essential for MRI. And at the same time, the underlying physics continues to inspire new ideas that bridge quantum mechanics and medical imaging technology.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 20 - Slide9.txt", "file_path": "Lecture 20\\Texts\\Slide9.txt", "content": "Now, I‚Äôm not an expert in electromagnet design, but let me share some essential information so you can understand the concept clearly.\nThe large superconducting magnet provides the strong, uniform B-zero field‚Äîthat‚Äôs our background magnetic field. But to create spatial encoding for imaging, we also need additional gradient fields.\n\nSo, how do we generate these gradient fields? A magnetic gradient means that the magnetic field changes linearly with position. In other words, the field strength increases or decreases in a straight-line fashion along one direction.\n\nTo achieve this, the MRI scanner uses three separate gradient coils, which produce fields we call B-X, B-Y, and B-Z. Each of these gradients corresponds to one of the three spatial directions: X, Y, and Z.\n\nIf you look at the illustration, you can see how the coils are wound differently to produce different kinds of gradients. The middle portion of each field‚Äîthe region near the center of the magnet‚Äîis approximately linear. That‚Äôs the part we actually use during imaging. Outside the center, the field is not perfectly linear, but that‚Äôs okay, because we only rely on this central, uniform region for accurate encoding.\n\nBy wiring the coils in specific patterns, we can make the magnetic field vary linearly along any chosen direction. So, for example, one coil arrangement gives you a gradient along the Z direction, another gives a gradient along Y, and another along X.\n\nWe don‚Äôt need to go deep into Maxwell‚Äôs equations here‚Äîthat would require advanced electromagnetic theory. For now, it‚Äôs enough to have a heuristic understanding: when we wind the coils in a certain configuration, the resulting magnetic field changes linearly in space, and that‚Äôs exactly what we need to distinguish signals from different locations inside the body.\n\nSo, remember ‚Äî the B-zero field provides the uniform background alignment, and the gradient coils introduce controlled, linear variations along X, Y, and Z, which allow the MRI system to encode spatial information for image reconstruction.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 20 - Slide10.txt", "file_path": "Lecture 20\\Texts\\Slide10.txt", "content": "When you read the textbook chapter on MRI hardware, pay special attention to why we need to generate a linear magnetic field. The basic idea is actually very simple, and this diagram illustrates it nicely.\n\nYou might remember from high school physics that whenever an electric current flows through a wire, it produces a magnetic field that circles around the wire. If we take that wire and bend it into a loop, the small circular magnetic fields from each segment combine together. The result is a field that looks like the one produced by a tiny bar magnet, pointing in the direction given by the right-hand rule ‚Äî curl your fingers along the current, and your thumb points toward the magnetic field direction.\n\nNow, if you use a single loop, the magnetic field strength is small. But if you wind many loops together ‚Äî for example, a coil with a hundred turns ‚Äî the magnetic field becomes roughly a hundred times stronger. That‚Äôs the basic principle behind electromagnets and, in our case, gradient coils.\nOn the left side of the figure, you see a loop with current flowing upward, generating a magnetic field that points to the right. On the right side, the current direction is reversed, producing a magnetic field that points to the left.\n\nWhen we place these two coils symmetrically and run equal but opposite currents, something interesting happens. At the center between the coils, the magnetic fields cancel each other out, giving a field strength near zero. But if you move slightly to one side, one field becomes stronger while the other becomes weaker. That means the magnetic field increases linearly as you move away from the center ‚Äî exactly the gradient we need.\nSo, in the central region, the total field varies in a nearly linear fashion, even though the actual fields from each coil are curved. This linear section is the useful part of the gradient field for imaging.\n\nIn summary:\u000bA single loop of current generates a circular magnetic field.\u000bMultiple loops amplify the field strength.\u000bTwo symmetric, oppositely oriented coils produce a linear gradient in the central region.\nThat‚Äôs the simple physical idea behind how gradient coils work ‚Äî elegant, practical, and directly derived from the basic laws of electromagnetism you learned in school.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 20 - Slide11.txt", "file_path": "Lecture 20\\Texts\\Slide11.txt", "content": "Now let‚Äôs connect this idea to the actual gradient coils used in MRI scanners. The linear field that we just discussed is formed between two opposing magnetic fields created by the left and right coil pairs. That configuration generates the Z-gradient field.\n\nHowever, for the X and Y gradients, we can‚Äôt arrange the coils in the same way‚Äîbecause the patient needs to pass through the tunnel of the magnet. So, the coil design must be adjusted while keeping the same physical principle.\n\nFor the Y-gradient, the setup uses a top coil and a bottom coil. Each coil forms what‚Äôs called a saddle-shaped loop, but it still maintains a closed current path. The magnetic field from the top coil points downward, while the field from the bottom coil points upward. In the middle, these two fields cancel each other, creating a region of zero field. As you move upward, the downward field weakens and the upward field strengthens, forming a linear Y-gradient.\nIf we rotate this entire configuration by ninety degrees, we can generate the X-gradient using exactly the same concept.\n\nWhen all these coil systems‚Äîthe X, Y, and Z gradients‚Äîare combined, we can generate any arbitrary gradient field, often represented as G-X, G-Y, and G-Z. By carefully controlling the current in each set of coils, we can produce gradients in any direction or combination needed for imaging.\nIn practice, the gradient coils are designed in multiple layers, and additional shim coils are included to fine-tune the magnetic field so it remains as linear as possible within the imaging region.\n\nAlthough the engineering details are complex, the fundamental idea is simple. Using Maxwell‚Äôs equations‚Äîjust four equations that describe how electric and magnetic fields behave‚Äîwe can simulate the magnetic field using finite element computation. By adjusting coil shapes, adding new loops, and fine-tuning current paths, engineers can iteratively refine the field design until it achieves excellent linearity.\nSo even though the hardware looks complicated, the underlying physics is straightforward: we excite the coils in such a way that their combined fields produce the precise gradient patterns required for MRI imaging.\n\nAnd one final point‚Äîduring imaging, these gradient coils rapidly switch on and off, producing mechanical stress that makes the loud knocking sounds we hear during MRI scans.\nThat‚Äôs the practical side of how gradient coils work‚Äîboth physically and acoustically.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 20 - Slide12.txt", "file_path": "Lecture 20\\Texts\\Slide12.txt", "content": "Now, let‚Äôs talk about the B-one field, which is the missing component we haven‚Äôt discussed yet. This field is generated by the radio-frequency coil, or R-F coil.\nThe most common type of RF coil used for transmitting the B-one field is called a birdcage coil, shown on the left. You can understand this coil design quite intuitively. It looks like the frame of a cylindrical cage ‚Äî a circular structure with evenly spaced conducting bars connected at both ends by rings.\n\nWhen an alternating current flows through these bars, it creates a rotating magnetic field inside the bore. This rotating field is the B-one field, which is applied perpendicular to the main magnetic field B-zero. The B-one field is responsible for exciting the protons and flipping the magnetization vector, M, away from its alignment with B-zero.\n\nOn the right side of the slide, you can see other kinds of RF coils as well. The surface coil, shown in the middle, is used for imaging regions close to the surface of the body ‚Äî for example, the shoulder or knee. Because it‚Äôs placed near the area of interest, it gives excellent signal-to-noise ratio locally, though it doesn‚Äôt cover deep tissues as well.\n\nThe third design, shown on the right, is a phased-array coil. This one combines multiple small coil elements arranged in an array. Each coil picks up signals from a different region, and the signals are combined by the system to produce a high-quality image. Phased arrays are very useful for spine imaging and other applications where we need both wide coverage and high sensitivity.\n\nIn summary:\u000bThe birdcage coil is typically used for transmission ‚Äî it creates a uniform B-one field, ideal for brain or whole-body imaging.\u000bThe surface coil is best for localized, high-resolution scans near the body surface.\u000bThe phased-array coil allows us to combine multiple receiver channels for stronger and faster imaging.\nSo, the RF coils are essential for both transmitting the B-one excitation field and receiving the MRI signal from the body, making them one of the most important components in the entire imaging chain.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 20 - Slide13.txt", "file_path": "Lecture 20\\Texts\\Slide13.txt", "content": "Now, let‚Äôs take a closer look at how the birdcage coil produces a uniform B-one field. If you consider the two cross-sections shown here, the idea is quite intuitive. When the current in one connecting wire flows in a particular direction, it generates a local magnetic field pointing downward. To produce a local field pointing upward, the current in the adjacent wire must flow in the opposite direction, forming a closed loop.\n\nThis basic configuration‚Äîcurrents flowing in opposite directions through adjacent loops‚Äîcreates magnetic fields that point downward in some regions and upward in others. When multiple loops or cross-sections are added around the cylinder, these individual magnetic fields combine to form a uniform field across the entire region inside the coil.\n\nMathematically, this can be shown by modulating the current as a sinusoidal function of the angular position, which we call phi. If you plot the current intensity as a function of the phi angle, it varies like a sine wave‚Äîstarting from zero at phi equals zero degrees, reaching a maximum at ninety degrees, and then becoming negative at one hundred eighty degrees. This sinusoidal modulation ensures that the magnetic field intensity remains uniform across the cross-section.\n\nTo make the B-one field rotate in space, the current itself is also modulated over time, so that the maximum and minimum of the field continuously move around the coil. As a result, the B-one field rotates smoothly, allowing it to flip the magnetization vector, M, in a controlled and consistent way during excitation.\n\nIn essence, this is how we achieve a uniform rotating magnetic field that interacts with the main B-zero field. Together, the B-zero field, B-one field, and the three gradient fields‚ÄîG-sub-X, G-sub-Y, and G-sub-Z‚Äîform the complete electromagnetic environment of the MRI system.\nWhat‚Äôs truly fascinating is that all of these ideas‚Äîthe generation of magnetic fields, their directions, and the way they add up‚Äîcan actually be explained using high school physics: Amp√®re‚Äôs law, the right-hand rule, and the principle of superposition.\n\nOf course, for real MRI coil design, we need precise optimization to make sure the fields are uniform and stable. That‚Äôs done through finite element analysis, a numerical technique that lets engineers compute the detailed magnetic field distribution and fine-tune the geometry of the coil.\nSo, while the physical concept is beautifully simple, the practical realization requires sophisticated computation and engineering.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 20 - Slide14.txt", "file_path": "Lecture 20\\Texts\\Slide14.txt", "content": "Now, let‚Äôs look at what happens when we use a surface coil instead of a full birdcage coil that covers the entire patient. A surface coil is a smaller coil placed directly near the region of interest‚Äîfor example, near the spinal cord or breast. Because it‚Äôs positioned so close to the tissue, it produces a very strong B-one field right next to the coil surface. However, the field strength drops off rapidly with distance, as you can see from the plot on the left. So the signal intensity is highest near the coil and becomes much weaker for tissues deeper inside the body.\n\nThis makes surface coils particularly useful for localized imaging, where we only need high-quality signals from a specific region, such as the spine or breast. They are also very important for parallel MRI, where multiple small coils are used together to collect data simultaneously from different parts of the body. This parallel setup helps speed up imaging and improve the overall signal-to-noise ratio.\n\nIn the past, implementing parallel MRI was quite challenging because it required careful calibration and coordination among many coils to cover the entire patient. But with modern electronics and coil arrays, this technique has become highly effective and widely used.\nSo, with this slide, we‚Äôve completed our overview of the MRI scanner hardware ‚Äî the superconducting magnet for the B-zero field, the RF coils for generating and detecting the B-one field, and the gradient coils for spatial encoding along X, Y, and Z directions.\n\nUnderstanding how these fields ‚Äî B-zero, B-one, and the gradient fields ‚Äî work together is enough to grasp the essential physics behind MRI. Once you‚Äôre comfortable with these concepts, you‚Äôll have a solid foundation for understanding the imaging process itself.\n\nIn the next section, we‚Äôll shift our focus to pulse sequences ‚Äî beginning with the spin echo sequence, which allows us to measure T-one-weighted, T-two-weighted, and proton-density images. Then, we‚Äôll move on to gradient echo sequences for faster imaging, and later discuss MRI angiography, diffusion-weighted imaging, spectroscopy, and MRI contrast agents, along with their safety considerations.\n\nThis marks the transition from the hardware foundation of MRI to the signal manipulation and image formation techniques that make this technology so powerful.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 20 - Slide15.txt", "file_path": "Lecture 20\\Texts\\Slide15.txt", "content": "We‚Äôve now completed our overview of the MRI scanner hardware‚Äîincluding how the system is configured and how the three key magnetic fields are generated: the B-zero field from the main magnet, the B-one field from the radio-frequency coil, and the gradient fields, G-X, G-Y, and G-Z, which are produced by the gradient coils.\n\nUnderstanding these fundamentals gives you a strong foundation for grasping how MRI actually works. You now know how each component contributes‚Äîhow the hardware creates, manipulates, and detects magnetic signals that form the basis of MRI imaging.\n\nNext, we‚Äôll move on to the pulse sequences, which define how we control these fields in time to generate different types of images. The first and most fundamental one we‚Äôll discuss is the spin echo sequence. This sequence is the key to producing T-one-weighted, T-two-weighted, and proton-density images‚Äîthe three major image contrasts used in clinical MRI.\n\nAfter that, we‚Äôll cover gradient echo sequences, which are designed for faster imaging and are commonly used in dynamic studies. Then we‚Äôll extend our discussion to MR angiography for blood flow visualization, diffusion-weighted imaging for tissue microstructure, and MR spectroscopy for biochemical analysis. Finally, we‚Äôll talk about MRI contrast agents‚Äîhow they work, and the important safety considerations associated with their use.\nSo from this point onward, we‚Äôll move from the hardware foundation of MRI into the timing and control of signals, which is where much of the image contrast and diagnostic power of MRI truly comes from.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 20 - Slide16.txt", "file_path": "Lecture 20\\Texts\\Slide16.txt", "content": "Now we‚Äôre entering one of the most important ideas in MRI ‚Äî phase and frequency encoding. Essentially, MRI operates in what we call a Fourier imaging mode, so understanding the Fourier transform is absolutely critical to understanding how images are formed.\nThis slide is really the key to everything that follows. If you can grasp what‚Äôs happening here, all the other pulse sequences ‚Äî no matter how complicated they look ‚Äî will make much more sense.\n\nIn the stationary reference frame, the overall signal that we detect ‚Äî or equivalently, the magnetization vector M ‚Äî is proportional to the proton density, written as rho of x, y. This function describes how many protons exist at each point in a given slice.\n\nWhen we apply phase encoding and frequency encoding gradients, we introduce exponential phase factors into the signal. Mathematically, this means we multiply the signal by e to the power of negative j times gamma times G-x times x times t, and also by e to the power of negative j times gamma times G-y times y times tau pe, where gamma is the gyromagnetic ratio, G-x and G-y are the gradient field strengths, and t and tau pe are the times for which those gradients are applied.\n\nThese exponential terms introduce both phase and frequency variations across the image slice. The phase encoding adds a phase shift that depends on position in the Y direction, and the frequency encoding controls how the signal oscillates in time for each X position.\n\nTogether, these variations give each location ‚Äî each small voxel ‚Äî a unique combination of frequency and phase. That‚Äôs what allows us to reconstruct a tomographic image. Without this encoding, all the information from the slice would be averaged together, and we‚Äôd lose spatial detail.\n\nNow, by performing a few mathematical steps ‚Äî specifically dividing and multiplying by two pi ‚Äî we can rewrite these expressions in a form that matches the Fourier transform. We define two variables: k-x equals gamma times G-x times t divided by two pi, and k-y equals gamma times G-y times tau pe divided by two pi.\n\nUsing these definitions, the detected signal S of k-x, k-y becomes proportional to the double integral of rho of x, y times e to the power of negative j two pi times (k-x times x plus k-y times y) integrated over x and y. This is exactly the two-dimensional Fourier transform of the object function, rho of x, y.\nSo, the measured data that we collect in MRI ‚Äî what we call k-space ‚Äî is simply the Fourier transform of the spatial proton density. Once we‚Äôve filled k-space by varying the gradient fields, we apply an inverse Fourier transform to reconstruct the image in real space.\n\nNow, in practice, the process can go in either order ‚Äî you can perform phase encoding first, followed by frequency encoding, or vice versa. For each phase-encoding step, we record one line of data in k-space, and by repeating this process many times, we gradually fill the entire k-space grid.\nThe green trajectory shown on the slide illustrates how we move through k-space: phase encoding determines the starting position, and frequency encoding defines the direction in which data are collected. Different pulse sequences change the way we traverse k-space ‚Äî sometimes line by line, sometimes in spirals or zigzags ‚Äî but the underlying principle is always the same.\n\nFinally, because of symmetry, if the image function is purely real, we theoretically need to sample only half of k-space. However, in practice, we usually sample the full space to ensure higher accuracy and image stability.\n\nSo, to summarize: phase encoding and frequency encoding translate spatial information into distinct frequency and phase variations. The signals we collect form the Fourier representation of the object in k-space. By applying the inverse Fourier transform, we convert this frequency-domain data back into a real, tomographic image.\nThis is the essence of the k-space theorem, and it‚Äôs the mathematical foundation of nearly all MRI imaging methods.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 20 - Slide17.txt", "file_path": "Lecture 20\\Texts\\Slide17.txt", "content": "Now that we understand how MRI encoding works through Fourier principles, let‚Äôs see how this idea is applied in one of the most fundamental pulse sequences ‚Äî the spin-echo imaging sequence.\n\nIn this diagram, the horizontal axis represents time, and you can see the major events in the sequence: the radio-frequency pulses, the gradients for slice selection, phase encoding, and frequency encoding, and finally, the signal readout by the analog-to-digital converter.\n\nThe process begins with a ninety-degree radio-frequency pulse, which flips the magnetization vector, M, from the z-axis into the x-y plane. Immediately after this, the spins start to dephase because of local field inhomogeneities ‚Äî some spins precess a bit faster, others a bit slower. As a result, the transverse magnetization gradually fades away.\n\nTo correct this dephasing, a one-hundred-eighty-degree pulse is applied halfway through the time interval known as T E, or echo time. This pulse effectively flips all the spins ‚Äî the ones that were ahead now fall behind, and those that were behind move ahead. After an equal amount of time, all the spins come back into phase, and an echo signal is formed. That‚Äôs why we call it a spin echo ‚Äî the system ‚Äúrephases‚Äù itself to produce a measurable signal.\nThe T R, or repetition time, is the total time between successive ninety-degree pulses. By changing T E and T R, we can emphasize different types of image contrast ‚Äî for example, T-one weighting, T-two weighting, or proton-density weighting.\n\nNow, if you look at the lower part of the diagram, you‚Äôll see three gradient waveforms:\u000bG - slice selects the imaging slice,\u000bG - phase applies a brief pulse to encode phase differences along the Y direction, and\u000bG - freq, or the frequency-encoding gradient, is turned on during the echo readout to separate spatial information along the X direction.\n\nThe signal is then digitized by the A/D converter, forming one line of data in k-space. By repeating the process with different phase-encoding gradients, we fill up all the lines of k-space. After that, an inverse Fourier transform converts the data into the final image.\n\nSo, in summary: the spin-echo sequence uses a ninety-degree pulse to excite the spins, a one-hundred-eighty-degree pulse to rephase them, and a carefully timed set of gradients to encode spatial information in both frequency and phase. This sequence forms the foundation of most MRI imaging techniques, and every other sequence ‚Äî including gradient echo, echo planar, and spiral imaging ‚Äî can be understood as a variation of this same principle.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 20 - Slide18.txt", "file_path": "Lecture 20\\Texts\\Slide18.txt", "content": "Now, let‚Äôs look at how multi-slice imaging works in MRI.\n\nIn MRI, slice selection allows us to excite only a specific region of the body instead of the entire volume. This is achieved by manipulating the resonance frequency of spins within a narrow slice using a gradient field. When we apply the gradient, the magnetic field strength changes linearly along the gradient direction. As a result, spins in different positions experience slightly different Larmor frequencies.\n\nBy sending in a radio-frequency pulse that matches the frequency range of one specific slice, we can flip the magnetization vector, M, only within that selected slice. The rest of the body remains unaffected.\n\nHowever, because the field varies linearly across the slice, the spins acquire different phase shifts ‚Äî spins in regions of stronger field accumulate larger phase factors, and those in weaker fields accumulate smaller ones. This variation is called dephasing.\n\nTo correct for this, we apply a refocusing, or rephasing pulse, that has the opposite polarity. This pulse restores the overall phase coherence. It doesn‚Äôt bring every spin perfectly back to its original state but rephases them on average, so that the net signal becomes measurable again.\nIn the context of slice selection, we call this process defocusing and refocusing. The refocusing gradient usually has half the area of the initial defocusing gradient because we only need to correct the average phase dispersion, not the exact microscopic alignment.\n\nA similar process occurs during frequency encoding. When the frequency-encoding gradient is applied, it introduces another phase variation. To recover the maximum signal strength, a negative gradient pulse with half the area of the first one is applied to refocus the spins again. Although the polarity of the gradient may appear the same in diagrams, the one-hundred-eighty-degree pulse inverts the spin directions, effectively flipping the polarity of the phase, so that the final echo appears consistent in direction.\n\nThe phase-encoding gradient then introduces controlled phase shifts across the slice, allowing each spatial location to contribute a unique signal component. These differences are what allow us to reconstruct distinct pixel values in the final image.\n\nIn the spin-echo sequence, the two key timing parameters are the echo time, T-E, and the repetition time, T-R. By carefully adjusting these parameters, we can control image contrast ‚Äî and, importantly, the time between echoes can also be used for additional slice selection.\n\nThat‚Äôs how we can acquire multiple slices within a single T-R period, as you see in the example here. Each slice is excited and read out in sequence using its own frequency offset, labeled as omega-one, omega-two, omega-three, and so on. This is called multi-slice imaging.\nIt‚Äôs a highly efficient way to cover a larger region of the body without increasing the total scan time.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 20 - Slide19.txt", "file_path": "Lecture 20\\Texts\\Slide19.txt", "content": "In multi-slice imaging, we can further optimize how slices are acquired. For example, we can excite slice three, then slice four, and then slice two, alternating the order to maximize time efficiency. This alternating strategy helps reduce the blurring or cross-talk that can occur when slices are too close together in time or space. By allowing enough separation between successive excitations, we achieve cleaner and more distinct images.\n\nNow, the spin-echo sequence we just discussed provides an excellent signal-to-noise ratio, because it effectively cancels out the dephasing caused by T-two-star effects. This gives us a pure T-two-weighted signal that‚Äôs highly reliable for tissue contrast. However, the main drawback is that the spin-echo sequence takes a long time. A full imaging session can easily last half an hour or more, which is often too slow for many clinical applications.\n\nTo address this limitation, we use gradient-echo imaging as an alternative. In a gradient-echo sequence, there is no one-hundred-eighty-degree refocusing pulse. This omission greatly shortens both the echo time, or T-E, and the repetition time, T-R. Because there‚Äôs no refocusing pulse, gradient-echo imaging does not compensate for magnetic field inhomogeneities, so the signal decays more quickly according to T-two-star relaxation rather than pure T-two.\nTo keep the signal strong and the scan fast, the echo time ‚Äî T-E ‚Äî is kept as short as possible, typically just a few milliseconds. In addition, the flip angle of the radio-frequency pulse is reduced to a small value, usually well below ninety degrees. This allows for rapid repetition of pulses and enables much faster imaging ‚Äî often completing a full scan in less than a minute.\n\nThere are several variations of gradient-echo sequences, including: FLASH, which stands for Fast Low-Angle Shot; FISP, or Fast Imaging with Steady Precession; GRASS, which uses Gradient-Refocused Acquisition in the Steady State; and SSFP, or Steady-State Free Precession. Each of these methods is built on the same principle: using small flip angles and gradient rephasing instead of a 180-degree pulse to achieve high-speed imaging.\n\nFor any given repetition time, T-R, there exists an optimal flip angle that maximizes the signal-to-noise ratio. This is known as the Ernst angle, given by the formula: alpha Ernst equals cosine inverse of e to the power of negative T-R divided by T-one. By choosing the Ernst angle, we balance image contrast and signal strength for efficient imaging.\n\nSo, to summarize: spin echo sequences give you excellent image quality but require long scan times, gradient echo sacrifices some signal stability for dramatically faster acquisition, and together, these two techniques form the foundation of nearly all MRI pulse sequences used in practice today.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 20 - Slide20.txt", "file_path": "Lecture 20\\Texts\\Slide20.txt", "content": "Now let‚Äôs look at three-dimensional gradient echo imaging. In this method, we extend the basic idea of gradient echo imaging into three spatial dimensions by using two separate phase-encoding gradients and one frequency-encoding gradient. Here‚Äôs how it works: we begin with a radio-frequency pulse that flips the magnetization vector, M, by a small angle alpha, much less than ninety degrees. Then, the system applies the first phase-encoding gradient, labeled G phase one, followed by a frequency-encoding gradient, labeled G freq, and finally a second phase-encoding gradient, G phase two. This sequence allows us to encode spatial information along all three axes ‚Äî x, y, and z ‚Äî so the final data form a three-dimensional Fourier transform. Mathematically, the reconstructed signal can be expressed as a triple integral over kx, ky, and kz, that is, rho of x, y, z equals the integral of S of kx, ky, kz times e to the power of j two pi times (kx x plus ky y plus kz z), integrated over dkx, dky, and dkz.\n\nEach phase-encoding step changes the gradient strength slightly, producing a different k-space plane. When we collect all these planes and perform a three-dimensional inverse Fourier transform, we obtain a volumetric image of the scanned region. Because the signal is read out immediately after excitation, it decays rather quickly due to T-two-star relaxation. However, this approach allows for very rapid data acquisition, enabling high-resolution three-dimensional imaging in a relatively short time. Compared with spin-echo sequences, the signal-to-noise ratio is somewhat lower because gradient echo does not compensate for field inhomogeneities. But the speed advantage is substantial ‚Äî scans can be completed much faster.\n\nTo achieve this balance, we use a small flip angle rather than a large one. If the flip angle is too large, the magnetization along the z-axis doesn‚Äôt have enough time to recover before the next excitation, and the signal from subsequent repetitions becomes weaker. By using a smaller flip angle and a short repetition time, or T-R, we can repeatedly excite and read out signals quickly, maintaining good image quality while minimizing scan duration. In short, three-dimensional gradient echo imaging sacrifices a bit of signal strength for a dramatic improvement in imaging speed. It is particularly useful for volumetric scans of the brain, heart, and other organs where time is critical.\n\nAnd if we push the concept even further, we arrive at echo planar imaging, which can acquire an entire image ‚Äî or even a full volume ‚Äî in just one or a few rapid readouts. Echo planar imaging is the foundation for real-time MRI and functional MRI studies, where both speed and temporal resolution are essential.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 20 - Slide21.txt", "file_path": "Lecture 20\\Texts\\Slide21.txt", "content": "Now we come to one of the fastest MRI imaging techniques ‚Äî Echo-Planar Imaging, or EPI. In this sequence, the magnetization vector, M, is first flipped into the x‚Äìy plane using a ninety-degree radio-frequency pulse. Next, slice selection is performed to isolate a specific plane ‚Äî for example, the x‚Äìy plane of the imaging region. Once the slice is selected, the imaging process begins with a combination of phase encoding and frequency encoding that alternate in an oscillating pattern. This means that the frequency-encoding gradient repeatedly switches direction ‚Äî first positive, then negative, then positive again ‚Äî while data are continuously acquired.\n\nAs a result, the signal alternates between positive and negative readouts, forming a continuous train of echoes. Each echo corresponds to one horizontal line in k-space, and the entire pattern forms a zig-zag trajectory across k-space, as shown on the right side of the slide. Let‚Äôs interpret this step by step: slice selection determines the imaging plane, typically the x‚Äìy plane. Phase encoding moves the position in k-space vertically ‚Äî that is, along the k-y direction ‚Äî shifting the data line by line. Frequency encoding determines the horizontal readout, or the k-x direction, as the data are collected. Each line of k-space is traversed in alternating directions: the first line moves from left to right, the next line from right to left, and so on. This continuous acquisition pattern allows us to collect a full image, or at least a major portion of it, in a single shot.\n\nHowever, as data acquisition progresses, the signal amplitude gradually decreases due to T-two-star decay, which limits how long useful data can be collected. Because of this decay, echo planar imaging may sometimes cover only part of k-space during one acquisition, requiring multiple passes to fill in the remaining data. Even with that limitation, EPI is dramatically faster than conventional three-dimensional gradient-echo imaging. In fact, it can complete a full image in just a fraction of a second. That‚Äôs why echo planar imaging has become the foundation for real-time MRI and functional MRI, where rapid temporal sampling is more important than perfect image contrast.\n\nAnd when we want to go even faster, we can move beyond echo planar imaging to spiral imaging, which traces k-space in smooth circular trajectories, further reducing acquisition time.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 20 - Slide22.txt", "file_path": "Lecture 20\\Texts\\Slide22.txt", "content": "Now, let‚Äôs move on to spiral imaging, a powerful and elegant approach that enables even faster data acquisition than echo-planar imaging. In spiral imaging, there is no separate phase encoding step. Instead, both gradient fields ‚Äî G x and G y ‚Äî continuously change their amplitudes over time. The relative strength of these two gradients determines the instantaneous slope of the trajectory in k-space.\n\nAt the very beginning, the system starts from the origin of k-space, meaning the initial point of the phase vector is zero. As time progresses, the amplitudes of G x and G y vary smoothly, causing the trajectory to move outward in a spiral pattern. The direction of this spiral is controlled by the relative phase and amplitude ratio between the two gradients. Initially, the trajectory moves along a specific direction for a short distance. Then, as the relative gradient amplitudes change, the slope and direction of the path gradually rotate. This continuous change allows the k-space trajectory to trace a smooth spiral curve, expanding outward from the center as higher spatial frequencies are collected.\n\nThe advantage of this approach is that it provides continuous control over both gradients, allowing highly efficient coverage of k-space. Depending on the design, the spiral can move from the center outward, from the periphery inward, or even use multiple interleaved spirals that fill k-space more densely and reduce artifacts. Because of this flexibility, spiral imaging offers excellent speed while still maintaining high spatial resolution. It captures low-frequency components first ‚Äî which contain the overall image contrast ‚Äî and then gradually collects higher-frequency components that define fine structural detail.\nAfter data acquisition, the spiral k-space data must be interpolated onto a rectangular grid before applying the two-dimensional inverse Fourier transform to reconstruct the image. This reconstruction yields T-one-weighted, T-two-weighted, or proton-density-weighted images, depending on how the pulse sequence parameters ‚Äî such as flip angle, T-E, and T-R ‚Äî are chosen. Understanding how data are collected in k-space is crucial. If you fully grasp how the gradients define the k-space trajectory ‚Äî whether linear, zigzag, or spiral ‚Äî the entire process of MRI image formation becomes intuitive. Otherwise, it can seem abstract or confusing.\n\nFinally, these flexible gradient-based trajectories also open the door to specialized MRI pulse sequences. For example, in MR angiography, we can tailor the sequence to highlight blood flow, just as CT angiography does for X-ray-based imaging. By manipulating the gradient and timing parameters, we can emphasize vascular structures and obtain clear, high-resolution images of the circulatory system. So, spiral imaging not only accelerates acquisition but also serves as a bridge to more advanced and application-specific MRI techniques, such as angiography, diffusion imaging, and functional MRI.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 20 - Slide23.txt", "file_path": "Lecture 20\\Texts\\Slide23.txt", "content": "Let‚Äôs now talk about Time-of-Flight Imaging, or TOF, which is a fundamental technique in Magnetic Resonance Angiography, or MRA. In a standard CT image, blood vessels and soft tissues often appear very similar in density. As a result, it‚Äôs difficult to distinguish the vasculature ‚Äî you can‚Äôt easily tell whether a vessel is open, narrowed, or completely blocked. To solve this problem in CT, we use a contrast agent ‚Äî typically iodine ‚Äî a liquid metal with a very high linear attenuation coefficient. When injected into the bloodstream, iodine increases the attenuation of X-rays in the blood, making vessels appear much brighter than the surrounding soft tissue. If a vessel that should be wide suddenly appears narrow or faint in the CT image, that suggests a vascular blockage, which could require stenting or further cardiac intervention. This is the basic principle behind CT angiography.\n\nNow, for MRI angiography, the principle is different, but the goal is the same ‚Äî to make the blood vessels appear distinct from the surrounding tissue. Here‚Äôs how it works: we first select a single imaging slice, and then apply a radio-frequency pulse, typically using an echo-planar imaging sequence, to flip the spins ‚Äî those small magnetic dipoles ‚Äî within that slice. At that instant, both the blood within the vessel and the stationary tissue generate signals that are quite similar, so initially, the vessel is not clearly visible. But if we wait a little longer, something interesting happens. The blood that was originally in the imaging slice flows out of the plane, and new blood ‚Äî containing unflipped spins ‚Äî flows in from outside. This is what we call the through-plane mechanism.\nBecause the newly arrived blood has not been excited by the RF pulse, it produces no signal in that moment. Meanwhile, the surrounding stationary tissue continues to emit its signal, which gradually decays but does not vanish immediately. As a result, when we collect the data after a short delay, the moving blood appears bright, while the stationary tissue looks relatively dark. This contrast difference allows us to visualize the vasculature clearly.\n\nThe Time-of-Flight effect is strongest when the blood flow is perpendicular to the imaging slice ‚Äî that is, when blood moves directly through the plane of excitation. If the vessel runs within the plane of the slice, then the same spins remain in the imaging region, and the through-plane mechanism does not work effectively.\n\nSo, to summarize: in CT angiography, iodine contrast enhances X-ray attenuation. In MR angiography, the motion of blood itself provides the contrast, through the time-of-flight effect and the through-plane mechanism. This principle forms the foundation of non-contrast MR angiography, allowing clear visualization of blood flow without any injected contrast agents.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 20 - Slide24.txt", "file_path": "Lecture 20\\Texts\\Slide24.txt", "content": "Now let‚Äôs move on to Phase-Contrast Imaging, which is another powerful method used in MR angiography. Unlike the time-of-flight technique that works mainly through the through-plane mechanism, phase-contrast imaging is an in-plane mechanism ‚Äî meaning it can detect blood flow in any direction, not just perpendicular to the slice. The basic idea is similar to what we‚Äôve seen in echo-planar imaging. We start with an alpha pulse that flips the magnetization vector, M, by an angle alpha for slice selection. All the spins within the selected slice then work together to produce a coherent signal. As usual, frequency encoding and phase encoding are applied to collect the Fourier information from that slice.\n\nHowever, in a normal Fourier-based MRI scan, the pixel intensities for blood and stationary tissue ‚Äî in terms of T-one, T-two, or proton density ‚Äî often appear very similar. This makes it difficult to distinguish flowing blood from static tissue. To separate them, we introduce a pair of bipolar gradient pulses ‚Äî one positive and one negative ‚Äî before data acquisition. The first, or positive, gradient pulse adds a phase shift to all spins, and the second, or negative, gradient pulse adds an equal but opposite phase shift. For stationary spins ‚Äî such as those in soft tissue ‚Äî these two effects cancel perfectly, leaving no net phase change.\n\nBut if the spins are moving, as in flowing blood, something different happens. Between the two gradient pulses, the blood cells physically move to a new position, so they experience slightly different local magnetic fields. As a result, the positive and negative gradients do not cancel out completely. The motion introduces an extra phase term that accumulates over time, and this additional phase is directly proportional to the velocity of the moving spins. By measuring the difference in phase between two acquisitions ‚Äî one with and one without motion encoding ‚Äî we can isolate the component of the signal that depends purely on blood flow. This produces a phase-contrast image, where bright regions correspond to areas of flow and darker regions to stationary tissue.\n\nIn mathematical form, this accumulated phase, phi, is given by an integral of the gradient over time: phi equals the integral from zero to tau over two of gamma times G-x times x of t, d-t, plus the integral from tau over two to tau of gamma times negative G-x times x of t, d-t. The net phase difference reflects the motion of the spins under the bipolar gradients. Conceptually, you can think of time-of-flight imaging as a passive effect, where the signal difference arises simply because new, unflipped blood enters the slice. In contrast, phase-contrast imaging is an active mechanism, where we intentionally encode blood motion into the phase of the signal.\n\nAnd unlike X-ray phase-contrast imaging ‚Äî which measures refractive index variations ‚Äî MRI phase-contrast imaging detects the phase shift of the magnetic vector caused by moving spins. It doesn‚Äôt matter whether the blood flows up, down, or sideways ‚Äî as long as the spins move through regions with slightly different magnetic fields, the phase difference can be detected and visualized. By exploiting these motion-induced phase shifts, we can generate clear vascular images and even quantify blood velocity or flow rate. Together with time-of-flight imaging, phase-contrast imaging provides a complete toolkit for visualizing the circulatory system without the need for injected contrast agents.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 20 - Slide25.txt", "file_path": "Lecture 20\\Texts\\Slide25.txt", "content": "Now, let‚Äôs talk about diffusion-weighted imaging, or DWI for short. Remember, our body is made up mostly of water, and these water molecules are constantly moving around. This motion ‚Äî or diffusion ‚Äî becomes physiologically and pathologically important, especially when something goes wrong at the cellular level. For example, when a cell membrane becomes swollen or damaged, water can leak out of the cell, altering the normal diffusion and perfusion patterns in that tissue. That‚Äôs why studying how water moves can tell us a lot about what‚Äôs happening inside the body ‚Äî particularly in the brain ‚Äî in cases like stroke, tumor, or inflammation.\n\nNow, conceptually, diffusion-weighted imaging is similar to angiography imaging, because both deal with motion ‚Äî but in DWI, we‚Äôre focusing on the microscopic motion of water molecules rather than blood flow. The pulse sequence looks a little different, but the underlying idea remains very similar. Here, we use a spin-echo sequence with a 90-degree excitation pulse followed by a 180-degree refocusing pulse. You‚Äôll notice two gradient pulses, placed symmetrically on both sides of the 180-degree pulse ‚Äî these are called diffusion-sensitizing gradients.\n\nWe already know that a 180-degree pulse flips the magnetization vector upside down. So, from the perspective of the first gradient pulse, the second one has the opposite polarity. If nothing in the tissue is moving ‚Äî meaning the water molecules are stationary ‚Äî then whatever phase shift is induced by the first gradient will be perfectly canceled out by the second one. In that case, there is no net signal change. But if the water molecules move between the two gradient pulses ‚Äî for example, diffusing from one position to another ‚Äî they experience slightly different magnetic field strengths at those two locations. As a result, the phase shifts from the two gradients no longer cancel out perfectly. This causes a measurable signal loss, which tells us that motion or diffusion has occurred.\n\nThe signal attenuation due to diffusion depends on three main factors: first, the gradient strength, represented by capital G; second, the duration of each gradient pulse, denoted by the Greek letter delta; and third, the time separation between the two gradients, represented by capital Delta. Together, these parameters determine how sensitive the sequence is to molecular motion. The diffusion-related signal intensity can be expressed as I of G n equals I zero times e to the power of negative D gamma squared G n squared delta squared times (Delta minus delta over three). This equation shows how the measured signal decreases with stronger diffusion.\n\nPractically, we don‚Äôt need to go through the derivation ‚Äî the key takeaway is that diffusion efficiency can be quantified and used as a diagnostic marker. In a typical spin-echo sequence, the gradients can be applied along the x, y, or z direction, depending on which axis of diffusion we want to measure. For example, diffusion in the through-plane direction ‚Äî perpendicular to the imaging slice ‚Äî can reveal how water molecules move between layers of tissue. When diffusion appears unusually high, it may indicate disrupted or damaged cellular structure, such as in ischemic tissue. So, by analyzing diffusion in this way, we gain valuable insight into both normal physiology and pathological conditions.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 20 - Slide26.txt", "file_path": "Lecture 20\\Texts\\Slide26.txt", "content": "Now, the diffusion rate we just talked about is not constant. It varies from place to place ‚Äî and more importantly, it also depends on direction. If you measure diffusion along one direction, you‚Äôll get a certain value. Measure it along another direction, and you‚Äôll likely get a different one. And along yet another direction, it changes again. So, unlike uniform motion that can be described by a simple vector ‚Äî where the magnitude represents the speed and the direction shows the path ‚Äî here, the diffusion rate itself depends on orientation. To describe such directional dependence, we need a more general concept than a vector ‚Äî and that‚Äôs where the tensor comes in.\n\nMathematically, we call this the diffusion tensor, often denoted by capital D. It‚Äôs a three-by-three symmetric matrix that characterizes how diffusion behaves along different spatial directions. Each element of this matrix describes the relationship between diffusion along one axis and its coupling with another axis ‚Äî for example, D one one, D one two, D one three, and so on. This means the diffusion process is anisotropic, or directionally dependent. In biological tissues such as white matter in the brain, water molecules tend to diffuse more easily along the fibers than across them, because the cell membranes and myelin sheaths restrict motion in the perpendicular directions. Similarly, in vasculature, blood and water tend to follow the principal axes of the vessels.\n\nNow, if we extend this idea further, we can describe the random diffusion process mathematically. The probability that a molecule starting at position r-naught moves to position r after some diffusion time, tau, can be expressed using a multivariate Gaussian function. This is written as: p of r given r-naught and tau equals one over (4 pi tau) to the power three-halves times the square root of the determinant of D, multiplied by e to the power of negative (1 over 4 tau) times (r minus r-naught) transpose, times D inverse, times (r minus r-naught). In simpler terms, this function describes the probability distribution of molecular displacement ‚Äî how far and in what direction the molecules are likely to move during diffusion.\n\nSo, diffusion tensor imaging, or DTI, is the MRI technique that measures this tensor for every voxel in the image. It gives us a complete three-dimensional map of how water diffuses in tissue ‚Äî allowing us to infer the orientation of fibers and structural connectivity in the brain and other organs.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 20 - Slide27.txt", "file_path": "Lecture 20\\Texts\\Slide27.txt", "content": "Now that we understand diffusion tensors, we can visualize them in a colorful way by encoding the three main spatial directions‚Äîx, y, and z‚Äîinto red, green, and blue channels respectively. So, at each location in the image, the color tells us the dominant direction of diffusion, and the brightness shows the degree of anisotropy, or how directional the diffusion is.\n\nFor example, a bright red region indicates that diffusion is strongest along the x-direction, green shows diffusion along y, and blue along z. When these combine, you get intermediate colors‚Äîjust like mixing red, green, and blue light‚Äîforming a vivid, color-coded diffusion map.\nThis type of image is called a fractional anisotropy, or FA map.\n\u000bIt tells us how strongly the diffusion is confined to one particular direction. In regions such as the white matter of the brain, where axonal fibers are highly aligned, the diffusion is strongly anisotropic‚Äîso these areas appear bright and vividly colored.\nWhat makes this especially powerful is that we are not only seeing the brain‚Äôs anatomy, but also its function and connectivity.\u000bWhile CT imaging primarily gives us anatomical information‚Äîshowing structures, shapes, and densities‚ÄîMRI can provide functional information, such as how water and blood move inside tissues.\n\nUsing diffusion-weighted and diffusion-tensor imaging, we can track how water molecules travel along nerve fibers, revealing the brain‚Äôs communication pathways. This is one of the reasons MRI is considered both a structural and a functional imaging modality.\nAnd with that understanding of diffusion and perfusion imaging, we are ready to move to the final concept of this section‚Äîspectral imaging, which explores the frequency domain information contained in MRI signals.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 20 - Slide28.txt", "file_path": "Lecture 20\\Texts\\Slide28.txt", "content": "Spectral imaging is like adding another layer of information to MRI. You can think of it as extending the same Fourier-based idea we‚Äôve been discussing ‚Äî but now, instead of looking at a single signal per voxel, we analyze multiple frequencies coming from that same voxel. Let‚Äôs pay attention to the sentence highlighted here: each voxel emits signals at multiple frequencies, rather than just a single frequency.\n\nIn conventional MRI, after phase and frequency encoding, every pixel or voxel produces one main signal ‚Äî one frequency ‚Äî corresponding to its local environment. But in spectral imaging, or what we often call MR spectroscopy, that same voxel emits several frequencies at once. Why does this happen? Because not all water molecules are in the same chemical environment. Some are bound within proteins, some interact with lipids, and others are part of metabolites. Each of these environments slightly shifts the resonance frequency of the hydrogen nuclei ‚Äî what we call a chemical shift.\n\nAs a result, a single voxel gives rise to multiple peaks ‚Äî each corresponding to different chemical compounds. For example, in the spectrum shown here, you can see peaks for creatine, choline, glutamate, glutamine, and N-acetylaspartate, or NAA. These molecules tell us about the brain‚Äôs biochemical composition ‚Äî for instance, NAA is a marker of healthy neurons, while choline indicates cell membrane turnover. So, spectral imaging gives us access to biochemical information, not just structural or functional detail. By measuring the exact frequencies and amplitudes of these peaks, we can infer what molecules are present and in what quantities.\n\nThe next natural question is ‚Äî how do we obtain these signals from a specific pixel or voxel? There are two main approaches, just like the passive and active methods we discussed earlier in angiography. Both techniques let us excite or isolate specific regions in space to record their unique spectral signatures ‚Äî and we‚Äôll explore those next.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 20 - Slide29.txt", "file_path": "Lecture 20\\Texts\\Slide29.txt", "content": "Now let‚Äôs look at one of the two methods used in MRI spectroscopy ‚Äî the PRESS method, which stands for Point-Resolved Spectroscopy. This method is commonly used to acquire the MR spectrum from a single voxel ‚Äî that is, a small three-dimensional volume within the body. Here‚Äôs the idea: you begin by applying a 90-degree RF pulse to select a specific slice of tissue, as shown in the figure. Then, you apply two 180-degree pulses, each combined with a gradient field ‚Äî one along the x direction and another along the y direction. These three pulses ‚Äî one 90-degree and two 180-degree ‚Äî together define a small box-shaped region in space.\n\nOnly the nuclei located inside that box experience all three RF pulses, and therefore, only that region contributes to the detected MR signal. Everything outside that voxel is effectively suppressed. Once the voxel is defined, the signal collected from it contains contributions from all the chemical compounds present there ‚Äî for example, metabolites like creatine, choline, and N-acetylaspartate. When you perform a Fourier transform on that signal, you get a spectrum, which shows the characteristic peaks of these molecules.\n\nSo essentially, PRESS isolates a specific voxel in three dimensions using three orthogonal gradients ‚Äî Gx, Gy, and Gz ‚Äî and extracts its spectrum. The next method, called chemical shift imaging, extends this idea further. It‚Äôs a bit more complex but very powerful. Instead of acquiring data from just one voxel, it gathers spectral information from multiple voxels across a slice ‚Äî creating a spatially resolved chemical map. This is a more advanced, and often more time-consuming, approach, but conceptually, it builds directly on what you see here.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 30, "slide_filename": "Slide30.txt", "slide_annotation": "Lecture 20 - Slide30.txt", "file_path": "Lecture 20\\Texts\\Slide30.txt", "content": "Now, let‚Äôs move to the second spectroscopy method ‚Äî CSI, which stands for Chemical Shift Imaging. This method extends the idea of PRESS. Instead of selecting a single voxel, CSI allows us to collect spectra from multiple voxels simultaneously, across an entire slice. Here‚Äôs how it works. First, you select an axial slice in the z-direction using a 90-degree RF pulse combined with a slice-selection gradient, as shown in the figure. Then, phase encoding is applied in the x and y directions ‚Äî that‚Äôs what the Gx and Gy gradients represent here.\n\nAs a result, the MRI system collects signals from many small voxels arranged in a grid pattern, as you can see on the right. Each voxel now contains its own chemical spectrum, showing peaks from various metabolites, just like the one we saw in PRESS. To reconstruct this data, we perform two-dimensional inverse Fourier transformations with respect to the spatial dimensions x and y, and a forward Fourier transformation with respect to time. This process gives a full spectrum for each voxel, mapping both spatial and chemical information across the slice.\n\nIn simpler terms, PRESS gives you the spectrum from one small volume, while CSI gives you a whole map of spectra from many volumes at once. It‚Äôs computationally heavier and takes longer to acquire, but it provides much richer biochemical information throughout the tissue. This technique is particularly valuable in brain spectroscopy, where you can visualize metabolic differences between normal and abnormal tissues. And just like other MRI techniques, the image quality and contrast can be further improved using specialized contrast agents ‚Äî typically compounds based on magnetic elements with unpaired electrons, which help enhance local magnetic field effects and improve visibility.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 31, "slide_filename": "Slide31.txt", "slide_annotation": "Lecture 20 - Slide31.txt", "file_path": "Lecture 20\\Texts\\Slide31.txt", "content": "Now, let‚Äôs look at an example of an MRI contrast agent ‚Äî Magnevist. MRI contrast agents are usually paramagnetic materials, meaning they have unpaired electrons that create local magnetic field variations. These variations enhance the local signal intensity and improve the signal-to-noise ratio, allowing us to better differentiate between tissues. When a contrast agent is introduced into the body, it tends to accumulate in regions with rich vasculature, such as tumors. This makes those areas appear brighter in the MRI image, helping radiologists identify abnormal growths or tissue changes.\n\nThe most common class of MRI contrast agents is based on metal ions such as gadolinium, manganese, or europium. Among these, gadolinium is particularly effective because the ion Gd¬≥‚Å∫ has seven unpaired electrons ‚Äî the maximum possible ‚Äî which gives it a very strong magnetic moment. However, gadolinium by itself is toxic, so it cannot be injected directly. Instead, it is safely enclosed in a chemical cage, called a chelate, that binds the gadolinium tightly. One of the most widely used formulations is Gadolinium DTPA, or Gadolinium Diethylenetriaminepentaacetic Acid, which is marketed under the trade name Magnevist.\n\nThis compound has been used clinically for many years to enhance MRI scans of the brain, spine, and other organs. It works by creating small magnetic inhomogeneities that shorten relaxation times, giving a strong contrast enhancement in vascular and pathological regions. However, although MRI itself is considered a green and non-ionizing imaging technique ‚Äî unlike CT, which involves radiation ‚Äî recent studies have shown that these contrast agents are not entirely harmless. Traces of gadolinium can sometimes remain in the body, leading to concerns about long-term safety, especially for patients who undergo repeated contrast-enhanced MRI scans.\n\nSo while MRI remains fundamentally safe, it‚Äôs important to be aware of potential risks associated with certain contrast materials, and to continue developing safer, more biocompatible alternatives for the future.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 32, "slide_filename": "Slide32.txt", "slide_annotation": "Lecture 20 - Slide32.txt", "file_path": "Lecture 20\\Texts\\Slide32.txt", "content": "Here we see an example of how a contrast agent like Gadolinium-DTPA, or Magnevist, works in clinical imaging. On the left is the chemical structure of Gd-DTPA. In this molecule, the gadolinium ion ‚Äî which has seven unpaired electrons ‚Äî is enclosed within a stable chelating cage. This cage prevents the toxic metal from freely interacting with biological tissue, while still allowing it to influence nearby water molecules. When water molecules come close to the contrast agent, they experience local magnetic field changes that speed up their T‚ÇÅ relaxation, producing a brighter signal on the MRI image. Water molecules that directly interact with the free binding site of the gadolinium ion experience what we call inner-sphere relaxation, resulting in a strong signal increase. Those that are nearby but not directly bound experience outer-sphere relaxation, which is weaker but still noticeable.\n\nOn the right, you can see a brain MRI taken after administration of Gd-DTPA. Notice the bright region within the brain ‚Äî this enhancement clearly reveals a tumor. The contrast agent has accumulated in that region because tumors typically have leaky vasculature, allowing the agent to diffuse more freely into the tissue. However, it‚Äôs important to mention that these contrast agents are not entirely safe. Although gadolinium is held within a chemical cage, studies have shown that residual traces of gadolinium can remain in the human body ‚Äî particularly in the brain and kidneys ‚Äî for a long time after injection. This has raised significant safety concerns.\n\nAs a result, the European Union has restricted or prohibited the clinical use of several popular gadolinium-based contrast agents. Researchers and manufacturers are now working to design safer alternatives ‚Äî either agents that can be excreted more efficiently or new imaging methods that require little or no contrast agent at all. So while MRI contrast imaging is extremely powerful, it reminds us that even advanced, ‚Äúnon-ionizing‚Äù imaging technologies come with important safety considerations.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 33, "slide_filename": "Slide33.txt", "slide_annotation": "Lecture 20 - Slide33.txt", "file_path": "Lecture 20\\Texts\\Slide33.txt", "content": "Now let‚Äôs talk about MRI safety, especially regarding the use of gadolinium-based contrast agents. Gadolinium, symbol Gd, is one of the 17 rare-earth elements and belongs to the lanthanide series of metals. It‚Äôs a paramagnetic material, meaning it has unpaired electrons that interact with magnetic fields. This property makes gadolinium extremely useful for enhancing the contrast in MRI scans, helping us visualize blood flow, tissue abnormalities, and tumors more clearly.\n\nHowever, gadolinium itself is highly toxic to humans. It has no natural biological role and is not found anywhere in the human body under normal conditions. The problem arises because gadolinium ions are similar in size and charge to calcium ions, which are essential for many cellular processes. When free gadolinium ions enter the body, they can interfere with calcium-dependent mechanisms, disrupting normal cell function. To minimize this risk, gadolinium is used in a chelated form, meaning it is locked inside a chemical cage ‚Äî such as DTPA ‚Äî to prevent the toxic metal from interacting directly with tissues.\n\nHowever, studies have shown that not all of the gadolinium is completely excreted from the body after an MRI. Small amounts can remain in the brain, kidneys, and other organs, even months or years later. This residual gadolinium may lead to inflammation, oxidative stress, neurological effects, or even genetic damage at the cellular level. Over the past decade, these concerns have prompted major regulatory changes. The European Medicines Agency has restricted or banned the clinical use of several widely used gadolinium-based contrast agents, particularly those that release gadolinium more easily.\n\nSo, while MRI itself is a non-ionizing and generally safe imaging modality, the contrast agents we use must be handled with caution. This is why the field is actively researching safer alternatives ‚Äî agents that are more stable, biodegradable, or even completely free of gadolinium ‚Äî to make MRI an even safer tool for medical diagnosis.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 34, "slide_filename": "Slide34.txt", "slide_annotation": "Lecture 20 - Slide34.txt", "file_path": "Lecture 20\\Texts\\Slide34.txt", "content": "So, as we‚Äôve just discussed, MRI contrast agents are not entirely harmless. Research in recent years has revealed that gadolinium-based agents ‚Äî while extremely useful for improving image clarity ‚Äî can sometimes remain in the body long after the scan, particularly in the brain and even within bone tissue. In response to this growing concern, the European Medicines Agency, or EMA, launched an extensive investigation. In 2017, their committee reviewed evidence showing that traces of gadolinium could persist in brain tissues for many months after a single injection. MRI scans even showed increased signal intensity in regions where gadolinium had accumulated.\n\nFollowing this review, the EMA decided to take precautionary action. They suspended several widely used linear gadolinium-based contrast agents, including MultiHance, Omniscan, Magnevist, and OptiMARK. These were once very common in clinical MRI but are now restricted in Europe due to the risk of residual gadolinium retention. This is not something you‚Äôll find discussed in most textbooks yet ‚Äî it‚Äôs a relatively recent development, but an important one to know. It reminds us that while MRI is a non-ionizing and generally safe imaging technique, it still comes with chemical safety considerations when contrast agents are used.\n\nNow, from here, I‚Äôll move quickly through a few additional slides that are not required for this course, but they‚Äôre worth knowing to give you a broader perspective. They illustrate how MRI, beyond diagnostic imaging, plays a major role in neuroscience and brain research, helping us study how the human brain functions and connects at a fundamental level.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 35, "slide_filename": "Slide35.txt", "slide_annotation": "Lecture 20 - Slide35.txt", "file_path": "Lecture 20\\Texts\\Slide35.txt", "content": "So far, we‚Äôve covered the main aspects of MRI ‚Äî the scanner hardware and the pulse sequences. These are the two core components that determine how MRI works and what kind of images we can obtain. Now, the remaining two topics ‚Äî Brain Initiatives and Novel Ideas ‚Äî are not required for this course, but I‚Äôll go through them briefly because they‚Äôre very interesting and show how MRI has evolved beyond traditional medical imaging.\n\nMRI is an exceptionally powerful tool, especially in brain research. It‚Äôs not only used to visualize anatomical structures, like the brain‚Äôs gray matter and white matter, but also to explore neurological function ‚Äî how different regions of the brain communicate and interact. Researchers around the world, through major brain initiatives, are using advanced MRI techniques to map brain connectivity, monitor neural activity, and understand complex cognitive processes such as memory, decision-making, and emotion.\n\nIn this way, MRI has become one of the most important technologies for both clinical diagnosis and fundamental neuroscience research.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 36, "slide_filename": "Slide36.txt", "slide_annotation": "Lecture 20 - Slide36.txt", "file_path": "Lecture 20\\Texts\\Slide36.txt", "content": "This has become one of the hottest topics in science ‚Äî not just in the United States, but also in Europe, China, and Japan. Governments around the world are investing heavily to unlock one of the greatest mysteries of all time: how the human brain works and how intelligence emerges. MRI plays a central role in this global effort because it allows researchers to look inside the living brain ‚Äî not only to see its structure, but also to monitor its activity in real time.\n\nHere you can see the timeline of the U.S. BRAIN Initiative, which was announced in 2013 by President Obama. The initiative brought together major agencies such as the NIH, NSF, DARPA, and later FDA and IARPA. Each agency contributes in its own way ‚Äî from developing new tools and technologies, to studying cognition, rehabilitation, and neurological regulation. The research priorities include identifying brain cell types, mapping neural circuits, developing technologies to monitor brain activity, and creating precise intervention tools.\n\nThe ultimate goal is to integrate all of these approaches to achieve a comprehensive understanding of how the brain functions ‚Äî from the molecular level to behavior. MRI is a key part of this vision, serving as one of the most powerful noninvasive tools for studying the human brain.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 37, "slide_filename": "Slide37.txt", "slide_annotation": "Lecture 20 - Slide37.txt", "file_path": "Lecture 20\\Texts\\Slide37.txt", "content": "Now let‚Äôs look at this fascinating experiment. Here, subjects were shown two different types of images ‚Äî human faces and buildings. The MRI scans you see here show how the brain reacts differently in each case. When a person looks at a face, certain brain regions light up ‚Äî you can see those areas in red and orange. When the same person looks at a building, other regions become more active. This happens because the brain‚Äôs oxygen-rich blood flow changes depending on what you are thinking or perceiving.\n\nWhen neurons in a region start firing, they consume more oxygen. The body responds by sending in oxygenated blood, which creates a stronger MRI signal in that area. This mechanism is called the BOLD effect, short for Blood Oxygen Level Dependent contrast. By analyzing these signal patterns statistically, we can tell what kind of object the person is viewing ‚Äî for example, a face versus a house. This was one of the early experiments demonstrating how brain activity reflects thought and perception.\n\nIn more recent studies, researchers have gone even further. Using advanced image analysis and machine learning, they can now reconstruct, with rough resolution, what a person is actually seeing ‚Äî essentially allowing us to peek into human perception. It‚Äôs an incredible step toward understanding how the brain encodes visual information. If you have time, I highly recommend watching a short video on this topic ‚Äî it‚Äôs truly amazing to see how far this technology has come.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 38, "slide_filename": "Slide38.txt", "slide_annotation": "Lecture 20 - Slide38.txt", "file_path": "Lecture 20\\Texts\\Slide38.txt", "content": "This short video, which I highly recommend you watch later, is truly fascinating. It explores how modern MRI technology can be used to study thought processes ‚Äî in other words, how it can ‚Äúread the mind.‚Äù\n\nIn this demonstration, researchers show that by monitoring brain activity patterns, we can detect what a person is recognizing or recalling, even without them speaking. For instance, imagine a criminal suspect claiming, ‚ÄúI didn‚Äôt commit the crime.‚Äù If you place this person in an MRI scanner and show images of various locations ‚Äî one being the actual crime scene ‚Äî the brain‚Äôs response will be noticeably different when that familiar location appears.\n\nThe brain‚Äôs recognition centers light up, revealing subconscious memory traces, even when the person tries to hide it. This shows the incredible potential of functional MRI ‚Äî not just for medical diagnosis, but also for applications in neuroscience, psychology, and even criminal investigation.\nThe video runs about ten minutes, and it‚Äôs well worth watching. It gives you a glimpse into how MRI can connect biology, cognition, and technology ‚Äî opening up entirely new frontiers for understanding the human mind.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 39, "slide_filename": "Slide39.txt", "slide_annotation": "Lecture 20 - Slide39.txt", "file_path": "Lecture 20\\Texts\\Slide39.txt", "content": "So far, we‚Äôve learned about the major medical imaging modalities ‚Äî CT, nuclear imaging, and MRI. Each of them has unique strengths and physical foundations. CT uses X-rays and attenuation; nuclear imaging uses radioactive tracers; and MRI uses magnetic resonance and the behavior of hydrogen protons.\n\nNow, one very exciting direction in modern imaging research is modality fusion ‚Äî combining these different imaging technologies into a single, integrated platform. For example, PET/CT and PET/MRI scanners combine anatomical and functional information together, so that we can see both the structure and the physiology in one unified image.\n\nBeyond this, people are exploring hybrid imaging ideas, where optical imaging, ultrasound, or even photoacoustic methods are combined with MRI or CT to capture complementary information. These approaches can greatly enhance diagnostic accuracy and help us understand the body‚Äôs processes at multiple scales ‚Äî from cellular activity all the way to organ-level structure.\n\nSo, while MRI itself is a powerful and mature modality, the fusion of imaging technologies is opening up new horizons ‚Äî what we might call the ‚Äúnext generation‚Äù of biomedical imaging systems. These are truly exciting times for both clinical medicine and research.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 40, "slide_filename": "Slide40.txt", "slide_annotation": "Lecture 20 - Slide40.txt", "file_path": "Lecture 20\\Texts\\Slide40.txt", "content": "So here we come to one of the most exciting directions ‚Äî imaging modality fusion.\n\nIn this triangle, you see the three major imaging technologies: CT, MRI, and PET or SPECT. Each of them has unique strengths ‚Äî CT provides excellent structural detail and shows calcification and bone; MRI gives superb soft-tissue contrast and functional information; and PET or SPECT provides metabolic and molecular insights.\n\nIn recent years, CT has already been combined with PET or SPECT, and MRI has also been paired with PET in what we call PET/MRI scanners ‚Äî Siemens, for example, developed a system called the MRI Pattern Scanner to take advantage of this synergy.\n\nBut the combination of CT and MRI remains one of the most challenging yet most promising directions. In cardiac imaging, for example, CT clearly shows the coronary vasculature and calcifications, while MRI reveals the soft tissue and functional aspects of the heart. Together, they can provide a comprehensive picture of both structure and physiology.\n\nUltimately, our goal ‚Äî and the vision our group has been promoting ‚Äî is to bring all imaging modalities together in one integrated system, so that we can visualize anatomy, function, and molecular information simultaneously. That would be the true ‚Äúall-in-one‚Äù scanner of the future ‚Äî the complete fusion of CT, MRI, and nuclear imaging.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 41, "slide_filename": "Slide41.txt", "slide_annotation": "Lecture 20 - Slide41.txt", "file_path": "Lecture 20\\Texts\\Slide41.txt", "content": "And this idea leads us to the next concept ‚Äî what we call Omni-tomography.\nOmni-tomography means all-in-one, all-at-once, and all-of-couplings. In other words, we want to bring all imaging modalities ‚Äî CT, nuclear imaging, and MRI ‚Äî together into a single integrated system.\n\nInstead of acquiring CT data first, then performing a nuclear scan, and finally running an MRI, the vision is to acquire all the information simultaneously, within one coordinated scan. Imagine a scanner capable of capturing structural, functional, and molecular information ‚Äî all at the same time, perfectly aligned in space and synchronized in time.\n\nThis concept was proposed and discussed in a perspective paper titled ‚ÄúTowards Omni-Tomography ‚Äî Grand Fusion of Multiple Modalities for Simultaneous Interior Tomography.‚Äù The paper outlines both the motivation and the technical strategies for making this vision possible.\nThe idea is ambitious ‚Äî it represents the ultimate form of imaging fusion ‚Äî but it‚Äôs also the natural next step in medical imaging, combining everything we‚Äôve learned from CT, MRI, and nuclear imaging into one unified platform.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 42, "slide_filename": "Slide42.txt", "slide_annotation": "Lecture 20 - Slide42.txt", "file_path": "Lecture 20\\Texts\\Slide42.txt", "content": "To move toward this vision of omni-tomography, we began by building a toy system ‚Äî a small prototype that integrates all the scanners together in one setup.\nHere you can see a conceptual model where CT, PET, SPECT, and MRI components are arranged concentrically around a common patient table. The idea is to allow all the modalities to operate within the same geometry so that they can collect data simultaneously from the same region of interest.\n\nIn this design, the PET ring and SPECT detectors are positioned together with an X-ray tube and detector pair for CT imaging, while a specially designed open MRI magnet provides the magnetic field without blocking the other modalities. This required careful electromagnetic shielding and mechanical design to ensure that each subsystem could operate without interference.\n\nAfter developing this integrated model, we focused on building a functional MRI subsystem, completing its design as a key step toward realizing a fully combined, all-in-one scanner. This system demonstrates the feasibility of truly simultaneous, multimodal imaging ‚Äî a step closer to our long-term goal of achieving comprehensive, unified tomography for both research and clinical applications.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 43, "slide_filename": "Slide43.txt", "slide_annotation": "Lecture 20 - Slide43.txt", "file_path": "Lecture 20\\Texts\\Slide43.txt", "content": "This slide shows the next step ‚Äî how we actually designed and integrated a CT‚ÄìMRI scanner. The idea is to combine the strengths of both modalities within a single system.\n\nHere you can see the components that make this possible. On the left, we have the CT subsystem, including the X-ray source and detector arranged in a compact, quasi-stationary ring. On the right are the MRI components, with magnetic coils and gradient coils designed to fit around the same imaging volume. The two systems operate together ‚Äî CT providing high-resolution structural information, while MRI provides soft-tissue contrast and functional details.\nAll these elements come together in the integrated setup shown at the bottom. This is a simultaneous CT‚ÄìMRI scanner, designed so that both modalities can collect data at the same time without interference.\n\nFor those of you who are interested in learning more, you can look up our paper published in Medical Physics titled ‚ÄúVision 20/20: Simultaneous CT‚ÄìMRI ‚Äî The Next Chapter of Multimodality Imaging.‚Äù\n\nIn addition to this high-end system, we are also exploring a low-cost version, aiming to make hybrid imaging more accessible for both research and educational purposes.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 44, "slide_filename": "Slide44.txt", "slide_annotation": "Lecture 20 - Slide44.txt", "file_path": "Lecture 20\\Texts\\Slide44.txt", "content": "Here, we move toward a more practical version of hybrid imaging ‚Äî a low-cost system that combines MRI and X-ray together. Instead of using a superconducting magnet as in a high-end MRI system, this design uses a permanent magnetic field, making it much simpler and more compact.\n\nThe concept is quite elegant. For each rotation, you obtain both an MRI projection and an X-ray projection at the same angle. By rotating the system around the patient, you can reconstruct the images together ‚Äî effectively achieving CT‚ÄìMRI fusion in a single setup.\n\nThis configuration opens many possibilities for future imaging systems. You can use MRI to guide X-ray imaging, combine anatomical and functional information in real time, and even explore new imaging modes for clinical or research applications.\n\nSo, while high-performance simultaneous CT‚ÄìMRI scanners represent the top tier of multimodal imaging, this hybrid spiral MR‚ÄìX-ray system offers a more accessible pathway to achieve similar goals ‚Äî bringing advanced imaging capabilities within reach for smaller labs or specialized applications.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 45, "slide_filename": "Slide45.txt", "slide_annotation": "Lecture 20 - Slide45.txt", "file_path": "Lecture 20\\Texts\\Slide45.txt", "content": "When a patient undergoes radiation therapy for cancer, a highly energetic beam is directed toward the tumor to destroy cancerous cells. However, the accuracy of this beam is critical ‚Äî we need to know precisely where the tumor is during treatment, and that‚Äôs where image guidance becomes essential.\n\nWhat you see here is a new concept that integrates interior MRI with a medical linear accelerator. The idea is to use MRI imaging to continuously visualize a small region of interest ‚Äî the area where the tumor resides ‚Äî while radiation therapy is being delivered. This allows us to target the tumor with submillimeter precision and adjust in real time if the tumor moves due to breathing or other physiological motion.\n\nThis work is being developed in collaboration with colleagues at UT Southwestern, where we are exploring how this interior MRI can guide radiation delivery and make cancer therapy safer and more effective.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 46, "slide_filename": "Slide46.txt", "slide_annotation": "Lecture 20 - Slide46.txt", "file_path": "Lecture 20\\Texts\\Slide46.txt", "content": "Building upon that idea, this slide shows our interior tomography approach for MRI-guided radiation therapy.\nHere, the MRI system is designed not to image the entire body but to focus only on a specific region of interest, such as the tumor area. This design uses superconducting coils and gradient coils arranged in a compact geometry that allows radiation beams to pass through while maintaining the ability to generate precise MRI images of the target zone.\n\nThis compact ‚Äúinterior MRI‚Äù setup makes it possible to integrate the MRI system directly into a linear accelerator ‚Äî the same device used for delivering radiation therapy. With this configuration, we can track the tumor in real time and adjust the beam accordingly, ensuring that radiation precisely targets the cancer while sparing surrounding healthy tissue.\n\nAlong this same direction, our latest research‚Äîpublished last year‚Äîintroduces an even more advanced concept based on polarized radio tracers, which bridges the gap between traditional gamma-ray imaging and MRI-based methods.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 47, "slide_filename": "Slide47.txt", "slide_annotation": "Lecture 20 - Slide47.txt", "file_path": "Lecture 20\\Texts\\Slide47.txt", "content": "Previously, we learned about gamma-ray imaging and radio tracers, which allow us to visualize physiological functions inside the body by detecting radiation emitted from injected tracers. Now, in this work, we take the concept one step further by combining emission and transmission tomography into a single, simultaneous process‚Äîwhat we call Simultaneous Emission‚ÄìTransmission Tomography, or SET.\n\nThis idea was presented at the Fully3D Conference in Xi‚Äôan, China, and it represents an important advance in multimodal imaging. Normally, in PET or SPECT, emission data and transmission data are collected separately‚Äîfirst one scan for anatomy and then another for function. But with SET, both types of data are acquired at the same time.\n\nBy combining emission and transmission information simultaneously, we can achieve more accurate reconstructions with improved spatial and contrast resolution. The images on the left show the structural information from the transmission data, while the ones on the right display emission activity from the radio tracer. When we fuse these two datasets, we obtain both anatomical and functional information in a single scan, reducing motion artifacts and improving quantitative accuracy.\n\nThis represents another step toward our broader goal‚Äîtrue multimodal imaging, where complementary information from different imaging mechanisms is acquired concurrently for the most comprehensive view of the human body.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 48, "slide_filename": "Slide48.txt", "slide_annotation": "Lecture 20 - Slide48.txt", "file_path": "Lecture 20\\Texts\\Slide48.txt", "content": "Now, let‚Äôs look at the motivation for Simultaneous Emission‚ÄìTransmission Tomography, or SET.\u000bThis idea comes from a concept called Polarized Nuclear Imaging, abbreviated as PNI, which builds on the magnetic resonance (MR) framework.\nIn conventional nuclear imaging, gamma-ray photons are emitted randomly in all directions. However, with a polarized radio tracer, the emission becomes directionally dependent. That means the gamma rays are no longer isotropic ‚Äî they are preferentially emitted along specific directions.\n\nAs shown in the figure, when nuclei are fully polarized along a magnetic field, the emitted gamma rays have a strong preference for the horizontal direction, and almost none are emitted vertically. By using RF pulses and magnetic field manipulations, similar to those in MRI, we can flip this polarization vector ‚Äî sometimes called the M vector ‚Äî to control the emission direction.\n\nIn other words, by flipping the spins with an RF pulse, we can switch from collecting gamma rays along the x‚Äìy plane to collecting them along the z direction. This enables us to encode spatial information into the atomic spin orientation, essentially merging nuclear imaging and magnetic resonance mechanisms.\nThe result is that polarized nuclear imaging provides much higher sensitivity than conventional MRI, since the signal is derived directly from gamma emission rather than from weak magnetic resonance. The only drawback is that it does not account for attenuation, which is usually handled by CT.\n\nOur recent paper builds on this idea ‚Äî showing that by using a polarized tracer, we can reconstruct both the radioactive tracer concentration and the attenuation background simultaneously, combining what MRI and CT each do best into a single integrated framework.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 49, "slide_filename": "Slide49.txt", "slide_annotation": "Lecture 20 - Slide49.txt", "file_path": "Lecture 20\\Texts\\Slide49.txt", "content": "With this polarized nuclear imaging framework, we can perform both emission and transmission tomography simultaneously.\u000bHowever, to make this practical, we need suitable polarized radioisotopes, or tracers.\n\nThis table lists a few existing candidates ‚Äî isotopes of xenon and krypton ‚Äî that can, in principle, be polarized and used for such imaging.\u000bFor example, xenon-131m has a half-life of about twelve days and a gamma emission energy around one hundred sixty-four kilo‚Äìelectron volts.\u000bXenon-127m decays faster ‚Äî with a half-life of sixty-nine seconds, emitting gamma rays between one hundred twenty-five and one hundred seventy kilo‚Äìelectron volts.\n\u000bAnd krypton-79m has a half-life of about fifty seconds, with gamma emission near one hundred thirty kilo‚Äìelectron volts.\nYou can see the challenge here: ideally, a good clinical tracer should have a half-life around thirty minutes, long enough for preparation and imaging, but short enough to minimize radiation dose. Unfortunately, none of these isotopes fit that range ‚Äî they are either too long or too short.\nThat‚Äôs why our team has been collaborating with chemists and nuclear physicists to identify or synthesize new polarizable tracers with more suitable half-lives and emission properties.\n\u000bIf such tracers can be developed, they would make simultaneous emission‚Äìtransmission tomography a realistic and powerful imaging tool for the future.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 50, "slide_filename": "Slide50.txt", "slide_annotation": "Lecture 20 - Slide50.txt", "file_path": "Lecture 20\\Texts\\Slide50.txt", "content": "", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 51, "slide_filename": "Slide51.txt", "slide_annotation": "Lecture 20 - Slide51.txt", "file_path": "Lecture 20\\Texts\\Slide51.txt", "content": "", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 52, "slide_filename": "Slide52.txt", "slide_annotation": "Lecture 20 - Slide52.txt", "file_path": "Lecture 20\\Texts\\Slide52.txt", "content": "Now, let‚Äôs look at how all of this comes together in the system design.\u000bIn this setup, you can selectively activate or turn on any voxel or pixel by controlling the magnetic gradients and RF pulses we just discussed.\nEach polarized nucleus‚Äîsuch as xenon or krypton‚Äîcan be individually manipulated using precise gradient fields in the x, y, and z directions. The coils you see here generate those magnetic field gradients, while the surrounding detectors collect the emitted gamma photons.\n\nSo, depending on which gradient and RF pulse combination you apply, you can excite a specific spatial region, measure its response, and reconstruct the image point by point.\n\nThis concept essentially merges nuclear emission detection with MRI-style spin control, enabling us to perform polarized nuclear imaging in a controlled and highly selective way. It‚Äôs a very elegant system, because we can combine magnetic resonance encoding with nuclear emission detection‚Äîbringing together the best of both imaging worlds.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 53, "slide_filename": "Slide53.txt", "slide_annotation": "Lecture 20 - Slide53.txt", "file_path": "Lecture 20\\Texts\\Slide53.txt", "content": "Now let‚Äôs talk about the data model behind this concept. As I mentioned earlier, each polarized tracer behaves like a tiny donut that can be flipped in any direction. If the polarization is not aligned with the detector, you won‚Äôt detect any signal. But once you flip it to the right orientation, that voxel ‚Äî or pixel ‚Äî becomes active, and we can measure it. In this model, for each voxel we have two unknowns: the local attenuation coefficient, which we call ‚Äúmu of r,‚Äù and the local tracer concentration, which we call ‚Äúlambda of r.‚Äù\n\nTo estimate these two unknowns, we make two measurements ‚Äî one from the left and one from the right ‚Äî denoted as ‚Äúm one of r‚Äù and ‚Äúm two of r.‚Äù These two measurements are expressed as: m one of r equals phi of r times the exponential of negative integral from minus infinity to r of mu of r d r, and m two of r equals phi of r times the exponential of negative integral from r to infinity of mu of r d r. Here, phi of r represents the gamma-ray flux, and mu of r is the attenuation coefficient along the ray path.\n\nIf we take the logarithm of the ratio ‚Äúm one of r over m two of r,‚Äù we obtain an expression involving the difference of two integrals of mu. By differentiating this relationship, we can recover mu of r directly as negative one-half times the derivative with respect to r of the logarithm of ‚Äúm one over m two.‚Äù Once mu of r is known, we can calculate phi of r, which equals the exponential of the integral of mu of r times m one of r, or equivalently, the exponential of the integral of mu of r times m two of r, depending on direction.\n\nIn simple terms, this means the gamma flux escaping from each side of an activated voxel is proportional to the tracer concentration at that point. So, with just two directional measurements per voxel, we can reconstruct both the attenuation coefficient and the tracer concentration ‚Äî achieving simultaneous emission and transmission tomography in a single scan. That‚Äôs the key idea behind SET, or Simultaneous Emission‚ÄìTransmission Tomography.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 54, "slide_filename": "Slide54.txt", "slide_annotation": "Lecture 20 - Slide54.txt", "file_path": "Lecture 20\\Texts\\Slide54.txt", "content": "Now, let‚Äôs look at how this approach can be extended to whole-body imaging. When we use a polarized radio tracer, we can perform both emission and transmission tomography within the same MRI framework. Because the tracer is polarized, it can emit gamma rays directionally ‚Äî and since the polarization can be flipped, we can measure signals from multiple directions without rotating any hardware.\n\nThis setup naturally integrates with MRI. So not only can we collect MRI signals, but at the same time, we can measure attenuation, just like in CT, and tracer concentration, as in nuclear imaging. In other words, we can combine the strengths of all three modalities ‚Äî CT gives us attenuation information, MRI gives us soft-tissue and molecular contrast, and nuclear imaging provides tracer distribution ‚Äî all in one coordinated system.\n\nHere, you see a concept for whole-body imaging. The patient lies inside a large cylindrical magnet, and the radio tracer inside the body is flipped periodically. Each time the tracer is flipped, gamma rays are emitted in opposite directions. These emissions are detected from both sides, allowing us to reconstruct a complete image of the entire body. A key advantage of this method is that the collimation ‚Äî or the directional selection of gamma photons ‚Äî is achieved magnetically. That means we no longer need a mechanical collimator, which normally blocks most photons and wastes energy.\n\nMagnetic collimation captures more of the emitted photons, improving sensitivity and reducing noise. So this design minimizes photon waste and integrates the best parts of CT, MRI, and nuclear imaging ‚Äî offering a very promising pathway for future multi-modality imaging systems.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 20", "slide_number": 55, "slide_filename": "Slide55.txt", "slide_annotation": "Lecture 20 - Slide55.txt", "file_path": "Lecture 20\\Texts\\Slide55.txt", "content": "For your homework, I‚Äôd like you to draw diagrams for all the pulse sequences we discussed in this lecture. That includes spin echo, gradient echo, diffusion-weighted imaging, and spectroscopy sequences. Along with each diagram, please explain the working principle ‚Äî how the sequence works and what kind of information it provides.\n\nAlso, please watch the YouTube video I mentioned earlier; it will help you better visualize how MRI can even be used for mind-reading research. Try to connect what you see in the video and what we discussed today about MRI signal generation and brain imaging. Then, take some time to read the textbook section on MRI so you‚Äôre well prepared for our next lecture on ultrasound imaging.\n\nThank you, and I‚Äôll see you in the next class.", "total_slides_in_lecture": 55}
{"lecture": "Lecture 21", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 21 - Slide1.txt", "file_path": "Lecture 21\\Texts\\Slide1.txt", "content": "Today, we‚Äôre beginning a very unique chapter in this course, focusing on ultrasound imaging. This is exciting because ultrasound is rather cost-effective, and becomes increasingly more important due to hardware improvements and AI techniques. In this lecture, we‚Äôll go through the fundamental principles behind ultrasound: what it is, how it works, and how it behaves in biological tissues.\n\nThen, in the next lecture, we‚Äôll move on to ultrasound imaging modes ‚Äî how the system collects, processes, and displays data. So, today is all about understanding the principles, the physics, and the key ideas that make ultrasound imaging possible.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 21 - Slide2.txt", "file_path": "Lecture 21\\Texts\\Slide2.txt", "content": "Here‚Äôs our course schedule to remind you where we are. After these two lectures on ultrasound, we‚Äôll finish the green book, which means we‚Äôll have covered everything from X-ray, CT, MRI, PET, to ultrasound imaging.\n\nAfter that, we‚Äôll move beyond the textbook to discuss optical imaging. This topic is not included in the green book, but they‚Äôre very important to know. Optical imaging is fascinating, and like US imaging it becomes increasingly relevant with advancement of AI methods in this area.\n\nFor now, your main focus should be on the ultrasound chapter in the green book. It‚Äôs about twenty pages long, so it‚Äôs quite manageable. I recommend reading it carefully ‚Äî it‚Äôs actually quite an enjoyable read, almost like a story.\n\nSo today, let me walk you through the first part of that chapter ‚Äî the foundation that connects the physics of sound with medical imaging.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 21 - Slide3.txt", "file_path": "Lecture 21\\Texts\\Slide3.txt", "content": "In this lecture, our main focus is on the physical principles of ultrasound. We‚Äôll start by understanding what an ultrasound wave really is ‚Äî how it‚Äôs generated, how it travels through biological tissue, and how it interacts with different types of materials inside the body. These are the physical foundations of ultrasound imaging.\n\nThen, we‚Äôll shift to the engineering aspects ‚Äî how we actually generate and detect these waves using an ultrasound transducer. You‚Äôll hear me mention the term ‚Äútransducer‚Äù a lot today. The word transduction means converting one form of energy into another. In our case, the transducer converts electrical energy into mechanical vibration to generate the ultrasound wave. When the reflected wave comes back, the same device converts the mechanical vibration back into electrical signals. So it‚Äôs a two-way process ‚Äî we transmit and receive using the same component.\n\nWe‚Äôll look at both single-unit and array-based transducers, and we‚Äôll talk about different types of resolution specific to ultrasound ‚Äî axial resolution, lateral resolution, and spatial resolution. Each of these determines how finely we can distinguish structures in an image.\n\nWe‚Äôll also talk about imaging contrast, which is very important for ultrasound. You‚Äôll learn about natural contrast, as well as how we can enhance contrast using microbubbles. Microbubbles are a very clever and modern idea ‚Äî tiny gas bubbles injected into the bloodstream to improve visibility in ultrasound. They‚Äôre not discussed much in your textbook, but I‚Äôll expand on them to give you a sense of current developments in ultrasound imaging.\n\nSo this is our outline for today. And as always, for every imaging modality we study, pay close attention to the outline ‚Äî it tells you exactly what to focus on. Try to understand each of these key ideas clearly ‚Äî the wave, the transducer, the resolution, and the contrast ‚Äî because together, they form the foundation for everything that follows in ultrasound imaging.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 21 - Slide4.txt", "file_path": "Lecture 21\\Texts\\Slide4.txt", "content": "Okay, now let‚Äôs talk about medical imaging again, focusing on the five major modalities we‚Äôve been studying.\u000bWe‚Äôve already gone through X-ray imaging, nuclear imaging, and MRI ‚Äî the three big ones. The next one is ultrasound, and after that, we‚Äôll finish with optical imaging, which, as you‚Äôll see later, is also very unique and quite powerful.\n\nUltrasound imaging has several clear advantages. First, it involves no ionizing radiation. That‚Äôs very important because, unlike X-rays or gamma rays, ultrasound uses mechanical sound waves, not electromagnetic radiation. So there‚Äôs no risk of DNA damage and no concern about radiation-induced cancer.\nAnother major strength is speed. Ultrasound provides images in real time ‚Äî you can literally move the probe and watch the image update instantly. The transducer, or ultrasound probe, can be gently moved across the body, for example, over the abdomen to monitor a baby during pregnancy, or over the chest to examine cardiac function. It‚Äôs a quick, flexible, and low-cost imaging modality.\n\nOf course, like every imaging method, ultrasound has its limitations. One limitation is the penetration depth. It can‚Äôt go very deep into the body, especially through hard tissue or bone. X-ray imaging, in contrast, is excellent for bone-tissue or tissue-air interfaces ‚Äî like lungs or bones ‚Äî because those materials have very different densities and attenuate X-rays differently. Ultrasound, on the other hand, depends on acoustic impedance, which determines how sound travels through a material. When there‚Äôs a large mismatch in impedance, such as between air and bone, most of the ultrasound energy reflects back instead of penetrating.\n\nThat‚Äôs why it‚Äôs difficult to image structures behind the lungs or skull. To get a clear image of the heart, for example, you have to find an acoustic window between the ribs ‚Äî an area where sound can travel through soft tissue effectively.\n\nUltrasound imaging mainly works on the principle of reflection and scattering. The system sends out sound waves; those waves hit structures inside the body ‚Äî tissue boundaries, organs, or small particles ‚Äî and the echoes are detected to form an image. There are also transmission-based ultrasound methods, but those are less common in clinical imaging.\n\nBeyond diagnosis, ultrasound is used for image-guided procedures, such as biopsies and minimally invasive surgeries, where live ultrasound images can be registered with prior CT or MRI scans to help guide the doctor precisely.\n\nAnd in recent years, ultrasound has even found a role in neuromodulation ‚Äî using focused ultrasound to stimulate neural tissue, potentially helping to treat conditions like depression or Parkinson‚Äôs disease. This is a very exciting and emerging field, showing that ultrasound is far more than just an imaging tool.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 21 - Slide5.txt", "file_path": "Lecture 21\\Texts\\Slide5.txt", "content": "Now, I know DARPA has funded huge research programs in this new frontier of ultrasound technology. It‚Äôs important to remember that ultrasound is not an electromagnetic wave. This makes it fundamentally different from X-ray, CT, nuclear imaging, or MRI.\n\nMRI, for example, uses radio-frequency (RF) signals, which are simply alternating magnetic fields ‚Äî still part of the electromagnetic spectrum. X-ray and nuclear imaging use high-energy photons ‚Äî X-rays and gamma rays ‚Äî which can behave like both waves and particles. All of those imaging types relate to Maxwell‚Äôs equations if we describe them as waves, or to Boltzmann‚Äôs equations if we describe them as particles.\nBut ultrasound is different ‚Äî it‚Äôs a mechanical wave, not electromagnetic. It physically vibrates particles in a medium ‚Äî air, water, or biological tissue. That‚Äôs why it needs a medium to travel; it can‚Äôt move through a vacuum.\n\nAs you can see in this frequency chart, the audible range for humans is roughly from 20 hertz to 20 kilohertz ‚Äî that‚Äôs what you and I can hear, like my speaking voice or birds singing. Ultrasound goes far beyond that, starting from around 20 kilohertz and reaching into the megahertz range.\n\nIn medical imaging, typical frequencies are between 2 MHz and 20 MHz, depending on the application. Lower frequencies penetrate deeper but give lower resolution; higher frequencies provide sharper images but can‚Äôt go as deep. Industrial and destructive-testing ultrasounds can reach up to hundreds of megahertz.\nSo, unlike the sound waves we can hear, these ultrasound waves are way too high-frequency for our ears. And with very high-power, focused ultrasound, we can actually go beyond imaging ‚Äî we can cut or heat tissue, like a kind of ultrasound knife. It‚Äôs used in certain treatments to destroy tumors or cauterize tissue.\n\nBut in this lecture, our focus remains on diagnostic ultrasound imaging ‚Äî understanding how these mechanical waves are formed, transmitted, and received to create medical images.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 21 - Slide6.txt", "file_path": "Lecture 21\\Texts\\Slide6.txt", "content": "Now let‚Äôs look at this picture, which illustrates a classic experiment that helps us understand what sound really is. This is Robert Boyle‚Äôs experiment, one of the earliest demonstrations of how sound needs a medium to travel.\n\nSo imagine this setup: we have a ringing alarm clock placed inside a large glass chamber with a valve on top. At first, the clock is ringing, and you can clearly hear the sound through the air inside the container. Then, we connect a vacuum pump and start to remove the air from the chamber.\n\nAs the air is pumped out, the sound of the clock becomes weaker and weaker until eventually, you can hardly hear anything at all. The clock is still ringing, but with almost no air inside the chamber, there‚Äôs nothing to carry the sound waves to your ears.\nThen, when you open the valve and let air rush back into the chamber, the sound suddenly becomes loud again.\n\nThis simple experiment beautifully demonstrates that sound waves require a medium ‚Äî such as air, water, or tissue ‚Äî to propagate. Unlike light or X-rays, which can travel through a vacuum, mechanical waves cannot.\n\nSo, in ultrasound imaging, the same principle applies: the sound waves travel through biological tissue, reflecting and scattering as they go, allowing us to measure and form images. Without a proper medium ‚Äî say, if there‚Äôs air between the transducer and the skin ‚Äî no ultrasound signal would be transmitted effectively. That‚Äôs why, as you‚Äôll see later, we always use ultrasound gel to eliminate air gaps and ensure good acoustic coupling between the probe and the body.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 21 - Slide7.txt", "file_path": "Lecture 21\\Texts\\Slide7.txt", "content": "Now, let‚Äôs take a closer look at what an ultrasound wave actually is.\u000bHere, the medium ‚Äî whether air, water, or biological tissue ‚Äî is made up of countless tiny molecules. When a transducer or some vibrating source starts pushing on those molecules, it moves them back and forth along one direction ‚Äî say, the z-axis.\n\nWhen the transducer pushes the molecules forward, they become compressed, and when it pulls back, they spread apart. This alternating process of compression and rarefaction creates a longitudinal wave that travels through the medium.\nIf you look carefully at the top figure, the denser areas represent compressions, where molecules are packed tightly together and pressure is high. The lighter areas show rarefactions, where the molecules are spread apart and the pressure is lower.\n\nNow, something very interesting happens here: the wave itself travels forward, but the individual molecules don‚Äôt actually move along with it. Each molecule simply vibrates back and forth around its equilibrium position ‚Äî it oscillates, but it doesn‚Äôt go anywhere overall.\n\nSo, the sound wave is really a transfer of energy through the medium, not a transfer of matter. The wave moves, but the particles stay roughly where they are, just vibrating locally. That‚Äôs an essential idea to keep in mind when we talk about how ultrasound propagates through tissue ‚Äî the energy travels forward, not the molecules themselves.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 21 - Slide8.txt", "file_path": "Lecture 21\\Texts\\Slide8.txt", "content": "Now that we know what a wave looks like, let‚Äôs think about how a wave is formed. This is a really fundamental question ‚Äî what makes a wave a wave?\nTo understand this, we need to bring together three important physical principles.\n\u000bFirst, the conservation of mass, which tells us that mass cannot be created or destroyed ‚Äî it must remain constant within the system.\u000bSecond, there‚Äôs the relationship between pressure and volume ‚Äî when you squeeze something, the pressure inside changes in a predictable way.\u000bAnd third, we have Newton‚Äôs second law, which relates force to mass and acceleration.\n\nIf you imagine a perfectly rigid body, and you push on one end, the entire object moves together as one piece ‚Äî there‚Äôs no wave because nothing inside deforms or compresses. But in real materials, there‚Äôs some elasticity ‚Äî things can compress slightly and bounce back. That elasticity allows small disturbances to propagate as waves.\nSo, when you push on a small region, it compresses and transmits that force to its neighbors, setting off a chain reaction that moves through the medium. The result is a wave.\n\nNow, if we were talking about electromagnetic waves, such as light or X-rays, we‚Äôd describe them using Maxwell‚Äôs equations. For mechanical waves, like ultrasound, we instead rely on these three mechanical principles: conservation of mass, the pressure‚Äìvolume relationship, and Newton‚Äôs second law.\n\nIn more advanced, graduate-level courses, we can actually derive the full wave equation from these laws step by step. But for now, the key point is understanding the intuition ‚Äî waves exist because a local disturbance in pressure or motion creates a self-sustaining pattern that propagates through the medium.\n\nAnd for me personally, this is the fun part ‚Äî not just using formulas, but really understanding why these waves behave the way they do. Once you grasp these core principles, you can always work out the details later. The details may change from one problem to another, but the underlying physics stays the same. That‚Äôs what gives you real insight ‚Äî and the ability to think beyond just using a device as a ‚Äúblack box.‚Äù\n\nSo now, with this foundation, let‚Äôs look specifically at how we form an ultrasound wave.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 21 - Slide9.txt", "file_path": "Lecture 21\\Texts\\Slide9.txt", "content": "Alright, so how exactly do we form an ultrasound wave? Let‚Äôs break this down step by step.\nThink about how ordinary sound travels through air. You can hear my voice because the sound waves ‚Äî or pressure waves ‚Äî travel from me to you through the air. Now imagine replacing the air with water molecules, or even biological tissue. The same principle applies.\n\nLet‚Äôs picture a small volume element, or what we might call a voxel, inside that material. Under normal conditions, that voxel is in equilibrium ‚Äî it has a certain pressure, volume, and position. Nothing‚Äôs changing.\n\nBut now, suppose we push on one side of that element toward the right. This causes a displacement, denoted by the lowercase letter w. When we push it, the volume of that little element decreases because the particles are squeezed closer together.\n\nAccording to the conservation of mass, the total amount of material inside that voxel stays the same ‚Äî but the pressure inside increases. That higher pressure creates a net force on neighboring elements, pushing them forward.\n\nBy Newton‚Äôs second law, the net force generates acceleration, which we call a. The acceleration then changes the velocity, denoted by u, and as the velocity builds up, the displacement w also changes further.\n\nSo, you can see that these quantities ‚Äî displacement (w), velocity (u), acceleration (a), pressure (p), and volume (v) ‚Äî are all linked together. They interact continuously as the disturbance moves through the medium.\n\nThis is not like pushing a rigid stick, where moving one end instantly moves the other. In a rigid body, everything moves together. But in an elastic medium, local motion depends on these dynamic relationships ‚Äî one region moves, affects the next, and that disturbance travels as a wave.\n\nSo, an ultrasound wave forms because of this ongoing interplay between pressure, volume, and motion. Push a little, the volume shrinks, pressure rises, acceleration increases, and that disturbance keeps propagating through the material ‚Äî and that‚Äôs your ultrasound wave.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 21 - Slide10.txt", "file_path": "Lecture 21\\Texts\\Slide10.txt", "content": "Now let‚Äôs take a closer look at how these relationships actually work.\u000bWhen we push on one side of the medium ‚Äî for example, this piston here ‚Äî we create a velocity, which we call u. The displacement, denoted as w, shows how far the boundary moves. In a simple case, displacement is proportional to velocity times time. Mathematically, we write this as w equals u times t.\nThat‚Äôs a linear relationship, meaning if you double the velocity, the displacement doubles in the same time period.\n\nBut, on the other side of the same element, the motion doesn‚Äôt happen immediately. The molecules there need time to respond, and that brings in acceleration, which we represent as a. The displacement caused by acceleration is a second-order relationship, expressed as w equals one-half times a times t squared.\nSo the first one ‚Äî w equals u times t ‚Äî is linear, and the second one ‚Äî w equals one-half a t squared ‚Äî is quadratic in time.\n\nThis means when you push on one side of a small tissue element, that side moves first, while the far side lags behind slightly. Because of this delay, the material between them becomes compressed, and pressure increases.\n\nThis process isn‚Äôt perfectly synchronized ‚Äî one side moves first, then the other side catches up. That‚Äôs why we get a pressure increment. The delay in motion creates compression, and that‚Äôs how the ultrasound wave starts to form.\n\nSo, putting it all together: the combination of displacement, velocity, and acceleration, together with changes in pressure and volume, gives us the basic mechanism of wave formation in tissue.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 21 - Slide11.txt", "file_path": "Lecture 21\\Texts\\Slide11.txt", "content": "Once we start driving one side of the material ‚Äî continuously pushing and pulling ‚Äî the wave begins to propagate through the medium.\nEach small element of the tissue responds a little later than the one before it. So, if you look along the depth of the tissue, every point is moving up and down in the same pattern, but with a small phase delay.\n\nThat delay is what makes the wave travel forward. You still see the same waveform shape, but each point along the medium is slightly out of sync ‚Äî slightly shifted in phase.\n\nThat‚Äôs what we call wave propagation ‚Äî the motion is passed from one particle to the next, not instantaneously, but through a sequence of tiny, delayed reactions.\nIf the material were completely rigid, like a solid stick, every part would move together at once, and there would be no wave. But because biological tissue is elastic, parts of it move at slightly different times ‚Äî and that‚Äôs exactly what allows the wave to form and travel.\n\nMathematically, if we focus on one physical quantity ‚Äî say displacement w, or pressure p ‚Äî and express all the other variables, like acceleration and velocity, in terms of that single variable, we can combine them into a single second-order differential equation.\n\nThat equation is called the wave equation, and it beautifully describes how the disturbance moves through the medium as a function of both space and time.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 21 - Slide12.txt", "file_path": "Lecture 21\\Texts\\Slide12.txt", "content": "Alright, now we‚Äôve arrived at one of the most elegant equations in all of physics ‚Äî the wave equation. In one dimension, it looks like this: partial squared W over partial Z squared equals one over C squared times partial squared W over partial T squared. So what does that mean? It means that the second derivative of displacement, W, with respect to space, Z, is equal to one over C squared times the second derivative of W with respect to time, T. In plain words, the way the wave changes in space is directly linked to how it changes in time. Space and time are tied together through this relationship. You can even think of it conceptually as ‚Äúspace equals time‚Äù ‚Äî not literally, but in a second-derivative sense. This symmetry is what gives rise to those beautiful, smooth, repeating sinusoidal waves we see in sound or vibration.\n\nNow, let‚Äôs talk about the speed of sound, denoted by C. C depends on two key material properties ‚Äî density, which we call rho, and compressibility, which we call kappa. The relationship is given by this equation: C equals one divided by the square root of kappa times rho. Here, rho ‚Äî written as the Greek letter œÅ ‚Äî represents the density of the tissue, and kappa ‚Äî the Greek letter Œ∫ ‚Äî represents compressibility, which is actually the inverse of the bulk modulus, or in other words, how stiff the material is.\n\nNow here‚Äôs the interesting part. The stiffer the tissue, the smaller the compressibility. And when compressibility is small, the wave travels faster. So, in biological tissues, sound typically moves at about fifteen hundred meters per second ‚Äî that‚Äôs roughly the speed in soft tissue or water. But in bone, which is much stiffer, the speed can reach around four thousand meters per second. And clinically, this becomes quite important. For example, a malignant tumor tends to be harder than normal tissue. That means sound travels faster through it. So in ultrasound elastography, we can actually measure that difference in speed to assess tissue stiffness ‚Äî and that helps identify abnormalities like tumors.\n\nSo this simple equation ‚Äî the wave equation ‚Äî doesn‚Äôt just tell us how sound moves; it also connects directly to the mechanical properties of the body ‚Äî its density, its stiffness, and its elasticity. That‚Äôs the physical foundation of how ultrasound works.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 21 - Slide13.txt", "file_path": "Lecture 21\\Texts\\Slide13.txt", "content": "Now I want to mention two important types of mechanical waves ‚Äî the so-called P-wave and S-wave, also known as longitudinal and transverse waves. Remember what we discussed earlier ‚Äî when you push a volume of air in one direction and then release it, the molecules move back and forth along that same line. This back-and-forth motion produces a longitudinal wave, where regions of compression and rarefaction travel in the same direction as the wave. That‚Äôs what we call a P-wave, or pressure wave. In ultrasound, this is the type of wave we use ‚Äî the longitudinal wave traveling through soft tissue.\n\nNow, if we compare that to a transverse wave, the vibration direction is completely different. Instead of moving along the same line, the oscillation happens perpendicular to the wave‚Äôs direction. So if the wave moves horizontally, the vibration is up and down. This is exactly what happens with electromagnetic waves, like light or radio waves ‚Äî their electric and magnetic fields vibrate at right angles to the direction the wave is moving.\n\nSo, to summarize: in a longitudinal wave, the vibration and propagation directions are the same, while in a transverse wave, they are orthogonal. Both are just different ways energy can propagate through a medium.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 21 - Slide14.txt", "file_path": "Lecture 21\\Texts\\Slide14.txt", "content": "Here again, we see how a longitudinal wave behaves in ultrasound. The sound originates from a source, like a vibrating tuning fork. In fact, in the old days, doctors would use a tuning fork to test hearing ‚Äî by striking it and moving it near the ear, they could tell if the patient could sense sound through air or through bone conduction. That tuning fork generates a longitudinal sound wave, where the compressions and rarefactions travel along the direction of vibration.\n\nNow, both longitudinal and transverse waves have some key characteristics. They each have a wavelength, which is the distance between two corresponding points ‚Äî like from one peak to the next, or from one compression to the next. That‚Äôs the spatial period of the wave. They also have amplitude, which represents how strong the wave is ‚Äî the height of a peak or the depth of a trough.\n\nIf you look at the shape of these waves, you‚Äôll notice they form a sinusoidal pattern ‚Äî this is not a coincidence. Sinusoidal waves naturally arise when solving the wave equation, whether it‚Äôs for mechanical or electromagnetic waves. Mathematically, they are fundamental solutions. And in Fourier analysis, we use sinusoidal components ‚Äî sines and cosines ‚Äî as building blocks to represent all kinds of signals. So in a way, every complex sound or image signal can be broken down into a sum of simple sine waves.\n\nFinally, remember that the speed of sound is directly connected to the wave equation. The constant you see there ‚Äî 1 over c squared ‚Äî represents how the wave travels through a given medium.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 21 - Slide15.txt", "file_path": "Lecture 21\\Texts\\Slide15.txt", "content": "So when we solve the wave equation, we find that the constant c represents the speed of sound in the medium. And this speed depends entirely on the material properties ‚Äî how dense and how stiff the material is. For example, sound waves travel very differently in air, in steel, or in biological tissue.\n\nIf we look at this table, we can see that the speed of sound in air is around 330 meters per second, in lung tissue about 600 meters per second, in fat about 1,450 meters per second, and in soft tissue around 1,540 meters per second. Water is very similar to soft tissue, around 1,480 meters per second, which is why we often use water-based gels in ultrasound coupling. In muscle, the speed is a bit higher, about 1,600 meters per second, and in bone, it reaches as high as 4,000 meters per second.\n\nSo, the denser and stiffer the material, the faster the sound travels through it. But within the same medium, like soft tissue, the speed stays constant ‚Äî it‚Äôs a property of the material itself. What we can vary is the frequency of vibration. When we increase frequency, the wavelength becomes shorter. So, higher frequency means shorter wavelength, and lower frequency means longer wavelength.\n\nThis relationship between frequency and wavelength is fundamental in ultrasound physics ‚Äî it helps determine both resolution and penetration in imaging.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 21 - Slide16.txt", "file_path": "Lecture 21\\Texts\\Slide16.txt", "content": "Now, it‚Äôs nice for you to know this ‚Äî as we look deeper into how a sound wave propagates, we can see that when the wave travels into a field, energy is actually being carried through the medium. Earlier, we talked about the wave equation in terms of displacement, and if you take the first derivative of displacement with respect to time, you get velocity. That‚Äôs what‚Äôs shown here in this equation: u  z equals dW over dT.\n\nA typical value for u z, the particle velocity, is around one to ten centimeters per second, which is much smaller than the speed of sound, c. So that‚Äôs something to remember ‚Äî the medium‚Äôs particles move slowly, even though the sound wave itself travels fast.\n\nNow, because the sound wave is propagating, it also introduces pressure into the field. That pressure at a given point can be computed as shown here: p equals rho times c times u z. It‚Äôs a very straightforward relationship ‚Äî the pressure is proportional to the density of the medium, the sound speed, and the particle velocity.\nSince the wave equation can be solved, and as I explained before, both the pressure and the velocity take on sinusoidal waveforms, we can represent each of them using an oscillating form ‚Äî exponential or sinusoidal ‚Äî like this: p of t equals p naught e to the j omega t, and u z of t equals u naught e to the j omega t. So now, you have both quantities ‚Äî pressure and velocity ‚Äî in a sinusoidal oscillation form.\n\nFrom here, we can define intensity, represented by I. The average intensity over a period, capital T, is defined as the product of p and u, averaged over time. Now, let‚Äôs think about this physically. Pressure, p, has the unit of force per unit area, so you can think of it as force distributed across a surface.\n\nIf you apply a force to move something over a certain distance, that‚Äôs work. So, this is essentially the work you did. But notice that in this case, we‚Äôre not talking about displacement or distance directly ‚Äî u is the rate of change of displacement, or in other words, velocity. That means we‚Äôre really talking about the rate at which work is done ‚Äî and the rate of doing work is power.\n\nSo, if you multiply pressure (which is force per area) by velocity (which is displacement over time), what you get is power per unit area, and that‚Äôs exactly what intensity represents. It‚Äôs how much power, or energy per unit time, is transmitted through a certain area in the medium.\n\nSo, putting it all together:\u000bPressure, p, corresponds to force per area.\u000bVelocity, u, is displacement per time.\u000bMultiply them together ‚Äî p times u ‚Äî and you get work per time per area, or power density, which we call intensity.\nThat‚Äôs how these three physical quantities ‚Äî velocity, pressure, and intensity ‚Äî are all linked. And even though these can be derived from the wave equation, here we‚Äôre just focusing on their basic definitions and physical meanings.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 21 - Slide17.txt", "file_path": "Lecture 21\\Texts\\Slide17.txt", "content": "So now you understand how we measure intensity, and how energy propagates through a medium. As the ultrasound wave travels through tissue, it doesn‚Äôt stay constant ‚Äî it gets attenuated. That means the amplitude decreases gradually, and the intensity drops as the wave penetrates deeper. Later, we‚Äôll see that the amplitude or intensity decays exponentially with depth, very similar to Beer‚Äôs law in optical physics.\n\nBecause of this, we often use relative intensity and relative power to describe how intensity changes with distance. And since these values can vary across a huge range ‚Äî sometimes millions of times between the incident pulse and the reflected echo ‚Äî it‚Äôs much easier to express them on a logarithmic scale.\nThis is where the decibel, or dB, scale comes in. It compresses large ratios into smaller, more manageable numbers. So, instead of dealing with enormous values, we talk about relative changes in decibels.\n\nFor ultrasound, pressure intensity is expressed in decibels. The unit of pressure is the Pascal, which is one Newton per square meter. The atmospheric pressure is about 0.1 megaPascal, while diagnostic ultrasound can have a peak pressure of around 1 megaPascal.\n\nNow, we describe relative intensity using the formula 10 log of I2 over I1. This represents the change in intensity between two points. If you‚Äôre dealing with power, however, we use 20 log of P2 over P1, because power is proportional to the square of amplitude.\n\nLet me explain that part carefully. In circuit theory, we know that power equals resistance times current squared ‚Äî P equals R times I squared. The same idea applies here. Since power is proportional to amplitude squared, we need a factor of two in the logarithmic expression. That‚Äôs why, for power, we multiply by 20 instead of 10.\n\nSo, in summary:\u000b‚Äì When we talk about intensity, we use 10 log because intensity is directly proportional to energy flow.\u000b‚Äì When we talk about power, we use 20 log because it depends on amplitude squared.\nA 10 dB change corresponds to a factor of 10 change in intensity.\u000bA 20 dB change corresponds to a factor of 100 change in intensity.\nThis logarithmic relationship is used throughout ultrasound physics because it helps us describe very large variations in intensity and power in a compact, practical way.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 21 - Slide18.txt", "file_path": "Lecture 21\\Texts\\Slide18.txt", "content": "So now, let‚Äôs do a quick review and apply what we just learned. We‚Äôll look at how to calculate remaining intensity and half-value thickness for ultrasound.\nFor the first example, we start with a 100 milliwatt ultrasound pulse that loses 30 decibels as it travels through tissue. Using the formula, negative 30 dB equals 10 log of I2 over 100 milliwatts, we can solve for I2. Rearranging, we get I2 equals 10 to the power of negative three times 100 milliwatts, which equals 0.1 milliwatt. So, after losing 30 dB, the intensity is reduced by a factor of one thousand.\n\nNow, let‚Äôs look at the second example ‚Äî the half-value thickness. This represents the depth at which the ultrasound intensity drops by 50 percent. That means I2 over I1 equals 0.5. Using the decibel formula again, we get 10 log of 0.5 equals negative 3 dB. So, when the intensity is reduced by half, it corresponds to a 3 decibel change.\n\nThis concept is very similar to what we use in X-ray or gamma-ray imaging, where we also talk about a ‚Äúhalf-value layer.‚Äù It tells us how deep the wave can travel before its intensity is halved.\n\nIn ultrasound, this is just a convenient way to express relative changes using logarithms ‚Äî there‚Äôs no physical constant behind it; it‚Äôs simply a mathematical convenience. The decibel scale helps us describe very large or very small intensity ratios in a practical and readable way.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 21 - Slide19.txt", "file_path": "Lecture 21\\Texts\\Slide19.txt", "content": "Okay, now we'll talk about acoustic impedance. This concept is actually not dramatically different from what we‚Äôve already learned in electrical circuits. Remember when we studied circuit analysis? We defined resistance as the ratio between voltage and current. Later, we introduced impedance, which included the effects of capacitance and inductance, and again it was defined as voltage over current. The only difference was that, in general, impedance can have a phase shift, so it‚Äôs expressed as a complex number ‚Äî meaning that both voltage and current can be complex, or we can simply describe them in terms of their phase relationship.\n\nNow, think about this analogy in the context of ultrasound. In an electrical circuit, voltage is the driving force ‚Äî the potential difference that pushes electrons through the conductor. In ultrasound, the driving force is pressure, P. The mechanical wave you push into the medium ‚Äî that‚Äôs the pressure wave. So, just as voltage drives electron motion, pressure drives the particle motion in the tissue.\n\nIn an electrical circuit, current is the flow of electrons. But in ultrasound, no electrons are flowing; instead, the medium‚Äôs molecules ‚Äî air molecules or soft tissue molecules ‚Äî move back and forth along one direction. At any given location, they have an instantaneous velocity, which we call u z. So, at each point in space, the particles have an instantaneous velocity that‚Äôs changing with time.\n\nBoth P and u keep changing with time, but when we solve the wave equation, we find something very interesting. It‚Äôs beyond the scope of this lecture to show the full derivation, but the result is that the ratio between pressure and velocity ‚Äî P over u ‚Äî is constant at any given location. Even though both pressure and velocity oscillate as the wave propagates, their ratio remains constant in amplitude. Because this ratio stays constant, we can define it as a characteristic property of the medium.\n\nThis constant ratio is what we call acoustic impedance, denoted by Z. If the ratio kept changing, it would depend on position or time, and we couldn‚Äôt define it as an intrinsic property of the material. But because it‚Äôs stable and characteristic, it gives us something meaningful ‚Äî a property that tells us how the medium resists the passage of sound.\n\nAcoustic impedance is extremely important in ultrasound imaging. When we use an ultrasound transducer, we have to make sure that the acoustic impedance of the transducer matches the acoustic impedance of the tissue we‚Äôre imaging ‚Äî for example, the abdomen. If there‚Äôs a big mismatch between them, the energy cannot enter the body effectively, and you can‚Äôt collect the echo signals efficiently. That‚Äôs why we use ultrasound gel ‚Äî it acts as a coupling layer that bridges the impedance gap between the probe and the skin.\n\nSo, to define it heuristically: acoustic impedance is like voltage over current in an electrical system. Here it‚Äôs pressure over particle velocity. We can show this mathematically, though I won‚Äôt go through the full derivation now. By solving the mechanical wave equation, we can prove that Z is constant and depends only on the material‚Äôs intrinsic properties ‚Äî specifically, density (rho) and sound speed (c).\n\nMathematically, we can write this as Z equals rho times c. And we know that the sound speed, c, can be expressed as one over the square root of rho times kappa, where kappa is the compressibility. So, substituting that into the equation, we get Z equals rho over the square root of rho times kappa, which simplifies to Z equals the square root of rho over kappa.\n\nThis shows that acoustic impedance depends on both density and compressibility, and it is a fundamental property of the material.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 21 - Slide20.txt", "file_path": "Lecture 21\\Texts\\Slide20.txt", "content": "Now, once we have this equation, Z equals the square root of rho over kappa, we can calculate or measure the acoustic impedance for different types of biological tissues. Equation 3.8 and the table shown here give you typical values.\n\nIf you look at the numbers, you can get a feeling for how impedance and sound speed vary across materials. In air, the sound speed is only about 330 meters per second, because air is very light and compressible. In biological tissues like the kidney or liver, the sound speed is much higher ‚Äî around 1,560 to 1,570 meters per second ‚Äî because these materials are denser and less compressible.\n\nNow, if you ask where sound travels the fastest inside the human body, the answer is bone, because bone is the densest and stiffest material we have. The table shows a speed of about 3,500 meters per second in bone, which is several times faster than in soft tissue.\nThese are just typical examples of tissue acoustic properties, but they are extremely important for understanding how ultrasound interacts with different parts of the body. The speed of sound, the density, and the impedance all influence how much of the sound wave is transmitted, reflected, or absorbed at tissue boundaries.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 21 - Slide21.txt", "file_path": "Lecture 21\\Texts\\Slide21.txt", "content": "So next, let‚Äôs talk about ultrasound‚Äìtissue interaction. Earlier, we mentioned how different imaging modalities interact with biological tissues. For example, X-ray and CT involve attenuation and scattering, while gamma-ray imaging behaves similarly. In MRI, we talk about T1 and T2 relaxation parameters to describe energy interactions within tissues.\n\nFor ultrasound, the same basic idea applies ‚Äî it‚Äôs all about energy propagation and how that energy interacts with biological structures. The first major effect we talk about is reflection. When a sound wave hits a boundary where there is a difference in acoustic impedance ‚Äî for example, between air and tissue, or between bone and tissue ‚Äî some of the energy is reflected. That‚Äôs what creates the echoes we use to form ultrasound images.\nReflection happens because of impedance mismatch. If the impedance difference is large, reflection is strong. If the impedances are well matched, reflection is weak, and more energy passes through.\n\nNow, with X-rays and gamma rays, reflection is not really an issue, because their wavelengths are extremely short and the photons are so energetic that they mostly pass straight through. In ultrasound, however, the wavelength is much larger, so reflection becomes significant.\nSo, remember this simple rule: reflection at a boundary means a change in direction.\n\nThe second effect is scattering. When ultrasound waves travel through tissue, they encounter many small structures ‚Äî cells, organelles, connective fibers ‚Äî that scatter energy in different directions. So, instead of traveling as a narrow beam, the energy spreads out or diffuses. You can think of this like Compton scattering in X-rays ‚Äî energy is diffused, and the propagation direction changes randomly.\n\nThe third effect is absorption, which converts mechanical energy into heat. In the case of X-rays, this corresponds to the photoelectric effect, where photon energy is absorbed completely. For ultrasound, as the wave passes through tissue, the molecules oscillate back and forth, and because of internal friction, some of the mechanical energy is lost as heat.\n\nThis leads to attenuation, which is the overall reduction in intensity. Attenuation happens because energy is both absorbed and scattered away. The wave‚Äôs amplitude becomes smaller, its pressure decreases, and the particle displacements become weaker as it moves deeper.\n\nIf we look at Figure 3.3, it illustrates how the beam behaves when it meets a boundary between two materials with impedances Z1 and Z2. If the wave hits perpendicularly, part of it is reflected back and part is transmitted forward. In more general cases, where the wave hits the surface at an angle, we define the incident angle (theta i), the reflected angle (theta r), and the transmitted angle (theta t). The incident angle equals the reflected angle, but the transmission angle can differ depending on the material properties.\n\nThe energy divides between reflection and transmission, and the total energy always equals the incident energy. The ratios of reflected to transmitted energy can be expressed in terms of pressure amplitude, and when you square those amplitudes, you get the intensity ratios.\n\nThese ratios depend on the acoustic impedance values of the two materials, Z1 and Z2, as well as the angles of incidence and transmission.\nIf Z1 equals Z2 ‚Äî meaning the materials are identical ‚Äî there will be no reflection at all, and all the energy will pass straight through. But if the impedances are very different ‚Äî for example, if one of them is close to zero ‚Äî then the reflection coefficient becomes one, meaning all the energy is reflected back.\n\nThat‚Äôs why impedance matching is so important in ultrasound imaging ‚Äî it ensures that most of the sound energy enters the body instead of being reflected away at the surface.\n\nSo, in summary, ultrasound‚Äìtissue interaction involves five main effects: reflection, refraction, scattering, absorption, and attenuation. Together, these determine how the ultrasound wave propagates, how it loses energy, and how we detect it for image formation.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 21 - Slide22.txt", "file_path": "Lecture 21\\Texts\\Slide22.txt", "content": "Alright, as I mentioned before, the intensity of the acoustic wave ‚Äî and likewise the pressure amplitude ‚Äî decreases as you go deeper and deeper into the medium. And the reason both intensity and pressure amplitude become smaller and smaller is due to attenuation.\n\nNow, why do we have attenuation? Because we have two main effects: absorption and scattering. Some of the energy is absorbed and turned into heat, while some is scattered ‚Äî redirected into other directions. Along the main beam direction, the intensity keeps getting smaller, and the pressure amplitude also decreases.\nThis is not a strange concept to us ‚Äî we‚Äôve seen exponential decay many times before. In nuclear imaging in X-ray imaging, exponential decay appears everywhere. It‚Äôs a very common phenomenon. And every time we see exponential decay, we can use it to understand how strongly our target interacts with the probing beam. Based on the attenuation rate, we can tell how dense or how light certain tissues are.\nThat‚Äôs really the imaging mechanism ‚Äî attenuation gives us contrast.\n\nNow, the same logic applies here as with X-rays. You don‚Äôt want something that absorbs energy too quickly. If all the X-rays ‚Äî or all the ultrasound ‚Äî are absorbed by the body, then the detector sees nothing. Every photon, or every bit of energy, gets eaten up, and no information is carried out.\nBut on the other hand, you also don‚Äôt want the ultrasound beam to pass through the body without any attenuation. If everything goes through unchanged, again, you get no information.\n\nThe ideal situation is somewhere in between ‚Äî you want about half the energy absorbed, and half transmitted. Not exactly fifty‚Äìfifty in every case, but roughly half. That balance gives you the maximum contrast and allows you to reconstruct a meaningful tomographic image.\n\nNow, I often make this analogy with exams. When I give you an examination, I try to set it up so that the class average is around fifty percent. Why? Because that gives me the best spread ‚Äî I can see which questions are tricky and which ones you can solve easily. It allows me to judge relative performance much more clearly. So, don‚Äôt worry if your average score isn‚Äôt high. I purposely don‚Äôt aim for a high average, because it gives me better information about the class as a whole. And in the end, your final letter grades ‚Äî A, B, or C ‚Äî are assigned consistently with the overall performance distribution. So nobody is unfairly graded.\n\nComing back to imaging, the same principle applies. The attenuation curve gives us the contrast we need ‚Äî not too much, not too little. And in ultrasound, we often use two equivalent forms to describe it: Œº, the attenuation coefficient for intensity, and Œ±, the attenuation coefficient for pressure. These are just two different ways to express the same exponential decay behavior. On a decibel scale, Œº and Œ± become interchangeable.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 21 - Slide23.txt", "file_path": "Lecture 21\\Texts\\Slide23.txt", "content": "Now, let‚Äôs talk about how attenuation depends on frequency.\nHere, the coefficient Œ± is proportional to a constant, which we call Œ±-naught, and it‚Äôs also proportional to frequency. Mathematically, we often write this as alpha of f equals alpha-naught times f to the power of b.\n\nFor diagnostic ultrasound, the value of b is about one, which means the relationship is almost linear. In other words, as the frequency increases, attenuation increases linearly.\nSo, higher frequency means stronger attenuation. And that makes perfect sense physically. Think about it: when you vibrate molecules more frequently, they move back and forth faster. Faster motion means more friction inside the tissue, and that friction converts energy into heat. So, higher frequency means you lose energy faster ‚Äî the wave decays in intensity and amplitude more quickly.\nIn simple terms: higher frequency equals higher energy loss equals less penetration.\n\nNow, for different types of materials, the constant Œ±-naught is different. If you look at the table on the slide, you‚Äôll see that fat has a smaller Œ±-naught value, while liver is a little higher. Cardiac muscle is higher still, and bone has a very large value ‚Äî meaning sound doesn‚Äôt penetrate bone very well.\n\nYou can use these numbers to get a sense of how attenuation works in different tissues. For example, let‚Äôs take fat at 5 megahertz. You can calculate attenuation as five megahertz times zero point six three decibels per centimeter per megahertz, which equals about three point one five decibels per centimeter. After traveling four centimeters, that‚Äôs twelve point six decibels of attenuation. The relative amplitude of the wave becomes ten to the power of negative twelve point six divided by twenty, which is about zero point two three four.\n\nSo, as frequency increases, attenuation increases. That‚Äôs why in medical ultrasound we always have to make a trade-off: high frequency gives you better resolution but less depth, while low frequency penetrates deeper but gives you lower resolution.\nThe takeaway is simple and important: higher frequency ‚Üí larger attenuation coefficient ‚Üí less penetration depth.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 21 - Slide24.txt", "file_path": "Lecture 21\\Texts\\Slide24.txt", "content": "Now let‚Äôs talk about some practical implications of reflection coefficients ‚Äî this is really an extension of what I already mentioned earlier.\nYou cannot penetrate ribs or air bubbles easily, because their acoustic impedances are very different from those of soft tissue. When there‚Äôs a large mismatch in impedance, most of the ultrasound energy is reflected, and very little gets through.\n\nAlso, if you try to do brain imaging in vivo, the skull will reflect almost all of the ultrasound. That‚Äôs why we don‚Äôt use ultrasound to image the brain the same way we use CT or MRI. The bone‚Äìtissue boundary is such a strong reflector that almost no energy gets inside.\nEven inside the brain, gray matter and white matter are acoustically very uniform. In a uniform medium, the ultrasound wave passes straight through without much change ‚Äî kind of like a transparent window. But that also means you don‚Äôt get contrast. It‚Äôs like when everyone in the class scores a perfect 100 ‚Äî I can‚Äôt tell who is better, because everyone looks the same.\n\nOn the other extreme, if you have a strong mismatch ‚Äî for example, at a tissue‚Äìbone interface ‚Äî the ultrasound wave cannot penetrate at all. Everything is reflected. That‚Äôs like an exam where everyone scores zero ‚Äî again, I can‚Äôt tell who‚Äôs better.\nSo, what we really need is a good balance ‚Äî a useful imaging window. In practice, this means finding acoustic windows that allow sound to enter effectively. For instance, when imaging the heart, we position the probe around the ribs to find a small gap where the sound can pass through. We also make sure to use ultrasound gel between the transducer and the skin, because that gel helps match the impedance between the probe and the tissue. Without it, the air gap would cause almost total reflection, and no signal would enter the body.\n\nSo the key idea is this: reflection depends on impedance mismatch. If there‚Äôs too little mismatch, there‚Äôs no contrast ‚Äî everything looks uniform. But if the mismatch is too large, nothing penetrates ‚Äî you can‚Äôt see anything. The useful imaging window lies in between.\nThis is something I‚Äôve mentioned before, and you can read through the bullet points on the slide to reinforce the idea. The goal is always the same: find the right balance between reflection and transmission to get good imaging quality.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 21 - Slide25.txt", "file_path": "Lecture 21\\Texts\\Slide25.txt", "content": "Alright, now let‚Äôs talk about something quite interesting ‚Äî the biological effects of ultrasound. When we introduce ultrasound energy into the body, it can lead to two kinds of effects: thermal effects and non-thermal effects.\n\nI‚Äôve already mentioned the thermal effects earlier, but let‚Äôs quickly recall them. When ultrasound travels through tissue, part of its mechanical energy gets converted into heat. This heating can cause several physiological changes ‚Äî for instance, it can increase collagen tissue extensibility, which makes soft tissues more flexible. It can also increase blood flow, which improves circulation and oxygen delivery. In addition, it can increase nerve conduction velocity, meaning that nerve impulses travel faster. This effect can also raise the pain threshold, allowing patients to tolerate more discomfort during therapy.\nMoreover, heating can enhance enzymatic activity ‚Äî many enzymes in the body function more efficiently at slightly elevated temperatures. And finally, ultrasound heating can help decrease muscle spasm, relaxing the tissue and improving mobility.\n\nNow, in addition to these thermal effects, ultrasound also causes non-thermal effects, and these are especially fascinating. Non-thermal effects are not caused by heating; instead, they result from mechanical interaction with the tissue ‚Äî the actual pressure waves and oscillations in the medium.\nFor example, ultrasound can increase cell membrane permeability, allowing molecules to move in and out of cells more easily. It can also increase vascular permeability, which means that blood vessels become slightly more ‚Äúleaky,‚Äù allowing nutrients or drugs to diffuse through more effectively. Both of these effects are very useful in therapeutic and drug delivery applications.\n\nUltrasound can also increase local blood flow without significant heating, helping tissue repair and regeneration. It can assist in the reduction of edema, which is the accumulation of fluid in tissues, and that makes it very useful for treating inflammation or swelling.\nOne of the most remarkable non-thermal effects is something called cavitation. Cavitation refers to the formation and oscillation of tiny bubbles ‚Äî microscopic gas pockets ‚Äî in the tissue fluid. At certain frequencies and intensities, these bubbles can oscillate and even collapse, releasing bursts of mechanical energy that can affect nearby cells or molecules.\n\nAs shown in the figure on this slide, you can see different frequency ranges where ultrasound is applied ‚Äî from very low frequencies used in physiotherapy to higher frequencies used in imaging, Doppler studies, and even lithotripsy for breaking kidney stones. The likelihood of cavitation increases with lower frequency and higher acoustic pressure. However, for standard medical diagnostic ultrasound, which operates between about 3 and 20 megahertz, cavitation is highly unlikely, so it‚Äôs very safe.\nAlso notice the lower figure ‚Äî it shows how ultrasound can enhance drug delivery. The red curve shows a larger uptake of drug when ultrasound is applied, compared to the blue curve, which shows little uptake without ultrasound. This demonstrates how ultrasound can help drugs penetrate biological barriers more effectively.\n\nSo, in summary, ultrasound has both thermal and non-thermal effects, and both can be used for different therapeutic and diagnostic purposes. The non-thermal effects, like increased permeability and cavitation, open up some exciting possibilities ‚Äî such as ultrasound-mediated drug delivery and neurological stimulation. These are emerging areas of research and clinical application.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 21 - Slide26.txt", "file_path": "Lecture 21\\Texts\\Slide26.txt", "content": "Now, let‚Äôs shift gears a little bit. Up to this point, we‚Äôve been focusing mainly on the physics of ultrasound ‚Äî the wave equation, acoustic impedance, and sound‚Äìtissue interactions. We‚Äôve covered how ultrasound propagates, how energy is absorbed or reflected, and how contrast is generated in imaging.\nBut now, we‚Äôre moving into a different section ‚Äî a more engineering-oriented discussion. These next few slides focus on the ultrasound scanner, or sometimes we call it the ultrasound probe or transducer. All these terms basically refer to the same device.\n\nSo, in this next part, we‚Äôll first talk about the single ultrasound unit, and then how multiple such units are combined to form arrays ‚Äî either linear arrays for one-dimensional scanning, or two-dimensional arrays for volumetric imaging. This is actually quite similar to how a CT scanner is organized ‚Äî you have multiple detectors arranged in a specific pattern to collect information from different angles.\n\nWe‚Äôll also talk about image resolution, contrast, and microbubbles, which are tiny gas-filled spheres used as ultrasound contrast agents. These help us visualize blood flow and enhance tissue contrast in medical imaging.\n\nSo again, just to summarize ‚Äî the first part of this lecture covered the physical foundations, while this next part is all about the engineering principles of the ultrasound scanner itself: how it‚Äôs built, how it operates, and how it detects signals.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 21 - Slide27.txt", "file_path": "Lecture 21\\Texts\\Slide27.txt", "content": "So, how do we actually generate ultrasound waves?\n\nEarlier, I mentioned that if you push and pull repeatedly ‚Äî like stretching and compressing a spring, or vibrating a rope ‚Äî you can generate waves. Those are good heuristic examples to understand the idea. But for medical ultrasound, we need something that can vibrate millions of times per second ‚Äî far beyond what we can do by hand. So, we use a physical mechanism called the piezoelectric effect.\n\nThe piezoelectric effect is truly fundamental to ultrasound imaging ‚Äî it‚Äôs the enabling technology behind both the generation and detection of ultrasound waves. In fact, this phenomenon was so important that it earned recognition at the Nobel Prize level.\nNow, the piezoelectric effect works in two directions, which we call the direct effect and the reverse effect.\nLet‚Äôs start with the direct effect. When sound vibrations ‚Äî that is, pressure waves ‚Äî hit a piezoelectric crystal, the alternating pressure causes the crystal to deform mechanically. As the pressure oscillates from positive to negative, the crystal expands and contracts slightly. This mechanical deformation generates an electrical potential across its surfaces. In other words, mechanical energy ‚Äî the sound vibration ‚Äî is converted into electrical energy. This is how the ultrasound echo signal is detected by the transducer and turned into an electrical signal for processing.\n\nNow, the reverse effect works the other way around. When we apply a voltage across the piezoelectric crystal, it causes the crystal to deform ‚Äî to change shape. When the voltage alternates ‚Äî positive, then negative, then positive again ‚Äî the crystal vibrates back and forth. If that electrical signal has a frequency in the ultrasound range, those vibrations produce ultrasound waves that propagate into the surrounding medium.\nSo you can see that the same piezoelectric element can do both jobs ‚Äî it can generate ultrasound waves and also detect the returning echoes.\nHere‚Äôs how it all fits together: when you send a sinusoidal electrical signal to the crystal, it vibrates and emits an ultrasound wave that travels into the body. The wave reflects from tissue boundaries and returns to the transducer. Those returning pressure waves deform the crystal again, and the deformation creates an electrical signal that can be measured.\n\nThat‚Äôs why we call it a two-way transduction process ‚Äî it goes from electrical energy to mechanical energy when we transmit, and back from mechanical to electrical energy when we receive.\nThis process ‚Äî the piezoelectric effect ‚Äî is the heart of every ultrasound transducer. It allows the probe to both send and receive sound waves, enabling us to form detailed medical images.\n\nSo, when you see the term ‚Äúpiezoelectric ceramics‚Äù or ‚Äúpiezoelectric crystals,‚Äù just remember: that‚Äôs what‚Äôs inside every ultrasound probe, converting electrical energy to sound and sound back to electrical energy, millions of times every second.\nAnd that‚Äôs the essence of how ultrasound imaging becomes possible.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 21 - Slide28.txt", "file_path": "Lecture 21\\Texts\\Slide28.txt", "content": "You might wonder, why does the piezoelectric effect actually happen? What‚Äôs going on inside the material that allows it to generate electricity when we apply pressure?\u000bThis little cartoon, or infographic, explains it beautifully.\n\nImagine you have a crystal structure, such as quartz, that is in perfect static balance ‚Äî both mechanically and electrically. In its no-stress state, the positive and negative charges are arranged symmetrically. The positive charges ‚Äî usually from the silicon atoms ‚Äî and the negative charges ‚Äî from the oxygen atoms ‚Äî balance one another out, so the overall crystal is electrically neutral. There is no net polarization, no voltage across the surfaces.\n\nNow, let‚Äôs apply a horizontal tension ‚Äî we‚Äôre stretching the crystal, pulling its sides apart. The crystal is stable but elastic, so it deforms slightly. When that happens, the charge centers shift ‚Äî the positive charges move upward, and the negative charges move downward. That means the top surface becomes slightly positive, and the bottom surface becomes slightly negative. In other words, the mechanical strain has produced an electrical potential difference between the two surfaces.\nNow imagine the opposite ‚Äî a compression instead of tension. When we compress the crystal, we push those atoms closer together, but in the opposite direction. The negative charges are forced upward, and the positive charges are pushed downward. Now the top surface becomes negative, and the bottom surface becomes positive.\n\nThis alternation between tension and compression causes repeated charge separation, which is what generates an alternating voltage when the material is vibrated continuously.\n\nSo what you see in this diagram ‚Äî the shift of positive and negative charge centers ‚Äî is the fundamental reason behind the piezoelectric effect. Mechanical deformation creates electrical polarization, and electrical excitation can, in turn, create mechanical motion.\nThis is the microscopic picture that explains how piezoelectric crystals act as transducers, converting mechanical vibration into electrical energy and back again. It‚Äôs this static-electric imbalance, produced by strain, that forms the very foundation of ultrasound imaging.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 21 - Slide29.txt", "file_path": "Lecture 21\\Texts\\Slide29.txt", "content": "Now, let‚Äôs take a look at the single-crystal transducer, which is one of the simplest yet most important components in ultrasound systems.\nIf you look at the diagram from your textbook, you can see all the key parts clearly labeled. At the heart of the transducer is the piezoelectric crystal, the active element that generates and receives ultrasound waves. On one side, there is a matching layer, which I mentioned earlier ‚Äî it‚Äôs used to match the acoustic impedance of the crystal to that of the tissue. Without this layer, much of the energy would be reflected at the interface, and very little sound would enter the body.\n\nBehind the crystal, you‚Äôll see a damping material. This is essential because it absorbs unwanted echoes and stops the crystal from ringing after each pulse. The damping material ensures that the emitted pulse is short and well-defined, giving us better axial resolution in the image.\n\nSurrounding everything, there‚Äôs an acoustic insulator or backing, which directs the ultrasound wave forward into the tissue rather than allowing it to leak backward into the housing. There‚Äôs also a conducting wire, or contact lead, which connects to the electrical circuitry, allowing us to apply voltage across the crystal and receive electrical signals from it.\nAll of these components ‚Äî the crystal, the matching layer, the damping material, and the insulator ‚Äî are enclosed in a metal or plastic housing, often with an ergonomic handle. Together they form what we call a single-element transducer.\nMathematically, this crystal has a natural resonant frequency, denoted as f-zero, given by the formula f-zero equals the speed of sound in the crystal divided by twice its thickness.\u000bIn other words,\u000bF-Zero equals c-crystal over 2 d.\n\nSo, the frequency of the ultrasound wave is determined by how fast sound travels in the crystal and how thick the crystal is. A thinner crystal produces higher frequency sound, while a thicker one produces lower frequency sound.\n\nThis simple relationship between frequency, sound speed, and crystal thickness is fundamental to designing ultrasound probes for different applications. High-frequency probes, which give better resolution, use very thin crystals, while low-frequency probes, which penetrate deeper, use thicker ones.\nSo, this diagram gives you the complete picture ‚Äî from physical design to mathematical principle ‚Äî of a single-crystal transducer.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 30, "slide_filename": "Slide30.txt", "slide_annotation": "Lecture 21 - Slide30.txt", "file_path": "Lecture 21\\Texts\\Slide30.txt", "content": "Now, once you put that single-crystal transducer inside a handheld housing, you get what we commonly call a probe ‚Äî or a transducer head.\nYou simply place this probe on top of the area you want to examine ‚Äî it could be the chest, the abdomen, the breast, an arm, or any other part of the body ‚Äî depending on what you are imaging. These probes are used for a variety of diagnostic and therapeutic purposes.\n\nOn this slide, you can see several examples of commercial ultrasound probes. Each one is designed for a different purpose ‚Äî some are curved for abdominal imaging, some are linear for vascular or musculoskeletal scans, and others are very small for intracavitary or ophthalmic use.\n\nNow, I‚Äôd like to share a more forward-looking idea ‚Äî something I‚Äôve been thinking about for quite a while. I believe that in the future, our everyday devices ‚Äî maybe even the iPhone ‚Äî could incorporate this kind of piezoelectric technology directly into the screen.\n\nJust imagine that the phone‚Äôs surface could act as a two-dimensional acoustic transducer array. Each tiny pixel on the screen could both generate and receive ultrasound signals. That means you could, in principle, perform a quick body scan right at home ‚Äî maybe before going to bed.\nYou could place the phone on your skin, and the device would automatically send out ultrasound waves, collect the echoes, and the AI software inside would analyze the data for you. It might tell you, ‚ÄúEverything looks fine,‚Äù or ‚ÄúYou may want to check this area further.‚Äù It could even show you which parts of the body were already scanned and which ones weren‚Äôt.\n\nBy combining piezoelectric technology, machine learning, and digital signal processing, we could one day make personal, professional-level imaging accessible to everyone ‚Äî right from their own phone.\n\nThis might sound futuristic, but remember ‚Äî all great innovations start as ideas. And this one could truly revolutionize preventive healthcare by bringing ultrasound diagnostics into the hands of ordinary people.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 31, "slide_filename": "Slide31.txt", "slide_annotation": "Lecture 21 - Slide31.txt", "file_path": "Lecture 21\\Texts\\Slide31.txt", "content": "So now let‚Äôs imagine what the damping effect really means ‚Äî and more importantly, why we need it in ultrasound imaging.\n\nThink about how an ultrasound pulse is sent into the body. Most of the time, ultrasound imaging doesn‚Äôt use a continuous wave; instead, it operates in what we call a pulsed mode. You send out a pulse, it travels into the body, reflects from different tissue layers, and then comes back to the transducer as an echo. This pulsed approach is extremely useful because it allows us to measure both depth and velocity ‚Äî that is, how deep a structure is and how fast it‚Äôs moving.\n\nNow, let‚Äôs say you apply a short electrical signal to the piezoelectric crystal ‚Äî it vibrates for a moment and produces a burst of ultrasound waves. Then, you stop the input signal. Ideally, the crystal should also stop vibrating immediately, just like striking a bell and having it stop ringing right away. But in reality, if there‚Äôs no damping, the crystal keeps vibrating for quite a while after the excitation stops, almost like a bouncing ball that never settles down.\n\nIf there‚Äôs no damping ‚Äî no internal friction ‚Äî the ball keeps bouncing up and down forever. That‚Äôs exactly what would happen to your transducer: it would continue ringing, creating a long, drawn-out pulse. But we don‚Äôt want that, because a long pulse means poor resolution ‚Äî the returning echoes overlap and you lose clarity.\nNow, if the transducer is placed in a medium with some friction ‚Äî like air or fluid ‚Äî those vibrations die out faster. The amplitude drops quickly, and you get a much shorter pulse. This reduction in vibration amplitude is called damping.\n\nSo, when we apply a damping material behind the crystal, it acts like friction for the vibrating transducer. It absorbs the leftover energy, preventing it from ringing too long. In the time domain, that means the vibration decays faster, giving us a short, clean pulse. In the frequency domain, the pulse now covers a wider range of frequencies ‚Äî in other words, a broader bandwidth with a lower amplitude peak.\n\nWithout damping, you have a narrow and tall frequency spectrum ‚Äî that‚Äôs a very sharp resonance around the central frequency. With damping, the spectrum is broader and flatter ‚Äî the amplitude is smaller, but the bandwidth is much wider, which is exactly what we want for imaging.\nWe define this relationship using something called the Q factor, or the quality factor. Mathematically, Q equals two pi times the central frequency divided by the bandwidth. So, Q equals 2œÄf‚ÇÄ divided by BW.\n\nA high-Q system means low damping ‚Äî long vibration, narrow frequency range. A low-Q system means strong damping ‚Äî short vibration, wide frequency range.\nFor diagnostic ultrasound, we actually prefer low-Q systems because we want short pulses and broad bandwidths. Typically, the Q factor is around 1 or 2, which provides an ideal balance between good axial resolution and manageable signal strength.\n\nSo, damping is what makes our ultrasound pulses short, our echoes sharp, and our images clear.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 32, "slide_filename": "Slide32.txt", "slide_annotation": "Lecture 21 - Slide32.txt", "file_path": "Lecture 21\\Texts\\Slide32.txt", "content": "Now that we understand damping, let‚Äôs move on to another important component ‚Äî the matching layer. You can see from this slide that the transducer structure includes this special layer right in front of the piezoelectric crystal, and this layer is absolutely essential for getting ultrasound energy efficiently into the body. Let me start with a quick personal story. I once mentioned that I had a kidney stone, and when you go for an ultrasound scan like that, you‚Äôll notice the doctor always applies a gel on the skin. The gel is completely harmless and painless, though yes, it can feel a little messy. But that gel is doing something very important ‚Äî it‚Äôs not just for comfort or for better contact. It actually helps solve what we call a matching problem.\n\nHere‚Äôs why. The piezoelectric material inside the transducer, usually something like PZT ‚Äî that stands for lead zirconate titanate ‚Äî has a very high acoustic impedance, which we call Z P or Z transducer. The human skin, on the other hand, has a much lower acoustic impedance, which we call Z skin. And between the two surfaces there are often tiny air bubbles, because the skin isn‚Äôt perfectly smooth. Now, whenever two materials with very different acoustic impedances are in contact, most of the ultrasound energy reflects back instead of transmitting forward. So the ultrasound wave generated by the crystal would mostly bounce back at the surface ‚Äî kind of like shouting into a canyon and hearing your echo. That‚Äôs bad for imaging because very little energy actually enters the body.\n\nSo what do we do? We introduce a matching layer ‚Äî a thin material layer whose acoustic impedance lies between those two values. This layer acts as a bridge, transferring acoustic energy more efficiently from the transducer into the skin. Here‚Äôs how it works. The vibration from the crystal first transfers its energy into the matching layer, and then from the matching layer into the skin. So instead of one big impedance jump, we now have two smaller steps ‚Äî from crystal to matching layer, and from matching layer to skin.\n\nMathematically, the amount of energy that gets transmitted from one medium into another depends on their respective acoustic impedances. The transmitted intensity between two media is proportional to the product of their impedances divided by the square of their sum ‚Äî that‚Äôs shown in the equation on the slide. To optimize this transmission, we choose the impedance of the matching layer carefully. The ideal acoustic impedance of the matching layer, which we call Z M, is the geometric mean of the two adjacent impedances. So we say, Z M equals the square root of Z transducer multiplied by Z skin. Again, that‚Äôs Z M equals the square root of Z transducer times Z skin.\n\nThis relationship minimizes reflection and maximizes transmission, allowing most of the ultrasound energy to enter the body and come back to the transducer as echoes. That‚Äôs why every ultrasound probe has a matching layer built in, and that‚Äôs also why we always use gel on the skin ‚Äî the gel acts as an acoustic coupling medium that functions as part of the matching layer system. And by the way, as you can see on the slide, this topic might appear in your homework, so make sure you remember it.\n\nTo summarize: without the matching layer, most of the ultrasound energy would just reflect at the skin‚Äôs surface. With the matching layer ‚Äî and with the gel acting as a coupling medium ‚Äî the energy is transmitted efficiently, first from the crystal to the gel, then from the gel to the body. This ensures stronger echoes, clearer signals, and higher-quality ultrasound images.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 33, "slide_filename": "Slide33.txt", "slide_annotation": "Lecture 21 - Slide33.txt", "file_path": "Lecture 21\\Texts\\Slide33.txt", "content": "Now, let‚Äôs take a closer look at something very familiar in every ultrasound exam ‚Äî the ultrasound gel. You‚Äôve probably seen this many times during a scan, whether it‚Äôs for the abdomen, the heart, or during pregnancy imaging. The sonographer or the doctor squeezes out some gel and spreads it on the patient‚Äôs skin before placing the transducer on top. It may seem simple, but it actually plays a crucial physical role in ultrasound imaging.\n\nRemember, we just discussed the matching layer ‚Äî that thin layer designed to help ultrasound energy travel efficiently from the transducer into the body. Well, the ultrasound gel is part of that same idea. It acts as an acoustic coupling medium, helping to eliminate the air gap between the transducer and the skin. Without the gel, there would be a very thin layer of air between the probe and the body surface, and that‚Äôs a serious problem.\n\nWhy? Because air has an extremely low acoustic impedance ‚Äî much lower than either the piezoelectric crystal or human tissue. When an ultrasound wave tries to go from the crystal into air, almost all of the energy reflects. In fact, more than ninety-nine percent of the signal would be lost. So the ultrasound beam would never enter the body; you‚Äôd see nothing but noise on the screen.\n\nThe gel solves this problem beautifully. It has an acoustic impedance value that lies roughly between the impedance of the transducer surface and that of the skin, just like a matching layer. When the gel fills in the microscopic irregularities on the skin and removes any trapped air bubbles, it creates a smooth, continuous path for the sound waves to travel through. That means more ultrasound energy gets transmitted into the body, and stronger echoes come back to the transducer.\n\nSo, in simple terms, the gel makes sure there‚Äôs no air, only sound transmission. It improves image quality dramatically, ensuring clear boundaries and accurate echo measurements. It also helps the probe glide smoothly across the skin, which is important for maintaining good contact during a scan.\n\nAnd practically speaking, the gel is made of a water-based polymer that‚Äôs safe, non-toxic, and easily cleaned off after the exam. Some gels are even warmed up before use to make the patient more comfortable. But whether it‚Äôs warm or cold, the gel is essential ‚Äî without it, even the best transducer couldn‚Äôt produce a good image.\n\nSo the next time you see that ultrasound gel being applied, remember it‚Äôs not just for comfort ‚Äî it‚Äôs a critical part of the acoustic system. It‚Äôs what allows the ultrasound energy to move efficiently from the transducer, through the skin, and into the body, giving us clear, high-quality medical images.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 34, "slide_filename": "Slide34.txt", "slide_annotation": "Lecture 21 - Slide34.txt", "file_path": "Lecture 21\\Texts\\Slide34.txt", "content": "So this is your homework, and here we start to look at what modern ultrasound transducers actually look like inside. In most modern ultrasound systems, the transducers are not just a single piezoelectric element like we saw earlier ‚Äî they‚Äôre made up of arrays of elements, either one-dimensional or two-dimensional. The image you see here is a schematic of a one-dimensional linear array transducer, which is very common in medical ultrasound imaging.\n\nYou can see that each piezo element is one tiny rectangular piece of piezoelectric material. Beneath these elements, we have the electrodes, which are used to apply oscillating sinusoidal electrical signals. These electrical signals cause the piezo elements to vibrate and generate ultrasound waves ‚Äî and when echoes return from the body, those same elements work in reverse, converting mechanical vibration back into electrical signals.\n\nNow, above these piezoelectric elements, you can see matching layers ‚Äî sometimes there are two, a first and a second matching layer. These are just like what we discussed earlier: they help transfer ultrasound energy more efficiently into the body by gradually matching the acoustic impedance between the crystal and the skin. The entire system is connected to a custom-designed flexible circuit that allows precise timing and control for each element, enabling electronic focusing and beam steering.\n\nBehind the piezo elements is the backing layer, which provides damping ‚Äî it absorbs unwanted vibrations so that each pulse is short and clean. This improves axial resolution by preventing long ‚Äúringing‚Äù of the crystal. So, when everything is working together ‚Äî the electrode, the matching layers, the gel, and the backing layer ‚Äî the ultrasound energy is efficiently transmitted into the body, and the echoes return through the same path.\n\nIn essence, you have a system that is symmetrically optimized for both transmission and reception. The signal path forward is carefully matched, and the reverse path follows the same principle, leading to a higher signal-to-noise ratio and better overall image quality.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 35, "slide_filename": "Slide35.txt", "slide_annotation": "Lecture 21 - Slide35.txt", "file_path": "Lecture 21\\Texts\\Slide35.txt", "content": "Now, let‚Äôs talk about the trade-off between resolution and penetration in ultrasound imaging. This is a very important concept that determines how clearly you can see structures at different depths inside the body.\n\nIf you want to get higher resolution, generally you need to increase the frequency of your ultrasound wave. Higher frequency means a shorter wavelength, and that allows you to distinguish smaller structures ‚Äî that‚Äôs why high-frequency ultrasound is used for imaging shallow organs like the thyroid or the eye.\n\nHowever, there‚Äôs a price you pay for this. Higher frequency sound waves are attenuated much faster as they travel through tissue. Remember that the attenuation coefficient, alpha, is proportional to frequency ‚Äî we can say alpha is proportional to f. So, as frequency increases, attenuation also increases. That means the ultrasound wave loses energy faster, and it can‚Äôt penetrate as deeply into the body.\n\nIn contrast, a lower frequency transducer can send energy much deeper, but it comes with lower resolution. For example, a 12 megahertz transducer provides excellent resolution but can‚Äôt reach very deep ‚Äî maybe just a few centimeters. On the other hand, a 3 megahertz transducer can reach much deeper into the abdomen, but the image will be coarser.\n\nSo, in practice, we always balance these two factors. You choose your transducer frequency based on the target you want to image ‚Äî high frequency for fine detail near the surface, and low frequency for deeper penetration. This is one of the most fundamental trade-offs in ultrasound imaging.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 36, "slide_filename": "Slide36.txt", "slide_annotation": "Lecture 21 - Slide36.txt", "file_path": "Lecture 21\\Texts\\Slide36.txt", "content": "Now, let‚Äôs move on to beam geometry, which gives us a deeper understanding of how ultrasound beams behave as they propagate through space. This figure from your textbook shows the beam shape produced by a single crystal transducer.\nWhen the transducer vibrates, it sends energy downward in a conical shape. The region closest to the transducer is called the near field, and beyond a certain point ‚Äî called the near field boundary ‚Äî the beam starts to diverge, entering what we call the far field.\n\nIn the near field, the beam diameter is roughly the same as the transducer‚Äôs aperture ‚Äî so if the transducer has a diameter ‚Äúa,‚Äù the beam width is about the same. But after the near field boundary, the beam starts to spread out or diverge. This divergence angle, which we call theta, is determined by the wavelength lambda and the transducer aperture ‚Äúa.‚Äù It‚Äôs given approximately by the equation:\n\ntheta equals arc sine of zero point six one times lambda divided by a.\n\nThis relationship shows that smaller apertures or larger wavelengths give wider beams ‚Äî meaning poorer lateral resolution.\nNow, within any cross-section of the beam, the intensity isn‚Äôt uniform. Along the central axis, the signal is strongest, and as you move away to the sides, the intensity decreases. The intensity profile across the beam can often be approximated by a Gaussian distribution, meaning it has a peak at the center and falls off symmetrically.\nTo describe this beam width, we use a quantity called full width at half maximum, or F-W-H-M, which literally means the width of the beam where the intensity drops to half of its peak value. For a Gaussian profile, the FWHM is approximately two point three six times sigma, where sigma is the standard deviation of the Gaussian. Mathematically, we write it as:\n\nFWHM equals two times the square root of two times the natural log of two, multiplied by sigma ‚Äî approximately equal to two point three six sigma.\nThis measurement gives us a good estimate of the lateral resolution of the ultrasound beam. The narrower the beam, the better the lateral resolution ‚Äî meaning you can distinguish two nearby structures more clearly.\n\nSo, beam geometry, frequency, aperture size, and attenuation ‚Äî all these factors together define how sharp and how deep your ultrasound image will be.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 37, "slide_filename": "Slide37.txt", "slide_annotation": "Lecture 21 - Slide37.txt", "file_path": "Lecture 21\\Texts\\Slide37.txt", "content": "Now, this slide explains the concept of lateral resolution ‚Äî sometimes called cross-beam resolution ‚Äî and it‚Äôs illustrated again by a figure from your textbook. Remember earlier, we talked about the beam profile and how it has a certain width, which we describe using the full width at half maximum, or F-W-H-M. Ideally, we want this width to be as small as possible, because that determines how well we can distinguish two objects that are side by side.\n\nIf two scatterers in the body ‚Äî say, two small reflecting points ‚Äî are separated by a distance smaller than the beam width, their echoes will overlap when received by the transducer. When that happens, the signals blend, and we can‚Äôt tell them apart. That‚Äôs what you see on the right side of the figure: the two curves merge into one, and the system records them as a single echo.\n\nOn the other hand, if the two scatterers are separated by a distance greater than the beam width ‚Äî specifically, greater than the full width at half maximum ‚Äî then the two echoes appear as two distinct peaks. That‚Äôs what we want. So the smaller the F-W-H-M, the better your lateral resolution, because you can separate two adjacent points in space.\n\nSo in summary, lateral resolution is determined primarily by the beam width. Narrow beams resolve closely spaced objects; wide beams cause overlap. This concept applies directly to your image sharpness ‚Äî particularly in the direction perpendicular to the beam path.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 38, "slide_filename": "Slide38.txt", "slide_annotation": "Lecture 21 - Slide38.txt", "file_path": "Lecture 21\\Texts\\Slide38.txt", "content": "Now let‚Äôs move on to ultrasound focusing, which is another crucial concept for improving lateral resolution. You can think of this just like focusing light through a lens in photography. The figure here from your textbook shows three cases: strong focusing, weak focusing, and a reference to how the focal plane and focal distance change.\n\nIf we have a single-crystal transducer ‚Äî often shaped like a disk ‚Äî it can have a concave surface, meaning it curves inward. The curvature acts like an optical lens, helping to converge the ultrasound beam to a smaller focal spot. When the curvature is stronger, the beam converges faster, giving us a shorter focal distance and a tighter focus. That means, at that focal point, you have very high resolution ‚Äî the beam is extremely narrow, and you can separate fine structures clearly. However, beyond that focal plane, the beam quickly diverges again, and the resolution deteriorates.\n\nIf the transducer surface is only slightly curved, the focusing is weaker. The focal distance becomes longer, and the beam stays more uniform downstream. The resolution isn‚Äôt as sharp at the focal point, but it‚Äôs more consistent along the depth of field. So there‚Äôs always a trade-off ‚Äî strong focusing gives you very high resolution over a small region, while weak focusing gives you moderate resolution over a wider range.\n\nNow, in both optics and acoustics, we use a quantity called the F-number, or F-ratio, to describe focusing strength. The F-number is defined as the radius of curvature divided by twice the aperture radius. Mathematically, that‚Äôs:\nF equals R divided by two A.\n\nHere, R is the radius of curvature, and A is the aperture radius. So, a smaller R or a larger aperture A gives you a smaller F-number ‚Äî meaning stronger focusing. Conversely, a large F-number corresponds to weaker focusing.\n\nSo what we‚Äôre really describing here is how lateral resolution changes as a function of focusing strength and imaging depth. When you design an ultrasound system, you must decide whether you want a narrow beam with strong focusing or a broader, more uniform beam with weaker focusing, depending on your imaging application.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 39, "slide_filename": "Slide39.txt", "slide_annotation": "Lecture 21 - Slide39.txt", "file_path": "Lecture 21\\Texts\\Slide39.txt", "content": "Alright, so we‚Äôve talked about lateral resolution ‚Äî how well we can distinguish two points that are side by side. Now let‚Äôs move on to the next concept, which is axial resolution, or sometimes called longitudinal resolution ‚Äî meaning how well we can distinguish two points along the direction of beam propagation.\n\nIf you look at this figure, it might seem a little tricky at first glance, but let‚Äôs walk through it carefully together. To understand axial resolution, remember that in ultrasound imaging we always send out short bursts, or pulses, of sound ‚Äî not continuous waves. We need these pulses, and as we mentioned before when talking about damping, that damping helps shorten the pulse. A shorter pulse means better resolution.\n\nSo imagine you send out one pulse. You already know the speed of sound in the medium, so after a short while, the pulse travels outward, interacts with the tissue, and part of it comes back as an echo. Based on the time interval between when we send the pulse and when we receive the echo, we can calculate the round-trip distance the sound wave has traveled.\n\nNow, if I send another pulse right after the first one, the same process repeats ‚Äî it goes out, reflects, and comes back. The key here is that axial resolution tells us how well we can resolve separate boundaries along that same line of travel. So the big question is: what‚Äôs the smallest distance between two reflecting surfaces that we can still tell apart? That minimum separable distance is what we call the axial resolution.\n\nIf those two boundaries are too close together, the echoes from them will overlap ‚Äî just like what you see in this figure ‚Äî and when that happens, all the signals merge, so you can‚Äôt tell which one is which. They just blur into one single reflection.\nFrom this physical reasoning, we can define the formula for axial resolution. It‚Äôs given by one-half times the pulse duration times the speed of sound. Let me say that clearly ‚Äî axial resolution equals one-half times pulse duration times c, where c represents the speed of sound in the medium.\n\nSo, mathematically, that‚Äôs:\u000bAxial resolution equals one-half multiplied by pulse duration multiplied by c.\nThis comes from a simple idea. For a first-order approximation, we assume the speed of sound is uniform throughout the tissue. Then we can look at which factors matter most. There are two major players here: the pulse duration and the sound speed. The smaller the pulse duration, the better the resolution. And actually, a smaller sound speed also improves resolution, because the wave doesn‚Äôt travel as far during each time interval.\n\nNow, picture this in your mind. You send out a pulse ‚Äî it has a certain length, from the front, which we call the leading edge, to the back, which we call the trailing edge. This whole pulse moves through the tissue. As it moves, it hits boundaries ‚Äî let‚Äôs call them surface A and surface B. The leading edge of the pulse reaches surface A first and reflects back, while the trailing part continues forward. Some portion of the pulse keeps going and hits surface B, and then that part reflects back too.\n\nSo the leading edge reflects first, and a moment later, the trailing edge and the transmitted component reach the second boundary. The reflected waves from surface A and surface B both travel back toward the transducer. Now, if these two boundaries are so close that the time difference between those reflections is shorter than the pulse duration, the two echoes overlap ‚Äî and we cannot tell them apart. But if the separation is large enough that the round-trip time between them is longer than half the pulse duration, we can distinguish the two.\n\nThat‚Äôs exactly how we define axial resolution ‚Äî it‚Äôs the distance between surface A and surface B that produces two distinguishable echoes. In symbols, we can write it as:\u000bDelta x equals one-half times pulse duration times c.\nLet me read that again in a smooth way:\u000b‚ÄúDelta x equals one-half times pulse duration times c.‚Äù\nThe one-half factor is there because the sound wave travels a round trip ‚Äî out to the boundary and then back ‚Äî so only half of that total distance corresponds to the actual separation between two reflectors.\n\nSo here‚Äôs the physical picture:\u000bIf the pulse duration is short, the leading and trailing edges of the pulse are close together, which means the echoes from A and B will be well separated in time ‚Äî giving you better axial resolution.\u000bIf the pulse duration is long, or if the boundaries are too close, then the echoes will merge together, and you‚Äôll lose that resolution.\nSo when we design ultrasound systems, we try to make the pulse as short as possible ‚Äî and that‚Äôs where the damping material plays a critical role. The damping shortens the pulse, improves temporal separation, and gives us sharper axial resolution.\n\nTo sum it up:\u000bAxial resolution represents the minimum distance between two boundaries along the beam path that can be resolved separately. It depends mainly on pulse duration and the speed of sound in the tissue. The shorter the pulse and the slower the sound, the finer the detail we can see along that direction.\n\nSo just review this figure carefully yourself. Once you visualize how the pulse moves forward, hits surface A, then surface B, and the echoes return, the logic becomes crystal clear. We now have both key ideas of spatial resolution ‚Äî lateral resolution and axial resolution ‚Äî and next, we‚Äôll move on to discuss how all this affects image contrast, which determines how clearly structures appear on the ultrasound image.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 40, "slide_filename": "Slide40.txt", "slide_annotation": "Lecture 21 - Slide40.txt", "file_path": "Lecture 21\\Texts\\Slide40.txt", "content": "Now, let‚Äôs talk about image contrast in ultrasound imaging. The term ‚Äúcontrast‚Äù here simply refers to how different tissues appear relative to one another based on the strength of the returning echoes. In ultrasound, acoustic waves interact with different tissues in different ways ‚Äî and that‚Äôs why we see some areas appearing brighter and others darker on the screen.\n\nWe use the prefixes ‚Äúhyper‚Äù and ‚Äúhypo‚Äù to describe this difference in the scattering amplitude of echoes. When a tissue produces echoes that have a higher scattering amplitude than the average, we call it ‚Äúhyperechoic.‚Äù That means the reflected signal is stronger than normal, and it shows up as a bright region on the ultrasound image. Conversely, when the tissue has a lower scattering amplitude than average, we call it ‚Äúhypoechoic.‚Äù In that case, the reflected signal is weaker, and it appears darker on the image.\n\nThese variations in brightness ‚Äî from hyperechoic to hypoechoic ‚Äî are what give us contrast. They allow us to distinguish one structure from another. For example, on the ultrasound image shown here, you can see how the liver, renal sinus fat, renal pyramids, and fluid-filled bowel loops all appear with different brightness levels. The renal sinus fat appears bright because it is hyperechoic ‚Äî it reflects ultrasound strongly. The renal pyramids or fluid-filled bowel loops appear darker because they are hypoechoic ‚Äî they scatter less energy.\n\nSome tissues, like the liver or renal cortex, tend to have uniform, mid-level echogenicity, meaning they reflect ultrasound in a moderate, consistent way. Others, like the anterior surface of the liver or the anterior surface of the kidney, are more reflective, showing distinct bright boundaries on the image.\n\nSo, in summary ‚Äî the way ultrasound waves interact with various tissues determines the level of brightness on the image. Hyperechoic areas are brighter and correspond to stronger scatterers. Hypoechoic areas are darker and correspond to weaker scatterers. This difference in signal amplitude is the foundation of contrast in ultrasound imaging.\n\nHowever, sometimes the natural contrast between tissues is not sufficient to clearly separate structures or identify pathology. In such cases, we use contrast agents to improve visibility. One of the most effective types of contrast agents in ultrasound imaging is the microbubble. These microbubbles, typically ranging from two to five micrometers in diameter, are introduced into the bloodstream to enhance contrast. They act as additional reflective surfaces that increase backscattering, especially in regions where the natural contrast is weak.\n\nBy introducing microbubbles, we can significantly improve the visualization of internal structures, making it easier to detect lesions, tumors, or subtle tissue changes. This approach is particularly useful in clinical cases where precise imaging is needed to make accurate diagnoses.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 41, "slide_filename": "Slide41.txt", "slide_annotation": "Lecture 21 - Slide41.txt", "file_path": "Lecture 21\\Texts\\Slide41.txt", "content": "Let‚Äôs look more closely at these microbubbles and how they work as ultrasound contrast agents.\nMicrobubbles are very small ‚Äî typically two to five micrometers in diameter ‚Äî and are encapsulated by a shell made of either albumin or a lipid layer. This shell makes them stable enough to circulate through the bloodstream without immediately collapsing. Their stability can be further enhanced by filling them with a high molecular weight gas, which diffuses out more slowly than regular air.\n\nThese microbubbles are produced outside the human body and then introduced into the bloodstream through an intravenous injection. Once inside the bloodstream, they interact with ultrasound waves. When the ultrasound beam hits them, the microbubbles start to oscillate ‚Äî they expand and contract in sync with the pressure variations of the sound wave.\n\nBecause the interface between the gas inside the bubble and the surrounding liquid is not acoustically transparent, a strong reflection occurs. This makes microbubbles act as powerful scatterers, producing bright, well-defined echoes. As a result, microvasculature and even small blood vessels become visible on ultrasound images, greatly improving the detection of blood flow and perfusion.\n\nMoreover, these microbubbles can be customized. Scientists can modify the shell using polymers or special coatings that allow the surface to be functionalized with antibodies. When certain antibodies are attached, the microbubbles can specifically bind to biological targets ‚Äî for example, tumor cells or inflammatory markers.\nWhen that happens, the microbubbles accumulate around those target regions. On the ultrasound image, those regions appear as bright, localized signals ‚Äî making it possible to identify molecular features, not just structural ones. In other words, microbubbles transform ultrasound from a purely anatomical imaging modality into a molecular and cellular imaging tool.\n\nSo, when you see very bright, clustered signals after microbubble injection, it often indicates the presence of something biologically distinct ‚Äî such as a tumor. This development marks a major advancement in medical ultrasound, extending it from traditional imaging into targeted diagnostics.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 42, "slide_filename": "Slide42.txt", "slide_annotation": "Lecture 21 - Slide42.txt", "file_path": "Lecture 21\\Texts\\Slide42.txt", "content": "Now let‚Äôs move one step further ‚Äî using microbubbles not just for imaging, but also for therapy.\nThis concept is known as acoustically enabled drug delivery. The idea is both simple and powerful. Suppose we inject a drug together with microbubbles into the bloodstream. These microbubbles circulate throughout the body, including through the small vessels, or microvasculature, around a tumor site.\n\nWhen we know the approximate location of a tumor, we can focus ultrasound waves precisely on that region. The ultrasound energy ‚Äî tuned to the right amplitude and frequency ‚Äî causes the microbubbles in that area to oscillate vigorously. If we increase the energy slightly, the bubbles rupture or collapse, releasing their drug payload right at the target site.\n\nThis process delivers the medication directly to the tumor, minimizing side effects and maximizing treatment effectiveness. It‚Äôs like having millions of tiny delivery capsules that release the drug only when and where an ultrasound activates them.\n\nThis technique opens the door to an entirely new field known as theranostics ‚Äî combining therapy and diagnostics. With this approach, ultrasound imaging can both visualize and treat disease simultaneously. The precision and non-invasiveness of this method make it an extremely promising direction for the future, particularly for treating cancers and other localized diseases.\n\nSo, using ultrasound and microbubbles together, we can not only image internal structures in great detail but also control drug delivery dynamically ‚Äî by sound waves themselves. This is an exciting frontier in biomedical engineering and medical imaging.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 21", "slide_number": 43, "slide_filename": "Slide43.txt", "slide_annotation": "Lecture 21 - Slide43.txt", "file_path": "Lecture 21\\Texts\\Slide43.txt", "content": "Alright, let‚Äôs wrap up today‚Äôs lecture. As we conclude, I‚Äôd like to give you the homework assignment for Ultrasound I.\n\nYou‚Äôll be working on two related problems, both designed to reinforce your understanding of the transducer and matching layer concepts we discussed earlier. Please refer to your green textbook for additional details and supporting equations ‚Äî it‚Äôs not difficult, and I‚Äôm confident you can handle it.\n\nThe first problem, number three point five, asks you to plot the transmitted frequency spectrum of an ultrasound beam from a transducer operating at a central frequency of one point five megahertz. Assume that the transducer is damped, and then repeat the plot for the beam returning to the transducer after passing through tissue and being backscattered.\n\nThe second problem, number three point six, focuses on the concept of the matching layer, which is one of the most important topics we covered today. The question is:\u000b‚ÄúTo improve the efficiency of a given transducer, the amount of energy reflected by the skin directly under the transducer must be minimized. A layer of material with an acoustic impedance Z M L is placed between the transducer and the skin. If the acoustic impedance of the skin is denoted by Z S, and that of the transducer crystal is denoted by Z C, show mathematically that the value of Z M L that minimizes the energy of the reflected wave is given by Z M L equals the square root of Z C multiplied by Z S.‚Äù\n\nThis derivation is straightforward, and you can find the full explanation in your textbook. The due date for this homework is one working week from today, so please make sure to submit it on time.\n\nWe‚Äôll continue our discussion next time, where we‚Äôll wrap up our study of ultrasound imaging. Remember ‚Äî the matching layer is crucial, both conceptually and practically, because it determines how efficiently energy moves from the transducer into the body. Understanding it deeply will also help you as we move toward more advanced imaging systems in the upcoming lectures.", "total_slides_in_lecture": 43}
{"lecture": "Lecture 22", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 22 - Slide1.txt", "file_path": "Lecture 22\\Texts\\Slide1.txt", "content": "Last time, we explained to you the physical and mathematical principles for ultrasound imaging.\n\nIn this lecture, we will be more specific about how we can form different kinds of ultrasound images. We‚Äôre moving from principles to practice‚Äîhow we send sound, receive echoes, and turn those echoes into the different image types for various clinical tasks.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 22 - Slide2.txt", "file_path": "Lecture 22\\Texts\\Slide2.txt", "content": "I feel quite good today, moving into the second part of ultrasound imaging. After this lecture, we‚Äôll teach the last imaging modality ‚Äîoptical imaging, in one lecture.\n\nNow, we‚Äôll follow the green textbook fairly closely. I strongly recommend that you read the chapter on ultrasound imaging ‚Äî it‚Äôs about twenty-three pages long. If you read through the chapter carefully, you‚Äôll have a very good understanding of everything we‚Äôre covering. I encourage you again to review or re-review that chapter, because it will really help you master today‚Äôs lecture.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 22 - Slide3.txt", "file_path": "Lecture 22\\Texts\\Slide3.txt", "content": "So here, I‚Äôm explaining the key points ‚Äî the way I think helps you summarize what really matters. But remember, you still need to read the textbook. It‚Äôs not that you can just sit back, listen to my talk, and skip the reading. No, you really have to read the textbook. That‚Äôs how you build a good memory and a solid foundation. Two ultrasound imaging lectures in a classroom is not enough if you only listen passively. What I‚Äôm giving you here is a general yet quick descriptioin of ultrasound imaging methods ‚Äî and specifically, how we actually form images.\n\nNow, this topic can be divided into two main parts. The first part is about imaging modes. We‚Äôll talk about different types of ultrasound imaging modes ‚Äî basically, A-mode, B-mode, C-mode, and M-mode, sometimes also called I-mode. What do these mean? Each mode represents a different way of collecting and displaying ultrasound data. There are multiple ideas behind them, and one very important concept that the textbook doesn‚Äôt explain in detail ‚Äî but I believe you really need to know ‚Äî is called Huygens‚Äô principle. I checked the pronunciation; it‚Äôs ‚ÄúHuygens‚Äô principle.‚Äù This is a very important physical idea. Understanding this principle will give you a much deeper grasp of how array-based scanning and beam forming work. It shows how you can focus ultrasound beams at different depths and times, giving you tremendous flexibility in image formation.\n\nIn the second part, we‚Äôll move on to Doppler imaging. This part deals with dynamic image formation ‚Äî it allows us to measure both the speed and the direction of motion, such as blood flow in vessels or motion of the heart chambers. Doppler imaging can be implemented in several ways: you can use continuous ultrasound waves, or you can send out sequences of pulses. It‚Äôs a very important imaging mode. We‚Äôll also discuss echo correlation and color Doppler. All these topics are explained clearly in the textbook, so please read that section carefully to strengthen your understanding.\n\nFinally, we‚Äôll look briefly at image artifacts and at some new things ‚Äî this last part is more for fun and inspiration. We won‚Äôt go into all the advanced new ideas in detail. The first three parts ‚Äî the imaging modes, Huygens‚Äô principle, and Doppler imaging ‚Äî are what you‚Äôll be tested on. So, let‚Äôs begin by asking: what exactly is the A-mode?", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 22 - Slide4.txt", "file_path": "Lecture 22\\Texts\\Slide4.txt", "content": "", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 22 - Slide5.txt", "file_path": "Lecture 22\\Texts\\Slide5.txt", "content": "Now let‚Äôs move on to the second one ‚Äî B-mode. B stands for brightness. That‚Äôs the keyword to remember here. B-mode is a very powerful imaging technique. You can think of it as an extension of A-mode ‚Äî instead of a single amplitude trace, you‚Äôre now using multiple A-mode scans to create a two-dimensional image.\n\nHere‚Äôs how it works: in A-mode, you send one ultrasound pulse into the tissue and receive echoes from a single line of depth. In B-mode, you repeat that process at many different positions. You scan line by line ‚Äî one A-mode here, another next to it, and so on. By combining all these one-dimensional signals, you form a two-dimensional image that shows amplitude, or intensity, as brightness. So, what you finally get is a picture ‚Äî an intensity map of the internal structures.\n\nYou‚Äôre not limited to scanning straight, parallel lines either. Just like in X-ray CT, you can use a fan-beam geometry ‚Äî instead of sending one beam directly forward, the transducer sweeps across an angle, covering a larger area. This sweeping motion allows you to reconstruct a full cross-sectional view of the tissue. I‚Äôll show you another short movie clip to give you a better visual sense of this process.\n\nNow, let‚Äôs look at the movies. First, you see the A-mode. Here ‚Äî look carefully. The pulse on the right is the ultrasound wave being transmitted into the biological tissue. The blue dots represent backscattered echoes ‚Äî reflections from different structures inside the tissue. Notice that the same transducer both sends and receives the signal. So, when the pulse goes out, the transducer then listens for the returning echoes.\n\nLet‚Äôs play that again. Watch the blue dots ‚Äî those are the echoes. You can see how they appear as small reflections. The tail of the pulse gets weaker as it travels deeper ‚Äî that‚Äôs where you receive the last few echoes. This gives you a one-dimensional signal profile, which is what we call A-mode.\nNow, look at the next one ‚Äî this is B-mode scanning. Each line in the image corresponds to one A-mode scan. When you stack many A-mode scans side by side, you form a two-dimensional brightness image. This example shows a phantom scan. You can see the B-mode image ‚Äî it‚Äôs not as crystal clear as a CT or MRI image. There are speckles, some noise, and scattering effects. But you can still recognize the major anatomical structures.\n\nUltrasound transducers are cost-effective, small, and compact ‚Äî that‚Äôs one of their biggest advantages. They can operate at high speed, producing real-time imaging. That‚Äôs B-mode scanning ‚Äî simple, efficient, and very practical for clinical use.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 22 - Slide6.txt", "file_path": "Lecture 22\\Texts\\Slide6.txt", "content": "Alright, let‚Äôs move on. So, we‚Äôve talked about A-mode and B-mode ‚Äî now we come to the next one: M-mode. The letter M stands for motion.\nM-mode is very similar to A-mode, but with one important difference. In A-mode, you send a pulse and get one amplitude profile at one point in time. In B-mode, you scan across space ‚Äî one A-mode line after another ‚Äî so the difference between two A-mode profiles represents a change in spatial location. You‚Äôre scanning across different positions or angles to build an image.\n\nIn M-mode, however, you stay at the same location ‚Äî the same angle ‚Äî and you repeatedly collect ultrasound signals over time. So instead of scanning across space, you‚Äôre scanning across time. What changes here are the internal structures that move, like the walls of a vessel or a heart chamber.\nFor example, imagine selecting a region of interest, such as part of the heart wall. As the heart beats, that wall expands and contracts. Even though the probe stays fixed, the echoes you receive from that region change over time. Each A-mode measurement gives you a one-dimensional profile, but now, because of motion, those profiles vary with every pulse. Sometimes the wall moves closer to the transducer; other times, it moves away.\n\nIf you keep recording these signals, you can build a motion profile. You can still display it as a two-dimensional image: the vertical axis represents depth ‚Äî just like a typical A-mode ‚Äî and the horizontal axis represents time. The result is a real-time record of motion. This is what we call motion mode, or M-mode.\nIt‚Äôs especially useful for studying moving organs, such as the beating heart, where you can visualize the rhythmic motion of cardiac walls and other dynamic structures over time.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 22 - Slide7.txt", "file_path": "Lecture 22\\Texts\\Slide7.txt", "content": "So, the last one‚Äîand I would say‚Äîis C-mode, C for coronal image. You know, three-dimensionally, if you cut a patient with a virtual plane this way, we usually call that the transverse plane. If you cut the patient this way, you call it sagittal. But if you cut the patient this way‚Äîthink I am facing you and you cut like this‚Äîthat is called the coronal image. So you have three views. I will show some images.\n\nYou do A-mode imaging along this direction. You have all the information, and the spatial dimension along this direction is your coordinate system‚Äîjust one axis. But you can do some echo-gated imaging. You send the pulse in that direction, you wait a certain amount of time, then you see‚Äîwithin a small time window‚Äîif the echo comes back. Given the delay time, you really target a specific depth. So from this one line, you roughly calculate: the ultrasound goes a round trip from the transducer to the plane and back. Suppose this round-trip time is, for example, 100 microseconds‚Äîlet‚Äôs say 100 milliseconds in my example. You only see the amplitude at 100 microseconds. That is the round-trip echo time‚Äîthe echo goes here, hits the plane, comes back‚Äîso, 100 microseconds.\n\nThis 100-microsecond sample is reported as your pixel value. Then, on this two-dimensional coronal plane, you keep scanning‚Äîline by line, pixel by pixel. You make sure the round-trip time is targeted to that depth position. You put all these amplitude values together, and you form a two-dimensional image. That is the so-called coronal image.\n\nHere is an example: you have a hand here. You put the ultrasound into the hand, and you record the echoes. With all the timing coordinated right, you form an internal image that cuts through the hand. You can see certain pieces of bone. There is some terminology I just learned‚Äîmetacarpals‚Äîthese light-blue bones; you can see these light-blue bones. The transducer is operated in C-mode here. There are some good ideas; details are in this article.\n\nYou have the image‚Äîthe image is really formed here. Then the image in a pixel here is reflected by a half-silvered mirror and viewed by the reader or doctor. Through this half-silvered mirror, you see the patient‚Äôs body, and the image you see is really superimposed onto the patient‚Äôs internal structure. It gives you the illusion that you see into the patient directly‚Äîyou see the beating heart‚Äîand you can maybe use this as guidance to put the catheter there, make a blood vessel open wider.\n\nThis is a good idea. It kind of works with augmented-reality things, and it‚Äôs a good idea. But the C-mode has nothing to do with this viewer. C-mode can be understood only with this sub-figure‚Äîso that‚Äôs the idea. And ultrasound imaging so far is straightforward. I have explained to you A, B, C, M‚Äîso you see later, a word you need to remember.\n\nOK, this is a single-crystal‚Äìbased transducer that can be used to do all these kinds of things. But when you want to form two-dimensional images, you need an easy scan, or you need to do multiple times‚Äîso you have a time dimension. With the time dimension, with a single transducer, you can still form two-dimensional images. Certainly, this is quite a lot like pencil-beam X-ray imaging. In the first generation, you have a pencil beam of X-rays and a single detector; you can do a scan. That is very inefficient.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 22 - Slide8.txt", "file_path": "Lecture 22\\Texts\\Slide8.txt", "content": "So that‚Äôs why we say: we have to use ultrasound transducer arrays. This will allow you to do parallel data acquisition. Parallel data acquisition is always desirable. And in the case of clinical‚ÄîI said chiral‚Äîscenarios, faster imaging speed will reduce most artifacts and improve the report. So, a lot of good points to make.\n\nThere are many types of ultrasound transducers. You have a linear array; you can use it in different ways. You have this linear phased array. And there is a one-point-five-D array‚Äîso this is a one-dimensional array, but you have several one-dimensional rows; the number is not too big. If the number is very big‚Äîso the x- and y-directions are kind of symmetric‚Äîyou have a truly 2-D array.\n\nPhased means each ultrasound pixel‚Äîeach element‚Äîdoes not fire in the same way. Each pixel you can think of as a source, as a vibrator. You can make a single piezoelectric material vibrate in the way you want. You can control the vibration with electrical signals through the so-called piezoelectric effect‚Äîthe reaction plane. And once we have a two-dimensional array or a 1.5-D or 1-D array, each pixel‚Äîeach transducer‚Äîcan be individually controlled. \n\nWith that individual control, we can freeze‚Äîcomputer. Let me see how to deal with it. Let me close the program. I will actually play a little bit with this. These are the reasons‚ÄîI think a computer problem. This is the tool.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 22 - Slide9.txt", "file_path": "Lecture 22\\Texts\\Slide9.txt", "content": "OK, think about a linear array. A linear array is to put multiple piezoelectric transducers along a line. So you have multiple elements, and you can selectively make a subset of transducers on. This is the light‚Äîthis subset is selected in this case. These elements are selected. You use these three as your ultrasound sources, so this beam will go down the road this way. \n\nYou do scanning as if this were just a single transducer aperture. Then you can define the next line‚Äîyou just group elements differently. Then you have a neighboring line, line 2, sending ultrasound waves downward. OK. Next instant, you generate line 3. So you have a line of piezoelectric elements; the elements remain, but each time, the first second‚Äîyou use this part; the second second, you use the second group. You keep doing this.\n\nThe physical effect is that you send one beam here; next, you send another beam. You keep doing A-mode scanning along many lines. But you do the beam scanning to form a B-mode image not by manually changing location or angle‚Äîyou do it mechanically or electronically. This is very convenient. You have a reform, so you use the acoustic transducer array, and you can perform scanning easily. And not only scanning, you can also do beam forming‚Äîa lot of flexibility.\n\nSo, here, I would like to explain to you.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 22 - Slide10.txt", "file_path": "Lecture 22\\Texts\\Slide10.txt", "content": "So, the very famous Huygens‚Äô principle. This slide is to guide the idea. Once you have this idea clear, the different schemes for ultrasound scanning and beam forming with an array-type ultrasound transducer will become very clear to you. The so-called Huygens‚Äô principle basically says: if you have a source‚Äîjust a simple example‚Äîif you have a point source, it will generate a spherical wave. The wave will keep moving out, and at a certain time, you have a wavefront here.\n\nThink of the ultrasound wave: you have compressions and you have rarefactions. That boundary is the wavefront. The principle suggests that if you decompose the current wavefront into small elements, you get a small element here, a small element there‚Äîmultiple small elements. Because the wavefield keeps vibrating‚Äîlike an ultrasound wave‚Äîthe small volume element keeps getting smaller and bigger, oscillating around a nominal volume; the pressure keeps changing, following a simple sinusoidal curve.\n\nBy this reasoning, if you view things individually, you isolate a small voxel‚Äîyou see the voxel keep changing, vibrating, oscillating. You can think: OK, this voxel is a single, small, secondary ultrasound source‚Äîor for electromagnetic waves, a small secondary source. Treat that element as a source and ask: how do we predict or calculate the next wavefront?\n\nOne way is to solve the original wave equation and follow the whole propagation process. The other way‚Äîthe Huygens way‚Äîis to forget the original source and say: at a given time, the current wavefront‚Äîshown here, A to A-prime‚Äîcan be decomposed into many small segments. Each individual segment is regarded as a secondary small source. Each small source keeps changing, and each small source generates its own small spherical wave. The next wavefront is the superposition of all those secondary spherical waves added together, forming the new wavefront.\n\nSo this is Huygens‚Äô principle. The current wavefront is decomposed into numerous small sources, and the next wavefront is computed from all these secondary sources. In words, we say: any current point on a wavefront is a new source‚Äîa secondary source. And all such new secondary sources, collectively, determine the future wavefront. All the subsequently generated waves added together will form the next wavefront. That is the idea.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 22 - Slide11.txt", "file_path": "Lecture 22\\Texts\\Slide11.txt", "content": "Looking at this picture, you think: this is a current wavefront. You have multiple small secondary sources here. Each source forms a small secondary spherical wave. These waves together‚Äîthe envelope formed by all these secondary spherical waves added together‚Äîform the next wavefront. In this first case, the next wavefront has the same shape as the previous or current wavefront, so a spherical wavefront propagates as a spherical wavefront. That looks right.\n\nIf you have a plane wave propagating forward, the plane wavefront can also be decomposed into sources‚ÄîA, B, C, D, and so on. Each is a secondary spherical wave. Again, the envelope of all these small spherical waves forms a plane, which is a new wavefront parallel to your current wavefront, parallel to the previous wavefront. That illustrates how a plane wave propagates along one direction‚Äîthe direction of beam propagation.\n\nNow, if you have a small aperture, the plane wave goes down to the opening. Because you have multiple yellow sources across that aperture, and each yellow source is treated as an individual point source vibrating out sound or light, you observe diffraction. The light or sound will no longer go perfectly straight; it will bend around the edges‚Äîturn around, so to speak.\nIf you have two slits, the waves from the two openings will interfere due to phase differences. At some points the two spherical waves‚Äîone from here and one from the other opening‚Äîwill add constructively; at other points they will cancel. If you place a screen there, you will observe bright and dark strips. This is interference.\n\nThe reflection, the refraction, and many old optical or ultrasonic phenomena can be explained similarly. You have a plane wave here; the wave is incident at a surface. At the surface, you again treat it with Huygens‚Äô sources. Depending on the location, this point is hit earlier and its secondary spherical wave is generated in this direction; that point is hit later and its spherical wave has a smaller extent. The envelope will be defined by straight lines‚Äîlike plane fronts‚Äîdepending on the speed of sound or light in the two media, the relative speed in the two media. You will see the transmitted beam change direction. The direction change is determined by the relative speeds of the wave in the first and second medium. This explains a lot of wave phenomena‚Äîvery cool.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 22 - Slide12.txt", "file_path": "Lecture 22\\Texts\\Slide12.txt", "content": "Now, let‚Äôs look at this example‚Äîan ultrasonic transducer array shown here inside the blue box. If you have a beam that is focused toward a certain direction, it shines in a specific mode, and you will have a wavefront forming within that blue region. So, what happens to the wave? Essentially, it behaves as if that entire front of energy is moving outward through the medium.\n\nBut here, instead of a continuous wavefront, we use a two-dimensional acoustic transducer to recreate that same wave behavior. In other words, we can replace the natural wavefront with this 2D transducer array. By doing so, we can electronically control each element in the array to operate exactly the way the original wavefront would. This means that the two-dimensional transducer can emulate the same effect as the real wave field.\n\nWith this approach, we can do the same kinds of operations‚Äîfocusing, scanning, and steering the beam‚Äîwith great flexibility. As long as each individual transducer element is driven properly, the array can reproduce the correct wave behavior. This is how we visualize and control wave propagation in a precise, programmable way.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 22 - Slide13.txt", "file_path": "Lecture 22\\Texts\\Slide13.txt", "content": "Let me make this clearer using a linear phased array example. If you fire all the pulses at the same time, all the elements are activated together, and the ultrasound beam travels straight ahead in one direction.\n\nHowever, if you adjust the timing slightly, you can shape the beam. For instance, if you fire elements 1 and 5 a little earlier, as shown on the time axis here, where this point is time zero, and then fire the others with slight delays, you create different wavefronts that add up. At first, two spherical waves are generated from elements 1 and 5. Then, a bit later, elements 2 and 4 emit their waves, and finally, the central element fires last. When all these small wavelets combine, they form a curved wavefront, producing a focused ultrasound field downward, with the focal spot located here.\n\nNow, the distance of this focus‚Äîhow deep it forms from the transducer surface‚Äîdepends on the relative time delays between the elements. If you apply the delay more gradually, the focus will form more deeply. By changing the delay pattern, you can also tilt the beam‚Äîto the left-hand side or the right-hand side‚Äîby linearly shifting the delay across elements. This gives you complete flexibility in how you steer and focus the beam.\n\nThe essential idea here follows Huygens‚Äô principle‚Äîeach element acts as an individual source, generating its own waveform with a controlled relative phase. By adjusting the phase and timing, you can create different acoustic or optical effects. This is how beam steering and dynamic focusing are achieved in a phased array.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 22 - Slide14.txt", "file_path": "Lecture 22\\Texts\\Slide14.txt", "content": "Now, this is a phased array, and you can extend the same concept to the two-dimensional case. Instead of having one line of elements, you now have a grid‚Äîa 2D array of transducer elements.\n\nBy controlling the phase and timing in two dimensions, you can purposely focus along the principal axis, or you can steer the beam left, right, up, or down‚Äîeven in a circular pattern if you wish. This allows for electronic focusing and beam steering in both dimensions, giving you precise control over the acoustic field.\n\nSo, in summary, the 2D phased array operates the same way as the 1D version but adds another level of control. It enables full 3D imaging capabilities, higher flexibility, and more advanced ultrasound applications.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 22 - Slide15.txt", "file_path": "Lecture 22\\Texts\\Slide15.txt", "content": "Now, let‚Äôs look at the annular array. You can make the ultrasound beam radially symmetric, or what we sometimes call recirculately symmetric. Basically, you can design a circular symmetric arrangement of piezoelectric elements, like the one shown here. With this type of array, you cannot do circular scanning‚Äîbecause it‚Äôs already circularly symmetric‚Äîbut you can focus along the principal direction.\n\nThe advantage is that you can control the focal distance very easily. You can make the focus point closer or farther, depending on the time delay and how you drive those circular rings. So you can adjust the focal distance to be small or large. That‚Äôs something you can easily do with this annular array structure.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 22 - Slide16.txt", "file_path": "Lecture 22\\Texts\\Slide16.txt", "content": "So, the beam-forming idea is that you introduce time delays across the array. Imagine you have scatterers located at different positions, labeled E1, E2, and E3. These are the echoes or signals you receive from different points inside the tissue.\n\nOn the receiver side, you receive all these signals and keep guiding or aligning them properly. If you add those signals together‚Äîbut with individual delays as shown here‚Äîyou can make them sum coherently. The tall bars in the diagram indicate a larger time delay. Different modulations of these delays determine which echo signal you want to detect.\n\nAt this location, the phase difference between the central element and the peripheral elements will be large, so the relative delay will be large too. You take the signal from the central channel and keep it. Then, you take the signal from the outer element‚Äîit takes a longer time for the echo to reach that peripheral element. That longer time becomes the time delay.\n\nYou add all the signals together, each with its own individualized time delay. When the time delays are properly distributed like this, and you add the five signals together, you form a strong echo‚Äîfor example, for echo one. But if you use a uniform time delay for all, you are focusing to infinity‚Äîthat‚Äôs a remote point. So, by changing the distribution of time delays, you can guide coherent echoes at different depths.\n\nThis is the basic beam-forming idea. And of course, scanning and beam forming can be combined to achieve the best image quality and overall performance.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 22 - Slide17.txt", "file_path": "Lecture 22\\Texts\\Slide17.txt", "content": "Now, let me just copy this figure to help explain the idea. This shows a one-dimensional ultrasound array. If you use just this group of transducers, you can certainly do beam forming. You perform focusing in this way‚Äîbased on the relative time delays across elements.\n\nYou can adjust the focus at a certain depth‚Äîfrom the transducer surface to the focal plane‚Äîand make the focus spot here. You receive signals mainly from this region of interest. Then, this group of signals can be moved laterally.\nFrom left to right, you perform scanning‚Äîline by line‚Äîso you get information along each line. When you repeat this line-by-line process, you effectively build up a two-dimensional image.\n\nSo, in summary, with beam forming, you control how the echoes are added together in time, and with scanning, you move the beam position across the field. Together, they form the foundation of modern ultrasound imaging.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 22 - Slide18.txt", "file_path": "Lecture 22\\Texts\\Slide18.txt", "content": "Now, let‚Äôs connect the ideas we discussed earlier. You can sweep the ultrasound beam from left to right, and when you perform ultrasound imaging, you can use this sweeping approach to create what we call compound imaging. Using a phased array transducer, you can steer the ultrasound beams toward the left side, collect multiple image lines‚Äîeach one similar to an A-mode line‚Äîand combine them to form a B-mode image over the region of interest. Then, you slightly change the beam angle and sweep across again, this time pointing the beams toward the right side.\n\nWhat you get are multiple images of the same region, each captured from a different angle. When you combine or ‚Äúadd‚Äù these images together, you average the results across those angles. The benefit is that the underlying structures‚Äîlike the organs or tissues‚Äîstay consistent across frames, but the speckle noise and random scattering patterns vary from one angle to another. When you sum them together, the random noise cancels out while the consistent features are reinforced, giving you a much smoother, clearer image with reduced artifacts.\n\nSo, this is what we call compound imaging. It‚Äôs essentially a noise reduction technique achieved by imaging the same region from multiple directions. The figure here shows how the image looks smoother, with far fewer speckles compared to a single acquisition. This method is similar in principle to noise averaging or multi-angle integration.\n\nIn short, this is another powerful way to perform ultrasound imaging‚Äîwhether A-mode, B-mode, or C-mode‚Äîbased on Huygens‚Äô principle and the use of one- or two-dimensional transducer arrays. You can scan, focus, and form images in many flexible ways. So, read the green textbook carefully and make sure you understand these imaging principles. That completes the first part of this lecture.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 22 - Slide19.txt", "file_path": "Lecture 22\\Texts\\Slide19.txt", "content": "The second part of this lecture focuses on dynamic imaging, particularly on how we measure motion, such as blood flow velocity or heart wall movement. To do this, we rely on the Doppler effect.\n\nSo, what exactly is the Doppler effect? It‚Äôs a physical principle that describes how the observed frequency of a wave changes when there is relative motion between the source and the observer. In ultrasound, this effect allows us to calculate how fast and in what direction blood is moving inside the body.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 22 - Slide20.txt", "file_path": "Lecture 22\\Texts\\Slide20.txt", "content": "Here‚Äôs a simple cartoon that helps you visualize the concept. You‚Äôve probably experienced the Doppler effect in everyday life. Imagine you‚Äôre standing at a train station. When the train comes toward you, the pitch of its whistle sounds higher. When the train moves away, the pitch sounds lower.\n\nThe same thing happens with the siren of a police car. When the car is moving toward you, the sound waves are compressed, giving you a higher frequency or a sharper tone. When the car moves away, the sound waves are stretched, and you perceive a lower frequency.\n\nIn other words, the perceived frequency depends directly on the relative velocity between the sound source and the observer. This change in frequency with motion is the Doppler effect, and it forms the physical foundation for Doppler ultrasound imaging, where we measure blood flow speed and direction inside the body.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 22 - Slide21.txt", "file_path": "Lecture 22\\Texts\\Slide21.txt", "content": "Now, in your textbook, you have this diagram that shows how the Doppler shift occurs in flowing blood. Here, we are looking at a small vessel with red blood cells moving inside. The transducer sends ultrasound energy toward the blood vessel wall or directly into the blood stream. The wall or the moving red blood cells reflect the ultrasound energy back to the transducer.\n\nIf the wall or blood cells were perfectly stationary, the reflected frequency would be exactly the same as the incident frequency. However, since these scatterers are moving, the reflected frequency becomes slightly different. The vertical component of the velocity‚Äîrepresented here as v cos Œ∏‚Äîis what contributes to this difference. This component corresponds to the velocity of motion along the ultrasound beam direction.\n\nThe small difference between the transmitted and received frequencies is what we call the Doppler frequency shift. This effect is universal‚Äîit appears not only in ultrasound but also in optical and electromagnetic waves. The Doppler shift tells us how motion changes the observed frequency, and that is the basis for Doppler imaging in medical ultrasound.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 22 - Slide22.txt", "file_path": "Lecture 22\\Texts\\Slide22.txt", "content": "Here is the key paragraph from your textbook. The diagram is reduced in size, but the principle remains the same. You have motion, an incident frequency, and a reflected frequency. The derivation goes as follows: the frequency reflected by the moving blood cell or boundary is given by Equation 3.40. Here, c is the speed of sound in the tissue, v is the blood flow velocity, Œ∏ is the angle between the beam and the direction of motion, and the whole term is divided by Œª, the wavelength.\n\nNow, how do we derive this? The idea is straightforward. The wavelength times the frequency gives you the speed of the sound wave, meaning c = f √ó Œª. The term v cos Œ∏ represents the velocity component of motion in the direction of the beam. Together, these describe the relative velocity between the transducer and the moving target.\n\nThe incident frequency of the ultrasound is denoted f ·µ¢, which equals c divided by Œª. The difference between the incident frequency and the reflected frequency is shown in the last equation. This frequency difference, called the Doppler frequency shift, allows you to calculate the velocity component along the beam direction. So, from the measured shift, you can infer how fast the blood or tissue is moving toward or away from the transducer.\n\nThis may not be immediately clear from the formulas, so let‚Äôs use a simple cartoon to understand why this Doppler shift happens.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 22 - Slide23.txt", "file_path": "Lecture 22\\Texts\\Slide23.txt", "content": "So, you think‚Äîlook at this. The transducer sends out ultrasound waves at the speed of c toward a stationary object. In this case, the speed c equals frequency times wavelength. The wavelength is basically how far the wave moves during one vibration. And how many times do you vibrate? That‚Äôs the frequency. So, frequency times wavelength equals the speed c.\n\nNow, in one unit of time, in this stationary case, you see four pulses. I should make that plural‚Äîfour pulses are sent into the object. This is the reference situation. This simple example gives you the baseline idea, and I just drew more examples to make it clearer.\n\nNow, think about the moving situation. I say, when the object is moving, you will see the apparent frequency change. How does that happen? Think about this: let‚Äôs say this same stationary object now starts a steady motion toward the transducer. This is just for simplicity. In fact, the object‚Äôs motion is also at a certain velocity, denoted v, and the sound itself travels with velocity c.\n\nSo, in one unit of time, the sound will travel a distance c. In that same unit time, you have four pulses sent into the object. But now, in the second case, the object is moving‚Äîalso with velocity v‚Äîso within that same unit time, the moving object will cover a certain distance. Because it covers that distance, if it were stationary, it would have received four pulses. But now, since it moves toward the transducer, those four pulses that were still traveling through space are ‚Äúcaught‚Äù or ‚Äúeaten up‚Äù by the moving object.\n\nSo, this object will intercept not just the original four pulses, but also an additional four pulses that were on their way. Altogether, you now have eight pulses entering the object in one unit of time. Let‚Äôs assume the object is a perfect reflector.\n\nIn the first case‚Äîwhen the object is stationary‚Äîyou have four pulses entering, and it will reflect four pulses within that unit time. So, you receive four reflected pulses, and that gives you the same frequency as the transmitted one‚Äîno frequency change. The object vibrates four times, and you see four reflected waves per unit time.\n\nBut in the second case‚Äîwhen the object is moving toward the transducer‚Äîit takes eight pulses within the same time period. If it is a perfect reflector, those eight pulses are now reflected back within one unit of time. So, the reflected signal contains eight pulses per unit time, meaning the reflected frequency is doubled.\n\nThis is how a moving object changes the reflected frequency. You get a signal reflected back with a higher frequency, and the difference between the stationary and moving cases is the Doppler effect.\n\nSo, this is the idea. Think about this simple diagram, and then review the equations again. If you think about the connection between this cartoon and those equations, you will gain a much better understanding of the formula‚Äîespecially the one where we talk about the velocity component v times cosine Œ∏.\n\nOnly the portion of the velocity component along the beam direction contributes to the observed frequency shift. That‚Äôs the part of motion you can measure as the carrying frequency. The incident frequency is the transmitted one, and the frequency shift‚Äîthe Doppler shift‚Äîis this difference.\nThis normalization factor is simply the speed of sound, c. So, think about it this way: half of the transmitted power goes out, then it comes back after reflection. That‚Äôs the basic Doppler imaging process.\n\nDoppler imaging is an important part of the ultrasound chapter. To really understand Doppler imaging, the key is to understand this formula. If you follow the derivation, it might not be totally clear at first, but if you understand this cartoon analogy, you can see how the Doppler frequency shift is generated.\n\nThe same idea applies across all situations. So, follow these formulas systematically. Then you‚Äôll see what‚Äôs going on‚Äîwhy there‚Äôs a connection between the Doppler frequency shift and the velocity component that you‚Äôre measuring along the ultrasound beam direction.\n\nIf the motion is parallel to the beam, the Doppler shift is at its maximum. If the motion is perpendicular to the beam, you cannot measure it. You can only measure the Doppler frequency shift along the beam direction.\n\nThis is a simple idea, but it gives you a very good understanding of what‚Äôs happening physically.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 22 - Slide24.txt", "file_path": "Lecture 22\\Texts\\Slide24.txt", "content": "Now, let‚Äôs talk about how Doppler imaging is actually performed. You can use a continuous wave (CW) method, where you keep sending ultrasound waves continuously, or a pulsed wave (PW) method, where you send one pulse at a time‚Äîa train of pulses‚Äîto measure the Doppler information. Another important technique is echo correlation, where you send one signal, receive it back, then send another signal, and again receive it back. By comparing the relative phase changes through correlation, you can detect motion.\n\nCorrelation tells you how much one signal matches another. In the foundation part, we mentioned the Cauchy‚ÄìSchwarz inequality, which states that if two waveforms are perfectly correlated, they represent the same event at a specific location. When the reflector moves, the correlation pattern shifts, which reveals how much the object has displaced between the two measurements.\n\nYou can also combine Doppler imaging with B-mode imaging. The B-mode provides an anatomical background‚Äîessentially, a grayscale structural image. If you have multiple two-dimensional B-mode slices, you can reconstruct a three-dimensional ultrasound volume. On top of this anatomical structure, you can superimpose the velocity distribution obtained from Doppler measurements.\n\nVelocity is a vector quantity, meaning it has both amplitude and direction. You can visualize these vectors using color coding. For example, motion in one direction may appear in red, motion in the opposite direction may appear in green, and another perpendicular direction may be shown in blue. The color intensity represents the velocity amplitude. With this color flow encoding, you can visualize the complete velocity field superimposed on the anatomical image. This combined visualization‚Äîstructural plus velocity‚Äîis the essence of Doppler imaging. Now let‚Äôs go step by step to see how each method works in practice.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 22 - Slide25.txt", "file_path": "Lecture 22\\Texts\\Slide25.txt", "content": "These concepts are quite straightforward, and it‚Äôs good to know them as part of your general understanding. In continuous wave Doppler, the transducer is divided into two separate parts‚Äîone part for transmission and the other for reception. The transmitting part continuously sends ultrasound waves toward the region of interest, while the receiving part continuously collects the reflected signals.\n\nSo, you have a single transducer housing two elements. One is always sending, and the other is always listening. The signal that is sent out has a fixed transmission frequency, while the reflected signal may have a slightly different frequency because of the Doppler shift caused by motion. What you send and what you receive differ slightly in frequency, and that difference directly relates to the motion along the ultrasound beam direction.\nThe region of interest is where the transmitted and received beams overlap. This overlapping region is the sensitive zone from which the Doppler signals are detected. If the reflection occurs outside that overlap, the signal will not be received effectively.\n\nIn summary, continuous wave Doppler requires two separate elements‚Äîone for emission and one for detection‚Äîbecause a single element cannot transmit and receive continuously at the same time. So, CW Doppler uses continuous waves for both sending and receiving, providing constant velocity measurement but without specific depth localization.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 22 - Slide26.txt", "file_path": "Lecture 22\\Texts\\Slide26.txt", "content": "Now let‚Äôs take a closer look at what actually happens to the signal. You have one signal going in and another signal coming out. The input signal is transmitted at a certain frequency, which we call f-i, while the received signal includes a small Doppler frequency shift, noted as delta-f. So, the frequency of the returning signal becomes f-i plus delta-f. This delta-f can be either positive or negative, depending on whether the target is moving toward or away from the transducer. \n\nUsing digital signal processing, we can compare these two signals. The received signal is multiplied by the reference signal generated by the transmitter, and this multiplication is performed by a circuit called a mixer, which effectively combines the two signals. Mathematically, this creates two components‚Äîone at a very high frequency, which is the sum of the two, and one at a much lower frequency, which is their difference.\n\nSince the transmitted frequency f-i is in the high ultrasound range and the Doppler shift delta-f is quite small, we use a low-pass filter to remove the high-frequency component and keep only the low-frequency term that carries the Doppler information. After filtering, the output signal is proportional to A over two times cosine of two-pi times delta-f times t, where A is the amplitude of the signal. This low-frequency signal represents the Doppler shift itself and allows us to estimate the velocity v using the formula you saw earlier: delta-f equals two times f-i times v times cosine theta divided by c. \n\nSo, by measuring delta-f, we can determine how fast the object is moving along the ultrasound beam direction. However, remember that continuous-wave Doppler does not provide depth information. It tells you the velocity but not the exact location of the moving target, because the transducer is transmitting and receiving continuously along the entire beam path. So, continuous-wave Doppler gives accurate velocity data, but without depth resolution.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 22 - Slide27.txt", "file_path": "Lecture 22\\Texts\\Slide27.txt", "content": "The second mode we use is called pulsed wave Doppler. In this mode, we do not send ultrasound continuously. Instead, we transmit a series of short pulses ‚Äî almost like delta functions ‚Äî separated in time. It‚Äôs not exactly a delta function, but it‚Äôs very narrow in duration. \n\nSo, at time one, you send out the first pulse into the tissue along the time axis. Then, after receiving the echo back, at time two, you send out the second pulse, and so on. Each pulse travels into the tissue, reflects from structures, and returns to the transducer. The process repeats again and again, one pulse at a time, to measure motion and flow at specific depths within the body.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 22 - Slide28.txt", "file_path": "Lecture 22\\Texts\\Slide28.txt", "content": "Now, I‚Äôve included here two images from another textbook that show this more graphically. You can see how the pulses are sent one after another into the biological tissue. Between each transmitted pulse, there is a fixed time delay. That time interval is the pulse repetition period, and the inverse of that is called the pulse repetition frequency. This repetition rate is an important parameter because it determines how deep your ultrasound can penetrate and how fast you can sample motion.\n\nEach pulse travels into the tissue and produces echoes that return at different times depending on the depth of the reflecting structure. These returning echoes ‚Äî let‚Äôs call them echo one, echo two, echo three ‚Äî come from slightly different layers or scattering surfaces inside the tissue. If there is no motion, the interval between successive echoes remains the same. But if there is motion ‚Äî for instance, blood flowing or tissue moving ‚Äî that spacing changes slightly. By measuring the change in the timing or phase of these echoes, we can estimate velocity.\n\nIn pulsed wave Doppler, we can also target a specific depth, often referred to as the gate depth. This means we can focus the measurement on a particular region of interest within the body. The operator can adjust this gate ‚Äî make it deeper, shallower, wider, or narrower ‚Äî to control the resolution and sensitivity of the measurement. By analyzing how the returned echoes shift over time, we can extract information about both motion and flow at that specific location.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 22 - Slide29.txt", "file_path": "Lecture 22\\Texts\\Slide29.txt", "content": "Let‚Äôs imagine we send a pulse to a stationary target ‚Äî say, a student sitting still. The pulse travels to that target and comes back after a certain time. Suppose it takes exactly one hundred milliseconds for the sound to make the round trip. If we send a second pulse after one second, that second echo will return at the same delay ‚Äî one hundred milliseconds ‚Äî if the target hasn‚Äôt moved. Both echoes will look identical, because the distance hasn‚Äôt changed.\n\nBut now, if that student moves slightly toward us, the second echo will arrive a bit earlier ‚Äî the return time becomes shorter. The peak of the echo signal shifts forward in time. This small timing change, or phase shift, tells us that motion has occurred. If the target moves rhythmically ‚Äî for example, the surface is oscillating ‚Äî the measured signal amplitude will fluctuate up and down around the average line.\n\nWe record this oscillating signal over time and then perform a Fourier transform on it. This mathematical operation converts the time-domain signal into a frequency-domain representation. The resulting frequency shift corresponds to the Doppler frequency, which reflects the motion or velocity of the surface or fluid. Usually, the motion is not perfectly uniform but has some periodic pattern. By analyzing these Doppler frequencies, we can quantify that motion precisely.\n\nSo, this is the basic idea behind pulsed wave Doppler. It‚Äôs slightly more complex than continuous wave Doppler, but it has a major advantage ‚Äî it allows us to measure motion at a specific depth. We‚Äôll look at some examples and waveforms shortly to illustrate how this works in practice.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 30, "slide_filename": "Slide30.txt", "slide_annotation": "Lecture 22 - Slide30.txt", "file_path": "Lecture 22\\Texts\\Slide30.txt", "content": "Now, let‚Äôs talk about the pulsed wave Doppler mode. In this method, we don‚Äôt use a continuous wave. Instead, we send out short, separated pulses‚Äîeach one acting somewhat like a delta function. It‚Äôs not exactly a delta function, but it behaves like one in time. \n\nSo, at time one, you send out a pulse into the tissue, and after a short delay, you receive the echo back. Then, at time two, you send the next pulse, and again you wait for the return signal. You keep sending these pulses one after another, in sequence. Each pulse travels into the tissue, interacts with the structures, and returns as a backscattered echo. The transducer then captures this echo signal, allowing you to measure reflections from specific depths‚Äîwhat we call the range gate.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 31, "slide_filename": "Slide31.txt", "slide_annotation": "Lecture 22 - Slide31.txt", "file_path": "Lecture 22\\Texts\\Slide31.txt", "content": "And I pause the wave, and I have two more ways to show you. So, let me just show you again. I hope the computer will not mess up. But I think good for you to see two more ways here. Just try. Okay. Okay. Try to pause the state, which means the reflector or surface does not have a\n\nLet‚Äôs go a little deeper into how the pulsed wave behaves. Imagine you‚Äôre sending a sequence of pulses toward a target that is not moving. You send one pulse, two pulses, three pulses‚Äîeach time the echo returns with the same amplitude and phase. If you line up all these received echoes at the same gated time instant, they all align perfectly. The phase remains constant, meaning the surface is stationary.\n\nNow, let‚Äôs imagine a different case‚Äîone where the reflector is moving or oscillating back and forth. In that case, when you line up the received echoes, you‚Äôll notice that their amplitudes and phases change from pulse to pulse. At a fixed gated time instant, the amplitude might be high for the first echo, lower for the second, and different again for the third. When you plot those amplitude values sequentially, they form a sinusoidal curve, representing a cyclic or oscillating motion.\n\nIf the target moves in a steady direction‚Äîsay, a uniform motion instead of an oscillation‚Äîthe resulting signal will not be sinusoidal but rather a linear change over time. So, each echo provides a single sampled data point from a given location, and by analyzing how those points evolve over a train of pulses, you can determine the relative motion of the scatterer or reflecting surface.\n\nIn your textbook, you‚Äôll find several formulas that describe this mathematically, but the key idea is quite straightforward. The pulse wave mode works by collecting one data point from each returning echo at a fixed range. If there is no motion, the points stay constant; if there is motion, the points vary in amplitude or phase. By examining those variations, we can determine relative motion, which can then be related back to Doppler frequency and ultimately to velocity. However, if the object moves too fast, the returning echoes can overlap or become aliased, causing artifacts in the signal. This is one of the limits of the pulsed wave Doppler technique.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 32, "slide_filename": "Slide32.txt", "slide_annotation": "Lecture 22 - Slide32.txt", "file_path": "Lecture 22\\Texts\\Slide32.txt", "content": "Now, let‚Äôs look at another approach‚Äîthe correlation method, which is quite intuitive. You start with a transducer that sends out a pulse of ultrasound and records the entire backscattered signal as a function of time. That gives you a complete waveform, not just a single data point. Then, you send a second pulse, and again you record the returned signal. By the time the second pulse arrives, the scatterers‚Äîsuch as red blood cells‚Äîwill have moved slightly compared to their original positions.\n\nTo find out how much motion occurred, we compare the two signals using correlation. You look for the time delay, denoted as tau, where the two signals are most similar. This time delay tells you how far the scatterers have moved between the two measurements. When you multiply that tau by the sound speed along the beam direction, you get the relative displacement. From that, you can calculate the velocity or even derive the Doppler frequency if you prefer.\n\nSo, this is the basic idea behind the correlation method‚Äîit‚Äôs simple but very effective. By comparing the returning waveforms over time, we can accurately measure how fast and in what direction the blood or tissue is moving. This is often used alongside Doppler analysis to enhance motion estimation accuracy.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 33, "slide_filename": "Slide33.txt", "slide_annotation": "Lecture 22 - Slide33.txt", "file_path": "Lecture 22\\Texts\\Slide33.txt", "content": "Now, this slide shows how the Doppler effect can be used to measure and visualize velocity. The Doppler information can be superimposed directly on the B-mode image. The B-mode gives you the anatomical structure ‚Äî for example, you can clearly see a large blood vessel here. Once you have that anatomical background, you can add the velocity field on top of it using color coding.\n\nIn color Doppler imaging, the motion is encoded using colors, typically the RGB scheme. Motion toward the transducer is displayed in red, while motion away from the transducer is shown in blue. Sometimes a green component is used to indicate the variance of the signal ‚Äî that is, how much fluctuation or turbulence there is in the flow. If the blood flow is smooth and uniform, you won‚Äôt see much green at all. But if the flow becomes turbulent or irregular, you‚Äôll see more green appearing in the image.\n\nThis color map gives a one-dimensional view of motion ‚Äî only along the direction of the ultrasound beam. It doesn‚Äôt tell you about lateral movement, but it‚Äôs very effective for examining flow in vessels. In more advanced cases, you can apply a Fourier transform to the Doppler signal to obtain the velocity distribution, because the motion is not always at a single frequency. Blood flow, for instance, has multiple velocity components due to pulsation and vessel geometry. So, color Doppler provides a dynamic, intuitive way to visualize those variations on top of anatomical images.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 34, "slide_filename": "Slide34.txt", "slide_annotation": "Lecture 22 - Slide34.txt", "file_path": "Lecture 22\\Texts\\Slide34.txt", "content": "Here we move to spectral Doppler imaging, as shown in Figure 3.23 from your textbook. This method provides more detailed information about the range of velocities within a vessel. You can see how the signal amplitude varies over the cardiac cycle ‚Äî during systole, when the heart contracts, and diastole, when it relaxes. During systole, the flow velocity is higher, which shifts the Doppler frequency upward. During diastole, the flow slows down, so the Doppler shift becomes smaller.\n\nThe result is a spectrum showing how signal amplitude changes with Doppler frequency over time. Each vertical line in the display represents the distribution of frequencies ‚Äî or velocities ‚Äî at a given instant. When plotted together, this forms the spectral waveform, which you often see in clinical Doppler displays. By analyzing these patterns, clinicians can assess blood flow speed, direction, and uniformity through the cardiac cycle.\n\nSo, to summarize, Doppler imaging includes several complementary modes: continuous wave, pulsed wave, and correlation methods, along with color and spectral Doppler. Continuous and pulsed modes measure velocity; color Doppler visualizes it; and spectral Doppler reveals its frequency content. Together, they provide a comprehensive view of motion and flow in ultrasound imaging ‚Äî anatomically and dynamically.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 35, "slide_filename": "Slide35.txt", "slide_annotation": "Lecture 22 - Slide35.txt", "file_path": "Lecture 22\\Texts\\Slide35.txt", "content": "Finally, let‚Äôs talk briefly about ultrasound image artifacts. Like any imaging modality, ultrasound is not perfect ‚Äî and various artifacts can appear due to limitations in the system or the physics of sound propagation. These artifacts may come from reflection, refraction, attenuation, or motion. Some are simply distortions, while others can mimic real anatomical structures.\n\nUnderstanding and recognizing these artifacts is extremely important, because they can affect how you interpret the image. We‚Äôll look at examples of these artifacts and discuss what causes them, and how they can sometimes even be used advantageously in image interpretation.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 36, "slide_filename": "Slide36.txt", "slide_annotation": "Lecture 22 - Slide36.txt", "file_path": "Lecture 22\\Texts\\Slide36.txt", "content": "Now, let‚Äôs talk about time-gain compensation. When we deal with very high-frequency ultrasound‚Äîultrasound signals in the megahertz range‚Äîthe signal amplitude can be extremely small. If we simply record the raw signal amplitude or intensity as it is, without doing any correction, you‚Äôll find that the deeper features inside the tissue appear much weaker than the ones near the surface.\n\nThis happens because of attenuation and wave scattering. As the ultrasound wave travels deeper, part of the energy is absorbed, and part of it is scattered in different directions. Both effects cause the returning signal‚Äîthe echo‚Äîto decay exponentially with depth. Near the surface, the reflected wave is strong, but as you move deeper, the signal strength drops off very quickly. The decay roughly follows an exponential curve.\n\nTo correct for this loss, we apply what‚Äôs called time-gain compensation, or TGC. The idea is simple: as the echo arrives later in time‚Äîwhich corresponds to greater depth‚Äîyou gradually increase the amplification factor. In other words, you boost the gain as a function of depth to counteract the exponential decay. The amplification curve roughly follows the opposite shape of the attenuation curve. When you multiply the received signal by this depth-dependent gain, the resulting image becomes more uniform in brightness.\n\nThis compensation makes the ultrasound image look much better and more interpretable. Without it, the deeper regions would appear dark and faint, while the shallow regions would look overly bright. With TGC, you recover a more balanced image. However, remember that this correction is based on assumptions about tissue homogeneity‚Äîit works well when the tissue properties are roughly uniform. In real biological tissue, conditions are not perfectly homogeneous, so the quantitative accuracy is not perfect.\n\nIt‚Äôs similar to what happens in CT imaging when we apply beam-hardening correction‚Äîwe can reduce the artifacts, but it‚Äôs never a perfect fix. So, keep in mind that in ultrasound, different echo times correspond to different depths, and the deeper the echo, the stronger the amplification you need to apply. That‚Äôs the principle of time-gain compensation.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 37, "slide_filename": "Slide37.txt", "slide_annotation": "Lecture 22 - Slide37.txt", "file_path": "Lecture 22\\Texts\\Slide37.txt", "content": "Now, let‚Äôs talk about the next type of artifact, which is caused by what we call side lobes. In ultrasound imaging, no matter how you modulate the transducer elements ‚Äî whether you use a one-dimensional or a two-dimensional array, or even a single transducer ‚Äî you can never create a perfectly straight, narrow beam. Each transducer element generates a spherical wave, and when these waves combine through interference, they form one strong main beam along the principal direction, plus several weaker beams on the sides ‚Äî these are the side lobes.\n\nPhysically, this happens because we‚Äôre dealing with waves, and waves naturally spread out. Even if we adjust the phase and amplitude across the array, we can‚Äôt perfectly confine all the energy into a single pencil-thin line. The best we can achieve is a main lobe with high intensity, surrounded by smaller side lobes with lower intensity. If you‚Äôve studied Fourier analysis, this is similar to what happens when you approximate a sharp function ‚Äî you always get some rippling, known as ringing. Here, the same principle applies to the ultrasound beam.\n\nNow, these side lobes can create misleading signals in the image. For example, if you‚Äôre scanning a region with a single bright reflector, the main lobe gives a strong echo at the correct location. But the side lobes can also pick up reflections from off-axis points, which then appear as false echoes in the image ‚Äî often as smaller or dimmer copies of the real structure. So, you might see multiple bright spots, even though only one real reflector exists. In clinical practice, recognizing this artifact is essential ‚Äî you may move the transducer slightly to confirm which echo is genuine and which is an artifact.\n\nAnother related effect is reverberation, which happens when echoes bounce back and forth between two reflective surfaces ‚Äî such as tissue layers or an air-tissue interface. These multiple reflections cause several delayed echoes to appear on the image, even though only one actual surface exists. They appear as repeated, evenly spaced lines, decreasing in intensity with depth. You can see examples of these side lobe patterns and reverberation artifacts in the slide images here. Understanding them helps you identify what‚Äôs real and what‚Äôs simply an effect of wave interference and multiple reflection.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 38, "slide_filename": "Slide38.txt", "slide_annotation": "Lecture 22 - Slide38.txt", "file_path": "Lecture 22\\Texts\\Slide38.txt", "content": "Let‚Äôs continue with more examples of ultrasound artifacts. One common type is caused by multiple reflections between layers. Imagine sending one ultrasound pulse into a structure like the lung. The wave first reflects from the top surface, giving you the first echo. Some energy penetrates deeper, hits the bottom surface, and reflects. But on its way, that echo may bounce again from the top surface and return once more to the transducer. As a result, even though you sent only one pulse, you detect several echoes arriving at different times. This produces a sequence of repeated bands on the image ‚Äî very similar to hearing multiple ‚Äúhellos‚Äù echoing in a mountain valley after shouting once.\n\nAnother important artifact is acoustic shadowing. This occurs when a dense object ‚Äî for instance, a gallstone or kidney stone ‚Äî strongly reflects or absorbs the ultrasound energy. Because the wave cannot penetrate beyond it, there‚Äôs no signal coming from the tissue behind the object. On the image, this appears as a dark region, or ‚Äúshadow,‚Äù extending below the bright reflective surface. You can clearly see this in the figure on the right, where the shadow forms behind a gallstone.\n\nIn the images on this slide, you can see both types of artifacts ‚Äî reverberation lines appearing as repeated echoes, and acoustic shadowing creating dark regions behind dense materials. Recognizing these patterns is essential for accurate diagnosis. They‚Äôre not always errors; sometimes they actually help identify the nature of the structure ‚Äî for example, confirming the presence of a stone or a metallic implant. But conceptually, you should always understand how these artifacts form and what they tell you about the ultrasound-tissue interaction.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 39, "slide_filename": "Slide39.txt", "slide_annotation": "Lecture 22 - Slide39.txt", "file_path": "Lecture 22\\Texts\\Slide39.txt", "content": "Now, in the last part of this lecture, let‚Äôs take a look at a few interesting new ideas in ultrasound imaging. We won‚Äôt go deep into equations or technical details here ‚Äî I just want to give you a brief overview of some of the most exciting developments happening right now in this field.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 40, "slide_filename": "Slide40.txt", "slide_annotation": "Lecture 22 - Slide40.txt", "file_path": "Lecture 22\\Texts\\Slide40.txt", "content": "We have a faculty member, Professor Pingkun Yan, who has done excellent work in collaboration with Philips, a leading medical imaging company. Their research focuses on combining ultrasound and MRI imaging for prostate cancer surgery. The idea is to use MRI before the operation and ultrasound during the operation, so you can merge or ‚Äúfuse‚Äù the information from both modalities.\n\nMRI imaging provides excellent spatial resolution and sensitivity ‚Äî it can clearly show anatomical and biological details, which makes it ideal for locating the tumor before surgery. However, MRI scanners are very expensive and not cost-effective for real-time surgical use. They also have slow temporal resolution, meaning it takes time to acquire the image. On the other hand, ultrasound imaging is fast, inexpensive, and easy to use in real time, but its spatial resolution and sensitivity are not as high.\n\nBy combining the two, surgeons can perform preoperative MRI imaging to identify the problem, and then, during the operation, use ultrasound as real-time guidance. Through image registration, or fusion, the live ultrasound image can be aligned with the preoperative MRI. This allows the surgeon to visualize the tumor location with both high spatial detail and real-time feedback. The fusion system developed by Dr. Yan‚Äôs group has produced very promising results in clinical trials.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 41, "slide_filename": "Slide41.txt", "slide_annotation": "Lecture 22 - Slide41.txt", "file_path": "Lecture 22\\Texts\\Slide41.txt", "content": "Now, if you look at the images here, you‚Äôll see what happens if the image registration between ultrasound and MRI is not done properly ‚Äî the structures won‚Äôt line up, and the surgeon can‚Äôt accurately target the tumor. Without good alignment, it‚Äôs impossible to rely on the fused images for surgery.\n\nTo solve this, Dr. Yan‚Äôs team developed machine learning algorithms to automatically register ultrasound and MRI images. Using these intelligent registration techniques, the alignment quality improves dramatically ‚Äî for example, in their results, the similarity index increases from about 0.24 to nearly 0.99. That‚Äôs a huge improvement.\n\nWith this level of precision, surgeons gain much higher confidence during the procedure, leading to better surgical outcomes. So this hybrid imaging approach ‚Äî combining MRI‚Äôs high spatial resolution with ultrasound‚Äôs real-time imaging capability ‚Äî represents a powerful direction for image-guided surgery. It‚Äôs a very active research area with many exciting developments and a strong clinical impact.\n\nNext, we‚Äôll look at another hybrid direction ‚Äî combining ultrasound imaging with optical imaging.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 42, "slide_filename": "Slide42.txt", "slide_annotation": "Lecture 22 - Slide42.txt", "file_path": "Lecture 22\\Texts\\Slide42.txt", "content": "In the next lecture, we‚Äôll move into optical imaging, but before that, let‚Äôs look at how ultrasound and optical imaging can be combined to create a powerful new imaging technique known as photoacoustic tomography. The idea is shown here. Imagine you inject an animal with contrast agents ‚Äî in this case, graphene-based microbubbles. Once these microbubbles are introduced, they enhance the ultrasound signal because the presence of microbubbles makes acoustic reflections stronger.\n\nNow, graphene has an interesting property: it can absorb light energy and convert it into heat. So, when you shine near-infrared light pulses onto the tissue, the graphene microbubbles absorb the light and heat up slightly. This heating causes a tiny thermal expansion ‚Äî the microbubbles expand and contract in response to the pulsed laser. Each laser pulse causes a brief expansion followed by cooling, and this mechanical vibration generates an acoustic wave, much like a sound pulse. These acoustic waves are then detected by an ultrasound transducer.\n\nSo, you can think of the process as a chain: near-infrared excitation leads to graphene absorption, which causes heating and thermal expansion, generating acoustic waves that are picked up by an ultrasonic detector. From these signals, you can construct an image for diagnosis. The advantage is that the contrast is introduced optically, while the detection is done acoustically, giving both sensitivity and depth penetration. In addition, some of these microbubbles can be used for therapeutic applications ‚Äî for example, targeted drug delivery. There‚Äôs a lot of exciting research happening in this area, exploring both diagnostic and therapeutic uses.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 43, "slide_filename": "Slide43.txt", "slide_annotation": "Lecture 22 - Slide43.txt", "file_path": "Lecture 22\\Texts\\Slide43.txt", "content": "Here you see some examples of photoacoustic tomography from Professor Lihong Wang, who made pioneering contributions to this field. This imaging method can be applied at multiple scales ‚Äî from single cells and small biological structures to whole animals and even human patients. The remarkable thing is that it combines the strengths of both optical and ultrasound imaging.\n\nPurely optical imaging provides excellent contrast but limited penetration depth ‚Äî the light scatters too easily, so you can only see shallow structures. Ultrasound, on the other hand, penetrates much deeper and gives good spatial resolution, but the contrast is not as strong because it depends mainly on acoustic properties. When you combine the two in photoacoustic imaging, you get the best of both worlds: strong optical contrast and deeper acoustic penetration.\n\nThis hybrid approach allows researchers to visualize structures ranging from microvasculature to organ-level features, making it a powerful tool for both biological research and clinical applications. It‚Äôs a perfect example of how multi-modality imaging can overcome the limitations of individual techniques.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 44, "slide_filename": "Slide44.txt", "slide_annotation": "Lecture 22 - Slide44.txt", "file_path": "Lecture 22\\Texts\\Slide44.txt", "content": "Finally, here‚Äôs something still speculative ‚Äì an iPhone-based ultrasound imager. With this device, we could turn an iPhone into a miniature ultrasound device. Imagine using your phone not just as a communication tool but as a health monitoring instrument. For example, we might use the phone‚Äôs screen or back surface as a transducer array to measure things like blood velocity, blood pressure, or even blood sugar.\n\nLet‚Äôs consider blood pressure first. Traditionally, blood pressure is measured with a cuff that compresses the vessel, but perhaps we could use Doppler ultrasound principles instead. When blood flows through a vessel, the pressure from the heartbeat changes both the velocity and the cross-sectional area of the vessel. When the pressure is high, the vessel expands; when it‚Äôs low, the vessel contracts. The velocity of blood flow also changes accordingly and can be measured. The shape of vessels can be measured as well.\n\nIf we could measure both the velocity (through the Doppler shift) and the vessel wall motion (by detecting the relative distance between the top and bottom surfaces), then, with proper calibration and machine learning, we might estimate blood pressure accurately ‚Äî just by placing the iPhone over the skin. Over time, the system could learn your personal profile and give real-time blood pressure readings without the discomfort of a cuff. The same principle could even be extended to use an optical sensor for measuring blood sugar related signals, by analyzing light interactions with tissue. These ideas are still pre-mature, but with advances in materials, transducers and sensors, they may become realistic in the future.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 22", "slide_number": 45, "slide_filename": "Slide45.txt", "slide_annotation": "Lecture 22 - Slide45.txt", "file_path": "Lecture 22\\Texts\\Slide45.txt", "content": "Alright, so this is the last slide for today. I‚Äôm not going to give you any regular homework, but it‚Äôs a good time for you to start reviewing or re-reviewing the ultrasound chapter ‚Äî what I usually call the green chapter. For my classroom teaching, I want to take a moment to share a few thoughts about how traditional classroom learning compares with what we‚Äôre trying to do here.\n\nTraditional teaching is often about knowledge transfer. In other words, you‚Äôre given a bunch of information ‚Äî formulas, numbers, and rules ‚Äî and you‚Äôre expected to become familiar with what other people have already done. You learn the steps, you can repeat them, you can reproduce the same results, and you can memorize the equations. There‚Äôs nothing wrong with that ‚Äî it‚Äôs still a good foundation. But our goal goes a bit beyond that.\nWhat I really want is to inspire you to have new ideas. To reach that point, you have to truly understand what‚Äôs going on beneath the surface. For example, when we studied the Doppler system earlier, I didn‚Äôt just want you to memorize the formula. I wanted you to see the reasoning behind it ‚Äîto connect the physics to what you actually observe. That‚Äôs the kind of understanding that helps you innovate. We encourage you not to simply copy what others are doing. Don‚Äôt just see a bridge someone has built and make the same bridge again ‚Äî instead, learn how to design your own. \n\nYou need to create, to innovate, to generate something new. That‚Äôs far more important.\nNow, when I talk about reviewing, we still use a closed-book or closed-form style of study. You can use the book, the slides, and the solutions. Even if you don‚Äôt understand every detail right away, I want you to read through the green chapter. Learn the concepts, understand the examples, and become familiar with the existing answers. \n\nAt the same time, try to think hard where you can improve or add new functionality, such as iPhone-based possibilities.\nYou need to think beyond memorization ‚Äî think about problem identification. When you review the material, try to notice how pioneers and earlier researchers identified their problems. How did they get their ideas? How did they use the tools available to them? Even if the textbook doesn‚Äôt directly give you an answer, you can always look for more ‚Äî use Google Scholar, use Web of Science, explore the literature, and ask ChatGPT and other AI models. Learn to find what you need and to solve problems independently.\n\nAnd finally, motivation matters. The traditional way of teaching doesn‚Äôt always emphasize new ideas or encourage creativity. But in real research, motivation drives discovery. That‚Äôs what I want to underline. So, when you review and study, don‚Äôt just focus on memorizing the content ‚Äî look for the underlying meaning behind it. Try to understand the ideas, the motivation, and the reasoning that led to them.\nSo anyway ‚Äî that‚Äôs all for today. Take some time to reflect, review the green chapter, and start thinking creatively about how you can connect what you‚Äôve learned to new ideas. That‚Äôs how innovation really begins.", "total_slides_in_lecture": 45}
{"lecture": "Lecture 23", "slide_number": 1, "slide_filename": "Slide1.txt", "slide_annotation": "Lecture 23 - Slide1.txt", "file_path": "Lecture 23\\Texts\\Slide1.txt", "content": "Today, we‚Äôll be covering the final imaging modality in our course‚Äîoptical imaging. This topic isn‚Äôt included in the green textbook, so this lecture stands alone. Think of it as a story‚Äîone that unfolds step by step to show you how light can be used to visualize and analyze biological structures and functions.\n\nAs we go along, try to follow the flow of this story rather than memorizing isolated facts and details. Optical imaging combines fascinating principles from physics, biology, and engineering, and by learning how these ideas connect, you‚Äôll develop a deep intuition for the topic.\n\nNow, let‚Äôs approach this lecture with curiosity. Enjoy the journey, stay engaged, and you‚Äôll come away with a clear picture of how optical imaging works and why it plays such an important role in modern biomedical imaging.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 2, "slide_filename": "Slide2.txt", "slide_annotation": "Lecture 23 - Slide2.txt", "file_path": "Lecture 23\\Texts\\Slide2.txt", "content": "As you can see from our course schedule, today‚Äôs topic is Optical Imaging, listed here as Lecture 23. This is our final imaging modality in the sequence, coming right after the ultrasound sessions.  This lecture will connect what we‚Äôve learned about other imaging techniques to the world of light-based imaging.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 3, "slide_filename": "Slide3.txt", "slide_annotation": "Lecture 23 - Slide3.txt", "file_path": "Lecture 23\\Texts\\Slide3.txt", "content": "Today‚Äôs lecture on optical imaging will be divided into four main parts.\u000bWe‚Äôll begin with optical microscopy, which is the foundation of optical imaging. This remarkable technology was the first to let us see individual cells and even cell division ‚Äî truly a revolutionary step in science. In this part, we‚Äôll discuss the basic idea of visible light as a segment of the electromagnetic wave and how it interacts with biological tissues. Understanding these optical and tissue properties gives us the foundation we need for everything that follows.\n\nNext, we‚Äôll move to optical coherence tomography, or OCT. I‚Äôll explain its basic principle ‚Äî a fascinating concept based on light interference ‚Äî and show you some of its clinical applications. While optical microscopy dominates biology labs, OCT is widely used in medical settings, especially in eye clinics. These two methods ‚Äî microscopy and OCT ‚Äî are the most established optical imaging techniques with real clinical and research impact.\n\nAfter that, we‚Äôll briefly explore diffuse optical imaging, including techniques such as diffuse optical spectroscopy (DOS), diffuse optical tomography (DOT), fluorescence molecular tomography (FMT), and bioluminescence tomography (BLT). These are powerful research tools for small-animal and molecular imaging, though they are not yet commonly used in hospitals. Finally, we‚Äôll touch on X-ray optical coupling, which includes X-ray luminescence computed tomography (XLCT) and X-ray micro-modulated luminescence tomography (XMLT) ‚Äî exciting research directions that bridge X-ray and optical techniques.\n\nBecause this lecture stands alone without a textbook chapter, I‚Äôve reorganized the material into a single, coherent story that combines what used to be two separate lectures. Follow along closely ‚Äî treat this as a guided story rather than a set of isolated facts. At the end, you‚Äôll find a homework question asking you to summarize key ideas like OCT, DOT, and BLT. Optionally, you can also transcribe parts of the lecture to help form a draft chapter. My goal is to make the lecture clear, logical, and enjoyable ‚Äî so if you simply follow the story, you‚Äôll understand the essential points.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 4, "slide_filename": "Slide4.txt", "slide_annotation": "Lecture 23 - Slide4.txt", "file_path": "Lecture 23\\Texts\\Slide4.txt", "content": "Now, let‚Äôs begin our story ‚Äî and at the beginning, there is light.\n\u000bLight is part of the electromagnetic, or EM, wave spectrum, which spans an enormous range of wavelengths. We‚Äôve already talked about the shorter wavelengths, such as gamma rays used in nuclear imaging and X-rays used in computed tomography. On the other end, we have radio waves and microwaves, which are much longer and used in applications like MRI and even cooking.\n\nVisible light, however, occupies only a very narrow portion of this vast spectrum. Specifically, it ranges roughly from 400 nanometers to about 1,000 nanometers in wavelength. Within this band, our eyes perceive different colors ‚Äî violet, blue, green, yellow, and red ‚Äî depending on the wavelength. You‚Äôve seen this effect when light passes through a prism, spreading into a rainbow of colors.\n\nSome animals can detect wavelengths beyond our range. For example, certain species can see ultraviolet or even X-ray light, as confirmed by electroretinogram studies. Among humans, individual sensitivity also varies, and a small percentage of people are color-blind, perceiving only shades of gray.\n\nFor medical imaging, each modality uses a different part of the spectrum depending on its purpose. In optical imaging, we focus on the portion we can directly see ‚Äî the visible and near-infrared range. These wavelengths penetrate tissue to a useful depth, making them ideal for studying cells, tissues, and biological processes using light. Understanding this basic physical foundation will help you appreciate how optical imaging works and why it‚Äôs such a powerful technique.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 5, "slide_filename": "Slide5.txt", "slide_annotation": "Lecture 23 - Slide5.txt", "file_path": "Lecture 23\\Texts\\Slide5.txt", "content": "Now let‚Äôs talk about how light interacts with biological tissue.\u000b\nJust like X-rays interact with the human body in medical imaging, visible light also interacts with biological tissues similarly. As shown in this diagram, you can see the different behaviors that occur when light reaches the surface of tissue. There‚Äôs the incident light, which hits the surface, and part of it gets reflected right away. Some of the light, however, penetrates into the tissue and becomes refracted light. Inside the tissue, that light can undergo scattering ‚Äî bouncing around in different directions. Some of the scattered photons may come back out, which we call remitted light, while others continue to travel through and emerge as transmitted light.\n\nThese are the basic mechanisms of light‚Äìtissue interaction. You can think of them as reflection, scattering, and transmission. When we send a beam of light into a piece of tissue, some of that light just passes straight through, continuing in roughly the same direction ‚Äî that‚Äôs the transmitted portion. The rest is either reflected or scattered randomly. The light intensity that we detect decreases as the beam travels deeper, and that attenuation happens because of two main effects ‚Äî absorption and scattering.\n\nAbsorption means that some of the light energy is absorbed by the tissue and converted into heat. Scattering, on the other hand, means that photons are deflected from their original path in different directions. Together, absorption and scattering reduce the intensity of the transmitted beam ‚Äî very much like what happens in X-ray imaging. So, when we think about optical imaging, we‚Äôre dealing with these same physical ideas ‚Äî transmission, absorption, scattering, and reflection ‚Äî but now in the visible and near-infrared light range, where tissue behaves quite differently.\n\nSo in summary, when you shine light into biological material, part of it is reflected, part of it penetrates and scatters, and part of it is absorbed. The balance among these processes depends on the tissue thickness and optical properties. These interactions are the foundation of optical imaging and determine how deeply light can penetrate and what kind of information we can obtain about biological structures.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 6, "slide_filename": "Slide6.txt", "slide_annotation": "Lecture 23 - Slide6.txt", "file_path": "Lecture 23\\Texts\\Slide6.txt", "content": "Now, let‚Äôs look more closely at scattering in biological tissue.\u000b\nScattering happens because of all the small structures within cells and tissues. The exact way light scatters depends on the size, shape, and refractive index of those structures. We can actually classify scattering mechanisms based on the relative scale of the structures compared to the wavelength of light. When the structures are much smaller than the wavelength, we call that Rayleigh scattering. When they‚Äôre comparable in size to the wavelength, we call that Mie scattering.\n\nI‚Äôm not going to dive into the mathematical formulas or the detailed physics behind these mechanisms ‚Äî that would take us too far. But the key point is this: visible light directly interacts with cellular and molecular features. Every component inside a cell ‚Äî the membrane, the nucleus, mitochondria, lysosomes, and so on ‚Äî all participate in scattering. Depending on their size, each component affects light differently. So the light that comes out of tissue actually carries information about the microstructure inside.\n\nThis is why optical imaging is so biologically informative. It tells us about cells and molecules directly ‚Äî something that X-rays or gamma rays can‚Äôt do. Optical imaging gives us the finest biological resolution among all imaging modalities we have today. The spatial scale of interaction is on the order of micrometers ‚Äî that‚Äôs the cellular scale.\n\nA typical cell is about 10 to 30 micrometers in diameter ‚Äî that‚Äôs 10 to 30 millionths of a meter. Different cell types vary, of course, and within each cell, there‚Äôs a huge amount of structure. You have the nucleus, the nucleolus, the chromatin, the mitochondria, the endoplasmic reticulum, the Golgi apparatus, many channels and vesicles ‚Äî all these substructures scatter light differently. Each layer contributes to the complexity of how light behaves in tissue.\n\nYou know, cells are often called the ‚Äúbuilding blocks of life.‚Äù If you open any major journal like Science or Nature, you‚Äôll see that phrase everywhere ‚Äî and it‚Äôs true. Cells form the basic computational and functional units of biological systems. Later, when we talk about machine intelligence, you‚Äôll see how we borrow this idea ‚Äî we use ‚Äúneurons‚Äù in artificial neural networks to mimic the way biological neurons process information.\n\nSo, to summarize, scattering happens at multiple levels ‚Äî from whole cells to tiny organelles ‚Äî and it gives us an incredible amount of information about biological organization. The optical properties vary with each structure, making tissue highly complex but also rich in information. And just as in X-ray imaging, we have concepts like transmission, absorption, fluorescence, and scattering. When all these are considered together, we call the combined attenuation ‚Äî it‚Äôs very similar terminology. So you can see, optical imaging and X-ray imaging share parallel physics, but optical imaging operates at much smaller, biologically relevant scales.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 7, "slide_filename": "Slide7.txt", "slide_annotation": "Lecture 23 - Slide7.txt", "file_path": "Lecture 23\\Texts\\Slide7.txt", "content": "Now, in addition to scattering and absorption, we also have another very important mechanism called fluorescence.\u000bIn X-ray imaging, we already know about X-ray fluorescence ‚Äî when we send X-rays into a material, they can produce secondary X-rays with different energies. Similarly, in optical imaging, we can have fluorescence in the visible range. Certain biological molecules or engineered proteins can absorb light of one color and emit light of another color with a longer wavelength.\n\nFor example, proteins can be tagged with fluorescent markers. One of the most famous is the green fluorescent protein, or GFP. When you genetically attach GFP to a specific protein ‚Äî let‚Äôs say to liprin or another biomolecule ‚Äî they form a pair. The biological part represents your target or biomarker, and GFP serves as the fluorescent reporter. When you shine a blue laser light ‚Äî about 488 nanometers in wavelength ‚Äî onto this combination, the GFP absorbs the blue light and emits bright green light. So, when you see that green fluorescence, you know where your biomarker is located.\n\nIn other words, by detecting the emitted fluorescence, you can infer the spatial distribution of your biological target. This is the basic principle behind fluorescence imaging. You can even design these proteins so that they target different molecules ‚Äî for instance, one may emit green, another red, another yellow ‚Äî allowing you to track multiple biological processes at once. This kind of optical labeling gives you the ability to visualize molecular events inside living systems in real time.\n\nFluorescence is a very powerful concept. It lets us do what we call ‚Äúbio-design.‚Äù Using genetic and bioengineering tools, we can attach different fluorescent proteins ‚Äî green, pink, red ‚Äî to different biomolecules. When those molecules move or react inside a cell, we can literally see where and when they act, based on the color of light emitted.\n\nThis work was so groundbreaking that it earned a Nobel Prize, awarded to several researchers from Japan and California who helped develop and refine these fluorescent proteins. With fluorescence imaging, we can observe living biological systems directly ‚Äî not just fixed samples. We simply illuminate the sample with the appropriate light to excite the fluorescent probe, and then detect the emitted light, which is usually at a longer wavelength. Both the excitation and emission are in the visible range, making it easy to detect and analyze.\n\nSo overall, fluorescence enables us to perform cellular and molecular imaging in a very direct, intuitive way. You can send in light, stimulate the fluorophore, and watch as it emits a different color of light ‚Äî giving you a real-time window into biological activity. This is the essence of fluorescence, and it forms one of the most important foundations of modern optical imaging.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 8, "slide_filename": "Slide8.txt", "slide_annotation": "Lecture 23 - Slide8.txt", "file_path": "Lecture 23\\Texts\\Slide8.txt", "content": "Now let‚Äôs talk about bioluminescence. So, what exactly is bioluminescence?\u000bYou‚Äôve probably seen it yourself ‚Äî on a summer night when the air is warm and the grass is glowing here and there with tiny flashing lights. Those little bugs you see flying around with glowing tails ‚Äî fireflies ‚Äî are perfect examples of bioluminescent creatures. Or, if you‚Äôve ever been near the ocean at night, sometimes you can see a faint glow in the water ‚Äî that‚Äôs also bioluminescence, produced by microscopic marine organisms.\n\nSo bioluminescence simply means light that is produced by a living organism. It‚Äôs very similar in appearance to fluorescence ‚Äî both involve light emission from certain molecules or proteins ‚Äî but the mechanism is quite different. In fluorescence, you must first shine an external light, such as a laser or lamp, to excite the molecules, and then they emit light as they relax back to their original energy state. But in bioluminescence, you don‚Äôt have to do anything at all. The organism itself produces light through a chemical reaction that happens naturally inside its body. No laser, no excitation source ‚Äî just spontaneous light emission.\n\nThink of those insects again: they‚Äôre not being ‚Äúshined on‚Äù by any light source, yet you can clearly see them glowing green or pink as they fly. That‚Äôs bioluminescence ‚Äî living creatures generating visible light on their own. It‚Äôs sometimes called cold light, because it doesn‚Äôt involve heat like a light bulb does.\n\nFrom the imaging point of view, this phenomenon is extremely useful. Scientists can isolate the light-emitting proteins from these organisms ‚Äî the so-called bioluminescent proteins ‚Äî and then combine them with other target molecules, such as specific genes or cancer markers. When those target genes are expressed in cells, the bioluminescent signal becomes visible. In other words, if a cancer cell expresses a certain gene, it will literally light up ‚Äî allowing us to observe biological processes directly, inside living tissue.\n\nSo, thanks to optical imaging, we can now visualize biological activity in vivo, meaning within a living organism, in real time. This is a major step forward for modern biomedical research. Bioluminescence, like fluorescence, represents another important form of light‚Äìtissue interaction ‚Äî one where living systems themselves emit light through molecular mechanisms. In the next part, we‚Äôll discuss how we can use these natural light-emitting processes to actually perform imaging.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 9, "slide_filename": "Slide9.txt", "slide_annotation": "Lecture 23 - Slide9.txt", "file_path": "Lecture 23\\Texts\\Slide9.txt", "content": "Alright, now let‚Äôs take a moment to look at something simple and familiar ‚Äî the optical lens.\u000bYou‚Äôve seen these before: there are two basic types of lenses ‚Äî converging lenses and diverging lenses. I think most of you learned this back in high school, right? Maybe in Physics II, or in your earlier science classes. No? Well, in any case, this is something that every student should know at least conceptually, and if you don‚Äôt, that‚Äôs perfectly fine ‚Äî we‚Äôll go through it together now.\n\nSo, here‚Äôs the basic idea. When you send in a parallel beam of light into a converging lens, the rays bend, or refract, toward each other and meet at a single point called the focal point. For a diverging lens, it‚Äôs the opposite ‚Äî the rays spread apart after passing through the lens. If you trace those diverging rays backward, they seem to come from a single point on the same side of the lens ‚Äî that‚Äôs the virtual focal point.\n\nNow, how do we actually form an image using these light rays? Let‚Äôs think about a simple example. If you have a real object in front of a converging lens, a ray parallel to the optical axis will bend and pass through the focal point on the other side. Another ray that passes through the optical center of the lens continues in a straight line, unchanged in direction. Where those two rays meet, that‚Äôs where the image forms. This is what we call ray tracing, and it‚Äôs a fundamental rule of geometrical optics.\n\nThe trick to making this work lies in the curvature of the lens surfaces. If you design the curvature properly, you can make all the parallel beams focus precisely at the same point. Usually, we approximate the surface as spherical when the lens is thin ‚Äî that‚Äôs why it‚Äôs called the thin lens approximation. So as long as the curvature is correct and the lens is thin, parallel rays will converge or diverge exactly as expected.\n\nThere‚Äôs also an important rule here: when a light ray passes through the optical center ‚Äî the very middle of the lens ‚Äî its direction does not change. It just goes straight through. That‚Äôs one of the key rules for tracing rays. Another important principle is reversibility ‚Äî light follows the same path backward and forward. So, if a ray travels from the object through the lens to form an image, the light could just as easily travel the opposite way, from that image point back through the lens to the object.\n\nIf you take the time to think about it carefully, this all makes sense geometrically. The symmetry of the two curved surfaces ensures that when a ray passes through the center, the small refraction at one surface is canceled by the opposite refraction at the other, keeping the ray direction unchanged. That‚Äôs why it goes straight through without bending. All these properties ‚Äî convergence, divergence, optical center, and reversibility ‚Äî together form the essential rules for understanding how lenses create images.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 10, "slide_filename": "Slide10.txt", "slide_annotation": "Lecture 23 - Slide10.txt", "file_path": "Lecture 23\\Texts\\Slide10.txt", "content": "Now, let‚Äôs summarize what we just discussed by looking at the rules for ray tracing.\u000bThere are a few simple but very important rules that describe how light behaves when passing through a lens. Rule number one says that a ray passing through the center of a lens will not be deflected ‚Äî it keeps its direction. This happens because the lens is symmetric, and the small amount of bending at one surface is canceled by an equal and opposite bending at the other. So, any ray that goes right through the center remains straight.\n\nRule number two tells us that rays parallel to the optical axis will converge to the focal point on the opposite side after passing through a converging lens, or will appear to come from a focal point on the same side in the case of a diverging lens. And if you reverse the process ‚Äî sending rays from the focal point through the lens ‚Äî they‚Äôll come out parallel to the optical axis. This is a beautiful symmetry and is very useful when tracing images.\n\nThen we have rule number three, which extends that idea. Parallel rays entering the lens at different angles all meet at a single focal plane after refraction. This rule also works in reverse. These three rules are the foundation of geometrical optics, and with them, you can quickly predict where and how an image forms.\n\nSo, when you combine these ideas, you can understand how a real image forms with a converging lens ‚Äî the rays physically meet at a point on the opposite side. For a diverging lens, the rays spread apart, and the image appears to form on the same side as the object ‚Äî that‚Äôs a virtual image. These concepts are very simple once you visualize how the rays travel.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 11, "slide_filename": "Slide11.txt", "slide_annotation": "Lecture 23 - Slide11.txt", "file_path": "Lecture 23\\Texts\\Slide11.txt", "content": "Here we can see some examples of actual lenses used in optical systems.\u000bThese are the real-world components that apply all those rules we just discussed. You can see lenses of different shapes and sizes ‚Äî convex, concave, cylindrical, plano-convex, biconvex, and so on. Each type has a specific purpose, but all follow the same physical principles of light refraction and focusing.\n\nIf you‚Äôre patient enough to spend time studying these lenses, you‚Äôll find that the underlying physics is straightforward and elegant. What‚Äôs fascinating is how we can shape these pieces of glass to control light precisely ‚Äî to focus it, spread it, or redirect it any way we want.\n\nIn fact, working with lenses can be quite enjoyable. When you actually draw ray diagrams and see them align perfectly, it‚Äôs like creating a piece of art. In optical labs, researchers and engineers spend a lot of time polishing lenses and adjusting their curvature to make sure light behaves exactly as intended. It‚Äôs a combination of science and craftsmanship.\n\nSo, remember ‚Äî even though the optical principles are simple, mastering them opens up an entire world of imaging technology. Everything from microscopes to cameras to telescopes depends on these very same ideas of ray tracing and lens design.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 12, "slide_filename": "Slide12.txt", "slide_annotation": "Lecture 23 - Slide12.txt", "file_path": "Lecture 23\\Texts\\Slide12.txt", "content": "Now let‚Äôs take a look at something even more fun ‚Äî the zoom lens.\u000bYou can see in this diagram and the photos on the right how the zoom lens works. It‚Äôs really cool. The zoom lens is designed so that by moving certain lens elements back and forth, you can continuously change the focal length. That means you can make the image appear closer or farther away without ever changing the focus on your object.\n\nHere you can see the different lens groups ‚Äî usually labeled L1, L2, L3 ‚Äî working together. When the internal lenses move relative to each other, the path of the light rays changes. The blue, yellow, and green paths you see here represent different light rays being bent and refocused as the lenses shift. By doing this, the system can magnify what you‚Äôre looking at ‚Äî that‚Äôs the ‚Äúzoom‚Äù part.\n\nSo, as you zoom in, you can magnify small details more and more. You start by seeing larger features, then zoom in again to reveal smaller and smaller structures ‚Äî down to cellular or even sub-cellular levels, depending on the optical system. The magnifying power increases with each step, allowing you to explore fine details that are invisible to the naked eye.\nThat‚Äôs the basic idea behind zoom optics. It‚Äôs not just about bringing things closer ‚Äî it‚Äôs about dynamically adjusting magnification while keeping the image in focus. And that ability to continuously zoom while maintaining clarity is one of the most powerful features of modern optical systems, especially in microscopy and photography.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 13, "slide_filename": "Slide13.txt", "slide_annotation": "Lecture 23 - Slide13.txt", "file_path": "Lecture 23\\Texts\\Slide13.txt", "content": "Now, let me talk about a very cool idea ‚Äî the confocal optical microscope.\u000bThis is truly a clever and elegant design in optical imaging. Let me first describe the essential concept. You can see here in the diagram that we start with a laser source ‚Äî this is our input light. The laser beam is reflected by a semi-transparent mirror, or what we call a beam splitter, and it‚Äôs directed downward through a set of lenses that focus the light onto a single tiny spot, known as the focal spot. That blue plane you see here represents the focal plane ‚Äî the layer of the specimen that‚Äôs currently in focus.\n\nNow, of course, light doesn‚Äôt only come from that exact focal plane. Some light is reflected or emitted from regions above or below the focal plane ‚Äî those are the out-of-focus areas. These are shown here as the dotted lines. The genius of the confocal design lies in how it handles these different signals. Only light from the true focal point is allowed to reach the detector. How? There‚Äôs a small pinhole aperture placed in front of the detector. This pinhole blocks light coming from out-of-focus regions, allowing only the in-focus light ‚Äî the sharply focused point ‚Äî to pass through.\n\nAs a result, the microscope effectively captures light from one small point at a time, dramatically increasing the resolution and contrast of the image. That‚Äôs why we call it confocal: there‚Äôs a focus point in the illumination path and another in the detection path ‚Äî both confocal, both matched to the same plane. The effect is that all blurred background light is rejected, giving you an image that‚Äôs crisp, clean, and precise.\n\nBut there‚Äôs more ‚Äî because this setup only collects light from one point at a time, we can‚Äôt capture a full image in one shot. Instead, the system uses an X-Y scanning device to move the laser spot across the sample, scanning point by point and line by line. Each time, one pixel of data is collected, and after scanning the entire area, you can reconstruct a two-dimensional image. If you then scan multiple focal planes at different depths, you can combine them to form a three-dimensional, volumetric image of the specimen.\n\nThis technique revolutionized optical microscopy. You can think of it as starting from the basic microscope ‚Äî the magnifying glass we all know ‚Äî and then taking a major leap forward. Traditional microscopes magnify, but they can‚Äôt reject the out-of-focus blur very well. The confocal microscope solves that problem beautifully by adding precision optics and scanning. It‚Äôs a milestone in optical imaging, giving us a much higher resolution and clearer view of microscopic structures.\n\nSo, in short, the confocal optical microscope is a powerful tool that combines focused laser illumination, spatial filtering, and precise scanning to achieve incredibly sharp images. It‚Äôs one of those designs that feels both simple and brilliant once you understand it ‚Äî truly a landmark in modern biomedical imaging.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 14, "slide_filename": "Slide14.txt", "slide_annotation": "Lecture 23 - Slide14.txt", "file_path": "Lecture 23\\Texts\\Slide14.txt", "content": "If you‚Äôre interested, you can click the link on this slide to watch an actual cell division video.\u000bWhat you‚Äôre seeing in that video is an image captured using the confocal microscopy principle that we just talked about. The image clearly shows living cells dividing, and the red-colored regions represent the nuclei or specific fluorescent markers that have been tagged so we can visualize the process in real time.\n\nThis is a great example of how confocal microscopy works in practice. The technology was so revolutionary that it became one of the most important tools in biological research. It‚Äôs amazing to think that this entire concept ‚Äî scanning one point at a time through a small aperture to reject out-of-focus light ‚Äî came from an idea developed when its inventor was still a PhD student. It shows how a simple but brilliant idea can completely change the way we see the microscopic world.\n\nEven though the confocal microscope may seem common today, the design is still considered incredibly clever. It‚Äôs a perfect example of elegant engineering ‚Äî combining optical precision with computational reconstruction to produce crisp, high-resolution images of living cells. Watching something as fundamental as a cell dividing, in real time, is truly fascinating. It reminds us how powerful and insightful optical imaging can be.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 15, "slide_filename": "Slide15.txt", "slide_annotation": "Lecture 23 - Slide15.txt", "file_path": "Lecture 23\\Texts\\Slide15.txt", "content": "Now, another important topic I want to mention is optical fiber.\u000bHere you can see how light travels through the fiber ‚Äî the structure has a core made of a material with a high refractive index, surrounded by a cladding with a lower refractive index. This design is intentional because it causes the light traveling inside the core to undergo total internal reflection. Every time the light hits the boundary between the core and cladding, it reflects back in, staying trapped inside the fiber.\n\nThe result is that the light ‚Äî and the energy it carries ‚Äî can travel a long distance without significant loss. So, you can think of the optical fiber as a very efficient light guide. It delivers light, or an optical signal, from one point to another with minimal attenuation, even over long distances and through curved paths. It‚Äôs a beautiful example of using the laws of physics ‚Äî in this case, total internal reflection ‚Äî for practical engineering.\n\nNow, imagine you place this kind of fiber inside the body, say along the heart wall or into a living tissue. The optical fiber can both transmit light and collect light that‚Äôs reflected or emitted back from the tissue. In the schematic below, you see a pair of fibers ‚Äî one for transmitting and one for receiving. The transmitted light excites or illuminates the target region, and the returning signal is collected and carried back through the receiving fiber.\n\nThis setup is the foundation for optical biopsy and miniaturized confocal microscopy. Using fiber optics, we can bring the principles of confocal microscopy directly inside a living organism ‚Äî even inside a human patient. That means we can perform microscopic imaging in vivo, observing tissues at the cellular level without the need for large or invasive instruments. So optical fibers not only deliver signals; they open the door to new types of medical imaging that can look inside the body in ways that were impossible before.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 16, "slide_filename": "Slide16.txt", "slide_annotation": "Lecture 23 - Slide16.txt", "file_path": "Lecture 23\\Texts\\Slide16.txt", "content": "Now, let‚Äôs talk about optical biopsy, which is another fascinating application of optical imaging.\u000bTraditionally, when we talk about a biopsy, we mean the physical process of removing a small piece of tissue from the body so it can be examined under a microscope. That‚Äôs what doctors do tens of millions of times every year ‚Äî they take a tissue sample, send it to the lab, and after several days or even weeks, you finally get the results. It‚Äôs a reliable method, but it‚Äôs slow, invasive, and can be uncomfortable for the patient.\n\nNow imagine if you could see those same microscopic structures instantly ‚Äî without ever cutting into the tissue. That‚Äôs the idea behind optical biopsy. Instead of using mechanical tools to remove a sample, we use light. You simply insert a tiny, flexible optical fiber, shine light onto the tissue, and collect the reflected or emitted signal. The information is transmitted back through the fiber and processed in real time.\n\nIn other words, you can obtain the same kind of diagnostic information that you would from a traditional biopsy ‚Äî but noninvasively, and in milliseconds rather than days or weeks. The optical fiber acts as both the light source and the detector, allowing you to see cellular and subcellular details immediately inside the living body.\n\nThis is truly an exciting step toward real-time, in vivo diagnosis. With optical biopsy, you can visualize tissue structure clearly and evaluate whether it‚Äôs normal or diseased right at the point of care. And as we‚Äôll see in the next part of this lecture, the next major development in this direction is optical coherence tomography (OCT) ‚Äî a powerful technique that extends this same concept of optical imaging into three-dimensional, high-resolution tissue visualization.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 17, "slide_filename": "Slide17.txt", "slide_annotation": "Lecture 23 - Slide17.txt", "file_path": "Lecture 23\\Texts\\Slide17.txt", "content": "Now, we‚Äôve just learned about ultrasound, and we‚Äôve also talked about confocal microscopy ‚Äî so let‚Äôs put them into context.\u000bAs you can see in this comparison plot, confocal microscopy gives us excellent resolution ‚Äî down to about one micron ‚Äî but the penetration depth of visible light is quite limited. Optical light simply can‚Äôt penetrate very deeply into tissue. For example, if I shine a light on my hand, you can‚Äôt see it clearly through the other side. Human tissue isn‚Äôt optically transparent, so the light is mostly scattered or absorbed before it can travel far.\n\nOn the other hand, ultrasound penetrates much deeper ‚Äî several centimeters, even up to tens of meters in other contexts. That‚Äôs why it‚Äôs such a powerful clinical imaging tool for viewing inside the body. But its resolution is much lower than that of optical methods ‚Äî typically in the range of hundreds of micrometers.\n\nSo, what about optical coherence tomography (OCT)? OCT sits right in between these two techniques. It offers resolution close to that of confocal microscopy ‚Äî on the order of a few micrometers ‚Äî but with a much greater imaging depth, typically a few millimeters. That‚Äôs what makes OCT so useful: it bridges the gap between high-resolution optical imaging and deep-penetrating ultrasound.\n\nThese three ‚Äî ultrasound, confocal microscopy, and OCT ‚Äî form the most clinically and biologically relevant imaging modalities for soft tissue. They‚Äôre not just theoretical; they‚Äôre widely used, well-established, and extremely powerful in both research and clinical applications.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 18, "slide_filename": "Slide18.txt", "slide_annotation": "Lecture 23 - Slide18.txt", "file_path": "Lecture 23\\Texts\\Slide18.txt", "content": "That‚Äôs why next, we‚Äôre going to focus on Optical Coherence Tomography, or OCT ‚Äî the green section in our outline.\u000bOCT is one of the most important and practically useful techniques in optical imaging. If you‚Äôve been following the flow of this lecture ‚Äî from optical microscopy to confocal microscopy ‚Äî OCT is the next logical step. The last two topics in this lecture are more research-oriented, but OCT is something you‚Äôll see everywhere, from biology labs to hospitals.\n\nSo what makes OCT different from the previous techniques? Well, if you think about it physically, traditional or confocal microscopy uses light to illuminate a sample and then collects the reflected or emitted light. But conceptually, it treats light as particles, or photons ‚Äî bouncing around, scattering, and being absorbed.\n\nIn OCT, we take a different approach. Here, we treat light as a wave ‚Äî a coherent electromagnetic wave, like a laser beam. This shift in perspective is key. Because once you treat light as a wave, you can use interference ‚Äî the way waves combine when they meet ‚Äî to extract incredibly fine details about the structure of the tissue.\n\nThis coherence-based approach allows OCT to perform precision measurements of distance and structure on the order of micrometers ‚Äî even smaller than the wavelength of light itself. So, if confocal microscopy gives us sharp 2D optical sections, OCT goes further. It lets us measure depth and reconstruct 3D images of tissue layers, much like an optical version of ultrasound.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 19, "slide_filename": "Slide19.txt", "slide_annotation": "Lecture 23 - Slide19.txt", "file_path": "Lecture 23\\Texts\\Slide19.txt", "content": "Alright, now let‚Äôs go back a little bit. This part is some trigonometry stuff, okay? So just follow me step by step. Suppose you have one wave ‚Äî I‚Äôll call it y one equals a cosine omega t. That‚Äôs your first wave. Then you have another wave, same amplitude, but traveling a slightly different optical path. That one is y two equals a cosine omega t plus phi.\nNow, this phi is the phase shift ‚Äî it tells us how much the second wave is delayed compared to the first one. When these two waves come back together and recombine, they won‚Äôt be perfectly in sync anymore. Because the optical paths are different, there‚Äôs a small phase difference between them.\n\nNow, when you add these two waves together ‚Äî that‚Äôs what happens inside the OCT interferometer ‚Äî the total wave is the sum of those two cosine terms. So, mathematically, y equals y one plus y two equals a cosine omega t plus a cosine omega t plus phi.\nNow, we can simplify that using a trigonometric identity. You might remember this one: cos A plus cos B equals two times cosine of A plus B divided by two, multiplied by cosine of A minus B divided by two. So if we apply that to our case, A is omega t, and B is omega t plus phi. Plugging that in, we get this: y equals two a cosine phi over two, times cosine omega t plus phi over two. Let me say that clearly ‚Äî y equals two a cosine phi over two times cosine omega t plus phi over two.\nNow look ‚Äî the amplitude of this combined wave depends on phi, the phase difference. That means as phi changes, the amplitude changes. So, a phase change directly changes the amplitude we measure.\n\nNow here‚Äôs another important point. In real optical systems, we can‚Äôt measure the electric field itself ‚Äî the light oscillates way too fast, hundreds of trillions of times per second. What we can measure is intensity. And intensity is proportional to the square of the amplitude.\nSo let‚Äôs take that amplitude ‚Äî two a cosine phi over two ‚Äî and square it. That gives us intensity proportional to two times a cosine phi over two squared. Or, in simpler words, I equals four a squared cosine squared phi over two. Let me repeat that slowly so you can hear it clearly ‚Äî I equals four a squared cosine squared phi over two.\nNow, a squared term is just the reference intensity. We usually call that I naught. So when we replace a squared with I naught, we get the final formula: I equals four I naught cosine squared phi over two. That‚Äôs the key relationship. I equals four I naught cosine squared phi over two.\n\nNow, let‚Äôs think about what that really means. Even a tiny change in phi ‚Äî a tiny phase shift ‚Äî can cause a big change in intensity because of that cosine squared term. If phi equals zero, meaning the two waves are perfectly in phase, the cosine of zero is one, and the intensity is maximum ‚Äî four times I naught. If phi equals one hundred eighty degrees, meaning the waves are half a wavelength out of phase, the cosine of ninety degrees is zero, and the intensity drops all the way to zero.\nSo, with just a small phase change ‚Äî half a wavelength ‚Äî the brightness goes from full to completely dark. That‚Äôs interference. That‚Äôs how powerful this relationship is. Now, remember, the wavelength of light is extremely small ‚Äî around four hundred to one thousand nanometers. That means this interference can detect changes in optical path length on the order of nanometers. That‚Äôs unbelievably precise.\n\nSo, the big idea here is that a small change in phase produces a measurable change in amplitude ‚Äî or in the brightness that we detect. That‚Äôs the fundamental principle of OCT. And this intensity, I, is modulated by cosine squared phi over two. So as phi changes ‚Äî from zero to three hundred sixty degrees ‚Äî the intensity goes from bright to dark and back again. That creates an interference fringe pattern.\n\nThose bright and dark fringes are what we analyze in OCT. Each fringe corresponds to a small change in path length, and by counting or analyzing those fringes, we can calculate the depth with extremely high accuracy ‚Äî down to micrometers or even nanometers.\n\nSo, remember this equation ‚Äî I equals four I naught cosine squared phi over two. That‚Äôs the one you want to keep in mind. It‚Äôs the bridge between phase change and amplitude change. And that‚Äôs the beauty of optical coherence ‚Äî we can‚Äôt see the phase directly, but we can measure the amplitude. The change in brightness tells us how the phase has shifted.\nThat‚Äôs why OCT is such a precise imaging technique. Two beams ‚Äî one reference, one sample ‚Äî recombine, interfere, and the resulting intensity pattern gives us incredibly fine depth information. So again, the simple rule: a small change in phase leads to a measurable change in amplitude. That‚Äôs the essence of optical coherence tomography.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 20, "slide_filename": "Slide20.txt", "slide_annotation": "Lecture 23 - Slide20.txt", "file_path": "Lecture 23\\Texts\\Slide20.txt", "content": "Now, I mentioned optical path several times, so let‚Äôs take a moment to understand what I actually mean by that. Suppose you have coherent light ‚Äî right light that is coherent means all the waves maintain a fixed phase relationship with one another. This light can include multiple colors or wavelengths ‚Äî red, orange, green, purple, and so on ‚Äî each with its own wavelength, lambda one, lambda two, lambda three, and so on. So, when you look at this broadband light, it‚Äôs really a mix of many slightly different frequencies that are all still coherent enough to interfere with each other.\n\nNow, imagine this light entering an interferometer setup. The light from the source first hits a beam splitter. This beam splitter is partially transparent, meaning part of the light passes straight through while the rest is reflected upward. That splits the beam into two optical paths. The first beam passes through the splitter, hits a fixed mirror ‚Äî which is a perfect reflector ‚Äî bounces back, passes through the beam splitter again, and goes down to the detector. That‚Äôs path one. The second beam, the reflected one, travels upward to a movable mirror, also a perfect reflector. It bounces back down, and because the beam splitter is semi-transparent, some of that returning light passes through to the same photodetector. That‚Äôs path two.\n\nSo, now you have two optical paths ‚Äî one fixed, one adjustable ‚Äî and the light from both paths recombines at the detector. If those two paths are the same length, the two beams arrive perfectly in phase, meaning their peaks and valleys line up. When that happens, they interfere constructively, and the total signal at the detector is at its maximum. But if you move that movable mirror by half a wavelength, the light in one arm travels a little farther, and the two waves arrive completely out of phase ‚Äî one is at a peak while the other is at a trough. They cancel each other out, and the detector sees zero intensity. So, by moving that mirror just a tiny bit ‚Äî even by half a wavelength ‚Äî you can make the signal go from bright to dark. This is the basic principle of interferometric measurement.\n\nMathematically, this is described by the formula we derived earlier, which follows that same ‚Äúone plus cosine‚Äù relationship. The cosine term represents the phase modulation, and the argument of that cosine depends on the difference in optical path length multiplied by the frequency of the light. That product ‚Äî the path difference times the frequency ‚Äî gives you the phase angle. So, for a single wavelength, say the red light, the signal can be written as I equals two I naught times one plus cosine of two pi delta L times nu. Now, if you use several wavelengths ‚Äî orange, green, purple ‚Äî each will have its own cosine term because each frequency is slightly different.\n\nSince light interference is a linear process, you can add all these contributions together. When you do that, you get a sum of cosine terms, each oscillating at slightly different frequencies. The overall signal, therefore, looks like a modulated waveform ‚Äî a burst of interference fringes that fade in and out. If you had only one wavelength, you‚Äôd see a clean, continuous sine wave pattern of bright and dark fringes. But when you combine multiple wavelengths, the slightly different frequencies cause the waves to occasionally line up and then drift out of phase again. This creates the clustered or ‚Äúenvelope-shaped‚Äù interference pattern we often see in broadband interferometry.\n\nSo, the total intensity can be written as gamma of delta L equals two times the sum of I naught i times cosine of two pi delta L times nu i, where the sum is taken over all wavelengths. Each component is modulated by its own cosine term, and when all are added together, they form this envelope pattern. When the mirror is positioned such that all the wavelengths line up, you get strong constructive interference. As you move the mirror further, the wavelengths gradually fall out of phase, and the interference fades away.\n\nThis is the essential idea behind interference of coherent or partially coherent light. Two optical paths ‚Äî one fixed and one movable ‚Äî produce an interference signal that varies with the optical path difference. For multiple wavelengths, the pattern becomes modulated and forms an interference envelope. And by analyzing that envelope, you can precisely determine the optical path length difference. This concept forms the foundation for optical coherence tomography, where we use these interference patterns to extract depth information with extremely high precision.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 21, "slide_filename": "Slide21.txt", "slide_annotation": "Lecture 23 - Slide21.txt", "file_path": "Lecture 23\\Texts\\Slide21.txt", "content": "So now, let‚Äôs think about what happens if we have not just a few components, but many, many wavelength components all mixed together ‚Äî just like a rainbow of light. Imagine several sine-wave components of different colors, each slightly different in frequency. When we add them together, the result no longer looks like a clean, single-frequency wave.\n\nInstead, you start to see an envelope pattern ‚Äî a burst that looks like a Gaussian-shaped profile, or what we often call a coherence envelope. The more components you add ‚Äî red, orange, green, blue, all slightly shifted ‚Äî the narrower and sharper that envelope becomes. This figure visually shows exactly that. With two components, you can still see the oscillations clearly. Add three components, and the interference gets tighter. Add seven components, and now the oscillations begin to cluster and fade away at the edges, forming a smooth, localized shape. \n\nSo, if you keep increasing the number of wavelengths, the interference pattern becomes confined in space ‚Äî like a short pulse ‚Äî and its overall shape starts resembling a Gaussian curve. That‚Äôs the idea behind partial coherence. The more wavelengths you combine, the shorter the coherence length becomes, and the interference fringes appear only within that small region.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 22, "slide_filename": "Slide22.txt", "slide_annotation": "Lecture 23 - Slide22.txt", "file_path": "Lecture 23\\Texts\\Slide22.txt", "content": "Now, if you keep adding more and more wavelength components ‚Äî so many that they form a continuous spectrum ‚Äî the math also changes. Instead of summing over discrete frequency components, we move into a limit where the summation becomes an integral. Each small intensity component contributes a little bit of energy within a narrow frequency band, and when you take the limit, the total becomes an integral over the full frequency range. \n\nMathematically, this looks like gamma of delta L equals two times I naught times the integral of S of nu times cosine of two pi delta L nu, d nu. Here, S of nu represents the power spectrum of the light source, describing how much energy each frequency contributes.\nNow, if this looks familiar, it should ‚Äî because this integral is nothing but a Fourier transform. You might remember that in a standard Fourier transform, we use an exponential kernel, e to the power of j two pi something, but since cosine is an even function, this is essentially the real part of that transform. So, using only the cosine kernel still gives us the same kind of frequency-to-space relationship. If you were to include sine terms too, you‚Äôd have the full complex exponential, but for our optical system, the cosine component is enough.\n\nThis means that what the interferometer is really doing ‚Äî physically ‚Äî is performing a Fourier transform of the source spectrum. The detected interference pattern is the cosine transform of the spectral density. So, when you move the mirror and record how the intensity changes, you‚Äôre effectively mapping out the coherence function of the light. When the optical path difference matches within the coherence length of the source, you get visible fringes. As soon as you move beyond that range, the interference fades away because the different frequency components go out of sync. That‚Äôs why interference fringes are only observed when the two optical paths are matched within that coherence length.\n\nNow, think about how we use that in imaging. As the movable mirror scans back and forth, you collect this varying signal over time. Some parts of the sample are closer, some are deeper, so their reflected signals arrive at slightly different delays. When you record and reconstruct all those depth-dependent interference signals, you essentially form an image ‚Äî one line of depth at a time. And by stacking those lines together, you can build a full cross-sectional image. That‚Äôs the basic principle of how optical coherence tomography captures structure in depth using interference.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 23, "slide_filename": "Slide23.txt", "slide_annotation": "Lecture 23 - Slide23.txt", "file_path": "Lecture 23\\Texts\\Slide23.txt", "content": "This same principle is used in many important applications. One of the most famous examples is the Michelson Interferometer, which was not just used in imaging but also in fundamental physics experiments. In fact, it played a crucial role in shaping modern physics. The Michelson setup is just what we described ‚Äî a beam splitter, a fixed mirror, and a movable mirror. Light travels along two paths, reflects back, and the interference pattern tells us how the path lengths differ. The optical path length difference is given by delta L equals two times the difference between L two and L one. The phase difference between the two beams is phi equals two pi delta L divided by lambda. And the detected intensity at the photodetector is given by I equals I one plus I two plus two times the square root of I one times I two times cosine phi.\n\nNow, if the two beams arrive in phase, meaning the optical path difference is an integer multiple of the wavelength, that gives constructive interference ‚Äî maximum brightness. Mathematically, that happens when two pi delta L divided by lambda equals two m pi, or simply delta L equals m lambda, where m is an integer ‚Äî zero, one, two, and so on. But if the path difference equals an odd multiple of half wavelengths ‚Äî like delta L equals m plus one-half times lambda ‚Äî then the waves cancel out, producing destructive interference, or darkness at the detector.\n\nWhat‚Äôs fascinating is that this same interferometer was used by Michelson and Morley in one of the most famous experiments in physics history. They wanted to test whether the speed of light changes depending on the motion of the Earth ‚Äî whether it moves through something called the ‚Äúether.‚Äù The experiment was performed at Case Western University, and at that time, people expected to see a shift in the interference pattern as the apparatus rotated, meaning that light would travel slightly faster in one direction than another. But no shift was observed. The result was completely null. That means the speed of light was constant in all directions, independent of the motion of the source or the observer.\n\nThat single observation changed the world of physics. It showed that light doesn‚Äôt behave the way classical mechanics predicted. A few years later, Albert Einstein took that result and built his theory of Special Relativity around it ‚Äî declaring the speed of light to be constant for all observers. So this simple optical instrument ‚Äî based on the same interference principles we‚Äôve been discussing ‚Äî essentially marked the birth of modern physics. The Michelson Interferometer not only explains how OCT works, but also represents the bridge between classical and modern physics. It‚Äôs a truly beautiful piece of science ‚Äî elegant, powerful, and revolutionary.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 24, "slide_filename": "Slide24.txt", "slide_annotation": "Lecture 23 - Slide24.txt", "file_path": "Lecture 23\\Texts\\Slide24.txt", "content": "Now let‚Äôs talk about the principle of OCT. It‚Äôs really the same idea as what we just discussed with the Michelson interferometer, but now applied to imaging. You can see here, we have a light beam that comes from a low-coherence light source ‚Äî so not a pure single wavelength, but a broadband light with many wavelengths mixed together. This beam first passes through a collimation lens to make the light travel in a parallel direction. Then it reaches the beam splitter, where the light is divided into two paths. One part of the beam goes upward toward the reference mirror, and the other part travels sideways toward the sample under test.\n\nThe light going to the reference mirror bounces back after reflection and returns to the beam splitter. Meanwhile, the light that goes toward the sample also gets reflected ‚Äî but from within the tissue itself. Think of this like sending the beam into the sample; it penetrates a little, hits various microstructures at different depths, and each layer reflects part of the light back. Those reflected signals travel back toward the beam splitter and are then combined with the reference beam.\n\nNow, depending on the phase difference between the reference signal and the reflected signal from the sample, the two waves interfere either constructively or destructively. This means that at each position ‚Äî at each depth ‚Äî the detector receives a slightly different intensity. You measure those variations point by point, and as the reference mirror scans along the axial direction, you record all the interference signals that come from different depths of the sample. Each point corresponds to a small reflection site inside the tissue.\n\nSo what happens next is that these interference signals are detected by the photodetector, digitized, and sent to a computer for processing. One depth scan gives you a one-dimensional line ‚Äî we call that an A-scan. When you move the beam laterally across the sample ‚Äî for example, in the x or y direction ‚Äî you collect many A-scans side by side, building up a full cross-sectional image. That‚Äôs the essential idea of optical coherence tomography ‚Äî measuring optical interference point by point, and reconstructing the internal microstructure of a sample in depth.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 25, "slide_filename": "Slide25.txt", "slide_annotation": "Lecture 23 - Slide25.txt", "file_path": "Lecture 23\\Texts\\Slide25.txt", "content": "Now, let‚Äôs look at one of the most common and important clinical uses of OCT ‚Äî eye imaging. OCT is widely used for retinal examination. Here, you can see a cross-sectional OCT image of a normal human retina. The bright layers represent strong reflections ‚Äî usually the interfaces between different tissue layers ‚Äî while the darker regions show areas where light penetrates deeper before scattering. You can see here, the scale bar indicates about 250 micrometers ‚Äî that‚Äôs just one-quarter of a millimeter ‚Äî so this technique provides incredibly fine resolution.\n\nEach layer of the retina ‚Äî from the nerve fiber layer at the top to the photoreceptor layer at the bottom ‚Äî has its own distinct optical properties. Because of this, the reflected interference pattern from each layer is slightly different. By analyzing these interference patterns, OCT can reveal the microstructure of the retina in exquisite detail.\n\nWhat I‚Äôve explained so far is the basic principle, but in practice, it‚Äôs a bit more sophisticated. You need laser scanning systems to move the beam precisely, and you use Fourier analysis to extract the depth information from the interference signals. By doing this repeatedly and precisely, you can reconstruct full three-dimensional images of the retina. OCT has become a standard tool in ophthalmology, helping doctors detect and monitor diseases like glaucoma, macular degeneration, and diabetic retinopathy.\n\nSo, from the same principle we learned earlier ‚Äî optical interference ‚Äî you can create a detailed, noninvasive image of the living eye. And not only that, this same technology can be adapted for other parts of the body. For example, you can even make a small optical biopsy probe or a catheter to use inside other organs.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 26, "slide_filename": "Slide26.txt", "slide_annotation": "Lecture 23 - Slide26.txt", "file_path": "Lecture 23\\Texts\\Slide26.txt", "content": "Now let‚Äôs take that idea one step further. Suppose instead of imaging the eye, we use a tiny flexible optical fiber ‚Äî the same kind of fiber used in telecommunications ‚Äî and we put it inside the body. That‚Äôs exactly what‚Äôs done in intravascular OCT for cardiac studies. A miniature OCT probe is built right into a catheter ‚Äî that‚Äôs the small tube used in heart procedures. This fiber can be guided through the blood vessels, all the way to the coronary arteries.\n\nOnce the probe is in place, OCT can scan the inside of the blood vessel, creating high-resolution images of the artery wall. With this, doctors can see if there‚Äôs a buildup of plaque, a blood clot, or any kind of blockage. They can even distinguish between different types of plaque ‚Äî for example, whether it‚Äôs hard and calcified or soft and lipid-rich. This distinction is extremely important because soft, unstable plaques are the ones most likely to rupture and cause a heart attack.\n\nSo, using OCT, clinicians can visualize the inside of the heart‚Äôs arteries in real time, with microscopic precision ‚Äî something that traditional imaging methods like X-ray angiography can‚Äôt show. It‚Äôs minimally invasive, it‚Äôs fast, and it provides information that‚Äôs both structural and functional.\n\nYou can think of this as doing an ‚Äúoptical biopsy‚Äù inside the blood vessel ‚Äî without removing any tissue. This is why OCT has become such a powerful tool, not only in ophthalmology but also in cardiology and other clinical applications. It‚Äôs an elegant combination of physics, optics, and medicine ‚Äî a simple idea, but a very powerful and precise technology that continues to transform how we see inside the human body.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 27, "slide_filename": "Slide27.txt", "slide_annotation": "Lecture 23 - Slide27.txt", "file_path": "Lecture 23\\Texts\\Slide27.txt", "content": "Okay, how are we doing on time? We‚Äôve wrapped up the second part, and now we‚Äôll continue into the third and fourth parts. In the first part I walked you through the key ideas. We talked about optical microscopy and the confocal idea ‚Äî a wonderful idea. No Nobel Prize there, unlike OCT, but still pretty amazing. Then we looked at how interference lets you do imaging ‚Äî not only optical coherence tomography, but interferometric imaging in general. The principle itself is profound, even when there isn‚Äôt a Nobel attached to it.\n\nYou may have read the news about gravitational wave detection. How do they detect those waves? The waves create tiny vibrations ‚Äî unbelievably small ‚Äî that you would think are impossible to measure. The trick is interferometry. Two long arms, miles long, two optical paths that pick up a minute difference, and that difference is magnified by interference. There are setups in the United States and Europe, and even programs in China. Some people are pushing to use satellites to make the arms even longer. Interferometry is incredibly helpful whenever you have wave behavior and you want extremely precise measurements.\n\nSo the two ideas ‚Äî confocal microscopy and optical coherence tomography ‚Äî sit inside this larger family of interferometric imaging. It‚Äôs not only optical; X-rays and even gravitational wave observatories use the same core idea. In the end, it all goes back to adding waves: cosine omega t plus cosine omega t plus phi. That simple concept has powered multiple Nobel-level advances. Now, let‚Äôs turn to something different: diffuse optical imaging. You may not know these methods yet, but you will.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 28, "slide_filename": "Slide28.txt", "slide_annotation": "Lecture 23 - Slide28.txt", "file_path": "Lecture 23\\Texts\\Slide28.txt", "content": "Light diffusion is the heart of the third part. These third and fourth parts aren‚Äôt in common clinical use yet, but in research settings, they‚Äôre very active. With X-ray imaging, rays go essentially straight. Think of that purple straight line: the signal is a line integral along the path. Everything contributes additively, and you can explain it with the linear attenuation coefficient along that ray. Any change in the measurement must come from features located along that ray ‚Äî that‚Äôs a strong localization claim, and it‚Äôs very clear.\n\nOptical imaging is different. Biological tissue scatters light strongly. I showed you that little cartoon of scattering ‚Äî Rayleigh at small scales, Mie at larger ones ‚Äî and you‚Äôve all seen this in real life. In a dark room, shine a bright laser pointer and you see a diffuse glow. Put the laser behind your finger: can you see details behind the finger? No. With X-ray, the projection already looks like a picture. With optical light, even if you send a parallel laser beam, what you see is a cloudy smear. And yet, diffuse optical imaging says, despite that strong scattering, we still want to reconstruct an image.\n\nSo put it in simple words: we want to make a good image behind the finger. That sounds almost impossible, and it is hard. That‚Äôs why diffuse optical imaging is not as practical as X-ray and some other modalities. But optical interactions carry rich biological information, so we study them. Our job as engineers is to turn ‚Äúimpossible‚Äù into ‚Äúfeasible.‚Äù That‚Äôs the challenge ‚Äî and that‚Äôs why we dig into diffusion models and inverse problems to see what can be recovered.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 29, "slide_filename": "Slide29.txt", "slide_annotation": "Lecture 23 - Slide29.txt", "file_path": "Lecture 23\\Texts\\Slide29.txt", "content": "First step: be quantitative about how light propagates in tissue that is highly and strongly scattering. We decompose a piece of heterogeneous tissue into many small elements ‚Äî in three dimensions, these are typically tetrahedra ‚Äî and we assume each small element is uniform. Then we launch photons into the mesh and watch what happens.\n\nWhen a photon hits a boundary where optical properties change, it can reflect or transmit, with angles set by the physics. Once inside a region, it travels some distance, then scatters; all of these events are probabilistic. We use random number generators ‚Äî this is the dice-rolling part ‚Äî to decide whether a step reflects or transmits, how far the photon goes before the next interaction, and in what direction it scatters. We trace one photon step by step: maybe it reflects, maybe it refracts, maybe it travels, scatters, and then gets absorbed ‚Äî good news and bad news, if it‚Äôs absorbed, you‚Äôre done with that photon. Or it might exit the tissue and reach a detector.\n\nThat‚Äôs the story for a single photon. Then you launch another photon. And another. You keep going ‚Äî millions, even billions of photons ‚Äî to build up statistics. This Monte Carlo approach lets you model refraction, reflection, scattering, absorption, and escape to the detector, all according to the measured or assumed light‚Äìtissue interaction properties. From those simulated detections, you can predict measurements and eventually tackle the inverse problem of forming an image in highly scattering tissue.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 30, "slide_filename": "Slide30.txt", "slide_annotation": "Lecture 23 - Slide30.txt", "file_path": "Lecture 23\\Texts\\Slide30.txt", "content": "So once you run that Monte Carlo simulation, you can actually form a picture like this. In our lab, we once developed one of the fastest numerical simulators of its kind ‚Äî it‚Äôs called a tetrahedron-based, inhomogeneous Monte Carlo optical simulator. Quite a mouthful, right? But what it really means is that we used a mesh made of small tetrahedral elements to simulate photon transport in realistic biological tissue.\n\nHere, you see a simulated mouse model. It‚Äôs not just a cartoon ‚Äî it‚Äôs a detailed anatomical mesh. You can see the heart, lungs, liver, stomach, spleen, and kidneys, all represented as separate organs with different optical properties. We place a small bioluminescent light source ‚Äî say, something glowing inside the mouse ‚Äî and then we let it emit photons. Those photons scatter through the tissue, and our simulator tracks where they go and how much light reaches the surface.\n\nOn the right, you can see the simulated fluence maps, color-coded to show photon density. The color bar on the side tells you how strong the signal is ‚Äî from deep blue, meaning very few photons, up through green, yellow, and bright red, meaning a high photon density. It‚Äôs quite beautiful when you think about it: millions of photons bouncing, scattering, being absorbed ‚Äî and yet out of all that chaos, you can reconstruct where the light came from inside the body.\n\nThis simulator was developed in our group, and I‚Äôm proud to say this work has become foundational. In fact, my former student, Qing-Sung, who worked on this, recently got a fantastic offer from Google to continue research in machine learning. So you can see how the skills we learn here ‚Äî simulation, modeling, and data interpretation ‚Äî connect to cutting-edge fields. It‚Äôs not just physics, it‚Äôs computation, it‚Äôs innovation. This is the kind of work that combines optical science, biology, and computing into something truly powerful.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 31, "slide_filename": "Slide31.txt", "slide_annotation": "Lecture 23 - Slide31.txt", "file_path": "Lecture 23\\Texts\\Slide31.txt", "content": "Now let‚Äôs talk about what happens at a quantitative level ‚Äî how photons actually move through scattering media like tissue. When light enters tissue, it doesn‚Äôt travel far before it gets scattered, because the medium is highly scattering. Statistically, we describe this using something called the mean free path. The mean free path is the average distance a photon travels before being scattered. For biological tissue, that‚Äôs typically on the order of one-tenth of a millimeter ‚Äî a very short distance.\n\nBelow that distance, we say the photons are in the ballistic region ‚Äî they travel almost straight, like X-rays, without scattering much. But once they start scattering, things get complicated. The direction changes slightly with each scattering event, and after several scatterings, the photon is completely randomized.\n\nThat‚Äôs why X-ray imaging works so well ‚Äî the X-rays go straight, so you know exactly which direction they came from. But in optical imaging, photons quickly lose that directional information. You detect a photon, but you can‚Äôt tell whether it came directly from your target or if it was scattered off several other structures first. It‚Äôs like hearing an echo in a cave ‚Äî you know sound reached you, but you don‚Äôt know exactly from where.\n\nTo describe this loss of directionality, we use another parameter called the transport mean free path. After traveling one transport mean free path, the photon‚Äôs direction has essentially been randomized ‚Äî it has lost all memory of where it came from. In typical biological tissue, this distance is about one millimeter.\n\nThat means that within the first millimeter, you can still recover some useful information ‚Äî that‚Äôs where diffuse optical imaging and near-infrared spectroscopy operate. But beyond that depth, it becomes extremely challenging. You lose the ability to form high-resolution images because photons have wandered too much. It‚Äôs not totally impossible, but it‚Äôs very hard. That‚Äôs one of the fundamental physical limits of optical imaging.\n\nSo to summarize: photons in tissue move only about a tenth of a millimeter before scattering, and after about a millimeter, their paths are completely randomized. That‚Äôs why optical imaging is powerful for shallow structures ‚Äî like the cortex, or the retina ‚Äî but much more difficult for deep tissue imaging. Later, we‚Äôll see how researchers combine optics with X-rays to overcome that limitation.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 32, "slide_filename": "Slide32.txt", "slide_annotation": "Lecture 23 - Slide32.txt", "file_path": "Lecture 23\\Texts\\Slide32.txt", "content": "Now let‚Äôs move to another branch ‚Äî diffuse optical spectroscopy, or DOS. And yes, I know what some of you are thinking ‚Äî DOS sounds like the old computer operating system. If you remember that, you‚Äôre not that young! But in this case, DOS stands for something different ‚Äî diffuse optical spectroscopy.\nHere, we‚Äôre not trying to make an image. Instead, we‚Äôre doing spectral analysis ‚Äî we shine light in, we collect what comes out, and we analyze the overall absorption and scattering to learn about the tissue‚Äôs composition.\n\nImagine my finger here. If I shine a broadband light ‚Äî that means light with many colors ‚Äî through my finger, different wavelengths will be absorbed differently. Blue light, for instance, is absorbed more strongly than red, which is why your finger looks red when you shine a flashlight through it.\n\nNow, by measuring how much light gets through at two or more wavelengths, we can solve for unknowns in the tissue. Usually, the main absorber in tissue is blood, and in blood, the key molecules are hemoglobin and oxyhemoglobin ‚Äî that‚Äôs hemoglobin with oxygen bound to it. The ratio between those two forms gives us oxygen saturation, which is one of the most important physiological parameters for life.\n\nMathematically, it‚Äôs quite straightforward. The transmitted intensity follows Beer‚Äôs law ‚Äî I equals I naught times e to the power of minus mu-a times L, where mu-a is the absorption coefficient and L is the path length. The absorption coefficient depends on how much hemoglobin and oxyhemoglobin are present and how strongly each absorbs light at that wavelength. By measuring intensity at two wavelengths, we can solve for two unknowns ‚Äî the concentrations of Hb and HbO‚ÇÇ.\n\nIn fact, this is exactly the principle used in a pulse oximeter ‚Äî that little clip you put on your finger in the hospital. It shines red and infrared light through your finger, measures how much of each gets through, and calculates oxygen saturation using this model.\n\nSo, this is diffuse optical spectroscopy. It doesn‚Äôt form an image, but it gives us valuable quantitative information ‚Äî blood oxygen levels, water content, and even tissue metabolism. In a sense, it‚Äôs the simplest and most practical use of optical interaction with tissue ‚Äî turning light absorption into a window on physiology. And the best part? It‚Äôs completely noninvasive ‚Äî just light in, light out, and a bit of math in between.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 33, "slide_filename": "Slide33.txt", "slide_annotation": "Lecture 23 - Slide33.txt", "file_path": "Lecture 23\\Texts\\Slide33.txt", "content": "Alright, now the second idea here is what I call the tiredly idea ‚Äî it‚Äôs really a continuation of the first part. We want to do tomography. We really want to do tomography ‚Äî just like we do X-ray tomography. So, let‚Äôs think about how we actually do X-ray or gamma-ray tomography.\nIn those systems, you send a pencil beam ‚Äî a narrow, collimated beam ‚Äî straight through the object. What you measure is basically a line integral. For nuclear imaging, it‚Äôs sometimes a quasi-line integral because the photons are affected by an attenuation kernel, but fundamentally, the information you get comes from along that single line.\n\nSo, you get one line-integral measurement, and that becomes one linear system equation. In that equation, all the unknowns ‚Äî those pixel or voxel values ‚Äî are weighted according to how much of the beam passes through each small region. That weighting is the fraction of the path that cuts through each voxel. So, mathematically, it‚Äôs like this: one unknown times its weighting factor, plus another unknown times its weighting factor, plus another, and so on ‚Äî all along that beam line. That gives you one linear equation.\nNow you can see how this builds up. You have many, many linear equations ‚Äî one for each ray, one for each angle. The weighting factors are all known because they come from the imaging geometry ‚Äî the way you arrange your source and detector. The only unknowns are all those mu values ‚Äî the attenuation coefficients inside each pixel or voxel ‚Äî which we call mu one, mu two, mu three, and so on.\n\nNow, how does this relate to the Beer‚ÄìLambert law? Let‚Äôs go back to that. You have an incoming intensity, called I naught, and you measure an outgoing intensity, I. According to Beer‚Äôs law, the transmitted intensity equals the input intensity multiplied by e to the power of negative mu times delta, where mu is the attenuation coefficient, and delta is the distance through that small region.\nSo, suppose the beam passes through one region with attenuation mu one and thickness delta one, then into a second region with mu two and thickness delta two, and so on. After the first region, the output of that layer becomes the input for the next. Multiply all those together, and inside the exponential, you get the total, negative the sum of mu times delta over all the layers.\n\nSo mathematically, we say I equals I naught times e to the power of minus the sum of mu times delta. That‚Äôs what we measure. Now, if we take the natural logarithm of both sides, it becomes linear. Log of I naught over I equals the sum of mu times delta. That‚Äôs the key point: each measurement gives you one linear equation in the unknown mu values.\nAll the geometry ‚Äî those delta distances ‚Äî are known, and the intensities I and I naught are measured. So that‚Äôs how we form the reconstruction problem for tomography.\nNow, this all works nicely for X-ray imaging, where photons travel in straight lines. But in optical tomography, things are very different. When we send optical light into biological tissue, we still have all those local attenuation coefficients, all those mu‚Äôs ‚Äî but the light doesn‚Äôt travel straight. It scatters everywhere. So we can‚Äôt just send one clean laser beam, measure on the other side, and call that a line integral. The light spreads out in all directions, taking many different paths.\n\nEach measurement still has weighting factors, but now those weighting factors are far more complicated because the light is diffusing rather than traveling straight. And when we add up all those equations, they still look linear in the unknown mu‚Äôs, but all the equations start to look very similar to one another. That‚Äôs the main problem.\nIt‚Äôs like this: suppose you have three unknowns ‚Äî x, y, and z. And all your equations look like x plus y plus z equals one. Then the next one says x plus y plus z equals one point zero zero one. Then another says x plus y plus z equals zero point nine nine nine. You have a lot of measurements, yes, but they‚Äôre almost identical. You can‚Äôt solve for x, y, and z because the equations don‚Äôt give you independent information.\n\nIn tomography, we call that an ill-posed problem. You have a lot of data, but not enough unique or independent information to get a stable solution.\nSo the analogy here is that in optical tomography, all the light measurements look very similar because scattering smears out the information. Each detector sees a blurred, mixed-up version of everything inside. The equations are still linear ‚Äî yes, still linear ‚Äî but they‚Äôre all nearly the same, so it becomes very difficult to separate one unknown from another.\nAnd that‚Äôs the essence of the challenge. Too much diffusion means too little unique information. That‚Äôs why optical tomography is so difficult. It‚Äôs mathematically ill-posed, physically diffusive, and highly sensitive to noise. The principle is beautiful, but the reconstruction is tricky.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 34, "slide_filename": "Slide34.txt", "slide_annotation": "Lecture 23 - Slide34.txt", "file_path": "Lecture 23\\Texts\\Slide34.txt", "content": "But researchers don‚Äôt give up easily, right? They keep trying. And they‚Äôve managed to make progress using infrared light, because infrared wavelengths are longer and scatter less than visible light. That gives you better penetration into tissue. So you can still get a measurable signal, even from deeper layers. And then, using some clever mathematical tricks, you can try to recover the internal absorption and scattering distribution ‚Äî the actual optical properties inside the tissue.\n\nNow, think about how useful that could be. Suppose you have a tumor. In that region, the absorption and scattering characteristics will be different from the surrounding healthy tissue. Water content, fat content, and hemoglobin concentration ‚Äî all these change when tissue becomes cancerous. For example, cancerous tissue tends to have more hemoglobin, because tumors grow new blood vessels to feed themselves ‚Äî we call that angiogenesis. So, in optical terms, the tumor region is ‚Äúhemoglobin-rich.‚Äù\n\nSo, people have written many papers using this principle. Under certain experimental conditions, the results do make sense ‚Äî the reconstructed maps of hemoglobin concentration and oxygenation often agree with what‚Äôs seen in pathology. You might see a region with higher hemoglobin absorption ‚Äî that‚Äôs where the tumor is. It‚Äôs exciting work. But the question remains: has it reached clinical use? Not really ‚Äî at least, not yet.\n\nAfter many years of research, diffuse optical tomography is still not mature. The principle is solid, the physics is beautiful, but the system is what we call ill-posed ‚Äî the solutions are unstable, sensitive to noise, and dependent on assumptions. It‚Äôs just not as robust or reliable as X-ray imaging.\nAnd that‚Äôs an important comparison. We‚Äôre all very proud of our X-ray imaging technology. Every hospital depends on it ‚Äî CT, fluoroscopy, mammography ‚Äî they‚Äôre all part of daily clinical life. If you took X-ray imaging away from a hospital, they‚Äôd practically have to close their doors! It‚Äôs that critical. So when you ask a doctor, ‚ÄúIf you could only keep one imaging modality in your clinic, which would it be?‚Äù ‚Äî none of them will pick optical imaging. \n\nThey‚Äôll all say, ‚ÄúWe‚Äôll keep X-ray.‚Äù That‚Äôs the truth.\nSo, yes ‚Äî optical tomography is very interesting. It‚Äôs innovative, and it provides functional and molecular information that X-rays can‚Äôt. But it‚Äôs still a research tool. X-rays, on the other hand, are robust, quantitative, and universally trusted in clinical medicine. That‚Äôs where we stand today.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 35, "slide_filename": "Slide35.txt", "slide_annotation": "Lecture 23 - Slide35.txt", "file_path": "Lecture 23\\Texts\\Slide35.txt", "content": "So anyway, one way to improve the localization capability ‚Äî meaning how precisely we can tell where the light comes from ‚Äî is through something called temporal gating. The idea is very clever. You see, when photons travel through tissue, they go all over the place ‚Äî scattering in every direction. But if we can measure when each photon arrives, and only collect the ones that arrive early, we can get a much cleaner signal.\n\nFor optical signals, the temporal resolution ‚Äî that is, the time resolution ‚Äî of modern detectors is extremely good. So we can actually do this kind of time gating. Temporal gating simply means we only collect the photons that arrive first, before they have time to bounce around and scatter too much.\nIn other words, those early-arriving photons are the ones that have traveled almost straight through, or maybe with just a few scatterings ‚Äî not too many. So by keeping only those early photons, we‚Äôre kind of reconstructing a straighter path through the tissue. This gives us a better localized image and sharper information. That‚Äôs the essence of temporal gating ‚Äî a brilliant idea to improve spatial resolution in optical imaging.\n\nNow, you can think of this as a kind of collimation mechanism. Earlier, we talked about mechanical collimation, where you physically restrict the beam using a tube ‚Äî that‚Äôs what we use in X-ray systems. I jokingly said, It‚Äôs my 'technical way' of collimation ‚Äî you just put a tube in front of the detector so only straight rays go through.\nAnd then we also mentioned electronic collimation, which you might remember from the pattern-imaging systems or coincidence detection in nuclear imaging. There, you don‚Äôt use a physical tube ‚Äî you use timing electronics, or coincidence circuits, to select only the photons coming from the right direction.\nNow here, temporal gating plays a similar role ‚Äî but for light. Instead of a physical or electronic filter, we use time as the filter. We only accept photons that arrive early enough, meaning those that haven‚Äôt wandered too much. That‚Äôs why I call it a ‚Äútemporal collimation‚Äù mechanism.\n\nAnd remember, I also mentioned, a while ago, that you can do something similar with polarized radiation? In that case, we used a magnetic collimation trick ‚Äî flipping the spin of particles back and forth to control their direction, kind of ‚Äúcheating‚Äù to get the same effect.\n\nAnyway, the big picture is this: optical imaging is not easy. It‚Äôs still a very active research topic. Even now ‚Äî well, I think maybe early this year, or perhaps late last year ‚Äî my group published a paper about improving optical tomography using advanced computational techniques. So we‚Äôre making progress. But I must say, it‚Äôs still far from routine clinical use. It‚Äôs fascinating physics and engineering, but not yet something you‚Äôd see in a hospital every day.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 36, "slide_filename": "Slide36.txt", "slide_annotation": "Lecture 23 - Slide36.txt", "file_path": "Lecture 23\\Texts\\Slide36.txt", "content": "Now, let‚Äôs move on and talk about fluorescence molecular tomography ‚Äî often abbreviated as FMT. Fluorescence and bioluminescence imaging are both very important tools, because they allow us to use fluorescent or luminescent probes to label biological molecules ‚Äî like proteins, genes, or even drugs ‚Äî so we can track them in real time.\nMost of the time, we use small animal models for this. Small animals ‚Äî mice, rats ‚Äî are used to model almost all kinds of human diseases. We don‚Äôt want to experiment directly on humans, of course, so we test our ideas on these animals first. We give them certain diseases ‚Äî cancer, bone degeneration, inflammation, all sorts of things ‚Äî and then we test how treatments work.\n\nOnce the biological principles look promising, we test drugs on these animal models to see if they behave as expected before we move on to human trials.\nSo, for example, suppose we label cancer cells with a red fluorescent protein. Then we label a drug molecule ‚Äî maybe an antibody or some therapeutic compound ‚Äî with a green fluorescent marker. Now, if the green-labeled drug binds to the red-labeled cancer cells, that means the drug has successfully targeted the tumor. In that case, the red signal would decrease or disappear, and we could monitor that process in vivo, meaning inside the living animal, in real time.\n\nThat‚Äôs the beauty of fluorescence molecular tomography ‚Äî we can visualize biological processes directly, without opening the body.\nNow, if you look at the illustration here, you see the setup. The animal is standing upright, which is not really a physiological position ‚Äî ideally, the animal should be lying down to minimize stress. But this figure is just to show the principle of the system.\n\nInside the animal, suppose both the cancer cells and the drug molecules are labeled with different fluorescent proteins ‚Äî maybe blue, green, yellow, or red, depending on what we want to track. At first, you might not see any fluorescence signal on the surface. But then, we shine in a femtosecond laser beam ‚Äî that‚Äôs an extremely short, high-intensity pulse of light ‚Äî at a specific wavelength. That excites the fluorescent molecules inside the body.\n\nThose molecules then emit light at longer wavelengths ‚Äî that‚Äôs the fluorescence emission. We collect those emission signals using an optical detector, usually a highly sensitive ICCD camera ‚Äî that‚Äôs an intensified charge-coupled device.\nNow, the emission filter plays a very important role. It blocks the excitation light ‚Äî the laser light that we sent in ‚Äî and only allows the emitted fluorescent light to pass through. This way, we measure only the signal that comes from the fluorescence, not the original illumination beam.\nSo, in the end, you get a set of projection views ‚Äî one from each rotation angle ‚Äî just like a CT scan. You rotate the animal, collect data from multiple views, and reconstruct a 3D distribution of fluorescent activity inside the body.\n\nOf course, it‚Äôs not as straightforward as CT, because light in tissue is highly scattered, so the reconstruction problem is much harder ‚Äî not very stable, not very unique, and sensitive to noise. But still, you can get useful information by combining advanced algorithms and multiple measurements.\nSo this system ‚Äî using a femtosecond laser, a rotating stage, and an ICCD detector ‚Äî is an example of fluorescence molecular tomography in small animals. It‚Äôs a powerful tool for biological and pharmaceutical research, allowing us to visualize how drugs move, how genes express, and how diseases progress ‚Äî all in a living system.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 37, "slide_filename": "Slide37.txt", "slide_annotation": "Lecture 23 - Slide37.txt", "file_path": "Lecture 23\\Texts\\Slide37.txt", "content": "Alright, so here we can see an example of what‚Äôs called FMT reconstruction, meaning fluorescence molecular tomography reconstruction. You can see the image of the animal here ‚Äî this is usually a mouse, and we‚Äôve done the imaging after injecting fluorescent probes. So, you collect the signals from different views, and then you reconstruct a 3D distribution of those fluorescent signals inside the body.\n\nNow, some research groups ‚Äî and I‚Äôve seen quite a few papers like this ‚Äî claim that they can do very high-quality reconstruction. And yes, to a certain degree, they can. These results look pretty good, and they do show useful biological information. But, to be honest, the results are still not very reliable yet. The reconstruction quality is limited by noise, by the scattering of light, and by how well the model matches the real tissue.\n\nIf you look at this picture, you can see the 3D reconstructed image with color bars on the right side. The colors correspond to different levels of fluorescent intensity. So, when you look at the image, you get a sense of where the fluorescent probe has accumulated inside the animal‚Äôs body. This helps identify regions of biological activity ‚Äî for example, where a particular enzyme is expressed, or where a drug is localized.\nThis example shows a three-dimensional reconstruction of fluorescence molecular tomography ‚Äî FMT. So, yes, it‚Äôs a practical imaging modality, and it really works, but it‚Äôs not for human use. There are a few reasons for that.\n\nFirst, most of the fluorescent probes used in small-animal imaging are not safe for human use ‚Äî they‚Äôre either toxic or not approved for clinical injection. Second, there‚Äôs a size issue ‚Äî humans are much bigger than mice, and optical light simply cannot penetrate that deeply into human tissue. The penetration depth of laser light is very limited.\nSo, for now, FMT is mainly a preclinical imaging technique ‚Äî something that‚Äôs used in research, mostly for small animals. It‚Äôs great for drug development, molecular biology, and early disease studies, but not yet practical for clinical imaging in humans.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 38, "slide_filename": "Slide38.txt", "slide_annotation": "Lecture 23 - Slide38.txt", "file_path": "Lecture 23\\Texts\\Slide38.txt", "content": "Now, before I joined RPI, I was actually on the faculty at Virginia Tech and Wake Forest University, both at the medical school and the engineering school. I had laboratories on both campuses. And one of the major projects we worked on ‚Äî a multi-million-dollar project ‚Äî was about using optical molecular tomography to monitor blood vessel growth.\nLet me explain the motivation behind it. When someone has cardiovascular disease, part of a blood vessel can get blocked ‚Äî completely closed off by plaque buildup. Right now, the standard treatment is to insert a stent to reopen the vessel and restore blood flow. But as you know, stents come with their own problems ‚Äî they can cause inflammation, thrombosis, and sometimes even re-narrowing of the artery over time.\n\nSo, the future solution that people are exploring is regenerative medicine ‚Äî to actually grow a new biological blood vessel from your own stem cells. The idea is that we can take a small segment of your blood vessel, or even some of your cells, and grow a new vessel in the lab ‚Äî a fully biological, living blood vessel that can eventually be implanted back into your body.\n\nTo do this, we use something called a bioreactor ‚Äî a special device that maintains controlled conditions for growing tissue. Inside the bioreactor, you have this new blood vessel growing while nutrient-rich liquid continuously circulates it, simulating what happens inside the body.\nNow, the question is ‚Äî how do you monitor this growth process without destroying the tissue? That‚Äôs where our imaging technology comes in.\nWe use OCT, or Optical Coherence Tomography, to capture the structural information ‚Äî basically, the microscopic anatomy of the growing vessel. You can actually see the layers, the texture, and the thickness of the vessel wall.\n\nThen we combine that with fluorescence imaging to observe what‚Äôs happening at the cellular and molecular level. For example, using fluorescence probes, we can track whether endothelial cells ‚Äî the cells that form the inner lining of blood vessels ‚Äî are growing properly. That‚Äôs critical because without a strong endothelial layer, the vessel won‚Äôt be functional or stable.\nSo, in our project, we integrated all these optical methods ‚Äî OCT for structure, diffuse optical tomography for optical property recovery, and fluorescence tomography for molecular activity ‚Äî all working together.\n\nThe advantage is that once you know both the structure and the optical properties of the tissue, you can use that information as a prior to improve the accuracy of fluorescence tomography. That means the reconstruction becomes more stable, more accurate, and better localized.\nAnd since these vessels are much smaller than an entire small animal ‚Äî just a few millimeters in diameter ‚Äî the light can penetrate through them more easily. So we can actually achieve better resolution and depth information.\n\nThis whole system, combining multi-probe and multi-modal optical molecular tomography, was designed for regenerative medicine ‚Äî specifically, to visualize and monitor bioengineered blood vessels both in the bioreactor and after implantation into living animals.\nSo that‚Äôs what we did a number of years ago ‚Äî a collaboration between my lab and my colleague, Dr. Shay Soker. It was supported by the NIH R01 BRP grant HL098912, running from 2010 to 2014. And that‚Äôs a really good example of how optical molecular imaging can move beyond small-animal studies toward real applications in regenerative medicine ‚Äî imaging living tissue as it grows, heals, and integrates into the body.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 39, "slide_filename": "Slide39.txt", "slide_annotation": "Lecture 23 - Slide39.txt", "file_path": "Lecture 23\\Texts\\Slide39.txt", "content": "Now, this next imaging modality is what we call bioluminescence tomography. It‚Äôs one of the most fascinating ‚Äî but also one of the most challenging ‚Äî tomography problems we can deal with.\nLet me explain why.\nIn fluorescence tomography, you can send in a laser beam ‚Äî you control where it goes, you can illuminate the tissue from different directions, and that makes it what we call an active imaging modality. You actively excite the fluorescent molecules, and they emit light back, which you can detect and reconstruct into an image.\n\nBut bioluminescence tomography is totally different. Here, the light comes from inside the animal itself ‚Äî not from any external laser. You don‚Äôt get to shine a laser and choose where to excite. The light is produced biologically, inside the tissue, by a chemical reaction, typically involving something like luciferase, the same enzyme that makes a firefly glow.\nSo, imagine you genetically modify an animal ‚Äî usually a mouse ‚Äî to express this bioluminescent probe inside its cells. The animal literally becomes like a little living firefly. When the probe reacts inside its body, it emits light that escapes through the tissue, and you can capture that light on the surface.\n\nThat‚Äôs beautiful to watch ‚Äî I still remember when we did this experiment back at the University of Iowa. We worked in a completely dark room, and when the mice began to glow faintly, you could actually see the light coming through their skin. It was really amazing.\nBut scientifically, this is very challenging, because you can‚Äôt control the direction of illumination ‚Äî there is no external laser input. All you can do is measure the light that escapes to the surface. That makes it a purely passive imaging problem, and therefore much harder than fluorescence tomography.\nSo, the question is: can we reconstruct where the bioluminescent light is coming from inside the animal, based only on what we observe on the surface? That‚Äôs the big challenge of bioluminescence tomography.\n\nWhen I was at Iowa, we actually got a multi-million-dollar NIH grant to tackle exactly this problem. My idea was to use a multi-modality approach ‚Äî to combine optical imaging with micro-CT or micro-MRI so we could obtain both the anatomical structure and the optical measurements together.\nHere‚Äôs how it works. First, we perform a micro-CT or micro-MRI scan of the animal. That gives us very detailed anatomical information ‚Äî we can segment the bones, the organs, the tissue layers. We then take that structure and build a finite element mesh, so now we have a 3D computational model of the animal‚Äôs anatomy.\n\nNext, we perform optical measurements ‚Äî diffuse optical tomography ‚Äî to estimate the optical properties of the tissues, like how much they scatter and absorb light. Once we have both the anatomical map and the optical map, we put them together. Now we have a very detailed, voxel-by-voxel model ‚Äî a so-called prior model ‚Äî of the animal‚Äôs body.\nWith that model, we can perform Monte Carlo simulations. This is similar to the photon simulation I showed you earlier. You put a light source at a certain location inside the model, and then you let thousands ‚Äî or even millions ‚Äî of photons propagate randomly in all directions. Each photon scatters, reflects, refracts, or gets absorbed, following the physical laws of light propagation in tissue. Eventually, some photons reach the surface, where they are detected.\n\nThis process tells us, for any assumed source position, what the light distribution on the surface would look like. We call that the forward problem ‚Äî going from a known source to the expected surface measurement.\nThen, we flip the problem around. This is the inverse problem ‚Äî we already know the surface light pattern (that‚Äôs what we measured), and we want to find the internal source distribution that could have produced it.\nSo we try different possible source locations ‚Äî say, one in the middle of the body, one near the liver, one near the kidneys ‚Äî and simulate each case. We compare the simulated surface pattern with the measured one. If they don‚Äôt match, we adjust the model, move the source slightly, and try again.\nAfter many iterations, when the simulated pattern matches the measured pattern well, we can say with confidence that the actual bioluminescent source is located in that region inside the animal.\n\nI remember one particular case ‚Äî we found two bioluminescent spots, one near the upper right and one near the left kidney. When we later sacrificed the mouse and examined it, we indeed found two small tumor nodules exactly in those locations. It was like magic ‚Äî a scientific magic trick!\nWe even published that work in Optics Express, showing this successful reconstruction. It was a very satisfying result.\n\nHowever, as with all research, the story isn‚Äôt all perfect. The reconstruction is extremely sensitive to errors in the model. If your optical properties are slightly off, or your anatomical registration isn‚Äôt perfect, the reconstructed source location can be wrong. That‚Äôs why, although it‚Äôs feasible, it‚Äôs not as robust or as straightforward as, say, X-ray CT.\nSo, to summarize ‚Äî bioluminescence tomography is feasible, it works, but it‚Äôs still very much a research topic, not yet a clinical tool. It‚Äôs beautiful, it‚Äôs challenging, and it‚Äôs fun to work on. And along the way, we also contributed several new ideas to this field ‚Äî such as combing tomography, helical scanning, interior tomography, and other concepts that pushed the boundaries of imaging research.\nSo yes ‚Äî this was, and still is, a very enjoyable area of scientific discovery.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 40, "slide_filename": "Slide40.txt", "slide_annotation": "Lecture 23 - Slide40.txt", "file_path": "Lecture 23\\Texts\\Slide40.txt", "content": "Now, let‚Äôs talk about another idea ‚Äî a really interesting one ‚Äî that tries to make fluorescence tomography and bioluminescence tomography even better. The idea here is to combine optical imaging with X-ray imaging ‚Äî a kind of hybrid approach.\n\nYou see, the limitation with traditional optical tomography is that it doesn‚Äôt give you good structural information. You can get molecular or functional data ‚Äî yes, you can tell where the light comes from or which cells are active ‚Äî but the anatomical detail is missing. On the other hand, X-ray CT gives you beautiful structure, very precise geometry, but not the molecular activity.\nSo, what if we could merge the two? What if we could combine the molecular sensitivity of optical imaging with the structural precision of X-ray imaging? That‚Äôs the motivation behind this work ‚Äî we call it X-ray Optical Fusion.\n\nNow, traditional micro-CT ‚Äî the standard kind ‚Äî has a limitation. It‚Äôs great for bone, great for dense structures, but not so good for soft tissue. The contrast inside soft organs is very weak because the attenuation difference is small. So, you can‚Äôt see the fine internal structures clearly.\n\nThat‚Äôs why researchers have started exploring X-ray Phase-Contrast CT, or PCCT. This is a newer type of X-ray imaging that doesn‚Äôt just rely on attenuation ‚Äî it actually detects the phase shift of the X-rays as they pass through tissue. This phase information is much more sensitive to subtle density changes, especially in soft tissue.\nSo, the hybrid idea is: combine Fluorescence Molecular Tomography (FMT) with X-ray Phase-Contrast CT. Together, they can provide both the functional molecular information from FMT and the high-resolution anatomical detail from PCCT.\n\nThere‚Äôs a paper on this ‚Äî you can see it here ‚Äî titled ‚ÄúFMT‚ÄìPCCT: Hybrid Fluorescence Molecular Tomography and X-ray Phase-Contrast CT Imaging of Mouse Models.‚Äù It was published in IEEE Transactions on Medical Imaging in 2014 by researchers from the Helmholtz Center in Munich and collaborators in Germany. This kind of work really represents the next step ‚Äî hybrid optical-X-ray imaging for small animal studies.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 41, "slide_filename": "Slide41.txt", "slide_annotation": "Lecture 23 - Slide41.txt", "file_path": "Lecture 23\\Texts\\Slide41.txt", "content": "Now, some of the results reported in this area are quite interesting. In these experiments, the researchers scanned small animals ‚Äî typically mice ‚Äî sometimes for an entire day, to acquire very high-quality data.\n\nInstead of using conventional micro-CT, they used a grating-based phase-contrast tomography system. I actually mentioned this technique before, when we talked about my visit to Japan ‚Äî the grating-based interferometric system. The setup uses a series of gratings ‚Äî a source grating, a phase grating, and an analyzer grating ‚Äî to measure very tiny changes in the X-ray phase as the beam passes through the sample.\n\nSo, what they did was essentially scan the animal ‚Äî or tissue samples ‚Äî using this grating-based X-ray phase-contrast CT. And they achieved beautiful soft-tissue visualization, something you simply can‚Äôt get from normal X-ray attenuation imaging.\n\nThese results, published by several European research groups, clearly demonstrated that grating-based phase-contrast tomography can reveal fine internal details ‚Äî such as blood vessels, organ microstructures, and even tumor margins ‚Äî without the need for any contrast agents.\nIt‚Äôs a big step forward for optical-X-ray fusion, because now the X-ray side of the hybrid system provides much richer anatomical detail to guide and constrain the optical reconstruction.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 42, "slide_filename": "Slide42.txt", "slide_annotation": "Lecture 23 - Slide42.txt", "file_path": "Lecture 23\\Texts\\Slide42.txt", "content": "At RPI, we‚Äôve been taking this concept even further. We‚Äôre working on developing a tightly integrated system for in vivo optical and X-ray imaging ‚Äî that means imaging live animals, not just tissue samples.\n\nIn the earlier work, what we call ex vivo imaging, the studies were done on isolated or preserved samples, which is great for testing physics, but not for observing biological dynamics. So, our goal is to move toward in vivo imaging, where we can study living processes in real time.\nOur approach is to build an orthogonal imaging chain ‚Äî basically, an X-ray phase-contrast system aligned at a right angle to an optical imaging system. One part of the setup handles the phase-contrast X-ray imaging, while the other captures fluorescence or bioluminescence signals. By merging these two datasets, we can combine the strengths of both modalities ‚Äî structural accuracy from X-rays and molecular sensitivity from optics.\n\nThe system includes components like a CCD camera, filter wheel, laser stage, isoflurane anesthesia line for live animal support, and a rotating gantry for tomographic data collection. The optical part uses mirrors, spectrometers, and digital micromirror devices for detecting and filtering the light.\nSo, what we‚Äôre working toward at RPI is a fully integrated hybrid imaging system ‚Äî one that can acquire X-ray phase-contrast data and optical molecular data simultaneously.\n\nNow, this is still an active research topic. Achieving truly precise, stable 3D tomography in such a hybrid setup is not easy. There are still open questions ‚Äî how to synchronize the modalities, how to register the datasets, how to compensate for motion, and how to achieve stable reconstructions.\nBut it‚Äôs an exciting direction. When you can successfully merge X-ray and optical data ‚Äî one showing the anatomy, the other revealing the function ‚Äî that‚Äôs when you get a truly powerful multimodal imaging system.\n\nSo, this is what we‚Äôre actively working on at RPI ‚Äî a tighter, more integrated system that pushes the boundary of X-ray‚Äìoptical fusion imaging. It‚Äôs challenging, but also very rewarding work.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 43, "slide_filename": "Slide43.txt", "slide_annotation": "Lecture 23 - Slide43.txt", "file_path": "Lecture 23\\Texts\\Slide43.txt", "content": "Alright, in this last part, let‚Äôs talk about something quite new and exciting ‚Äî X-ray optical coupling. If you look at the slide, you can see the last two names listed here: XLCT and XMLT. These stand for X-ray Luminescence Computed Tomography and X-ray Molecular Luminescence Tomography.\n\nNow, what we‚Äôre doing here is merging X-ray imaging and optical imaging ‚Äî two worlds that traditionally don‚Äôt overlap much. But when you think about it, it makes perfect sense. X-rays penetrate deeply and travel straight, while optical imaging provides rich molecular and functional information. So, if we can somehow couple these two, we can get deep, high-resolution, molecularly sensitive images ‚Äî the best of both worlds.\n\nSo this is what we mean by X-ray Optical Coupling ‚Äî using X-rays to stimulate optical signals inside tissue, and then using those emitted optical photons to form an image. It‚Äôs a very elegant idea, and it‚Äôs starting to gain serious research attention.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 44, "slide_filename": "Slide44.txt", "slide_annotation": "Lecture 23 - Slide44.txt", "file_path": "Lecture 23\\Texts\\Slide44.txt", "content": "Okay, so let‚Äôs go a bit deeper into the physics and materials behind this idea. This concept really reminds me of how fluorescent and bioluminescent proteins work. In fluorescence imaging, you have a fluorophore ‚Äî say, a protein that absorbs one color of light and emits another. In bioluminescence, you have a chemical reaction that produces light on its own, like fireflies.\n\nNow, what researchers have done is introduce a new type of material, called a nanophosphor ‚Äî it‚Äôs not a protein, not biological by nature, but rather an engineered material. Nanophosphors are nanoscale particles that have unique luminescent properties.\nHere‚Äôs how it works. When you shine an X-ray beam onto these nanophosphors, they emit visible or near-infrared light ‚Äî in other words, they glow. You can even make different kinds of nanophosphors that emit different colors ‚Äî green, red, near-infrared, far-infrared ‚Äî depending on the specific material and dopant you use.\nSo, it‚Äôs very much in parallel with fluorescence imaging, except for one big difference: in fluorescence, you excite the molecules using laser light, which is itself light, and therefore it scatters heavily inside tissue. But in this case, the excitation comes from X-rays, which travel straight through the body. So, the spatial resolution is much better because X-rays are not affected by scattering the way light is.\n\nOne example of this can be seen in the study shown here ‚Äî Photostimulated Near-Infrared Persistent Luminescence as a New Optical Readout from Chromium-Doped Lithium Gallium Oxide. This material, when irradiated by X-rays, stores the energy and then later emits near-infrared light when stimulated by low-energy light.\nThis concept ‚Äî using X-ray-excitable nanophosphors ‚Äî opens the door to a completely new kind of optical imaging. Instead of laser excitation, we use X-rays, and the resulting emitted light can be collected optically. That‚Äôs the core idea behind X-ray Optical Coupling.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 45, "slide_filename": "Slide45.txt", "slide_annotation": "Lecture 23 - Slide45.txt", "file_path": "Lecture 23\\Texts\\Slide45.txt", "content": "Now let‚Äôs look at this next example ‚Äî this is an article introducing X-ray Luminescence Computed Tomography, or XLCT. This is a very creative and forward-looking concept.\nThe essential idea is shown right here. You introduce X-ray-excitable nanophosphors into a small animal ‚Äî usually a mouse. These nanophosphors can be functionalized ‚Äî meaning you can coat their surface with polymers or peptides to make them target specific biomarkers. For example, if you want to target cancer cells, you attach a peptide that binds to receptors that are overexpressed in tumors.\n\nSo now, when these nanophosphors are injected into the body, they accumulate more densely in the tumor region than in normal tissue. Then, when you shine an X-ray beam, it goes straight through the body. Wherever it hits these nanophosphors, they emit optical light. And that light ‚Äî the X-ray-induced luminescence ‚Äî is detected by sensitive optical cameras around the animal.\n\nSo think about it: by scanning the X-ray beam and recording the emitted light, you can reconstruct the 3D distribution of these nanophosphors inside the body. That gives you a molecular image that is excited by X-rays and detected optically.\nThis is a very promising approach because it potentially makes X-rays a molecular imaging modality ‚Äî something traditionally reserved for optical, nuclear, or MR imaging. You can now use X-rays not just for structure, but also to identify specific molecular targets.\n\nOf course, there are still challenges. When you collimate the X-ray beam ‚Äî that means narrowing it to a small pencil beam ‚Äî you face physical limits. Mechanical collimators can only get so thin, and even a narrow beam will spread out slightly due to scattering and beam divergence. So the current spatial resolution of XLCT is about 1 to 2 millimeters.\nNow, the question becomes: how can we improve that? How can we go from millimeter resolution down to, say, 100 microns ‚Äî an order of magnitude better? That‚Äôs still an open research problem. It will require new hardware designs, smarter reconstruction algorithms, and probably better nanophosphor materials.\n\nBut the vision is clear ‚Äî this approach could bridge the gap between anatomical imaging and molecular imaging, between optical precision and X-ray penetration. It‚Äôs one of those rare ideas that brings together two worlds ‚Äî physics and biology ‚Äî into a single, unified imaging platform.\nSo that‚Äôs the direction of X-ray Luminescence CT and X-ray Optical Coupling ‚Äî still developing, but with tremendous potential for the future of biomedical imaging.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 46, "slide_filename": "Slide46.txt", "slide_annotation": "Lecture 23 - Slide46.txt", "file_path": "Lecture 23\\Texts\\Slide46.txt", "content": "Alright, now let‚Äôs move on to something very exciting ‚Äî X-ray micro-modulated luminescence tomography, or simply XMLT. This is one of the latest developments that builds upon everything we‚Äôve discussed so far.\n\nIf you‚Äôre really interested, I recommend reading the original paper. But let me give you the idea in simple terms.\nIn XMLT, we treat the X-ray beam as a wave, not just as a stream of particles. That means we can actually use certain X-ray optical components ‚Äî things that already exist on the market, like X-ray polycapillary lenses. These are special lenses that can focus X-rays, although not quite as tightly as optical lenses can focus visible light. Still, they can concentrate the X-ray beam to a small spot ‚Äî around 100 microns, or even smaller in some setups.\nAnd because X-rays penetrate tissue so well, this focused beam can reach deep inside the object. So now, imagine we focus the X-ray into the tissue ‚Äî we‚Äôre creating a tiny, well-defined excitation region.\n\nWhen we introduce nanophosphors into the sample ‚Äî the same luminescent particles we talked about earlier ‚Äî these particles light up only along the path of the focused X-ray beam. Everywhere else remains dark. That‚Äôs very important because it means the optical signal we collect corresponds precisely to the X-ray excitation path ‚Äî no background interference from outside that region.\nSo the spatial resolution of this imaging method is determined entirely by the X-ray focal spot size, by how small that beam is. Think of it like drawing with a very fine pen instead of a broad brush.\n\nAnd geometrically, this is not a pencil beam or a fan beam like in standard CT. It‚Äôs more like a double-cone beam ‚Äî a cone that converges to a focus point and then diverges again. So you get a cone of excitation in both directions.\nIn short, we perform tomography by collecting optical signals generated from these small, localized regions excited by the focused X-ray. We have published several papers on this topic, exploring the physics, the reconstruction algorithms, and the system design. It‚Äôs a fascinating direction for hybrid imaging research.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 47, "slide_filename": "Slide47.txt", "slide_annotation": "Lecture 23 - Slide47.txt", "file_path": "Lecture 23\\Texts\\Slide47.txt", "content": "Now, we‚Äôve been collaborating with General Electric ‚Äî GE ‚Äî to take this idea further.\nWith a micro-focused X-ray beam, we can excite nanophosphors deeply inside small animals, such as in the brain. You can imagine a setup like this: the X-ray beam enters an integrating sphere ‚Äî that‚Äôs a spherical chamber lined with a highly reflective surface. It has just one small opening where the X-ray beam enters and where the emitted photons exit.\nHere‚Äôs how it works. The X-ray beam excites the nanophosphors in a very small region. Those nanophosphors emit optical photons ‚Äî visible or near-infrared light. The integrating sphere reflects these photons multiple times ‚Äî bouncing them around the inner wall ‚Äî so that, eventually, almost every photon finds its way to the detector, usually a PMT, or photomultiplier tube. This design allows us to collect nearly all the emitted light with minimal loss.\nThat‚Äôs how we can do small-animal imaging efficiently.\n\nNow, if we scale up this concept, we can also envision doing human imaging, especially for the brain. The human cerebral cortex has about six layers, and together they‚Äôre roughly six millimeters thick. The interesting part ‚Äî the one related to higher-level brain function and intelligence ‚Äî lies not just on the surface, but deeper in those inner cortical layers.\nSo, how can we noninvasively reach those deeper layers? Well, one potential path is to use nanophosphors that can cross the blood‚Äìbrain barrier and label specific neural structures. Then, we use a micro-focused X-ray beam to excite those particles deep inside the brain.\n\nHere‚Äôs where it gets really exciting ‚Äî and, yes, a little bit like science fiction. When neurons fire, they produce action potentials ‚Äî tiny electrical currents. These currents can interact with the nanophosphors, changing their quantum emission properties ‚Äî for example, the color or intensity of the light they emit.\nSo in theory, by monitoring how the emitted light changes, we could infer neural activity ‚Äî we could literally watch the brain think. Imagine detecting color changes corresponding to neural firing patterns!\n\nNow, I‚Äôm not saying we‚Äôre there yet ‚Äî far from it. This is still in the conceptual stage. But it‚Äôs a very cool direction to pursue ‚Äî combining X-ray excitation, optical emission, and nanophosphor sensing to probe deep brain function noninvasively. It‚Äôs futuristic, but scientifically grounded.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 48, "slide_filename": "Slide48.txt", "slide_annotation": "Lecture 23 - Slide48.txt", "file_path": "Lecture 23\\Texts\\Slide48.txt", "content": "Let‚Äôs shift gears and talk about another exciting technology ‚Äî optogenetics.\nOptogenetics is a biological technique where light is used to control cells ‚Äî typically neurons ‚Äî that have been genetically modified to express light-sensitive ion channels. Depending on the wavelength of light you use, you can either activate or inhibit neural activity.\n\nFor example, blue light ‚Äî around 470 nanometers ‚Äî can open certain channels, allowing positively charged ions like sodium or calcium to enter the neuron, thereby triggering an electrical signal. On the other hand, yellow or red light ‚Äî around 570 or 590 nanometers ‚Äî can open other types of channels that inhibit activity, stopping the neuron from firing.\nSo, by simply changing the color of light, you can make a neuron fire or stop firing. It‚Äôs almost like using a remote control for the brain!\n\nResearchers have used this technique to study complex neurological conditions ‚Äî things like depression, Parkinson‚Äôs disease, and addiction ‚Äî where precise control of neural circuits is essential. You can shine light on specific regions of the brain and observe behavioral changes in animals ‚Äî say, a mouse moving left when you use one color, or right when you use another.\n\nHowever, there‚Äôs a limitation. Optical light doesn‚Äôt penetrate deeply ‚Äî it only reaches about one millimeter into tissue. If you want to reach deeper layers, you have to insert an optical fiber directly into the brain. And, of course, that‚Äôs invasive ‚Äî it causes tissue damage, and you can‚Äôt easily use it for large-scale or long-term experiments.\nSo, while optogenetics is an incredibly powerful tool, its reach is still limited by the physics of light scattering.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 49, "slide_filename": "Slide49.txt", "slide_annotation": "Lecture 23 - Slide49.txt", "file_path": "Lecture 23\\Texts\\Slide49.txt", "content": "Now, that brings us to our next idea ‚Äî something we call X-optogenetics.\nMy students and I proposed this concept a few years ago, and we even published an article titled ‚ÄúX-Optogenetics and U-Optogenetics: Feasibility and Possibilities.‚Äù\nThe idea is simple but powerful. We know that X-rays can penetrate deeply into tissue, and we also know that nanophosphors can convert X-rays into visible light. So, what if we could use X-rays as a remote light source to activate optogenetic proteins deep inside the brain?\n\nHere‚Äôs how it would work. We inject nanophosphors small enough to pass through the blood‚Äìbrain barrier ‚Äî so they can distribute evenly within the brain tissue. Then, we shine a focused X-ray beam into a specific region. The nanophosphors there absorb the X-rays and emit light locally ‚Äî right where the neurons are.\n\nThat emitted light can then activate the optogenetic channels ‚Äî just like in traditional optogenetics, but now without inserting any optical fibers. Everything is done noninvasively.\nIn our paper, we discussed this as a possible future imaging and neurostimulation technique. One of my undergraduate students ‚Äî who is now working full-time on this ‚Äî helped develop the idea. We‚Äôre currently characterizing different nanophosphor materials, testing their emission spectra, decay times, and compatibility with biological tissue.\n\nThis concept could open the door to deep-brain stimulation without surgery, combining the precision of optogenetics with the penetration of X-rays. It‚Äôs a very exciting frontier ‚Äî truly where physics meets neuroscience.", "total_slides_in_lecture": 50}
{"lecture": "Lecture 23", "slide_number": 50, "slide_filename": "Slide50.txt", "slide_annotation": "Lecture 23 - Slide50.txt", "file_path": "Lecture 23\\Texts\\Slide50.txt", "content": "Alright, that brings us to the end of today‚Äôs lecture.\n\nFor homework, I‚Äôd like you to do three things:\nFirst, review this lecture carefully and summarize the key ideas ‚Äî not just a quick summary, but really try to capture the main concepts and how they connect.\nSecond, transcribe one section of this lecture ‚Äî you can pick the first part, the second part, or the last two parts ‚Äî and write it down clearly. This will help reinforce your understanding.\nAnd third, here‚Äôs a fun one ‚Äî a creative thinking question. Imagine you could make a smartphone that can send and receive light in any way you want. What kind of medical imaging applications could you create?\n\nThink about it ‚Äî a smartphone already has a light source, a detector, computing power, and connectivity. What if it could emit polarized light, or detect fluorescence, or measure oxygen levels through tissue? Could it monitor glucose levels? Could it perform optical tomography?\n\nYou can be as creative as you want ‚Äî just stay within the limits of physics.\nSo, that‚Äôs your assignment: review, transcribe, and imagine. I‚Äôm really looking forward to seeing your ideas. And that‚Äôs all for today ‚Äî thank you!", "total_slides_in_lecture": 50}
